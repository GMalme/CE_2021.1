FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Taha, A
   Darwish, A
   Hassanien, AE
   ElKholy, A
AF Taha, Ayat
   Darwish, Ashraf
   Hassanien, Aboul Ella
   ElKholy, Ahmed
TI Arabian horse identification based on whale optimised multi-class
   support vector machine
SO INTERNATIONAL JOURNAL OF COMPUTER APPLICATIONS IN TECHNOLOGY
LA English
DT Article
DE Arabian horse identification; histogram of oriented gradient;
   multi-class support vector machine; whale optimisation algorithm
ID ORIENTED GRADIENTS; RECOGNITION; CATTLE; HISTOGRAM; FEATURES
AB In this study, a biometric identification approach for Arabian horse identification is proposed based on the optimised Multi-Class Support Vector Machine (MCSVM). The identification approach is performed in three phases: feature extraction, classification, and optimisation. The feature extraction phase uses Histogram of Oriented Gradient (HOG) to extract features vectors from muzzle print images of the Arabian horses and then stored in the database with its labels. The second phase is the classification phase which uses MCSVM for training and testing classification. Finally in the optimised MCSVM phase, three different swarms, Particle Swarm Optimisation (PSO), Grey Wolf Algorithm (GWA) and Whale Optimisation (WO), are used to optimise MCSVM parameters to enhance the identification accuracy of the Arabian horse. The results obtained show that the MCSVM achieves accuracy of 93.2% and increases to 97.4% with WO algorithm which achieves the best accuracy compared to PSO and GWA.
C1 [Taha, Ayat; ElKholy, Ahmed] Al Azhar Univ, Fac Sci, Cairo, Egypt.
   [Darwish, Ashraf] Helwan Univ, Fac Sci, Cairo, Egypt.
   [Hassanien, Aboul Ella] Cairo Univ, Fac Comp & Informat, Cairo, Egypt.
C3 Egyptian Knowledge Bank (EKB); Al Azhar University; Egyptian Knowledge
   Bank (EKB); Helwan University; Egyptian Knowledge Bank (EKB); Cairo
   University
RP Taha, A (corresponding author), Al Azhar Univ, Fac Sci, Cairo, Egypt.
EM ayat_taha@ymail.com; ashraf.darwish.eg@ieee.org; aboitcairo@gmail.com;
   AhmedElkholySc@yahoo.com
OI Hassanien, Professor Aboul Ella/0000-0002-9989-6681
CR Abd El Aziz M, 2017, EXPERT SYST APPL, V83, P242, DOI 10.1016/j.eswa.2017.04.023
   Ahrendt P, 2011, COMPUT ELECTRON AGR, V76, P169, DOI 10.1016/j.compag.2011.01.011
   Aljarah I, 2018, SOFT COMPUT, V22, P1, DOI 10.1007/s00500-016-2442-1
   Awad AI, 2013, COMM COM INF SC, V381, P143
   Bang AV, 2017, INT J COMPUT APPL T, V56, P172, DOI 10.1504/IJCAT.2017.088197
   Bowling M. B., 2008, Professional Animal Scientist, V24, P287
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Crepinsek M, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2480741.2480752
   Dietterich TG, 1994, J ARTIF INTELL RES, V2, P263
   El Hadad HM, 2015, PROCEDIA COMPUT SCI, V65, P864, DOI 10.1016/j.procs.2015.09.044
   Evangelin LN, 2017, INT J COMPUT APPL T, V56, P219, DOI 10.1504/IJCAT.2017.088196
   Gaber T, 2016, COMPUT ELECTRON AGR, V122, P55, DOI 10.1016/j.compag.2015.12.022
   Ghimire D, 2017, MULTIMED TOOLS APPL, V76, P7921, DOI 10.1007/s11042-016-3428-9
   Klindtworth M, 1999, COMPUT ELECTRON AGR, V24, P65, DOI 10.1016/S0168-1699(99)00037-X
   Lee WY, 2017, OPTIK, V136, P462, DOI 10.1016/j.ijleo.2017.02.017
   Minagawa H., 2002, AFITA 2002: Asian agricultural information technology & management. Proceedings of the Third Asian Conference for Information Technology in Agriculture, Beijing, China, 26-28 October, 2002, P596
   Mirjalili S, 2016, ADV ENG SOFTW, V95, P51, DOI 10.1016/j.advengsoft.2016.01.008
   Nazir M, 2018, CLUSTER COMPUT, V21, P539, DOI 10.1007/s10586-017-0921-5
   Nigam A, 2019, INT J COMPUT APPL T, V59, P214, DOI 10.1504/IJCAT.2019.098602
   Nigam S, 2018, MULTIMED TOOLS APPL, V77, P28725, DOI 10.1007/s11042-018-6040-3
   Noviyanto A., 2012, P 3 EUR C COMP SCI E, P110
   Noviyanto A, 2013, COMPUT ELECTRON AGR, V99, P77, DOI 10.1016/j.compag.2013.09.002
   Petersen W, 1922, J DAIRY SCI, V5, P249
   Rekha N., 2014, INT J ADV RES COMPUT, V3, P1345
   Roberts CM, 2006, COMPUT SECUR, V25, P18, DOI 10.1016/j.cose.2005.12.003
   Shu Chang, 2011, Tsinghua Science and Technology, V16, P216, DOI 10.1016/S1007-0214(11)70032-3
   Sidaoui B, 2017, INT J COMPUT APPL T, V55, P183
   Taha A., 2017, ARABIAN HORSE IDENTI
   Tharwat A, 2014, ADV INTELL SYST, V303, P217, DOI 10.1007/978-3-319-08156-4_22
   Triggs B., 2005, PROC CVPR IEEE, V1, P886, DOI DOI 10.1109/CVPR.2005.177
   Vlad M, 2012, 13 WSEAS INT C AUT I, P165
NR 31
TC 1
Z9 1
U1 1
U2 1
PU INDERSCIENCE ENTERPRISES LTD
PI GENEVA
PA WORLD TRADE CENTER BLDG, 29 ROUTE DE PRE-BOIS, CASE POSTALE 856, CH-1215
   GENEVA, SWITZERLAND
SN 0952-8091
EI 1741-5047
J9 INT J COMPUT APPL T
JI Int. J. Comput. Appl. Technol.
PY 2020
VL 63
IS 1-2
SI SI
BP 83
EP 92
DI 10.1504/IJCAT.2020.107910
PG 10
WC Computer Science, Interdisciplinary Applications
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA MP0OT
UT WOS:000551913800006
DA 2022-02-03
ER

PT C
AU Said, EH
   Nassar, DEM
   Ammar, HH
AF Said, Eyad Haj
   Nassar, Diaa Eldin M.
   Ammar, Hany H.
BE Dougherty, ER
   Astola, JT
   Egiazarian, KO
   Nasrabadi, NM
   Rizvi, SA
TI Image segmentation for Automated Dental Identification
SO IMAGE PROCESSING: ALGORITHMS AND SYSTEMS, NEURAL NETWORKS, AND MACHINE
   LEARNING
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT Conference on Image Processing - Algorithms and Systems, Neural
   Networks, and Machine Learning
CY JAN 16-18, 2006
CL San Jose, CA
AB Dental features are one of few biometric identifiers that qualify for postmortem identification; therefore, creation of an Automated Dental Identification System (ADIS) with goals and objectives similar to the Automated Fingerprint Identification System (AFIS) has received increased attention. As a part of ADIS, teeth segmentation from dental radiographs films is an essential step in the identification process. In this paper, we introduce a fully automated approach for teeth segmentation with goal to extract at least one tooth from the dental radiograph film. We evaluate our approach based on theoretical and empirical basis, and we compare its performance with the performance of other approaches introduced in the literature. The results show that our approach exhibits the lowest failure rate and the highest optimality among all full automated approaches introduced in the literature.
C1 [Said, Eyad Haj; Nassar, Diaa Eldin M.; Ammar, Hany H.] W Virginia Univ, Lane Dept Comp Sci & Elect Engn, POB 6109, Morgantown, WV 26506 USA.
C3 West Virginia University
RP Said, EH (corresponding author), W Virginia Univ, Lane Dept Comp Sci & Elect Engn, POB 6109, Morgantown, WV 26506 USA.
EM ehajsaid@yahoo.com; dmnassar@csee.wvu.edu; ammar@csee.wvu.edu
FU U.S. National Science FoundationNational Science Foundation (NSF)
   [EIA-0131079]; Office of Justice Programs, National Institute of
   Justice, and the U.S. Department of Justice [2001-RC-CX-K013]
FX This work is supported in part by the U.S. National Science Foundation
   under Award number EIA-0131079 to West Virginia University; the research
   is also supported under Award number 2001-RC-CX-K013 from the Office of
   Justice Programs, National Institute of Justice, and the U.S. Department
   of Justice. Points of view in this document are those of the authors and
   do not necessarily represent position of the U.S. Department of Justice.
CR *CJIS DIV, 2002, ADIS PROJ DIG RAD IM
   Fahmy G, 2004, LECT NOTES COMPUT SC, V3072, P789
   Gonzales RC, 2002, DIGITAL IMAGE PROCES
   JAIN A, 2000, COMMUN ACM, V43, P91, DOI DOI 10.1145/328236.328110
   Jain AK, 2004, PATTERN RECOGN, V37, P1519, DOI 10.1016/j.patcog.2003.12.016
   Khanna R., 1994, Proceedings. The Institute of Electrical and Electronics Engineers 28th Annual 1994 International Carnahan Conference on Security Technology (Cat. No.CH3437-1/94), P188, DOI 10.1109/CCST.1994.363768
   Nassar DEM, 2004, P 1 INT COMP ENG C I, P354
   Nomir O, 2005, PATTERN RECOGN, V38, P1295, DOI 10.1016/j.patcog.2004.12.010
   RATHEE S, 1992, IEEE T MED IMAGING, V11, P530, DOI 10.1109/42.192688
   SAID EH, IN PRESS IEEE T INFO
   Stimson PG., 1997, FORENSIC DENTISTRY
   WHITE SC, 2000, ORAL RADIOLOGY PRINC
   ZHOU J, IN PRESS PATTERN REC
   [No title captured]
NR 14
TC 1
Z9 1
U1 0
U2 1
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 0-8194-6104-0
J9 PROC SPIE
PY 2006
VL 6064
AR 60640X
PG 10
WC Computer Science, Artificial Intelligence; Imaging Science &
   Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BEF40
UT WOS:000237084700030
DA 2022-02-03
ER

PT J
AU El Sayed, AR
   El Chakik, A
   Alabboud, H
   Yassine, A
AF El Sayed, Abdul Rahman
   El Chakik, Abdallah
   Alabboud, Hassan
   Yassine, Adnan
TI Efficient 3D point clouds classification for face detection using linear
   programming and data mining
SO IMAGING SCIENCE JOURNAL
LA English
DT Article
DE 3D point clouds; face detection; face model; skin detection; data
   mining; linear programming
AB Most of the applications related to security and biometric rely on skin region detection such as face detection, adult 3D objects filtering, and gesture recognition. In this paper, we propose a robust method for skin detection on 3D coloured point clouds. Then, we extend this method to solve the problem of 3D face detection. To do so, we construct a weighted graph from initial coloured 3D point clouds. Then, we present a linear programming algorithm using a predictive model based on a data mining approach to classify and label graph vertices as skin and non-skin regions. Moreover, we apply some refinement rules on skin regions to confirm the presence of a face. Furthermore, we demonstrate the robustness of our method by showing and analysing some experimental results. Finally, we show that our method deals with many data that can be represented by a weighted graph such as 2D images and 3D models.
C1 [El Sayed, Abdul Rahman; Yassine, Adnan] Normandie Univ, FR CNRS 3335, UNIHAVRE, LMAH,ISCN, Le Havre, France.
   [El Chakik, Abdallah] Beirut Arab Univ, Tripoli, Lebanon.
   [Alabboud, Hassan] Lebanese Univ, Business & Adm Fac, Tripoli, Lebanon.
   [Yassine, Adnan] Normandie Univ, UNIHAVRE, ISEL, Le Havre, France.
C3 Centre National de la Recherche Scientifique (CNRS); Beirut Arab
   University; Lebanese University
RP El Chakik, A (corresponding author), Beirut Arab Univ, Tripoli, Lebanon.
EM a.alshakik@bau.edu.lb
CR Berg T, 2004, FORSYTH NEURAL INFOR
   Colombo A, 2006, PATTERN RECOGN, V39, P444, DOI 10.1016/j.patcog.2005.09.009
   Fergus R., 2004, CALTECH FACE DATABAS
   Hao S, 2009, IEEE I CONF COMP VIS, P213, DOI 10.1109/ICCV.2009.5459168
   Hawari K., 2011, INT J COMPUT APPL, V34, P6
   Hjelmas E, 2001, COMPUT VIS IMAGE UND, V83, P236, DOI 10.1006/cviu.2001.0921
   Huang HB, 2014, P INT CONF NAT COMPU, P524, DOI 10.1109/ICNC.2014.6975890
   Kheirkhah E., 2015, INDIAN J SCI TECHNOL, V8, P49
   Lin HJ, 2015, FACE DETECTION BASED, V2, P1144
   Mian AS, 2007, IEEE T PATTERN ANAL, V29, P1927, DOI 10.1109/TPAMI.2007.1105
   Moudani W, 2011, INT J COMB OPTIM PRO, V2, P27
   Niese Robert, 2007, Journal of Multimedia, V2, P1, DOI 10.4304/jmm.2.5.1-12
   Payet N, 2011, IEEE I CONF COMP VIS, P983, DOI 10.1109/ICCV.2011.6126342
   Rajput A., 2011, INT J COMPUT SCI SEC, V5, P201
   Smith TC, 2016, METHODS MOL BIOL, V1418, P353, DOI 10.1007/978-1-4939-3578-9_17
   Wang JZ, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 4, PROCEEDINGS, P575, DOI 10.1109/CISP.2008.270
   Yang M.-H., 1999, P SPIE C STOR RETR I, P458
NR 17
TC 2
Z9 2
U1 0
U2 5
PU TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN 1368-2199
EI 1743-131X
J9 IMAGING SCI J
JI Imaging Sci. J.
PY 2018
VL 66
IS 1
BP 23
EP 37
DI 10.1080/13682199.2017.1376772
PG 15
WC Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Imaging Science & Photographic Technology
GA GB0TC
UT WOS:000428759200002
DA 2022-02-03
ER

PT C
AU Alonso-Fernandez, F
   Bigun, J
AF Alonso-Fernandez, Fernando
   Bigun, Josef
GP IEEE
TI PERIOCULAR BIOMETRICS: DATABASES, ALGORITHMS AND DIRECTIONS
SO 2016 4TH INTERNATIONAL WORKSHOP ON BIOMETRICS AND FORENSICS (IWBF)
SE International Workshop on Biometrics and Forensics IWBF
LA English
DT Proceedings Paper
CT 4th International Conference on Biometrics and Forensics (IWBF)
CY MAR 03-04, 2016
CL Limassol, CYPRUS
DE Periocular biometrics; databases; segmentation; features;
   soft-biometrics
AB Periocular biometrics has been established as an independent modality due to concerns on the performance of iris or face systems in uncontrolled conditions. Periocular refers to the facial region in the eye vicinity, including eyelids, lashes and eyebrows. It is available over a wide range of acquisition distances, representing a trade-off between the whole face (which can be occluded at close distances) and the iris texture (which do not have enough resolution at long distances). Since the periocular region appears in face or iris images, it can be used also in conjunction with these modalities. Features extracted from the periocular region have been also used successfully for gender classification and ethnicity classification, and to study the impact of gender transformation or plastic surgery in the recognition performance. This paper presents a review of the state of the art in periocular biometric research, providing an insight of the most relevant issues and giving a thorough coverage of the existing literature. Future research trends are also briefly discussed.
C1 [Alonso-Fernandez, Fernando; Bigun, Josef] Halmstad Univ, Box 823, SE-30118 Halmstad, Sweden.
C3 Halmstad University
RP Alonso-Fernandez, F (corresponding author), Halmstad Univ, Box 823, SE-30118 Halmstad, Sweden.
EM feralo@hh.se; josef.bigun@hh.se
RI Alonso-Fernandez, Fernando/AAG-3239-2021
OI Alonso-Fernandez, Fernando/0000-0002-1400-346X
CR Adams J., 2010, P ICPR
   Alonso-Fernandez F., 2015, P IWBF
   Alonso-Fernandez F., 2012, LNCS, V7584
   Alonso-Fernandez F., 2010, IEEE TSMC A, V40
   Alonso-Fernandez F., 2015, IET BIOMETRICS
   Alonso-Fernandez F., 2014, P IWBF 2014
   Bakshi S., 2014, P INDICON
   Bakshi S., 2015, BIOCYBERNETICS BIOME, V35
   Barroso E., 2013, P CIBM
   Bharadwaj S., 2010, P BTAS
   Boddeti V.N., 2011, P IJCB
   Bowyer KW, 2008, COMPUT VIS IMAGE UND, V110, P281, DOI 10.1016/j.cviu.2007.08.005
   Cao Z. X., 2014, P ICIP
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   Denes L. J, 2002, CMURITR0225 ROB I
   Dong Y., 2011, P IJCB
   Fierrez J., 2007, PATTERN RECOGNITION, V40
   Gangwar A., 2014, P CISP
   Han H., 2014, IEEE TPAML, V37
   Hollingsworth K., 2010, P BTAS
   Hollingsworth K., 2012, IEEE TIFS, V7
   Jillela R., 2012, P BTAS 2010
   Jillela R., 2014, P ICIP
   Jillela R., 2013, HDB IRIS RECOGNITION, P281
   Joshi A., 2014, P ICIP
   Joshi A., 2012, P HIS
   Juefei-Xu F., 2010, P BTAS 2010
   Juefei-Xu F., 2012, P WACV
   Juefei-Xu F., 2014, P CVPRW
   Juefei-Xu F., 2011, P IJCB
   Juefei-Xu F., 2014, IEEE TIP
   Karahan S., 2014, P EUSIPCO
   Kasinski Andrzej, 2008, IMAGE PROCESSING COM, V13, P59
   Kumar A, 2010, PATTERN RECOGN, V43, P1016, DOI 10.1016/j.patcog.2009.08.016
   Kumari S., 2012, P ICMOC
   Le T.H.N., 2014, P IJCB
   Lyle J., 2010, P BTAS
   Lyle J., 2012, PATT RECOGN, V45
   Mahalingam G., 2014, IEEE TIFS
   Mahalingam G, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-36
   Martinez A., AR FACE
   Merkow J., 2010, P BTAS
   Mikaelyan A., 2014, P IEB SITIS
   Miller P., 2010, P SAC
   Miller P. E., 2010, P BTAS
   Nie L., 2014, P ICPR
   Nigam I, 2015, INFORM FUSION, V26, P1, DOI 10.1016/j.inffus.2015.03.005
   Oh B., 2012, P ICEA 2012
   Oh K., 2014, NEUROCOMPUT
   Padole C., 2012, P ICB
   Park U., 2011, IEEE TIFS
   Park U., 2009, P BTAS 2009
   Phillips P.J., 2011, P FG
   Phillips P. J., 2009, P ICB
   Phillips P. J., 2000, IEEE TPAMI
   Phillips P. J., 2005, P CVPR
   Pigeon S., 1997, P AVBPA
   Proenca H., 2010, IEEE TPAMI
   Proenca H., 2014, IET BIOMETRICS
   Proenca H., 2014, IEEE TIP, V23
   Proenca H., 2014, P IJCB 2014
   Raghavendra R., 2013, P ACPR
   Raja K. B., 2014, P IWBF
   Ricanek K., 2006, P FG 2006
   Ross A., 2012, P ICB
   Sadr J, 2003, PERCEPTION, V32, P285, DOI 10.1068/p5027
   Santos G., 2012, PAN RECOGN LETT, V33
   Santos G., 2013, P CIBIM
   Santos G., 2014, PATT RECOGN LETT
   Sequeira A. F., 2014, P VISAPP
   Sharma A., 2014, P ICIP
   Singh R., 2010, IEEE TIFS
   Smeraldi F, 2002, PATTERN RECOGN LETT, V23, P463, DOI 10.1016/S0167-8655(01)00178-7
   Smereka J.M., 2013, P CVPRW
   Tan C.-W., 2012, P ICPR
   Uhl A., 2012, P ICIAR
   Uzair M., 2013, P WACV
   Uzair M, 2015, NEUROCOMPUTING, V149, P854, DOI 10.1016/j.neucom.2014.07.049
   Viola P., 2004, IJCV, V57
   Woodard D., 2010, P ICPR
   Woodard D. L., 2010, P CVPRW
   Zhou Z., 2012, IEEE TSMC A, V42
NR 82
TC 2
Z9 2
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2381-6120
BN 978-1-4673-9448-2
J9 I W BIOMETRIC FORENS
PY 2016
PG 6
WC Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering
GA BF4ZS
UT WOS:000381804600017
DA 2022-02-03
ER

PT C
AU Gosciewska, K
   Frejlichowski, D
AF Gosciewska, Katarzyna
   Frejlichowski, Dariusz
BE Kamel, M
   Campilho, A
TI Classification of Tooth Shapes for Human Identification Purposes-An
   Experimental Comparison of Selected Simple Shape Descriptors
SO IMAGE ANALYSIS AND RECOGNITION (ICIAR 2015)
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 12th International Conference on Image Analysis and Recognition (ICIAR)
CY JUL 22-24, 2015
CL Niagara Falls, CANADA
DE Human identification; Teeth classification; Dental biometrics; Dental
   radiographs; Forensic odontology
ID RADIOGRAPHS; SYSTEM
AB The application of teeth as biometric features for human identification purposes is widely known thanks to their durability and distinguishability. Nowadays, due to both improved dental care and dental filling materials that are invisible on dental radiographs, the identification should focus on the analysis of tooth shapes, both crown and root, alongside their positions in the mouth. Such an approach requires the automation of digital radiograph processing methods, including: image enhancement, tooth contour extraction, tooth classification and numbering. This paper considers and examines the problem of tooth shape classification using simple shape descriptors and a template matching approach. An attempt is made to establish which simple shape descriptor gives the best classification results.
C1 [Gosciewska, Katarzyna; Frejlichowski, Dariusz] West Pomeranian Univ Technol, Fac Comp Sci & Informat Technol, PL-71210 Szczecin, Poland.
C3 West Pomeranian University of Technology
RP Frejlichowski, D (corresponding author), West Pomeranian Univ Technol, Fac Comp Sci & Informat Technol, Zolnierska 52, PL-71210 Szczecin, Poland.
EM kgosciewska@wi.zut.edu.pl; dfrejlichowski@wi.zut.edu.pl
RI Gosciewska, Katarzyna/AAL-7671-2020; Frejlichowski, Dariusz/H-8107-2016
OI Gosciewska, Katarzyna/0000-0002-6726-2174; 
CR Chen H, 2009, ENCY BIOMETRICS, P216
   Devi P., 2011, J INDIAN ACAD ORAL M, V23, P360, DOI [10.5005/jp-journals-10011-1169, DOI 10.5005/JP-JOURNALS-10011-1169]
   Frejlichowski D, 2009, METODY INFORM STOSOW, V4, P23
   Frejlichowski D, 2011, LECT NOTES COMPUT SC, V6979, P294, DOI 10.1007/978-3-642-24088-1_31
   Frejlichowski D, 2011, LECT NOTES COMPUT SC, V6855, P65, DOI 10.1007/978-3-642-23678-5_6
   Frejlichowski D, 2010, LECT NOTES COMPUT SC, V6112, P151, DOI 10.1007/978-3-642-13775-4_16
   Jain AK, 2005, PROC SPIE, V5779, P292, DOI 10.1117/12.604690
   James H, 2005, J Forensic Odontostomatol, V23, P1
   Lin PL, 2010, PATTERN RECOGN, V43, P1380, DOI 10.1016/j.patcog.2009.10.005
   Mahoor MH, 2005, PATTERN RECOGN, V38, P577, DOI 10.1016/j.patcog.2004.08.012
   Marana Aparecido Nilceu, 2011, Biometrics - Unique and Diverse Applications in Nature, Science, and Technology, P41
   Nassar DEM, 2007, PATTERN RECOGN, V40, P65, DOI 10.1016/j.patcog.2006.04.046
   Peura M., 1997, Proceedings of the Third International Workshop on Visual Form. Advances in Visual Form Analysis, P443
   Pushparaj V, 2012, J DIGIT IMAGING, P1
   Rosin P., 2005, HDB PATTERN RECOGNIT, P177, DOI [10.1142/9789812775320_0010, DOI 10.1142/9789812775320_0010]
   Trengrove H, 2011, J Forensic Odontostomatol, V29, P1
   Yin PY, 2008, PATTERN RECOGN, P43, DOI [10.1038/s41592-020-01018-x, DOI 10.5772/6237]
   Yuniarti A, 2012, TELKOMNIKA, V10, P137, DOI [10.12928/telkomnika.v10i1.771, DOI 10.12928/TELKOMNIKA.V10I1.771]
   [No title captured]
NR 19
TC 1
Z9 1
U1 0
U2 3
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-319-20801-5; 978-3-319-20800-8
J9 LECT NOTES COMPUT SC
PY 2015
VL 9164
BP 169
EP 177
DI 10.1007/978-3-319-20801-5_18
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Mathematical & Computational Biology; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Mathematical & Computational Biology; Robotics
GA BD8QH
UT WOS:000364183400018
DA 2022-02-03
ER

PT C
AU Walhazi, H
   Maalej, A
   Ben Amara, NE
AF Walhazi, Hajer
   Maalej, Ahmed
   Ben Amara, Najoua Essoukri
BE Sourin, A
   Charier, C
   Rosenberger, C
   Sourina, O
TI Mask2LFP: Mask-constrained Adversarial Latent Fingerprint Synthesis
SO 2020 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2020)
LA English
DT Proceedings Paper
CT 19th International Conference on Cyberworlds (CW)
CY SEP 29-OCT 01, 2020
CL ELECTR NETWORK
DE latent fingerprints; Generative Adversarial Networks; Image synthesis;
   Mask embedding
AB Latent fingerprints are one of the most valuable and unique biometric attributes that are extensively used in forensic and law enforcement applications. Compared to rolled/plain fingerprint, latent fingerprint is of poor quality in term of friction ridge patterns, hence a more challenging for automatic fingerprint recognition systems. Considering the difficulties of dusting, lifting, and recovery of latent fingerprint, this type of fingerprints remain expensive to develop and collect. In this paper, we present a novel approach for synthetic latent fingerprint generation using Generative Adversarial Network (GAN). Our proposed framework, named mask to latent fingerprint (Mask2LFP), uses binary mask of distorted fingerprint-like shapes as input, and outputs a realistic latent fingerprint. This work focuses on the generation of synthetic latent fingerprints. The aim is to alleviate the scarcity issue of latent fingerprint data and serve the increasing need for developing, evaluating, and enhancing fingerprint-based identification systems, especially in forensic applications.
C1 [Walhazi, Hajer; Maalej, Ahmed; Ben Amara, Najoua Essoukri] Univ Sousse, Ecole Natl Ingenieurs Sousse, LATIS Lab Adv Technol & Intelligent Syst, Sousse 4023, Tunisia.
   [Walhazi, Hajer] Univ Sousse, Inst Super Sci Appl & Technol Sousse, Sousse 4003, Tunisia.
   [Maalej, Ahmed] Univ Kairouan, Inst Super Math Appl & Informat Kairouan, Kairouan 3100, Tunisia.
C3 Universite de Sousse; Universite de Sousse; Universite de Kairouan
RP Walhazi, H (corresponding author), Univ Sousse, Ecole Natl Ingenieurs Sousse, LATIS Lab Adv Technol & Intelligent Syst, Sousse 4023, Tunisia.; Walhazi, H (corresponding author), Univ Sousse, Inst Super Sci Appl & Technol Sousse, Sousse 4003, Tunisia.
EM hajerwalhazi@gmail.com; maalejahmed@gmail.com;
   najoua.benamara@eniso.rnu.tn
FU Tunisian Ministry of Higher Education and Scientific Research within the
   PAQ COLLABORA project "Kit for the Detection and Authentication of
   Fingerprints"
FX This work has been funded by the Tunisian Ministry of Higher Education
   and Scientific Research within the PAQ COLLABORA project "Kit for the
   Detection and Authentication of Fingerprints" led by the GEOGLOB
   Research Lab, Faculty of Sciences of Sfax.
CR Attia M, 2019, IEEE SYS MAN CYBERN, P1855, DOI 10.1109/SMC.2019.8914499
   Bontrager P, 2018, INT CONF BIOMETR THE
   Cao K, 2018, INT CONF BIOMETR, P31, DOI 10.1109/ICB2018.2018.00016
   Cappelli R, 2002, INT C PATT RECOG, P744, DOI 10.1109/ICPR.2002.1048096
   Chen SH, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0111099
   Feng JJ, 2011, IEEE T PATTERN ANAL, V33, P209, DOI 10.1109/TPAMI.2010.77
   Haddada LR, 2019, INT J RF MICROW C E, V29, DOI 10.1002/mmce.21905
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson P, 2013, IEEE COMPUT SOC CONF, P154, DOI 10.1109/CVPRW.2013.30
   Lamia RH, 2019, INT MULTICONF SYST, P22, DOI 10.1109/SSD.2019.8893199
   Maltoni D., 2004, INT WORKSH BIOM TECH INT WORKSH BIOM TECH, P147
   Mistry V., 2019, ARXIV PREPRINT ARXIV
   National Institute of Standards and Technology, NIST SPECIAL DATABAS
   Singh H. P., 2020, SEGMENTATION TECHNIQ
   Wilson C. L., 2003, STUDIES FINGERPRINT
   Zhao Q, 2012, IEEE INT C AUTOMAT L, P277, DOI 10.1109/ICAL.2012.6308211
NR 16
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-7281-6497-7
PY 2020
BP 265
EP 271
DI 10.1109/CW49994.2020.00049
PG 7
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BS4FL
UT WOS:000718888200041
DA 2022-02-03
ER

PT C
AU Trumm, S
   Becken, W
   Benard, Y
   Esser, G
   Uttenweiler, D
AF Trumm, Stephan
   Becken, Wolfgang
   Benard, Yohann
   Esser, Gregor
   Uttenweiler, Dietmar
BE Fahnle, OW
   Futterer, G
   Rascher, R
   Haberl, A
TI Simulating the actual imaging in the individual eye A novel approach to
   calculating spectacle lenses
SO SEVENTH EUROPEAN SEMINAR ON PRECISION OPTICS MANUFACTURING
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT 7th European Seminar on Precision Optics Manufacturing
CY MAR 30-APR 01, 2020
CL ELECTR NETWORK
DE optical design; ophthalmic optics; spectacle lens design; higher-order
   aberrations; wavefront tracing; analytical approach; biometric
   parameters of the eye; mass customization
ID HIGHER-ORDER ABERRATIONS; LOCAL WAVE-FRONTS; DERIVATION; EQUATIONS
AB When developing optical systems, every element within the light path is considered in technical optics. In ophthalmic optics, however, spectacle lenses are usually designed to provide a given optical power at a position called vertex sphere, ignoring the actual imaging processing inside the eye.
   We have developed a novel technology (trade name DNEye (R) PRO) overcoming this practice. The computation of the wavefronts does not stop at the back surface of the spectacle lens but is continued right into the eye through its refracting surfaces. The assessment no longer takes place at the vertex sphere, but at the retina. This calculation is based on individual measurements of biometrical parameters of the eye and comprises the complex shapes of the wavefronts and of the refracting surfaces including their higher-order components.
   As a result, effects which arise from the individual structure of the eye and its components are considered giving rise to sharper imaging and better design retention.
C1 [Trumm, Stephan; Becken, Wolfgang; Benard, Yohann; Esser, Gregor; Uttenweiler, Dietmar] Rodenstock GmbH, Elsenheimerstr 33, D-80687 Munich, Germany.
RP Trumm, S (corresponding author), Rodenstock GmbH, Elsenheimerstr 33, D-80687 Munich, Germany.
EM stephan.trumm@rodenstock.com
CR Becken W., 2016, OPTICIAN, V2
   Becken W., 2016, OPTICIAN, V3
   Blendowske R, 2017, J OPT SOC AM A, V34, P1481, DOI 10.1364/JOSAA.34.001481
   Esser G, 2010, J OPT SOC AM A, V27, P218, DOI 10.1364/JOSAA.27.000218
   Esser G., 2016, FOCUS, V1
   Esser G, 2017, J OPT SOC AM A, V34, P441, DOI 10.1364/JOSAA.34.000441
   Esser G, 2012, ADV IMAG ELECT PHYS, V171, P1, DOI 10.1016/B978-0-12-394297-5.00001-5
   Esser G, 2011, J OPT SOC AM A, V28, P2442, DOI 10.1364/JOSAA.28.002442
   Forkel J, 2017, OPTOMETRY VISION SCI, V94, P208, DOI 10.1097/OPX.0000000000001016
   Gullstrand A., 1911, EINFUHRUNG METHODEN
   Gullstrand A., 1900, ALLGEMEINE THEORIE M
   International Organization for Standardization (ISO), 2017, 18476201706E ISOTR, VFirst
   MINKWITZ G, 1963, Opt Acta (Lond), V10, P223
   OYSTER CW, 1999, HUMAN EYE STRUCTURE
   Salmon TO, 2006, J CATARACT REFR SURG, V32, P2064, DOI 10.1016/j.jcrs.2006.07.022
   Sebastian S, 2015, J VISION, V15, DOI 10.1167/15.5.16
   Trumm S., 2014, FRING 2013 7 INT WOR, P301, DOI [10.1007/978-3-642-36359-7_51, DOI 10.1007/978-3-642-36359-7_51]
   Uttenweiler D., 2010, OPTICIAN, V6
   Uttenweiler D., 2010, OPTICIAN, V5
   Vasudevan Balamurali, 2016, J Optom, V9, P196, DOI 10.1016/j.optom.2015.11.001
   von Rohr M., 1911, BRILLE ALS OPTISCHES
NR 21
TC 0
Z9 0
U1 0
U2 0
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 978-1-5106-3866-2; 978-1-5106-3865-5
J9 PROC SPIE
PY 2020
VL 11478
AR 114780A
DI 10.1117/12.2564665
PG 9
WC Engineering, Manufacturing; Engineering, Electrical & Electronic; Optics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Optics
GA BS1SJ
UT WOS:000695453900009
DA 2022-02-03
ER

PT J
AU Noh, H
   Ahn, CG
   Kong, HJ
   Sim, J
AF Noh, HyungWook
   Ahn, Chang-Geun
   Kong, Hyoun-Joong
   Sim, JooYong
TI Ratiometric Impedance Sensing of Fingers for Robust Identity
   Authentication
SO SCIENTIFIC REPORTS
LA English
DT Article
ID BIOELECTRICAL-IMPEDANCE; BODY-COMPOSITION; RECOGNITION; SYSTEMS;
   TEMPERATURE
AB We present a novel biometric authentication system enabled by ratiometric analysis of impedance of fingers. In comparison to the traditional biometrics that relies on acquired images of structural information of physiological characteristics, our biological impedance approach not only eliminates any practical means of making fake copies of the relevant physiological traits but also provides reliable features of biometrics using the ratiometric impedance of fingers. This study shows that the ratiometric features of the impedance of fingers in 10 different pairs using 5 electrodes at the fingertips can reduce the variation due to undesirable factors such as temperature and day-to-day physiological variations. By calculating the ratio of impedances, the difference between individual subjects was amplified and the spectral patterns were diversified. Overall, our ratiometric analysis of impedance improved the classification accuracy of 41 subjects and reduced the error rate of classification from 29.32% to 5.86% (by a factor of 5).
C1 [Noh, HyungWook; Ahn, Chang-Geun; Sim, JooYong] Elect & Telecommun Res Inst, Med Informat Res Sect, Welf & Med ICT Res Dept, Daejeon 34129, South Korea.
   [Noh, HyungWook; Kong, Hyoun-Joong] Chungnam Natl Univ, Dept Biomed Engn, Coll Med, 266 Munwha Ro, Daejeon 35015, South Korea.
C3 Electronics & Telecommunications Research Institute - Korea (ETRI);
   Chungnam National University
RP Sim, J (corresponding author), Elect & Telecommun Res Inst, Med Informat Res Sect, Welf & Med ICT Res Dept, Daejeon 34129, South Korea.
EM jsim@etri.se.kr
OI NOH, HYUNG WOOK/0000-0001-6237-0021; Sim, Joo Yong/0000-0003-3779-7589
FU Institute for Information & communications Technology Promotion (IITP) -
   Korea government (MSIP) [2017-0-00409]; National Research Foundation of
   Korea (NRF) - Korea government (MSIT) [2019R1F1A1062312,
   2019R1F1A1063037]
FX We would like to thank all members of the Biomedical IT Research
   Department of ETRI for their advice and assistance. This work was
   supported by Institute for Information & communications Technology
   Promotion (IITP) grant funded by the Korea government (MSIP) (No.
   2017-0-00409, Study on biomedical imaging and recognition-sensors for
   acquisition and analysis of high quality bio-information) and the
   National Research Foundation of Korea (NRF) grant funded by the Korea
   government (MSIT) (No. 2019R1F1A1062312 and No. 2019R1F1A1063037). The
   authors declare that they have no conflict of interest.
CR Abo-Zahhad M, 2015, IET BIOMETRICS, V4, P179, DOI 10.1049/iet-bmt.2014.0040
   [Anonymous], 1996, Nutrition, V12, P749
   [Anonymous], 1996, AM J CLIN NUTR S, V64, p387S
   [Anonymous], 2019, DATA SCI NEWS
   [Anonymous], 2013, BODYSTAT
   [Anonymous], 2016, BAYOMETRIC
   Baik S. W., 2014, IMPLEMENTATION BIOEL, V6
   CATON JR, 1988, MED SCI SPORT EXER, V20, P489
   Chan HL, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00066
   Cheng YZ, 2006, APPL OPTICS, V45, P9238, DOI 10.1364/AO.45.009238
   Colubri A, 2019, ECLINICALMEDICINE, V11, P54, DOI 10.1016/j.eclinm.2019.06.003
   Cornelius C., 2012, WHO WEARS ME BIOIMPE, V10
   Cornelius C., 2013, USABLE SECURITY WIRE
   Cornelius C, 2014, MOBISYS'14: PROCEEDINGS OF THE 12TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P55, DOI 10.1145/2594368.2594369
   DEURENBERG P, 1988, EUR J CLIN NUTR, V42, P1017
   Dzissah DA, 2019, HEALTHC INFORM RES, V25, P106, DOI 10.4258/hir.2019.25.2.106
   Fingelkurts AA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0087507
   Franco S., 2002, DESIGN OPERATIONAL A
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Galbally J, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0069897
   Geiger J. T., 2013, 2013 IEEE WORKSH APP, P1, DOI DOI 10.1109/WASPAA.2013.6701857
   Goljan M, 2011, IEEE T INF FOREN SEC, V6, P227, DOI 10.1109/TIFS.2010.2099220
   Grosjean S., 2011, ELECT CIRCUIT MEASUR
   Gudivaka R, 1996, J APPL PHYSIOL, V81, P838, DOI 10.1152/jappl.1996.81.2.838
   Han CC, 2003, PATTERN RECOGN, V36, P371
   He X., 2008, FAKE IRIS DETECTION, DOI [10.1109/CCPR.2008.681-4, DOI 10.1109/CCPR.2008.681-4]
   Jain A. K., 2007, HDB BIOMETRICS
   James G., 2017, INTRO STAT LEARNING, V8th
   Kazimov T., 2015, INT RES J ENG TECHNO
   Kim SB, 2014, J ACUPUNCT MERIDIAN, V7, P33, DOI 10.1016/j.jams.2013.01.021
   Koehler JJ, 2001, LAW HUMAN BEHAV, V25, P493, DOI 10.1023/A:1012892815916
   LIANG MTC, 1993, MED SCI SPORT EXER, V25, P1231
   Lim CT, 2011, THEOR APPL MECH LETT, V1, DOI 10.1063/2.1101400
   Liu C, 2005, IEEE INT CONF ROBOT, P3262, DOI 10.1109/ROBOT.2005.1570613
   Lourenco A, 2011, COMPUT INTEL NEUROSC, V2011, DOI 10.1155/2011/720971
   Martinsen OG, 2007, IEEE T BIO-MED ENG, V54, P891, DOI 10.1109/TBME.2007.893472
   Medici G, 2005, EUR J CLIN NUTR, V59, P932, DOI 10.1038/sj.ejcn.1602165
   Menotti D, 2015, IEEE T INF FOREN SEC, V10, P864, DOI 10.1109/TIFS.2015.2398817
   Michael GKO, 2008, IMAGE VISION COMPUT, V26, P1551, DOI 10.1016/j.imavis.2008.06.010
   Prabhakar S., 2003, IEEE Security & Privacy, V1, P33, DOI 10.1109/MSECP.2003.1193209
   Reguraman M., 2016, BIOELECTRICAL IMPEDA, V21, P5
   Sanchez M, 2016, GREY ROOM, P6, DOI 10.1162/GREY_a_00193
   Seoane F, 2014, SENSORS-BASEL, V14, P7120, DOI 10.3390/s140407120
   Song SL, 2009, OPT ENG, V48, DOI 10.1117/1.3130242
   Soria D. I., 2008, IMPLEMENTATION ELECT, V85
   Splinter R., 2010, HDB PHYS MED BIOL
   Toli C.-A., 2014, MULTIMODAL BIOMETRIC, V457
   Vitha MF, 2007, BBA-BIOMEMBRANES, V1768, P107, DOI 10.1016/j.bbamem.2006.06.022
   Wahabi S, 2014, IEEE T INF FOREN SEC, V9, P2002, DOI 10.1109/TIFS.2014.2360430
   Wan CS, 2014, BMC PEDIATR, V14, DOI 10.1186/1471-2431-14-249
   Wayman J., 2005, BIOMETRIC SYSTEMS, p1 20, DOI [10.1007/1-84628-064-8_1, DOI 10.1007/1-84628-064-8_1]
   Whitman A, 1996, J ANIM SCI, V74, P80
   Xu H, 2001, ANAL CHEM, V73, P4124, DOI 10.1021/ac0102718
   Yang MX, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0071523
   Yang WC, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11020141
   Yuan L, 2012, PATTERN RECOGN LETT, V33, P182, DOI 10.1016/j.patrec.2011.09.041
   Yuksel ME, 2010, UBICOMM 2010: THE FOURTH INTERNATIONAL CONFERENCE ON MOBILE UBIQUITOUS COMPUTING, SYSTEMS, SERVICES AND TECHNOLOGIES, P1
   Yulita IN, 2018, HEALTHC INFORM RES, V24, P170, DOI 10.4258/hir.2018.24.3.170
   Zehngut N, 2015, IEEE IMAGE PROC, P522, DOI 10.1109/ICIP.2015.7350853
   Zhang L, 2009, 2009 16TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOLS 1-6, P1981, DOI 10.1109/ICIP.2009.5413734
   Zhang L, 2010, PATTERN RECOGN, V43, P2560, DOI 10.1016/j.patcog.2010.01.020
   Zhu Y, 2007, IEEE T INF FOREN SEC, V2, P391, DOI 10.1109/TIFS.2007.903846
NR 62
TC 0
Z9 0
U1 1
U2 1
PU NATURE PUBLISHING GROUP
PI LONDON
PA MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND
SN 2045-2322
J9 SCI REP-UK
JI Sci Rep
PD SEP 19
PY 2019
VL 9
AR 13566
DI 10.1038/s41598-019-49792-9
PG 12
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA IY7IM
UT WOS:000486568400001
PM 31537843
OA Green Published, gold
DA 2022-02-03
ER

PT J
AU Fu, SY
   He, HB
   Hou, ZG
AF Fu, Siyao
   He, Haibo
   Hou, Zeng-Guang
TI Learning Race from Face: A Survey
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Race classification; face recognition; image categorization; data
   clustering; face database; machine learning; computer vision
ID DEVELOPMENTAL INTERGROUP THEORY; FACIAL EXPRESSIONS; PERCEPTUAL
   DISCRIMINATION; RECOGNITION ALGORITHMS; PERSON CATEGORIZATION; AMYGDALA
   ACTIVITY; SOCIAL COGNITION; ERP EVIDENCE; SKIN TONE; 3D
AB Faces convey a wealth of social signals, including race, expression, identity, age and gender, all of which have attracted increasing attention from multi-disciplinary research, such as psychology, neuroscience, computer science, to name a few. Gleaned from recent advances in computer vision, computer graphics, and machine learning, computational intelligence based racial face analysis has been particularly popular due to its significant potential and broader impacts in extensive real-world applications, such as security and defense, surveillance, human computer interface (HCI), biometric-based identification, among others. These studies raise an important question: How implicit, non-declarative racial category can be conceptually modeled and quantitatively inferred from the face? Nevertheless, race classification is challenging due to its ambiguity and complexity depending on context and criteria. To address this challenge, recently, significant efforts have been reported toward race detection and categorization in the community. This survey provides a comprehensive and critical review of the state-of-the-art advances in face-race perception, principles, algorithms, and applications. We first discuss race perception problem formulation and motivation, while highlighting the conceptual potentials of racial face processing. Next, taxonomy of feature representational models, algorithms, performance and racial databases are presented with systematic discussions within the unified learning scenario. Finally, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potentially important cross-cutting themes and research directions for the issue of learning race from face.
C1 [Fu, Siyao; He, Haibo] Univ Rhode Isl, Dept Elect Comp & Biomed Engn, Kingston, RI 02881 USA.
   [Hou, Zeng-Guang] Chinese Acad Sci, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
C3 University of Rhode Island; Chinese Academy of Sciences
RP Fu, SY (corresponding author), Univ Rhode Isl, Dept Elect Comp & Biomed Engn, Kingston, RI 02881 USA.
EM fu@ele.uri.edu; he@ele.uri.edu; hou@compsys.ia.ac.cn
RI He, Haibo/ABF-3668-2020
OI He, Haibo/0000-0002-5247-9370
FU US National Science Foundation (NSF)National Science Foundation (NSF)
   [CAREER ECCS 1053717]; Army Research Office (ARO) [W911NF-12-1-0378];
   NSF-DFG Collaborative Research on "Autonomous Learning" [CNS 1117314];
   National Natural Science Foundation of China (NSFC)National Natural
   Science Foundation of China (NSFC) [61225017, 61175076]; Minzu
   University of China; Direct For Computer & Info Scie & EnginrNational
   Science Foundation (NSF)NSF - Directorate for Computer & Information
   Science & Engineering (CISE) [1117314] Funding Source: National Science
   Foundation
FX This work was supported by the US National Science Foundation (NSF)
   under grant CAREER ECCS 1053717, Army Research Office (ARO) under grant
   W911NF-12-1-0378, NSF-DFG Collaborative Research on "Autonomous
   Learning" (a supplement grant to CNS 1117314), and National Natural
   Science Foundation of China (NSFC) under grants 61225017 and 61175076.
   S. Fu would also like to thank G. Yang and Minzu University of China for
   the support in the development of this work.
CR Abate AF, 2007, PATTERN RECOGN LETT, V28, P1885, DOI 10.1016/j.patrec.2006.12.018
   Ahmad A., 2013, INT J COMPUT TECHNOL, V4, P234
   Ahmed A, 2008, LECT NOTES COMPUT SC, V5304, P69, DOI 10.1007/978-3-540-88690-7_6
   Akbari R., 2012, INT J SIGNLA PROCESS, V5, P85
   Allison M., 2013, P 26 INT FLOR ART IN, P332
   Anacleto J. C., 2010, ARXIV 1001 0418
   Anitha CM., 2010, INT J ENG SCI TECHNO, V2, P5158
   [Anonymous], 2000, NAT GENET, V24, P97
   Anzures G, 2011, INFANCY, V16, P640, DOI 10.1111/j.1532-7078.2010.00066.x
   Ball R., 2008, P 9 INT CONGR PHYSIO, P150
   Ball R, 2010, APPL ERGON, V41, P832, DOI 10.1016/j.apergo.2010.02.002
   Barbujani G, 2005, CURR GENOMICS, V6, P215, DOI 10.2174/1389202054395973
   Bastanfard A, 2007, INTERNATIONAL CONFERENCE ON MACHINE VISION 2007, PROCEEDINGS, P50
   Bellustin N., 2011, INT J ADV COMPUT SCI, V3, P112, DOI [10.14569/SpecialIssue.2011.010318, DOI 10.14569/SPECIALISSUE.2011.010318]
   Ben Azouz Z, 2006, VISUAL COMPUT, V22, P302, DOI 10.1007/s00371-006-0006-6
   Bentin S, 2000, COGN NEUROPSYCHOL, V17, P35, DOI 10.1080/026432900380472
   Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128
   Berretti S, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168759
   Berretti S, 2010, IEEE T PATTERN ANAL, V32, P2162, DOI 10.1109/TPAMI.2010.43
   Bickel B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239484
   Biehl M, 1997, J NONVERBAL BEHAV, V21, P3, DOI 10.1023/A:1024902500935
   Bigler RS, 2007, CURR DIR PSYCHOL SCI, V16, P162, DOI 10.1111/j.1467-8721.2007.00496.x
   Bigler RS, 2006, ADV CHILD DEV BEHAV, V34, P39, DOI 10.1016/S0065-2407(06)80004-2
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   BOWCOCK AM, 1994, NATURE, V368, P455, DOI 10.1038/368455a0
   Bowyer KW, 2006, COMPUT VIS IMAGE UND, V101, P1, DOI 10.1016/j.cviu.2005.05.005
   Bradtmiller B, HEAD FACE ANTHROPOME
   Brebnera J. L., 2011, THESIS
   Brewer M.B., 1998, INTERGROUP RELATIONS
   Brooks KR, 2010, PERCEPTION, V39, P1142, DOI 10.1068/p6703
   BRUCE V, 1986, BRIT J PSYCHOL, V77, P305, DOI 10.1111/j.2044-8295.1986.tb02199.x
   Bruyer R, 2004, PERCEPTION, V33, P169, DOI 10.1068/p5094
   BRYM RJ, 2006, SOCIOLOGY YOUR COMPA
   Bulthoff I., 2012, J VIS, V12, P1282
   Burchard E., 2002, GENOME BIOL, V3, P1, DOI DOI 10.1186/GB-2002-3-7-COMMENT2007
   Calder A.J., 2011, OXFORD HDB FACE PERC
   Calder AJ, 2005, NAT REV NEUROSCI, V6, P641, DOI 10.1038/nrn1724
   Catz O, 2009, ACTA PSYCHOL, V131, P143, DOI 10.1016/j.actpsy.2009.03.010
   Chang KI, 2005, IEEE T PATTERN ANAL, V27, P619, DOI 10.1109/TPAMI.2005.70
   Chen JM, 2012, J EXP SOC PSYCHOL, V48, P152, DOI 10.1016/j.jesp.2011.10.005
   Coon C.S., 1962, THE ORIGINS OF RACES
   Corneille O, 2007, J PERS SOC PSYCHOL, V93, P335, DOI 10.1037/0022-3514.93.3.335
   Cosmides L, 2003, TRENDS COGN SCI, V7, P173, DOI 10.1016/S1364-6613(03)00057-3
   Cowell AJ, 2005, INT J HUM-COMPUT ST, V62, P281, DOI 10.1016/j.ijhcs.2004.11.008
   Cunningham WA, 2004, PSYCHOL SCI, V15, P806, DOI 10.1111/j.0956-7976.2004.00760.x
   Dailey MN, 2010, EMOTION, V10, P874, DOI 10.1037/a0020019
   Dailey MN, 2002, J COGNITIVE NEUROSCI, V14, P1158, DOI 10.1162/089892902760807177
   Davidenko N, 2007, J VISION, V7, DOI 10.1167/7.4.6
   De Marsico M, 2013, LECT NOTES COMPUT SC, V8156, P472
   De Zhang, 2012, Biometric Recognition. 7th Chinese Conference, CCBR 2012. Proceedings, P300, DOI 10.1007/978-3-642-35136-5_36
   Dehon H, 2001, PERCEPTION, V30, P1107, DOI 10.1068/p3122
   Demirkus M., 2010, P SPIE BIOM TECHN HU
   Ding H., 2013, P IEEE INT C WORKSH, P1
   Dolan RJ, 2002, SCIENCE, V298, P1191, DOI 10.1126/science.1076358
   Dong H, 2001, ASIAN FACE IMAGE DAT
   Duan XD, 2010, 2ND IEEE INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER CONTROL (ICACC 2010), VOL. 2, P125, DOI 10.1109/ICACC.2010.5487194
   Earnest Les, 1989, COMMUN ACM, V32.2, P173
   Eberhardt JL, 2005, AM PSYCHOL, V60, P181, DOI 10.1037/0003-066X.60.2.181
   Eberhardt JL, 2003, PERS SOC PSYCHOL B, V29, P360, DOI 10.1177/0146167202250215
   EKMAN P, 1969, SCIENCE, V164, P86, DOI 10.1126/science.164.3875.86
   Encyclopedia Britannica and Race,, 2012, ENCY BRIT ULT REF SU
   Enlow DH, 1990, FACIAL GROWTH
   Evers V., 1997, THESIS
   Fang F, 2011, PLAST RECONSTR SURG, V127, P874, DOI 10.1097/PRS.0b013e318200afdb
   Farinella G, 2012, 2012 INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV), P383, DOI 10.1109/ICIEV.2012.6317383
   Farkas L.G., 1994, ANTHROPOMETRY HEAD F, V2nd
   Farkas LG, 2005, J CRANIOFAC SURG, V16, P615, DOI 10.1097/01.scs.0000171847.58031.9e
   Fasel B, 2003, PATTERN RECOGN, V36, P259, DOI 10.1016/S0031-3203(02)00052-3
   Freeman JB, 2010, NEUROREPORT, V21, P24, DOI 10.1097/WNR.0b013e3283320d54
   Fu SY, 2012, COMPUT INTEL NEUROSC, V2012, DOI 10.1155/2012/946589
   Fu SY, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1637, DOI 10.1109/IJCNN.2011.6033421
   Fu Y, 2010, INTELLIGENT VIDEO SURVEILLANCE: SYSTEMS AND TECHNOLOGY, P407
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   Fukushima K, 1982, NEOCOGNITRON SELF OR, P267
   Furl N, 2002, COGNITIVE SCI, V26, P797, DOI 10.1016/S0364-0213(02)00084-8
   Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557
   Gauthier I, 2000, J COGNITIVE NEUROSCI, V12, P495, DOI 10.1162/089892900562165
   Godil A., 2004, P SPIE S BIOM TECHN, P351
   Golby AJ, 2001, NAT NEUROSCI, V4, P845, DOI 10.1038/90565
   Goldinger SD, 2009, J EXP PSYCHOL LEARN, V35, P1105, DOI 10.1037/a0016548
   Grother P. J., 2010, 7709 NIST
   Guo G., 2010, P IEEE COMP SOC C CO, P79
   Guo GD, 2012, STUD COMPUT INTELL, V409, P101
   Gupta SD, 2010, METHODS MOL BIOL, V589, P97, DOI [10.1007/978-1-60327-114-1_10, 10.1109/SSIAI.2010.5483908]
   Gupta S, 2010, INT J COMPUT VISION, V90, P331, DOI 10.1007/s11263-010-0360-8
   Gutta S., 1999, IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339), P4084, DOI 10.1109/IJCNN.1999.830815
   Gutta S, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P194, DOI 10.1109/AFGR.1998.670948
   Gutta S, 2000, IEEE T NEURAL NETWOR, V11, P948, DOI 10.1109/72.857774
   Hadid A, 2013, NEUROCOMPUTING, V100, P197, DOI 10.1016/j.neucom.2011.10.040
   Hamilton D. L., 1994, SOCIAL COGNITION IMP, P291
   Haque A., 2005, P 27 ANN C COGN SCI, P899
   Hart AJ, 2000, NEUROREPORT, V11, P2351, DOI 10.1097/00001756-200008030-00004
   Haslanger S, 2000, NOUS, V34, P31, DOI 10.1111/0029-4624.00201
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   Heo J, 2012, IEEE T PATTERN ANAL, V34, P2341, DOI 10.1109/TPAMI.2011.275
   HILL H, 1995, P ROY SOC B-BIOL SCI, V261, P367, DOI 10.1098/rspb.1995.0161
   Hirsh AT, 2008, PAIN, V140, P231, DOI 10.1016/j.pain.2008.09.010
   Hma Salah S., 2013, P  IEEE INT C SING I, V21, P416
   Ho AK, 2011, J PERS SOC PSYCHOL, V100, P492, DOI 10.1037/a0021562
   Hollingsworth K, 2011, IMAGE VISION COMPUT, V29, P707, DOI 10.1016/j.imavis.2011.09.002
   Hosoi S, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P195, DOI 10.1109/AFGR.2004.1301530
   Huang D., 2011, COMP VIS PATT REC WO, P1
   Hugenberg K, 2010, PSYCHOL REV, V117, P1168, DOI 10.1037/a0020463
   Hugenberg K, 2008, SOC PERSONAL PSYCHOL, V2, P1052, DOI 10.1111/j.1751-9004.2008.00090.x
   Hwang BW, 2003, LECT NOTES COMPUT SC, V2688, P557
   Ito TA, 2004, PERS SOC PSYCHOL B, V30, P1267, DOI 10.1177/0146167204264335
   Ito TA, 2009, TRENDS COGN SCI, V13, P524, DOI 10.1016/j.tics.2009.10.002
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Jack RE, 2012, P NATL ACAD SCI USA, V109, P7241, DOI 10.1073/pnas.1200155109
   Jack RE, 2009, CURR BIOL, V19, P1543, DOI 10.1016/j.cub.2009.07.051
   Jain AK, 2004, PROC SPIE, V5404, P561, DOI 10.1117/12.542890
   Johnson KJ, 2005, PSYCHOL SCI, V16, P875, DOI 10.1111/j.1467-9280.2005.01631.x
   Jones CR, 2010, PERS SOC PSYCHOL B, V36, P1073, DOI 10.1177/0146167210375817
   KASSIN SM, 1989, AM PSYCHOL, V44, P1089, DOI 10.1037/0003-066X.44.8.1089
   Kaul C, 2014, SOC COGN AFFECT NEUR, V9, P326, DOI 10.1093/scan/nss138
   Klare B., 2010, P 4 IEEE INT C BIOM, P1, DOI DOI 10.1109/BTAS.2010.5634533
   Klare BF, 2012, IEEE T INF FOREN SEC, V7, P1789, DOI 10.1109/TIFS.2012.2214212
   Kruger N, 2013, IEEE T PATTERN ANAL, V35, P1847, DOI 10.1109/TPAMI.2012.272
   Kubota JT, 2012, NAT NEUROSCI, V15, P940, DOI 10.1038/nn.3136
   Kumar N, 2011, IEEE T PATTERN ANAL, V33, P1962, DOI 10.1109/TPAMI.2011.48
   Kurzban R, 2001, P NATL ACAD SCI USA, V98, P15387, DOI 10.1073/pnas.251541498
   Lagree S., 2011, 2011 IEEE International Conference on Technologies for Homeland Security (HST 2011), P440, DOI 10.1109/THS.2011.6107909
   Lagree S., 2011, P 22 MIDW ART INT CO, V14, P225
   LeCun Y., 1995, HDB BRAIN THEORY NEU
   Lei YH, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P701, DOI 10.1145/2348283.2348377
   Levin DT, 1996, J EXP PSYCHOL LEARN, V22, P1364, DOI 10.1037/0278-7393.22.6.1364
   Levin DT, 2002, PERCEPTION, V31, P567, DOI 10.1068/p3315
   Levin DT, 2000, J EXP PSYCHOL GEN, V129, P559, DOI 10.1037//0096-3445.129.4.559
   Li J., 2012, U.S. Patent, Patent No. [8 331 698 B2, 8331698]
   Li SZ., 2004, HDB FACE RECOGNITION
   Li X, 2013, INVEST OPHTH VIS SCI, V54, P3650, DOI 10.1167/iovs.12-11126
   Li Yanjun, 2008, PLoS One, V3, pe2166, DOI 10.1371/journal.pone.0002166
   Lieberman MD, 2005, NAT NEUROSCI, V8, P720, DOI 10.1038/nn1465
   Lin H., 2006, 6 WORLD C INT CONTR, P9988
   Lindsay R., 1983, EVALUATING WITNESS E
   Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5
   Love R, 2001, LANCET, V358, P476, DOI 10.1016/S0140-6736(01)05671-9
   Lu XG, 2006, LECT NOTES COMPUT SC, V3832, P554
   Lu XG, 2004, P SOC PHOTO-OPT INS, V5404, P114, DOI 10.1117/12.542847
   Luximon Y, 2010, TOOLS AND METHODS OF COMPETITIVE ENGINEERING, VOLS 1-2, P255
   Lyle J. R., 2010, BIOM THEOR APPL SYST, P1
   Mackal M. C., 2009, THESIS
   Macrae CN, 2000, ANNU REV PSYCHOL, V51, P93, DOI 10.1146/annurev.psych.51.1.93
   Maddox KB, 2004, PERS SOC PSYCHOL REV, V8, P383, DOI 10.1207/s15327957pspr0804_4
   Malskies C. R., 2011, P VIS MOD VIS, P353
   Manesh FS, 2010, I C CONT AUTOMAT ROB, P1644, DOI 10.1109/ICARCV.2010.5707882
   Meissner CA, 2001, PSYCHOL PUBLIC POL L, V7, P3, DOI 10.1037//1076-8971.7.1.3
   Moon H. K., 2013, US Patent, Patent No. [8 379 937, 8379937]
   Muhammad G., 2012, 2012 International Conference on Systems, Signals and Image Processing (IWSSIP), P421
   Muhammad G, 2012, INT J ARTIF INTELL T, V21, DOI 10.1142/S0218213012500194
   Natu V, 2011, NEUROIMAGE, V54, P2547, DOI 10.1016/j.neuroimage.2010.10.006
   Ng C.B., 2012, P PAC RIM INT C ART, P335
   O'Toole A. J., 1991, Connection Science, V3, P163, DOI 10.1080/09540099108946583
   O'Toole A. J., 2013, VIS COGN, V21, P1
   O'Toole AJ, 2002, TRENDS COGN SCI, V6, P261, DOI 10.1016/S1364-6613(02)01908-3
   O'Toole AJ., 1991, 13TH P ANN C COGN SC, P847
   Ocegueda O, 2013, IEEE T PATTERN ANAL, V35, P728, DOI 10.1109/TPAMI.2012.126
   Ofan RH, 2011, J COGNITIVE NEUROSCI, V23, P3153, DOI 10.1162/jocn_a_00014
   OTOOLE AJ, 1993, J OPT SOC AM A, V10, P405, DOI 10.1364/JOSAA.10.000405
   OTOOLE AJ, 1994, MEM COGNITION, V22, P208, DOI 10.3758/BF03208892
   Ou Y., 2005, IEEE INT C INF ACQ, P1
   Palmeri TJ, 2004, NAT REV NEUROSCI, V5, P291, DOI 10.1038/nrn1364
   Papesh MH, 2010, COGNITION, V116, P283, DOI 10.1016/j.cognition.2010.05.001
   Pessoa L, 2010, NAT REV NEUROSCI, V11, P773, DOI 10.1038/nrn2920
   Phelps EA, 2003, NEUROPSYCHOLOGIA, V41, P203, DOI 10.1016/S0028-3932(02)00150-1
   Phelps EA, 2000, J COGNITIVE NEUROSCI, V12, P729, DOI 10.1162/089892900562552
   Phillips P. J., 1996, 995 ARM RES LAB
   Phillips PJ, 2011, ACM T APPL PERCEPT, V8, DOI 10.1145/1870076.1870082
   Phillips PJ, 2010, IEEE T PATTERN ANAL, V32, P831, DOI 10.1109/TPAMI.2009.59
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   PLATZ SJ, 1988, J APPL SOC PSYCHOL, V18, P972, DOI 10.1111/j.1559-1816.1988.tb01187.x
   Poggio T., 2013, SCHOLARPEDIA, V8, P3516, DOI DOI 10.4249/SCHOLARPEDIA.3516
   Pratt JA, 2007, INTERACT COMPUT, V19, P512, DOI 10.1016/j.intcom.2007.02.003
   Qiu X.C., 2007, P ICIP, P405
   Ramanathan N., 2009, J VISUAL LANG COMPUT, V15, P3349
   Rehder B, 2005, COGNITIVE PSYCHOL, V51, P1, DOI 10.1016/j.cogpsych.2004.11.001
   RHODES G, 1988, PERCEPTION, V17, P43, DOI 10.1068/p170043
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Robinette K. M., 1999, Second International Conference on 3-D Digital Imaging and Modeling (Cat. No.PR00062), P380, DOI 10.1109/IM.1999.805368
   Roesch EB, 2011, J NONVERBAL BEHAV, V35, P1, DOI 10.1007/s10919-010-0095-9
   Roh MC, 2007, INT J PATTERN RECOGN, V21, P1017, DOI 10.1142/S0218001407005818
   Ronquillo J, 2007, SOC COGN AFFECT NEUR, V2, P39, DOI 10.1093/scan/nsl043
   Roomi S. M. M., 2011, Proceedings of the 2011 Third National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG 2011), P54, DOI 10.1109/NCVPRIPG.2011.19
   Rossion B., 2002, INT J NEUROSCI, V112, P1499
   Salah AA, 2008, J ELECTRON IMAGING, V17, DOI 10.1117/1.2896291
   SAMAL A, 1992, PATTERN RECOGN, V25, P65, DOI 10.1016/0031-3203(92)90007-6
   Samangooei S, 2010, MULTIMED TOOLS APPL, V49, P195, DOI 10.1007/s11042-009-0391-8
   Sarkar S, 2005, IEEE T PATTERN ANAL, V27, P162, DOI 10.1109/TPAMI.2005.39
   Schenke Wolf-Rudiger, 2011, IJCB, P1
   Schiller D, 2009, NAT NEUROSCI, V12, P508, DOI 10.1038/nn.2278
   Senior A, 2005, IEEE SECUR PRIV, V3, P50, DOI 10.1109/MSP.2005.65
   Serre T., 2006, THESIS
   Seung HS, 2000, SCIENCE, V290, P2268, DOI 10.1126/science.290.5500.2268
   Shakhnarovich G, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P16, DOI 10.1109/AFGR.2002.1004124
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Sinha P, 2006, P IEEE, V94, P1948, DOI 10.1109/JPROC.2006.884093
   Solomon CJ, 2013, APPL SOFT COMPUT, V13, P3298, DOI 10.1016/j.asoc.2013.02.010
   Stanley DA, 2011, P NATL ACAD SCI USA, V108, P7710, DOI 10.1073/pnas.1014345108
   Stark  L., 2010, P 4 IEEE INT C BIOM, P1
   Stephens JC, 2001, SCIENCE, V293, P489, DOI 10.1126/science.1059431
   Strom MA, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0041193
   Sukumar SR, 2008, IEEE IMAGE PROC, P1912, DOI 10.1109/ICIP.2008.4712154
   Tariq U., 2011, GENDER RACE IDENTIFI
   Tariq U, 2009, 2009 16TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOLS 1-6, P2441, DOI 10.1109/ICIP.2009.5414117
   Tin H. H. K., 2011, ACEE INT J INFORM TE, V01
   Toderici G, 2010, INT J COMPUT VISION, V89, P382, DOI 10.1007/s11263-009-0300-7
   Trawalter S, 2008, J EXP SOC PSYCHOL, V44, P1322, DOI 10.1016/j.jesp.2008.03.006
   Ullman S, 2002, NAT NEUROSCI, V5, P682, DOI 10.1038/nn870
   Wagner-Schuman M, 2011, INVEST OPHTH VIS SCI, V52, P625, DOI 10.1167/iovs.10-5886
   Wallis J, 2012, ATTEN PERCEPT PSYCHO, V74, P1712, DOI 10.3758/s13414-012-0359-z
   Wang JW, 2013, 2013 IEEE 10TH INTERNATIONAL CONFERENCE ON AND 10TH INTERNATIONAL CONFERENCE ON AUTONOMIC AND TRUSTED COMPUTING (UIC/ATC) UBIQUITOUS INTELLIGENCE AND COMPUTING, P17, DOI 10.1109/UIC-ATC.2013.19
   Wechsler H., 2012, 2012 IEEE WORKSH BIO, P1, DOI 10.1109/BIOMS.2012.6345776
   Wiese H, 2012, BIOL PSYCHOL, V89, P137, DOI 10.1016/j.biopsycho.2011.10.002
   Wilbraham DA, 2008, J VISION, V8, DOI 10.1167/8.15.5
   Wong JY, 2008, CLEFT PALATE-CRAN J, V45, P232, DOI 10.1597/06-175
   Wu B, 2004, INT C PATT RECOG, P914, DOI 10.1109/ICPR.2004.1334677
   Xianchao Qiu, 2005, Advances in Biometrics. International Conference, ICB 2006. Proceedings (Lecture Notes in Computer Science Vol.3832), P411
   Yin L., 2008, AUTOMATIC FACE GESTU, P1, DOI [10.1109/afgr.2008.4813324, DOI 10.1109/AFGR.2008.4813324]
   Yin LJ, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P362
   Yiting Xie, 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P143, DOI 10.1109/BTAS.2012.6374569
   Young SG, 2012, PERS SOC PSYCHOL REV, V16, P116, DOI 10.1177/1088868311418987
   Zarei A, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 1, P514, DOI 10.1109/ICMLA.2012.94
   Zawbaa H., 2012, ARXIV12052345
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhang D., 2010, P IEEE C COMP VIS PA, P108
   Zhang D, 2008, PROCEEDINGS OF 2008 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P62
   Zhang H, 2011, LECT NOTES COMPUT SC, V7098, P82, DOI 10.1007/978-3-642-25449-9_11
   Zhang Q., 2013, TELKOMNIKA INDONESIA, V11, P5076
   Zhong C, 2009, LECT NOTES COMPUT SC, V5558, P386, DOI 10.1007/978-3-642-01793-3_40
   Zhuang ZQ, 2005, J OCCUP ENVIRON HYG, V2, P567, DOI 10.1080/15459620500324727
   [No title captured]
NR 232
TC 69
Z9 72
U1 6
U2 67
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD DEC
PY 2014
VL 36
IS 12
BP 2483
EP 2509
DI 10.1109/TPAMI.2014.2321570
PG 27
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Engineering
GA AT5MW
UT WOS:000344988000012
PM 26353153
OA Green Submitted
DA 2022-02-03
ER

PT C
AU Tarare, S
   Anjikar, A
   Turkar, H
AF Tarare, Suchita
   Anjikar, Akhil
   Turkar, Hemant
GP IEEE
TI Fingerprint Based Gender Classification Using DWT Transform
SO 1ST INTERNATIONAL CONFERENCE ON COMPUTING COMMUNICATION CONTROL AND
   AUTOMATION ICCUBEA 2015
LA English
DT Proceedings Paper
CT First International Conference on Computing Communication Control and
   Automation (ICCUBEA)*
CY FEB 26-27, 2015
CL Pune, INDIA
DE fingerprint; wavelet; dwt; knn classifier; Euclidean distance; features
   of fingerprint
AB Each person's fingerprint structure is unique and is developed for biometric authentication systems than others because fingerprints have advantages such as: feasible, differ from each other (distinct), permanent, accurate, reliable and acceptable all over the world for security and person identity. Fingerprints are considered as legal proof of evidence in courts of law all over the world.
   Frequency domain based fingerprint classification can be done using discrete wavelet transform (dwt), which uses wavelet as its basis function which gives energy based features of an image. We are taking dataset of 100 male and 100 female fingerprints. K nearest neighbor (knn) classifier is used as a classifier which uses Euclidean Distance measure for classification and classifies testing fingerprint as male or female fingerprint.
   This paper describes the overall process of above scheme. DWT transform will give the features of some of the fingerprint images of dataset (training images) to create database of features which will be used as look up table for classification of unknown fingerprint and other fingerprints (testing fingerprints) will be used for testing. Knn classifier will assign one of two groups to testing fingerprint.
C1 [Tarare, Suchita; Anjikar, Akhil; Turkar, Hemant] RTMNU Nagpur Univ, Rajiv Gandhi Coll Engn & Res, Nagpur, Maharashtra, India.
C3 Rashtrasant Tukadoji Maharaj Nagpur University
RP Tarare, S (corresponding author), RTMNU Nagpur Univ, Rajiv Gandhi Coll Engn & Res, Nagpur, Maharashtra, India.
EM suchitatarare@gmail.com; akhil.anjikar09@gmail.com;
   hemantturkar@rediffmail.com
CR Acree MA, 1999, FORENSIC SCI INT, V102, P35, DOI 10.1016/S0379-0738(99)00037-7
   Arulkumaran Tom T., 2013, INT J ENG TRENDS TEC, V4
   Gornale S. S., 2013, AM INT J RES SCI TEC, P46
   Gupta S., 2014, INT J COMPUTER SCI M, V3, P1289
   HUNG DCD, 1993, PATTERN RECOGN, V26, P1661
   Kralik Miroslav, 2003, VARIABILITY EVOLUT, V11, P5
   MALTONI D, 2003, HDB FINGERPRINT RECO
   Mazumdar Susmita Ghosh, 2012, INT J ADV ENG TECHNO
   Muttan S., 2012, INT J BIOMETRICS BIO, V6, P58
   OJO O., 2012, INT J ADV RES ARTIFI, V1, P57
   Pankanti S, 2002, IEEE T PATTERN ANAL, V24, P1010, DOI 10.1109/TPAMI.2002.1023799
   Thepade S, 2014, J ENG-NY, V2014, DOI 10.1155/2014/439218
   Thepade Sudeep, 2012, INT J SCI TECHNOLOGY, V1, P105
   Yin YL, 2004, EURASIP J APPL SIG P, V2004, P495, DOI 10.1155/S1110865704310103
NR 14
TC 5
Z9 5
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-4799-6892-3
PY 2015
BP 689
EP 693
DI 10.1109/ICCUBEA.2015.141
PG 5
WC Automation & Control Systems; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Engineering
GA BF4DH
UT WOS:000380620000139
DA 2022-02-03
ER

PT J
AU Ursem, NTC
   Peters, IA
   Kraan-van der Est, MN
   Reijerink-Verheij, JCIY
   Knapen, MFCM
   Cohen-Overbeek, TE
AF Ursem, Nicolette T. C.
   Peters, Ingrid A.
   Kraan-van der Est, Mieke N.
   Reijerink-Verheij, Jacqueline C. I. Y.
   Knapen, Maarten F. C. M.
   Cohen-Overbeek, Titia E.
TI An Audit of Second-Trimester Fetal Anomaly Scans Based on a Novel
   Image-Scoring Method in the Southwest Region of the Netherlands
SO JOURNAL OF ULTRASOUND IN MEDICINE
LA English
DT Article
DE audit; fetal anomaly scan; quality assessment; training; ultrasound
ID QUALITY-CONTROL; PRENATAL DETECTION; IMPACT; BIOMETRY; REPRODUCIBILITY;
   FEASIBILITY; PERFORMANCE; POPULATION; MANAGEMENT; PREGNANCY
AB Objectives-Since 2007 the second-trimester fetal anomaly scan is offered to all pregnant women as part of the national prenatal screening program in the Netherlands. Dutch population-based screening programs generally have a well-described system to achieve quality assurance. Because of the absence of a uniform system to monitor the actual performance of the fetal anomaly scan in 2012, we developed a standardized image-scoring method. The aim of this study was to evaluate the scanning performance of all sonographers in the southwestern region of the Netherlands using this image-scoring method.
   Methods-Each sonographer was requested to set up a digital portfolio. A portfolio consists of five logbooks from five different pregnant women, each containing 25 fetal anatomical structures and six biometric measures of randomly selected fetal anomaly scans.
   Results-During the study period, 425 logbooks of 85 sonographers were assessed as part of the audit process. Seventy-three out of 85 sonographers (86%) met the criteria in the primary audit, and 12 sonographers required individual hands-on training. A successful assessment was achieved for 11 sonographers in the re-audit and one sonographer ceased her contract. Moreover, 2.1% of the required images were not digitally stored and therefore could not be reviewed.
   Conclusions-Quality assessment using the image-scoring method demonstrated that most of the sonographers met the expectations of the audit process, but those who had subpar performance met the expectations after retraining.
C1 [Ursem, Nicolette T. C.; Peters, Ingrid A.; Kraan-van der Est, Mieke N.; Reijerink-Verheij, Jacqueline C. I. Y.; Knapen, Maarten F. C. M.] Fdn Prenatal Screening Southwest Reg Netherlands, Wyternaweg 80,Na-1515,Na-1503, NL-3015 GE Rotterdam, Netherlands.
   [Ursem, Nicolette T. C.; Peters, Ingrid A.; Kraan-van der Est, Mieke N.; Cohen-Overbeek, Titia E.] Erasmus Univ, Med Ctr, Dept Obstet & Gynecol, Div Obstet & Prenatal Med, Rotterdam, Netherlands.
C3 Erasmus University Rotterdam; Erasmus MC
RP Ursem, NTC (corresponding author), Fdn Prenatal Screening Southwest Reg Netherlands, Wyternaweg 80,Na-1515,Na-1503, NL-3015 GE Rotterdam, Netherlands.
EM n.ursem@erasmusmc.nl
OI Peters, Ingrid/0000-0002-0374-3702
CR Abuhamad AZ, 2004, J ULTRAS MED, V23, P1023, DOI 10.7863/jum.2004.23.8.1023
   ACOG Committee on Practice Bulletins, 2004, OBSTET GYNECOL, V58, P1449
   American College of Obstetricians and Gynecologists, 2009, Obstet Gynecol, V113, P451, DOI 10.1097/AOG.0b013e31819930b0
   American Institute of Ultrasound in Medicine, 2013, J Ultrasound Med, V32, P1083, DOI 10.7863/ultra.32.6.1083
   Baardman ME, 2014, ULTRASOUND OBST GYN, V44, P58, DOI 10.1002/uog.13269
   Boyd PA, 2012, BJOG-INT J OBSTET GY, V119, P1131, DOI 10.1111/j.1471-0528.2012.03373.x
   Chervenak FA, 2007, CLIN PERINATOL, V34, P299, DOI 10.1016/j.clp.2007.03.007
   Dudley N, 2006, ULTRASOUND OBST GYN, V28, P352, DOI 10.1002/uog.3801
   Dudley NJ, 2002, ULTRASOUND OBST GYN, V19, P190, DOI 10.1046/j.0960-7692.2001.00549.x
   EIKNES SH, 1982, ACTA OBSTET GYN SCAN, V61, P53, DOI 10.3109/00016348209156952
   Ensing S, 2014, ULTRASOUND OBST GYN, V44, P154, DOI 10.1002/uog.13291
   Fleurke-Rozema JH, 2014, ULTRASOUND OBST GYN, V43, P553, DOI 10.1002/uog.12546
   Herman A, 1999, ULTRASOUND OBST GYN, V14, P388, DOI 10.1046/j.1469-0705.1999.14060388.x
   Hermann M, 2013, PRENATAL DIAG, V33, P770, DOI 10.1002/pd.4121
   Ivers N, 2012, COCHRANE DB SYST REV, V6, DOI DOI 10.1002/14651858.CD000259.PUB3
   National Institute of Public Health and Environment (RIVM), QUAL ASS FET NUCH TR
   Oosterhuis JJ, 2016, PRENATAL DIAG, V36, P555, DOI 10.1002/pd.4822
   Sairam S, 2009, ULTRASOUND OBST GYN, V33, P545, DOI 10.1002/uog.6323
   Salomon LJ, 2008, PRENATAL DIAG, V28, P822, DOI 10.1002/pd.2016
   Salomon LJ, 2011, ULTRASOUND OBST GYN, V37, P116, DOI 10.1002/uog.8831
   Salomon LJ, 2006, ULTRASOUND OBST GYN, V27, P34, DOI 10.1002/uog.2665
   The Dutch Society of Obstetrics and Gynecology (NVOG), STAND ROUT ULTR EX
   The Fetal Medicine Foundation, CERT COMP NUCH TRANS
   Tolsgaard MG, 2014, ULTRASOUND OBST GYN, V43, P437, DOI 10.1002/uog.13198
   Ursem NTC, 2014, NEDERLANDS TIJDSCHRI, V127, P460
   van Landsveld-Verhoeven C, 2015, EUR RADIOL, V25, P3322, DOI 10.1007/s00330-015-3738-8
   van Velzen CL, 2016, BJOG-INT J OBSTET GY, V123, P400, DOI 10.1111/1471-0528.13274
   van Velzen CL, 2015, ULTRASOUND OBST GYN, V45, P320, DOI 10.1002/uog.14689
   van Velzen CL, 2015, BJOG-INT J OBSTET GY, V122, P1421, DOI 10.1111/1471-0528.13417
   Verburg BO, 2008, ULTRASOUND OBST GYN, V31, P388, DOI 10.1002/uog.5225
NR 30
TC 7
Z9 7
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0278-4297
EI 1550-9613
J9 J ULTRAS MED
JI J. Ultrasound Med.
PD JUN
PY 2017
VL 36
IS 6
BP 1171
EP 1179
DI 10.7863/ultra.16.06055
PG 9
WC Acoustics; Radiology, Nuclear Medicine & Medical Imaging
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Acoustics; Radiology, Nuclear Medicine & Medical Imaging
GA EV6WF
UT WOS:000401914600011
PM 28299806
DA 2022-02-03
ER

PT C
AU Borza, D
   Yaghoubi, E
   Neves, J
   Proenca, H
AF Borza, Diana
   Yaghoubi, Ehsan
   Neves, Joao
   Proenca, Hugo
GP IEEE
TI All-in-one "HairNet": A Deep Neural Model for Joint Hair Segmentation
   and Characterization
SO IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2020)
LA English
DT Proceedings Paper
CT IEEE/IAPR International Joint Conference on Biometrics (IJCB)
CY SEP 28-OCT 01, 2020
CL ELECTR NETWORK
ID SOFT BIOMETRICS; RECOGNITION; LAST
AB The hair appearance is among the most valuable soft biometric traits when performing human recognition at-a-distance. Even in degraded data, the hair's appearance is instinctively used by humans to distinguish between individuals. In this paper we propose a multi-task deep neural model capable of segmenting the hair region, while also inferring the hair color, shape and style, all from in-the-wild images. Our main contributions are two-fold: 1) the design of an all-in-one neural network, based on depthwise separable convolutions to extract the features; and 2) the use convolutional feature masking layer as an attention mechanism that enforces the analysis only within the 'hair' regions. In a conceptual perspective, the strength of our model is that the segmentation mask is used by the other tasks to perceive - at feature-map level - only the regions relevant to the attribute characterization task. This paradigm allows the network to analyze features from non-rectangular areas of the input data, which is particularly important, considering the irregularity of hair regions. Our experiments showed that the proposed approach reaches a hair segmentation performance comparable to the state-of-the-art, having as main advantage the fact of performing multiple levels of analysis in a single-shot paradigm.
C1 [Borza, Diana] Babes Boylai Univ, Cluj Napoca 400000, Romania.
   [Yaghoubi, Ehsan; Proenca, Hugo] Univ Beira Interior, Inst Telecomunicacoes, P-6201001 Covilha, Portugal.
   [Neves, Joao] TomiWorld, P-3500106 Viseu, Portugal.
C3 Babes Bolyai University from Cluj; Universidade da Beira Interior
RP Borza, D (corresponding author), Babes Boylai Univ, Cluj Napoca 400000, Romania.
EM dianaborza@cs.ubbcluj.ro; Ehsan.yaghoubi@ubi.pt;
   JoaoNeves@tomiworld.com; hugomcp@di.ubi.pt
RI Proenca, Hugo/F-9499-2010
OI Proenca, Hugo/0000-0003-2551-8570
CR Borza D, 2018, LECT NOTES COMPUT SC, V11182, P438, DOI 10.1007/978-3-030-01449-0_37
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chang WY, 2017, IEEE COMPUT SOC CONF, P1963, DOI 10.1109/CVPRW.2017.246
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025
   Feldstein S, 2019, GLOBAL EXPANSION SUR
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Howard A. G., 2017, ARXIV
   Huang, 0749 U MASS
   Huang GB, 2007, IEEE I CONF COMP VIS, P237, DOI 10.1109/iccv.2007.4408858
   Hurlbert A, 2017, TEXT INST BOOK SER, P169, DOI 10.1016/B978-0-08-101270-3.00007-2
   Ileni TA, 2019, VISAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4, P59, DOI 10.5220/0007250500590066
   Julian P, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4617, DOI 10.1109/ICPR.2010.1134
   Karras T., 2017, PROGR GROWING GANS I
   King DB, 2015, ACS SYM SER, V1214, P1
   Krupka A., 2014, P 6 INT C ADV MUL, P102
   Lee K.-c., 2008, 2008 8 IEEE INT C AU 2008 IEEE INT C AUT, P1
   Levinshtein A, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P1, DOI 10.1109/CRV.2018.00011
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lipowezky U, 2008, IEEE CONV EL ELECT I, P51, DOI 10.1109/EEEI.2008.4736632
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Proenca H, 2017, IEEE T INF FOREN SEC, V12, P1637, DOI 10.1109/TIFS.2017.2680246
   Qin SY, 2017, IEEE INT CON MULTI, P103, DOI 10.1109/ICME.2017.8019339
   Ranjan R, 2017, IEEE INT CONF AUTOMA, P17, DOI 10.1109/FG.2017.137
   Rousset C, 2008, IEEE IMAGE PROC, P2276, DOI 10.1109/ICIP.2008.4712245
   Shen YH, 2014, SCI WORLD J, DOI 10.1155/2014/748634
   Sinha P, 2000, PERCEPTION, V29, P1005, DOI 10.1068/p2908no
   Sinha P, 2002, PERCEPTION, V31, P133, DOI 10.1068/p3101no
   Sinha P, 2006, P IEEE, V94, P1948, DOI 10.1109/JPROC.2006.884093
   Svanera M, 2016, IEEE IMAGE PROC, P933, DOI 10.1109/ICIP.2016.7532494
   Tome P, 2014, IEEE T INF FOREN SEC, V9, P464, DOI 10.1109/TIFS.2014.2299975
   Wang D., 2013, 2013 10 IEEE INT C W, P1
   Wang Y, 2014, INT C PATT RECOG, P450, DOI 10.1109/ICPR.2014.86
   Yacoob Y, 2006, IEEE T PATTERN ANAL, V28, P1164, DOI 10.1109/TPAMI.2006.139
NR 36
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-7281-9186-7
PY 2020
PG 10
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Mathematical & Computational Biology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Mathematical & Computational Biology
GA BS4XQ
UT WOS:000723870900050
DA 2022-02-03
ER

PT C
AU Holman, KW
   Kocher, DG
   Kaushik, S
AF Holman, Kevin W.
   Kocher, David G.
   Kaushik, Sumanth
BE Hayduk, MJ
   Pirich, AR
   Delfyett, PJ
   Donkor, EJ
   Barrios, JP
   Bussjager, RJ
   Fanto, ML
   Kaminski, RL
   Li, G
   Mohseni, H
   Taylor, EW
TI MIT/LL development of broadband linear frequency chirp for
   high-resolution ladar
SO ENABLING PHOTONICS TECHNOLOGIES FOR DEFENSE, SECURITY, AND AEROSPACE
   APPLICATIONS III
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT Conference on Enabling Photonics Technologies for Defense, Security, and
   Aerospace Application III
CY APR 09-10, 2007
CL Orlando, FL
DE laser radar; ladar; lidar; arbitrary waveform generation; linear
   frequency modulation; chirp; mode-locked laser
AB The development of a high-resolution laser radar (ladar) exhibiting sub-mm resolution would have a great impact on standoff identification applications. It would provide biometric identification capabilities such as three-dimensional facial recognition, interrogation of skin pore patterns and skin texture, and iris recognition. The most significant technical challenge to developing such a ladar is to produce the appropriate optical waveform with high fideltiy. One implementation of such a system requires a 1.5-THz linear frequency sweep in 75 mu s. Previous demonstrations of imaging with such waveforms achieved a 1 THz sweep in > 100 ms, and required additional corrections to compensate for sweep nonlinearity. The generation of high fidelity, temporally short frequency-swept waveforms is of considerable interest to the Dol) community. We are developing a technique that utilizes a novel method to generate a I THz sweep in 50 mu s from a mode-locked laser. As a proof-of-principle demonstration of this technique we have successfully generated a 20 GHz sweep in I Vs with a fidelity sufficient to produce better than -20 dB sidelobes for a range measurement without using any additional corrections. This method is scalable to produce the entire I THz sweep in 50 mu s.
C1 [Holman, Kevin W.; Kocher, David G.; Kaushik, Sumanth] MIT, Lincoln Lab, 244 Wood St, Lexington, MA 02420 USA.
C3 Lincoln Laboratory; Massachusetts Institute of Technology (MIT)
RP Holman, KW (corresponding author), MIT, Lincoln Lab, 244 Wood St, Lexington, MA 02420 USA.
FU United States Air Force under AF [FA8721-05-C-0002]
FX The authors thank R. M. Heinrichs for valuable discussions, and J. H.
   Kyung and L. Jiang for their contributions in the early stages of this
   work. This work is sponsored by the United States Air Force under AF
   Contract #FA8721-05-C-0002. Opinions, interpretations, recommendations
   and conclusions are those of the authors and are not necessarily
   endorsed by the United States Government
CR Bashkansky M, 2002, OPT LETT, V27, P1983, DOI 10.1364/OL.27.001983
   Buell W, 2005, P SOC PHOTO-OPT INS, V5791, P152, DOI 10.1117/12.609682
   CAPUTI WJ, 1971, IEEE T AERO ELECTR S, V7
   *DARPA DSO, 0511 BAA DARPA DSO
   McDonough R. N., 1991, SYNTHETIC APERTURE R
   Shimotsu S, 2001, IEEE PHOTONIC TECH L, V13, P364, DOI 10.1109/68.917854
   Takada K, 2001, IEEE PHOTONIC TECH L, V13, P577, DOI 10.1109/68.924025
NR 7
TC 5
Z9 5
U1 0
U2 2
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
BN 978-0-8194-6694-5
J9 PROC SPIE
PY 2007
VL 6572
AR 65720J
DI 10.1117/12.724272
PG 8
WC Optics; Physics, Applied
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Optics; Physics
GA BGJ60
UT WOS:000247643300015
DA 2022-02-03
ER

PT J
AU Susetianingtias, DT
   Madenda, S
   Fitrianingsih
   Adlina, D
   Rodiah
   Arianty, R
AF Susetianingtias, Diana Tri
   Madenda, Sarifuddin
   Fitrianingsih
   Adlina, Dea
   Rodiah
   Arianty, Rini
TI Retinal Blood Vessel Extraction using Wavelet Decomposition
SO INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS
LA English
DT Article
DE Blood vessels; extraction; fundus retina; identification; wavelet
AB One important part of the eye that is critical for processing visual information before it is sent through the optic nerve to the visual cortex is the retina. The retina of each individual has its own uniqueness that can be used as a characteristic feature in identifying, verifying, and authenticating. The traditional authentication process has various weaknesses such as forgetting the PIN code or losing the ID card used for obtaining system authentication. The results of extracted retinal blood vessels can be used as a feature in the formation of an individual identification system. In the imaging using a fundus camera, the retina's blood vessel has distinguishing shape and number of candidates from one human retina to another. In this research, researchers will develop an algorithm for extracting the retinal fundus image's blood vessels. The feature extraction is done by taking the fundus image feature which is the blood vessel as one of the unique characteristics in forming an individual identification system. The number of blood vessel candidates will then be calculated from the extracted blood vessel result. This research uses wavelet function by looking at the very complex texture of blood vessels using the approximation coefficient. The direction detail coefficient on the wavelet is also used to perform the extraction of retinal blood vessels where the structure of the retinal blood vessels in the fundus image is in all directions. The results of these blood vessel candidates will be used in further research to formulate a biometric system that is formed by unique features in the retinal fundus image which will be used to identify individuals using body traits.
C1 [Susetianingtias, Diana Tri] Gunadarma Univ, Dept Comp Syst, Depok, Indonesia.
   [Madenda, Sarifuddin] Gunadarma Univ, Doctoral Program Informat Technol, Depok, Indonesia.
   [Fitrianingsih] Gunadarma Univ, Dept Informat Syst Diploma, Depok, Indonesia.
   [Adlina, Dea; Rodiah] Gunadarma Univ, Dept Informat, Depok, Indonesia.
   [Arianty, Rini] Gunadarma Univ, Dept Informat Syst, Depok, Indonesia.
C3 Gunadarma University; Gunadarma University; Gunadarma University;
   Gunadarma University; Gunadarma University
RP Susetianingtias, DT (corresponding author), Gunadarma Univ, Dept Comp Syst, Depok, Indonesia.
FU DP2M RistekdiktiMinistry of Research and Technology of the Republic of
   Indonesia (RISTEK)
FX Thank you to DP2M Ristekdikti for research funding in the applied
   scheme. A sincere gratitude to Gunadarma University especially to the
   Gunadarma University Research Bureau for the opportunity to conduct
   research specifically in the field of Biometrics.
CR Ahmed A., 2005, 6TH IEEE INF ASS WOR
   Ashbourn J, 2004, PRACTICAL BIOMETRICS
   Ashbourn Julian, 2002, BIOMETRICS ADV IDEND
   Borah, 2015, IEEE INT C COMP COMM
   Bowling Brad, 2016, KANSKISS CLIN OPHTHA
   El-Bakry Hazem M., 2000, MANSOURA ENG J MANSO
   Fatima Joddat, 2013, SECURE PERSONAL IDEN
   Kalyani CH, 2017, J BIOM BIOSTAT, V8, P371, DOI DOI 10.4172/2155-6180.1000371
   Mudholkar, 2012, INT J COMPUTER SCI E, V2
   Prins C, 1998, COMPUTER LAW SECURIT, V14
   Sadikoglu F, 2016, PROCEDIA COMPUT SCI, V102, P26, DOI 10.1016/j.procs.2016.09.365
   Sasidharan, 2014, INT J COMPUTER TREND, V17
   Soutar C, 2002, SECURE, P46
   Spinella Edmund, 2003, BIOMETRIC SCANNING T
   Susetianingtias Diana Tri, 2017, J THEORETICAL APPL I
   Tuyls P, 2004, LECT NOTES COMPUT SC, V3087, P158
   Unar JA, 2014, PATTERN RECOGN, V47, P2673, DOI 10.1016/j.patcog.2014.01.016
   Wacks R., 1989, PERSONAL INFORM PRIV
   Wang Zhenchang, 2018, DIAGNOSTIC IMAGING O
NR 19
TC 0
Z9 0
U1 1
U2 1
PU SCIENCE & INFORMATION SAI ORGANIZATION LTD
PI WEST YORKSHIRE
PA 19 BOLLING RD, BRADFORD, WEST YORKSHIRE, 00000, ENGLAND
SN 2158-107X
EI 2156-5570
J9 INT J ADV COMPUT SC
JI Int. J. Adv. Comput. Sci. Appl.
PD APR
PY 2020
VL 11
IS 4
BP 351
EP 355
PG 5
WC Computer Science, Theory & Methods
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA LU0YR
UT WOS:000537489900048
DA 2022-02-03
ER

PT C
AU Montero, D
   Unzueta, L
   Goenetxea, J
   Aranjuelo, N
   Loyo, E
   Otaegui, O
   Nieto, M
AF Montero, David
   Unzueta, Luis
   Goenetxea, Jon
   Aranjuelo, Nerea
   Loyo, Estibaliz
   Otaegui, Oihana
   Nieto, Marcos
BE Farinella, GM
   Radeva, P
   Braz, J
   Bouatouch, K
TI Multi-Stage Dynamic Batching and On-Demand I-Vector Clustering for
   Cost-effective Video Surveillance
SO VISAPP: PROCEEDINGS OF THE 16TH INTERNATIONAL JOINT CONFERENCE ON
   COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS -
   VOL. 5: VISAPP
LA English
DT Proceedings Paper
CT 16th International Joint Conference on Computer Vision, Imaging and
   Computer Graphics Theory and Applications (VISIGRAPP) / 16th
   International Conference on Computer Vision Theory and Applications
   (VISAPP)
CY FEB 08-10, 2021
CL ELECTR NETWORK
DE Face Recognition; Face Clustering; Video-Surveillance
ID REPRESENTATION
AB In this paper, we present a cost-effective Video-Surveillance System (VSS) for face recognition and online clustering of unknown individuals at large scale. We aim to obtain Performance Indicators (PIs) for people flow monitoring in large infrastructures, without storing any biometric information. For this purpose, we focus on how to take advantage of a central GPU-enabled computing server, connected to a set of video-surveillance cameras, to automatically register new identities and update their descriptive data as they are reidentified. The proposed method comprises two main procedures executed in parallel. A Multi-Stage Dynamic Batching (MSDB) procedure efficiently extracts facial identity vectors (i-vectors) from captured images. At the same time, an On-Demand I-Vector Clustering (ODIVC) procedure clusters the i-vectors into identities. This clustering algorithm is designed to progressively adapt to the increasing data scale, with a lower decrease in its effectiveness compared to other alternatives. Experimental results show that ODIVC achieves state-of-the-art results in well-known large scale datasets and that our VSS can detect, recognize and cluster in real time faces coming from up to 40 cameras with a central off-the-shelf GPU-enabled computing server.
C1 [Montero, David; Unzueta, Luis; Goenetxea, Jon; Aranjuelo, Nerea; Loyo, Estibaliz; Otaegui, Oihana; Nieto, Marcos] Vicomtech, Mikeletegi 57, Donostia Sansebastian 20009, Spain.
RP Montero, D (corresponding author), Vicomtech, Mikeletegi 57, Donostia Sansebastian 20009, Spain.
RI Unzueta, Luis/L-6867-2014
OI Unzueta, Luis/0000-0001-5648-0910
CR Agarwal A, 2019, 36 ICML, V97, P92
   Amigo E, 2009, INFORM RETRIEVAL, V12, P461, DOI 10.1007/s10791-008-9066-8
   Chaudhari S T, 2010, Proceedings of the Third International Conference on Emerging Trends in Engineering and Technology (ICETET 2010), P520, DOI 10.1109/ICETET.2010.83
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng Q., 2019, IEEE T IND INFORM, DOI [DOI 10.1109/TII.2019.2909473, 10.1109/TII.2019.2909473.]
   Ioffe S, 2015, ARXIV150203167, P448
   Jankar JR, 2017, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INTELLIGENT SUSTAINABLE SYSTEMS (ICISS 2017), P250, DOI 10.1109/ISS1.2017.8389408
   Lim K.-S., 2018, PDCAT, P84
   Lin WA, 2018, PROC CVPR IEEE, P8128, DOI 10.1109/CVPR.2018.00848
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Mayer C., 2015, J AIRPORT MANAGEMENT, V9, P144
   Maze B, 2018, INT CONF BIOMETR, P158, DOI 10.1109/ICB2018.2018.00033
   Nasir M., 2018, J PARALLEL DISTR COM, V126
   Otto C, 2018, IEEE T PATTERN ANAL, V40, P289, DOI 10.1109/TPAMI.2017.2679100
   Radul A., 2019, ARXIVABS191011141
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shi YC, 2018, IEEE T INF FOREN SEC, V13, P1626, DOI 10.1109/TIFS.2018.2796999
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   TensorFlow A., 2017, IMPLEMENTATION CONTR
   Tsakanikas V., 2017, COMPUTERS ELECT ENG, V70
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang ZD, 2019, PROC CVPR IEEE, P1117, DOI 10.1109/CVPR.2019.00121
   Whitelam C, 2017, IEEE COMPUT SOC CONF, P592, DOI 10.1109/CVPRW.2017.87
   Yadwadkar NJ, 2019, PROCEEDINGS OF THE WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS (HOTOS '19), P184, DOI 10.1145/3317550.3321443
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang ZP, 2016, IEEE T PATTERN ANAL, V38, P918, DOI 10.1109/TPAMI.2015.2469286
NR 26
TC 0
Z9 0
U1 1
U2 1
PU SCITEPRESS
PI SETUBAL
PA AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL
BN 978-989-758-488-6
PY 2021
BP 436
EP 443
DI 10.5220/0010236204360443
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BR6JA
UT WOS:000661288200044
OA hybrid
DA 2022-02-03
ER

PT J
AU Mehboob, R
   Dawood, H
   Dawood, H
   Ilyas, MU
   Guo, P
   Banjar, A
AF Mehboob, Rubab
   Dawood, Hassan
   Dawood, Hussain
   Ilyas, Muhammad Usman
   Guo, Ping
   Banjar, Ameen
TI Live fingerprint detection using magnitude of perceived spatial stimuli
   and local phase information
SO JOURNAL OF ELECTRONIC IMAGING
LA English
DT Article
DE biometric systems; fingerprints; live; spoof; local and global features;
   perceived spatial stimuli; quantization; ridges valleys; ridge frequency
ID CLASSIFICATION; VEIN; PERSPIRATION; FEATURES; FUSION
AB Fingerprint recognition systems are widely used for authentication purposes in security systems. However, fingerprint recognition systems can easily be spoofed by imitations of fingerprints using various spoof materials. A compact and discriminative set of features is needed to discriminate between live and spoof fingerprints. We explore combined Shepard magnitude and orientation for live fingerprint detection using independent quantization of global and local features extracted in spatial and frequency domain. The spatial domain features that are extracted comprise of the magnitude of perceived spatial stimuli that is computed from the net variation of perceived edge information. Rotation invariance is achieved by extracting local features based on phase information of significant frequency components in the frequency domain. The concatenated feature vector associated with a fingerprint image is represented as a two-dimensional histogram. The support vector machine classifier is used to classify the fingerprint as either live or spoof. Experiments are performed on three databases, i.e., the fingerprint liveness detection (LivDet) competition databases of 2011, 2013, and 2015. Results showed a reduction in average error rate to 5.8, 2.2, and 5.3 on LivDet 2011, 2013, and 2015, respectively. (C) 2018 SPIE and IS&T
C1 [Mehboob, Rubab; Dawood, Hassan] Univ Engn & Technol, Dept Software Engn, Taxila, Pakistan.
   [Dawood, Hussain; Ilyas, Muhammad Usman; Banjar, Ameen] Univ Jeddah, Fac Comp & Informat Technol, Jeddah, Saudi Arabia.
   [Ilyas, Muhammad Usman] Natl Univ Sci & Technol, Sch Elect Engn & Comp Sci, Dept Elect Engn, Islamabad, Pakistan.
   [Guo, Ping] Beijing Normal Univ, Sch Syst Sci, Beijing, Peoples R China.
C3 University of Engineering & Technology Taxila; University of Jeddah;
   National University of Sciences & Technology - Pakistan; Beijing Normal
   University
RP Dawood, H (corresponding author), Univ Engn & Technol, Dept Software Engn, Taxila, Pakistan.
EM hasan.dawood@uettaxila.edu.pk
RI GUO, Ping/AAG-2160-2019; Ilyas, Muhammad Usman/Q-4791-2019; Dawood,
   Hussain/G-7453-2017; Dawood, Hassan/AAZ-8114-2021
OI GUO, Ping/0000-0002-7122-1084; Ilyas, Muhammad
   Usman/0000-0003-0308-4361; Dawood, Hussain/0000-0003-2653-9541; 
CR Abhyankar A, 2009, PATTERN RECOGN, V42, P452, DOI 10.1016/j.patcog.2008.06.012
   Cao K, 2013, PATTERN RECOGN, V46, P3186, DOI 10.1016/j.patcog.2013.05.008
   Chen J, 2010, IEEE T PATTERN ANAL, V32, P1705, DOI 10.1109/TPAMI.2009.155
   Choi H, 2011, IEEE T INF FOREN SEC, V6, P338, DOI 10.1109/TIFS.2010.2103940
   Chugh T., 2017, INT JT C BIOM
   Coli P, 2007, 2007 IEEE WORKSHOP ON AUTOMATIC IDENTIFICATION ADVANCED TECHNOLOGIES, PROCEEDINGS, P169, DOI 10.1109/AUTOID.2007.380614
   Darlow LN, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.2.023027
   Ding YH, 2016, INT CONF BIOMETR
   Dubey RK, 2016, IEEE T INF FOREN SEC, V11, P1461, DOI 10.1109/TIFS.2016.2535899
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Ghiani L., 2013, IEEE 6 INT BIOM THEO
   Ghiani L., 2013, 6 IAPR IEEE INT C BI
   Ghiani L., 2012, 21 INT C PATT REC, P2
   Ghiani L, 2017, IET BIOMETRICS, V6, P224, DOI 10.1049/iet-bmt.2016.0007
   Gragnaniello D., 2013, WORKSH BIOM MEAS SYS
   Gragnaniello D, 2015, IEEE T INF FOREN SEC, V10, P849, DOI 10.1109/TIFS.2015.2404294
   Gragnaniello D, 2015, PATTERN RECOGN, V48, P1050, DOI 10.1016/j.patcog.2014.05.021
   Grailu H, 2017, MULTIMED TOOLS APPL, V76, P9959, DOI 10.1007/s11042-016-3590-0
   Gupta P, 2015, DIGIT SIGNAL PROCESS, V38, P43, DOI 10.1016/j.dsp.2014.12.003
   Jain AK, 2011, IEEE T PATTERN ANAL, V33, P88, DOI 10.1109/TPAMI.2010.59
   Jia X., 2013, P 2013 INT C BIOM IC
   Jia XB, 2016, MULTIMED TOOLS APPL, V75, P1099, DOI 10.1007/s11042-014-2359-6
   Jiang Y., 2018, J ELECT COMPUT ENG, V2018, P1, DOI DOI 10.1155/2018/2343891
   Jin CL, 2007, LECT NOTES COMPUT SC, V4817, P168
   Kasban H, 2016, NEUROCOMPUTING, V171, P910, DOI 10.1016/j.neucom.2015.07.030
   Kim S, 2016, PATTERN RECOGN LETT, V77, P58, DOI 10.1016/j.patrec.2016.03.015
   Kim W, 2017, IEEE SIGNAL PROC LET, V24, P51, DOI 10.1109/LSP.2016.2636158
   Kobayashi T, 2013, PROC CVPR IEEE, P747, DOI 10.1109/CVPR.2013.102
   Kumar A, 2013, PROC CVPR IEEE, P3438, DOI 10.1109/CVPR.2013.441
   Kundargi J., 2018, FINGERPRINT LIVENESS
   Labati R. D., 2016, PATTERN RECOGNIT LET
   Liu EY, 2016, IEEE T INF FOREN SEC, V11, P1893, DOI 10.1109/TIFS.2016.2541345
   Liu GH, 2008, PATTERN RECOGN, V41, P3521, DOI 10.1016/j.patcog.2008.06.010
   Liu GH, 2010, PATTERN RECOGN, V43, P2380, DOI 10.1016/j.patcog.2010.02.012
   Marasco E, 2012, PATTERN RECOGN LETT, V33, P1148, DOI 10.1016/j.patrec.2012.01.009
   Mathew JJ, 2015, IEEE SIGNAL PROC LET, V22, P1336, DOI 10.1109/LSP.2015.2404827
   Menotti D, 2015, IEEE T INF FOREN SEC, V10, P864, DOI 10.1109/TIFS.2015.2398817
   Nixon KA, 2005, P SOC PHOTO-OPT INS, V5779, P214, DOI 10.1117/12.606643
   Nogueira RF, 2016, IEEE T INF FOREN SEC, V11, P1206, DOI 10.1109/TIFS.2016.2520880
   Nogueira RF, 2014, 2014 IEEE WORKSHOP ON BIOMETRIC MEASUREMENTS AND SYSTEMS FOR SECURITY AND MEDICAL APPLICATIONS (BIOMS) PROCEEDINGS, P22, DOI 10.1109/BIOMS.2014.6951531
   Nosaka R., 2011, 5 PAC RIM S IM VID T, V2, P82
   Nosaka R., 2012, P AS C COMP VIS, P15
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ojansivu V., 2008, 2008 19 INT C PATT R, P1, DOI DOI 10.1109/ICPR.2008.4761377
   Park E., 2016, 2016 INT C BIOM SPEC
   Peralta D, 2018, INT J INTELL SYST, V33, P213, DOI 10.1002/int.21948
   Qian JJ, 2013, PATTERN RECOGN, V46, P2724, DOI 10.1016/j.patcog.2013.03.005
   Reddy PV, 2008, IEEE T BIOMED CIRC S, V2, P328, DOI 10.1109/TBCAS.2008.2003432
   Song W, 2011, PATTERN RECOGN LETT, V32, P1541, DOI 10.1016/j.patrec.2011.04.021
   Tan BZ, 2010, PATTERN RECOGN, V43, P2845, DOI 10.1016/j.patcog.2010.01.023
   Wang C., 2015, CHIN C BIOM REC
   Wong WJ, 2013, PATTERN RECOGN LETT, V34, P1221, DOI 10.1016/j.patrec.2013.03.039
   Wu Q.M.J, 2018, SOFT COMPUT
   Xia ZH, 2018, MULTIMED TOOLS APPL, V77, P18187, DOI 10.1007/s11042-017-5517-9
   Xia ZH, 2017, SIGNAL IMAGE VIDEO P, V11, P381, DOI 10.1007/s11760-016-0936-z
   Yambay D., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P208, DOI 10.1109/ICB.2012.6199810
   Yang JF, 2012, PATTERN RECOGN LETT, V33, P623, DOI 10.1016/j.patrec.2011.11.002
   Yuan CS, 2016, CHINA COMMUN, V13, P60, DOI 10.1109/CC.2016.7559076
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882
   Zhang Y., 2016, CHIN C BIOM REC
NR 60
TC 3
Z9 3
U1 0
U2 10
PU IS&T & SPIE
PI BELLINGHAM
PA 1000 20TH ST, BELLINGHAM, WA 98225 USA
SN 1017-9909
EI 1560-229X
J9 J ELECTRON IMAGING
JI J. Electron. Imaging
PD SEP
PY 2018
VL 27
IS 5
AR 053038
DI 10.1117/1.JEI.27.5.053038
PG 13
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering; Optics; Imaging Science & Photographic Technology
GA GZ2PJ
UT WOS:000449229800040
DA 2022-02-03
ER

PT J
AU Wu, LY
   Cheng, JZ
   Li, SL
   Lei, BY
   Wang, TF
   Ni, D
AF Wu, Lingyun
   Cheng, Jie-Zhi
   Li, Shengli
   Lei, Baiying
   Wang, Tianfu
   Ni, Dong
TI FUIQA: Fetal Ultrasound Image Quality Assessment With Deep Convolutional
   Networks
SO IEEE TRANSACTIONS ON CYBERNETICS
LA English
DT Article
DE Deep convolutional neural network (DCNN); fetal ultrasound (US); local
   phase; quality control
ID STANDARD PLANE LOCALIZATION; NODULES
AB The quality of ultrasound (US) images for the obstetric examination is crucial for accurate biometric measurement. However, manual quality control is a labor intensive process and often impractical in a clinical setting. To improve the efficiency of examination and alleviate the measurement error caused by improper US scanning operation and slice selection, a computerized fetal US image quality assessment (FUIQA) scheme is proposed to assist the implementation of US image quality control in the clinical obstetric examination. The proposed FUIQA is realized with two deep convolutional neural network models, which are denoted as L-CNN and C-CNN, respectively. The L-CNN aims to find the region of interest (ROI) of the fetal abdominal region in the US image. Based on the ROI found by the L-CNN, the C-CNN evaluates the image quality by assessing the goodness of depiction for the key structures of stomach bubble and umbilical vein. To further boost the performance of the L-CNN, we augment the input sources of the neural network with the local phase features along with the original US data. It will be shown that the heterogeneous input sources will help to improve the performance of the L-CNN. The performance of the proposed FUIQA is compared with the subjective image quality evaluation results from three medical doctors. With comprehensive experiments, it will be illustrated that the computerized assessment with our FUIQA scheme can be comparable to the subjective ratings from medical doctors.
C1 [Wu, Lingyun; Cheng, Jie-Zhi; Lei, Baiying; Wang, Tianfu; Ni, Dong] Shenzhen Univ, Natl Reg Key Technol Engn Lab Med Ultrasound, Guangdong Key Lab Biomed Measurements & Ultrasoun, Sch Biomed Engn, Shenzhen 518060, Peoples R China.
   [Li, Shengli] Nanfang Med Univ, Dept Ultrasound, Affiliated Shenzhen Maternal & Child Healthcare H, Shenzhen 518000, Peoples R China.
C3 Shenzhen University; Southern Medical University - China
RP Ni, D (corresponding author), Shenzhen Univ, Natl Reg Key Technol Engn Lab Med Ultrasound, Guangdong Key Lab Biomed Measurements & Ultrasoun, Sch Biomed Engn, Shenzhen 518060, Peoples R China.
EM nidong@szu.edu.cn; nidong@szu.edu.cn
RI Lei, Baiying/AAY-5515-2020
OI Lei, Baiying/0000-0002-3087-2550; Wang, Tianfu/0000-0002-1248-1214
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [81571758, 61571304, 61501305]; National Key
   Research and Development Program of China [2016YFC0104703]; Shenzhen
   Basic Research Project [JCYJ20150525092940982, JCYJ20140509172609164];
   Natural Science Foundation of SZU [2016089]; Open Fund Project of Fujian
   Provincial Key Laboratory of Information Processing and Intelligent
   Control (Minjiang University) [MJUKF201711]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 81571758, Grant 61571304, and Grant
   61501305, in part by the National Key Research and Development Program
   of China under Grant 2016YFC0104703, in part by the Shenzhen Basic
   Research Project under Grant JCYJ20150525092940982 and Grant
   JCYJ20140509172609164, in part by the Natural Science Foundation of SZU
   under Grant 2016089, and in part by the Open Fund Project of Fujian
   Provincial Key Laboratory of Information Processing and Intelligent
   Control (Minjiang University) under Grant MJUKF201711. This paper was
   recommended by Associate Editor M. Shin. (dagger Lingyun Wu and dagger
   Jie-Zhi Cheng contributed equally to this work.) (*Corresponding author:
   Dong Ni).
CR [Anonymous], 1999, VIDERE J COMPUT VIS, V1, P1
   Belaid A, 2011, IEEE T INF TECHNOL B, V15, P138, DOI 10.1109/TITB.2010.2090889
   Chen H, 2016, THIRTIETH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1167
   Chen H, 2015, LECT NOTES COMPUT SC, V9349, P507, DOI 10.1007/978-3-319-24553-9_62
   Chen H, 2015, IEEE J BIOMED HEALTH, V19, P1627, DOI 10.1109/JBHI.2015.2425041
   Chen SH, 2017, IEEE T MED IMAGING, V36, P802, DOI 10.1109/TMI.2016.2629462
   Cheng JZ, 2016, SCI REP-UK, V6, DOI 10.1038/srep24454
   Ciompi F, 2015, MED IMAGE ANAL, V26, P195, DOI 10.1016/j.media.2015.08.001
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104
   Du B, 2017, IEEE T CYBERNETICS, V47, P1017, DOI 10.1109/TCYB.2016.2536638
   Dudley NJ, 2002, ULTRASOUND OBST GYN, V19, P190, DOI 10.1046/j.0960-7692.2001.00549.x
   Glorot X., 2011, J MACH LEARN RES
   Grau V, 2005, LECT NOTES COMPUT SC, V3749, P589
   Guo YR, 2014, LECT NOTES COMPUT SC, V8674, P308, DOI 10.1007/978-3-319-10470-6_39
   Hacihaliloglu I, 2009, ULTRASOUND MED BIOL, V35, P1475, DOI 10.1016/j.ultrasmedbio.2009.04.015
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kovesi P., 1997, 10 AUSTR JOINT C ART, V190, P2
   Krizhevsky A., 2012, INT C NEUR INF PROC, V25, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
   Lawn JE, 2005, LANCET, V365, P891, DOI 10.1016/S0140-6736(05)71048-5
   Ni D, 2014, ULTRASOUND MED BIOL, V40, P2728, DOI 10.1016/j.ultrasmedbio.2014.06.006
   Rahmatullah B, 2012, LECT NOTES COMPUT SC, V7512, P402, DOI 10.1007/978-3-642-33454-2_50
   Rahmatullah B, 2011, I S BIOMED IMAGING, P6, DOI 10.1109/ISBI.2011.5872342
   Roth HR, 2016, IEEE T MED IMAGING, V35, P1170, DOI 10.1109/TMI.2015.2482920
   Rueda S, 2014, IEEE T MED IMAGING, V33, P797, DOI 10.1109/TMI.2013.2276943
   Salomon LJ, 2006, ULTRASOUND OBST GYN, V27, P34, DOI 10.1002/uog.2665
   Shi J, 2016, NEUROCOMPUTING, V194, P87, DOI 10.1016/j.neucom.2016.01.074
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Sihong Chen, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P53, DOI 10.1007/978-3-319-46723-8_7
   Smith GCS, 1997, BRIT J OBSTET GYNAEC, V104, P186, DOI 10.1111/j.1471-0528.1997.tb11042.x
   Song YY, 2017, IEEE T MED IMAGING, V36, P288, DOI 10.1109/TMI.2016.2606380
   Song YY, 2016, I S BIOMED IMAGING, P1159, DOI 10.1109/ISBI.2016.7493472
   Suk HI, 2014, NEUROIMAGE, V101, P569, DOI 10.1016/j.neuroimage.2014.06.077
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Ville Y, 2008, ULTRASOUND OBST GYN, V31, P1, DOI 10.1002/uog.5248
   Wager S., 2013, ADV NEURAL INFORM PR, P351
   Wang M, 2015, IEEE T CYBERNETICS, V45, P1561, DOI 10.1109/TCYB.2014.2356136
   Wei Y., 2017, IEEE T CYBERNETICS, V47, P1
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhen XT, 2016, MED IMAGE ANAL, V30, P120, DOI 10.1016/j.media.2015.07.003
NR 39
TC 75
Z9 82
U1 0
U2 49
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2168-2267
EI 2168-2275
J9 IEEE T CYBERNETICS
JI IEEE T. Cybern.
PD MAY
PY 2017
VL 47
IS 5
BP 1336
EP 1349
DI 10.1109/TCYB.2017.2671898
PG 14
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Computer Science, Cybernetics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Automation & Control Systems; Computer Science
GA ES8HN
UT WOS:000399797000018
PM 28362600
DA 2022-02-03
ER

PT J
AU Liu, EY
   Zhao, H
   Liang, JM
   Pang, LJ
   Chen, HT
   Tian, J
AF Liu, Eryun
   Zhao, Heng
   Liang, Jimin
   Pang, Liaojun
   Chen, Hongtao
   Tian, Jie
TI Random local region descriptor (RLRD): A new method for fixed-length
   feature representation of fingerprint image and its application to
   template protection
SO FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE
LA English
DT Article
DE Fixed-length feature; Cancelable biometric; Fingerprint recognition;
   Random local region descriptor (RLRD)
ID MINUTIAE; ALGORITHM
AB Minutia based features are the most widely used features in fingerprint recognition. However, the minutiae based fingerprint matching algorithms have some drawbacks that limit their applications in template protection. Because the minutia sets are unordered, it is difficult to determine the correspondence between two minutia sets and cannot be used in some known template protection schemes directly (e.g., fuzzy commitment, wrap around). In this paper, we propose a new fixed-length feature representation: random local region descriptor (RLRD) feature. The RLRD feature is extracted by randomly and uniformly selecting a set of points, where the order of points is determined by a random seed. For each point, a real fixed-length feature vector is extracted based on Tico's sampling structure. The real RLRD feature vector can be further transformed into a bit vector for secure sketches working in the Hamming space. The experimental results on FVC2002 DB1 and DB2 show the advantages of the RLRD feature over some other fixed-length fingerprint feature vectors in terms of equal error rate (EER), genuine accept rate (GAR) and false accept rate (FAR). (C) 2011 Elsevier B.V. All rights reserved.
C1 [Liu, Eryun; Zhao, Heng; Liang, Jimin; Pang, Liaojun; Chen, Hongtao; Tian, Jie] Xidian Univ, Sch Life Sci & Technol, Life Sci Res Ctr, Xian 710071, Shaanxi, Peoples R China.
   [Liu, Eryun; Chen, Hongtao] Xidian Univ, Sch Elect Engn, Xian 710071, Shaanxi, Peoples R China.
   [Tian, Jie] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
C3 Xidian University; Xidian University; Chinese Academy of Sciences;
   Institute of Automation, CAS
RP Liang, JM (corresponding author), Xidian Univ, Sch Life Sci & Technol, Life Sci Res Ctr, Xian 710071, Shaanxi, Peoples R China.
EM jiminliang@gmail.com
RI Life, FP/M-9555-2013; Tian, Jie/M-5976-2013; Liang, Jimin/B-5394-2014;
   Tian, Jie/H-1190-2011
OI Liang, Jimin/0000-0003-1428-5804; Tian, Jie/0000-0003-0498-0432
FU 863 programNational High Technology Research and Development Program of
   China [2008AA01Z411]; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China (NSFC) [60902083,
   60803151, 60875018]; Beijing Natural Science FundBeijing Natural Science
   Foundation [4091004]; Fundamental Research Funds for the Central
   UniversitiesFundamental Research Funds for the Central Universities
FX This paper is supported by the 863 program under Grant No. 2008AA01Z411,
   the National Natural Science Foundation of China under Grant Nos.
   60902083, 60803151, 60875018, the Beijing Natural Science Fund under
   Grant No. 4091004 and the Fundamental Research Funds for the Central
   Universities.
CR Amornraksa T, 2006, ELECTRON LETT, V42, P522, DOI 10.1049/el:20064330
   Benhammadi F, 2007, PATTERN RECOGN, V40, P189, DOI 10.1016/j.patcog.2006.06.031
   Chang EC, 2007, LECT NOTES COMPUT SC, V4642, P750
   Chen XJ, 2006, IEEE T IMAGE PROCESS, V15, P767, DOI 10.1109/TIP.2005.860597
   Feng JJ, 2008, PATTERN RECOGN, V41, P342, DOI 10.1016/j.patcog.2007.04.016
   Golic JD, 2008, IEEE T INFORM THEORY, V54, P2026, DOI 10.1109/TIT.2008.920211
   Gray, 1953, U.S. Patent, Patent No. [2,632,058, 2632058, 2 632 058]
   He YL, 2006, IEEE T PATTERN ANAL, V28, P850, DOI 10.1109/TPAMI.2006.119
   Jain AK, 2000, IEEE T IMAGE PROCESS, V9, P846, DOI 10.1109/83.841531
   Jin ATB, 2004, PATTERN RECOGN, V37, P2245, DOI 10.1016/j.patcog.2004.04.011
   Jin ATB, 2004, IMAGE VISION COMPUT, V22, P503, DOI 10.1016/j.imavis.2003.12.002
   Juels A., 2002, P 2002 IEEE INT S IN
   Linnartz JP, 2003, LECT NOTES COMPUT SC, V2688, P393
   Liu EY, 2010, J NETW COMPUT APPL, V33, P221, DOI 10.1016/j.jnca.2009.12.002
   Macmillan NA., 1991, DETECTION THEORY USE
   Maio D, 2005, NEUROCOMPUTING, V69, P242, DOI 10.1016/j.neucom.2005.06.003
   Maio D, 2002, INT C PATT RECOG, P811, DOI 10.1109/ICPR.2002.1048144
   Maltoni D., 2009, HDB FINGERPRINT RECO, V2nd
   Nagar A., PATTERN REC IN PRESS
   Nanni L, 2008, PATTERN RECOGN, V41, P3461, DOI 10.1016/j.patcog.2008.05.013
   Nanni L, 2009, EXPERT SYST APPL, V36, P12414, DOI 10.1016/j.eswa.2009.04.041
   Sutcu Yagiz, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563111
   Tico M, 2003, IEEE T PATTERN ANAL, V25, P1009, DOI 10.1109/TPAMI.2003.1217604
   Tico M., 2001, CIRC SYST 2001 ISCAS, V2
   TUYLS P, 2008, SECURITY NOISY DATA
   Wang Y, 2007, IEEE T PATTERN ANAL, V29, P573, DOI 10.1109/TPAMI.2007.1003
   Xu HY, 2009, IEEE T INF FOREN SEC, V4, P397, DOI 10.1109/TIFS.2009.2021692
   Yang JC, 2008, NEUROCOMPUTING, V71, P1939, DOI 10.1016/j.neucom.2007.12.034
NR 28
TC 9
Z9 10
U1 0
U2 10
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0167-739X
EI 1872-7115
J9 FUTURE GENER COMP SY
JI Futur. Gener. Comp. Syst.
PD JAN
PY 2012
VL 28
IS 1
BP 236
EP 243
DI 10.1016/j.future.2011.01.001
PG 8
WC Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 834GN
UT WOS:000295947900027
DA 2022-02-03
ER

PT J
AU Li, L
   Feng, XY
   Xia, ZQ
   Jiang, XY
   Hadid, A
AF Li, Lei
   Feng, Xiaoyi
   Xia, Zhaoqiang
   Jiang, Xiaoyue
   Hadid, Abdenour
TI Face spoofing detection with local binary pattern network
SO JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA English
DT Article
DE Face spoofing detection; Deep learning; Local binary pattern
ID LIVENESS DETECTION; IMAGE; RECOGNITION; SCALE
AB Nowadays, face biometric based access control systems are becoming ubiquitous in our daily life while they are still vulnerable to spoofing attacks. So developing robust and reliable methods to prevent such frauds is unavoidable. As deep learning techniques have achieved satisfactory performances in computer vision, they have also been applied to face spoofing detection. However, the numerous parameters in these deep learning based detection methods cannot be updated to optimum due to limited data. Local Binary Pattern (LBP), effective features for face recognition, have been employed in face spoofing detection and obtained promising results. Considering the similarities between LBP extraction and convolutional neural network (CNN) that the former can be accomplished by using fixed convolutional filters, we propose a novel end-to-end learnable LBP network for face spoofing detection. Our network can significantly reduce the number of network parameters by combing learnable convolutional layers with fixed-parameter LBP layers that are comprised of sparse binary filters and derivable simulated gate functions. Compared with existing deep leaning based detection methods, the parameters in our fully connected layers are up to 64x savings. Conducting extensive experiments on two standard spoofing databases, i.e., Relay-Attack and CASIA-FA, our proposed LBP network substantially outperforms the state-of-the-art methods.
C1 [Li, Lei; Feng, Xiaoyi; Xia, Zhaoqiang; Jiang, Xiaoyue; Hadid, Abdenour] Northwestern Polytech Univ, Sch Elect & Informat, Xian, Shaanxi, Peoples R China.
   [Hadid, Abdenour] Univ Oulu, Ctr Machine Vis & Signal Anal CMVS, Oulu, Finland.
C3 Northwestern Polytechnical University; University of Oulu
RP Li, L (corresponding author), Northwestern Polytech Univ, Sch Elect & Informat, Xian, Shaanxi, Peoples R China.
EM lilei_npu@mail.nwpu.edu.cn
RI Xia, Zhaoqiang/AAC-4021-2019
OI Xia, Zhaoqiang/0000-0003-0630-3339; Li, Lei/0000-0003-4498-6126
FU National Aerospace Science and Technology Foundation; National Nature
   Science Foundation of ChinaNational Natural Science Foundation of China
   (NSFC) [61702419]
FX This work is partly supported by the National Aerospace Science and
   Technology Foundation and the National Nature Science Foundation of
   China (No. 61702419).
CR Adelson E. H., 1984, RCA ENG, V29
   Agarwal A., 2016, IEEE INT C BIOM THEO, P1, DOI [10.1109/GLOCOMW.2016.7848951, DOI 10.1109/BTAS.2016.7791171]
   Akhtar Z., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P283, DOI 10.1109/BTAS.2012.6374590
   Anjos A., 2011, BIOM IJCB 2011 INT J, P1, DOI DOI 10.1109/IJCB.2011.6117503
   [Anonymous], 2015, ACM MULTIMEDIA, DOI DOI 10.1145/2733373.2807412
   Bao W, 2009, PROCEEDINGS OF 2009 INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND SIGNAL PROCESSING, P233
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Boulkenafet Z, 2016, INT CONF BIOMETR
   Boulkenafet Z, 2015, IEEE IMAGE PROC, P2636, DOI 10.1109/ICIP.2015.7351280
   Chingovska I., 2012, P INT C BIOM SPEC IN, DOI DOI 10.1109/VTCFALL.2012.6399116
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Courbariaux M., 2015, ADV NEURAL INFORM PR, pP 3123, DOI 10.5555/2969442.2969588
   de Freitas Pereira Tiago, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P121, DOI 10.1007/978-3-642-37410-4_11
   de Freitas Pereira T., 2013, BIOM ICB 2013 INT C, P1, DOI DOI 10.1109/ICB.2013.6612981
   Duan YQ, 2018, IEEE T PATTERN ANAL, V40, P1139, DOI 10.1109/TPAMI.2017.2710183
   Erdogmus N., 2013, SPOOFING 2D FACE REC, P1
   Feng LT, 2016, J VIS COMMUN IMAGE R, V38, P451, DOI 10.1016/j.jvcir.2016.03.019
   Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139
   Hadid A., 2011, BIOM IJCB 2011 INT J, P1, DOI [DOI 10.1109/IJCB.2011.6117510, 10.1109/IJCB.2011.6117510]
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hubara I., 2016, ADV NEURAL INFORM PR, P4107
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Ji Z, 2016, IEEE IMAGE PROC, P1474, DOI 10.1109/ICIP.2016.7532603
   Juefei-Xu F, 2017, PROC CVPR IEEE, P4284, DOI 10.1109/CVPR.2017.456
   KARSON CN, 1983, BRAIN, V106, P643, DOI 10.1093/brain/106.3.643
   Kim S, 2014, SENSORS-BASEL, V14, P22471, DOI 10.3390/s141222471
   Kim W, 2015, IEEE T IMAGE PROCESS, V24, P2456, DOI 10.1109/TIP.2015.2422574
   Komulainen J, 2013, INT CONF BIOMETR
   Kose N, 2012, 2012 INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV), P1027, DOI 10.1109/ICIEV.2012.6317336
   Krizhevsky A., 2012, INT C NEUR INF PROC, V25, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
   Li H., 2016, P INT C IM PROC THEO, P1
   Li Y., 2014, P 9 ACM S INF COMP C, V14, P413
   Lu JW, 2018, IEEE T PATTERN ANAL, V40, P1979, DOI 10.1109/TPAMI.2017.2737538
   Lu JW, 2017, IEEE T IMAGE PROCESS, V26, P2352, DOI 10.1109/TIP.2017.2678163
   Lu JW, 2015, IEEE T PATTERN ANAL, V37, P2041, DOI 10.1109/TPAMI.2015.2408359
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Omar L., 2015, UK BRIT MACHINE VISI, DOI 10.5244/C.29.BMVW.5
   Pan G, 2007, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2007.4409068
   Pan G, 2011, TELECOMMUN SYST, V47, P215, DOI 10.1007/s11235-010-9313-3
   Parkhi O.M., 2015, BRIT MACH VIS C
   Patel K, 2016, LECT NOTES COMPUT SC, V9967, P611, DOI 10.1007/978-3-319-46654-5_67
   Pavlidis I, 2000, IEEE WORKSHOP ON COMPUTER VISION BEYOND THE VISIBLE SPECTRUM: METHODS AND APPLICATIONS, PROCEEDINGS, P15, DOI 10.1109/CVBVS.2000.855246
   Pinto A, 2015, IEEE T IMAGE PROCESS, V24, P4726, DOI 10.1109/TIP.2015.2466088
   Phan QT, 2016, IEEE IMAGE PROC, P404, DOI 10.1109/ICIP.2016.7532388
   Rahim M.A., 2013, GLOBAL J COMPUT SCI, V13, P1
   Rumelhart DE, 1995, BACKPROPAGATION THEO
   Sepas-Moghaddam A, 2017, IEEE IMAGE PROC, P3815, DOI 10.1109/ICIP.2017.8296996
   Simonyan K., ABS14091556 COMP RES
   Smith DF, 2015, IEEE T INF FOREN SEC, V10, P736, DOI 10.1109/TIFS.2015.2398819
   Smith S. W., 1997, SCI ENG GUIDE DIGITA
   Tan XY, 2010, LECT NOTES COMPUT SC, V6316, P504, DOI 10.1007/978-3-642-15567-3_37
   Tirunagari S, 2015, IEEE T INF FOREN SEC, V10, P762, DOI 10.1109/TIFS.2015.2406533
   Wen D, 2015, IEEE T INF FOREN SEC, V10, P746, DOI 10.1109/TIFS.2015.2400395
   Xia ZQ, 2018, IET BIOMETRICS, V7, P56, DOI 10.1049/iet-bmt.2017.0193
   Xia ZQ, 2017, SIGNAL PROCESS-IMAGE, V59, P109, DOI 10.1016/j.image.2017.06.008
   Xu ZQ, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P141, DOI 10.1109/ACPR.2015.7486482
   Yang J, 2014, ADV MATER RES-SWITZ, V850-851, P373, DOI 10.4028/www.scientific.net/AMR.850-851.373
   Yang L., 2009, P 2 INT C IM SIGN PR, P1, DOI DOI 10.1109/CCPR.2009.5344092
   Zhang BH, 2007, IEEE T IMAGE PROCESS, V16, P57, DOI 10.1109/TIP.2006.884956
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882
   Zhang GC, 2004, LECT NOTES COMPUT SC, V3338, P179
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao PZ, 2016, 2016 IEEE INTERNATIONAL INSTRUMENTATION AND MEASUREMENT TECHNOLOGY CONFERENCE PROCEEDINGS, P1
   Zhiwei Zhang, 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P26, DOI 10.1109/ICB.2012.6199754
   Zhiwei Zhang, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P436, DOI 10.1109/FG.2011.5771438
NR 66
TC 22
Z9 22
U1 2
U2 10
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1047-3203
EI 1095-9076
J9 J VIS COMMUN IMAGE R
JI J. Vis. Commun. Image Represent.
PD JUL
PY 2018
VL 54
BP 182
EP 192
DI 10.1016/j.jvcir.2018.05.009
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GJ3EC
UT WOS:000435167800016
DA 2022-02-03
ER

PT C
AU de Souza, GB
   Papa, JP
   Marana, AN
AF de Souza, Gustavo Botelho
   Papa, Joao Paulo
   Marana, Aparecido Nilceu
GP IEEE
TI On the Learning of Deep Local Features for Robust Face Spoofing
   Detection
SO PROCEEDINGS 2018 31ST SIBGRAPI CONFERENCE ON GRAPHICS, PATTERNS AND
   IMAGES (SIBGRAPI)
SE SIBGRAPI - Brazilian Symposium on Computer Graphics and Image Processing
LA English
DT Proceedings Paper
CT 31st Conference on Graphics, Patterns and Images (SIBGRAPI)
CY OCT 29-NOV 01, 2018
CL Foz do Iguacu, BRAZIL
AB Biometrics emerged as a robust solution for security systems. However, given the dissemination of biometric applications, criminals are developing techniques to circumvent them by simulating physical or behavioral traits of legal users (spoofing attacks). Despite face being a promising characteristic due to its universality, acceptability and presence of cameras almost everywhere, face recognition systems are extremely vulnerable to such frauds since they can be easily fooled with common printed facial photographs. State-of-the-art approaches, based on Convolutional Neural Networks (CNNs), present good results in face spoofing detection. However, these methods do not consider the importance of learning deep local features from each facial region, even though it is known from face recognition that each facial region presents different visual aspects, which can also be exploited for face spoofing detection. In this work we propose a novel CNN architecture trained in two steps for such task. Initially, each part of the neural network learns features from a given facial region. Afterwards, the whole model is fine-tuned on the whole facial images. Results show that such pre-training step allows the CNN to learn different local spoofing cues, improving the performance and the convergence speed of the final model, outperforming the state-of-the-art approaches.
C1 [de Souza, Gustavo Botelho] UFSCar Fed Univ Sao Carlos, Rod Washington Luis,Km 235, BR-13565905 Sao Carlos, SP, Brazil.
   [Papa, Joao Paulo; Marana, Aparecido Nilceu] UNESP Sao Paulo State Univ, Ave Eng Luiz Edmundo Carrijo Coube 14-01, BR-17033360 Bauru, SP, Brazil.
C3 Universidade Federal de Sao Carlos; Universidade Estadual Paulista
RP de Souza, GB (corresponding author), UFSCar Fed Univ Sao Carlos, Rod Washington Luis,Km 235, BR-13565905 Sao Carlos, SP, Brazil.
EM gustavo.botelho@gmail.com; papa@fc.unesp.br; nilceu@fc.unesp.br
RI Papa, Joao Paulo/ABC-6283-2020; Marana, Aparecido Nilceu/A-6334-2008
OI Papa, Joao Paulo/0000-0002-6494-7514; Marana, Aparecido
   Nilceu/0000-0003-4861-7061
FU FAPESPFundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP)
   [2014/12236-1, 2017/05522-6, 2016/19403-6]; CAPESCoordenacao de
   Aperfeicoamento de Pessoal de Nivel Superior (CAPES)
   [88881.132647/2016-01]; NVIDIA; Banco do Brasil
FX The authors are grateful to FAPESP (grants #2014/12236-1, #2017/05522-6
   and #2016/19403-6), CAPES (grant #88881.132647/2016-01), to Dr. Anil K.
   Jain for the doctoral exchange period, to NVIDIA, and to Banco do
   Brasil.
CR Akhtar Z, 2016, J ELECTR COMPUT ENG, V2016, DOI 10.1155/2016/4721849
   Atoum Y., 2017, P INT JOINT C BIOM
   Ba J., 2016, ARXIV161006258
   Bengio Yoshua, 2013, Statistical Language and Speech Processing. First International Conference, SLSP 2013. Proceedings: LNCS 7978, P1, DOI 10.1007/978-3-642-39593-2_1
   Boulkenafet Z, 2015, IEEE IMAGE PROC, P2636, DOI 10.1109/ICIP.2015.7351280
   Bradski G., 2000, OPENCV LIB
   Canziani A., 2017, ARXIV160507678V4
   Chiachia G, 2014, IEEE T INF FOREN SEC, V9, P2089, DOI 10.1109/TIFS.2014.2359543
   Chingovska I., 2012, P INT C BIOM SPEC IN
   Dusenberry M., 2015, EIGENFACES CREATING
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Galbally J., 2007, DATABASE, V1, P4
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jain A. K., 2011, INTRO BIOMETRICS
   Jia Y., 2014, ARXIV14085093
   Kingma D.P, 2015, ICLR, P1
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lewis MB, 2003, INT J IMAG SYST TECH, V13, P3, DOI 10.1002/ima.10040
   Li LM, 2016, PROCEEDINGS OF THE 2016 11TH INTERNATIONAL SYMPOSIUM ON ANTENNAS, PROPAGATION AND EM THEORY (ISAPE), P1, DOI 10.1109/ISAPE.2016.7833664
   Lucena O., 2018, P INT C IM AN REC
   Ma S., 2016, P IEEE INT C SOFTW E
   Maatta J., 2011, P INT JOINT C BIOM
   Mathias M, 2014, LECT NOTES COMPUT SC, V8692, P720, DOI 10.1007/978-3-319-10593-2_47
   Menotti D, 2015, IEEE T INF FOREN SEC, V10, P864, DOI 10.1109/TIFS.2015.2398817
   Mita T., 2005, P IEEE INT C COMP VI, V1
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Parveen S., 2016, COMPUTER, V5, P1
   Patel K, 2016, IEEE T INF FOREN SEC, V11, P2268, DOI 10.1109/TIFS.2016.2578288
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
   Pereira TF, 2012, P ACCV WORKSH, P121
   PURCELL DG, 1986, B PSYCHONOMIC SOC, V24, P118
   Ratha NK, 2001, LECT NOTES COMPUT SC, V2091, P223
   Santos DFS, 2017, SIBGRAPI, P155, DOI 10.1109/SIBGRAPI.2017.27
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Souza G., 2017, P IB C PATT REC
   Szegedy C., 2015, ARXIV150203167V3
   Tan XY, 2010, LECT NOTES COMPUT SC, V6316, P504, DOI 10.1007/978-3-642-15567-3_37
   Tronci R., 2011, P INT JOINT C BIOM
   TURK M, 1991, P IEEE C COMP VIS PA
   Viola P, 2001, P 2001 IEEE COMP VIS
   Xu ZQ, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P141, DOI 10.1109/ACPR.2015.7486482
   Yang J., 2014, ABS14085601 CORR
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang Z, 2012, IEEE INT C BIO BIO W
NR 45
TC 5
Z9 5
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1530-1834
BN 978-1-5386-9264-6
J9 SIBGRAPI
PY 2018
BP 258
EP 265
DI 10.1109/SIBGRAPI.2018.00040
PG 8
WC Computer Science, Artificial Intelligence; Imaging Science &
   Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BM1JP
UT WOS:000459886600034
OA Green Submitted
DA 2022-02-03
ER

PT J
AU Francese, S
   Bradshaw, R
   Ferguson, LS
   Wolstenholme, R
   Clench, MR
   Bleay, S
AF Francese, S.
   Bradshaw, R.
   Ferguson, L. S.
   Wolstenholme, R.
   Clench, M. R.
   Bleay, S.
TI Beyond the ridge pattern: multi-informative analysis of latent
   fingermarks by MALDI mass spectrometry
SO ANALYST
LA English
DT Review
ID LASER-DESORPTION/IONIZATION-TIME; TOF-MS; SMALL MOLECULES; GC/MS
   ANALYSIS; MATRIX; FINGERPRINTS; DESORPTION; DRUGS; PEPTIDES; PROTEINS
AB After over a century, fingerprints are still one of the most powerful means of biometric identification. The conventional forensic workflow for suspect identification consists of (i) recovering latent marks from crime scenes using the appropriate enhancement technique and (ii) obtaining an image of the mark to compare either against known suspect prints and/or to search in a Fingerprint Database. The suspect is identified through matching the ridge pattern and local characteristics of the ridge pattern (minutiae). However successful, there are a number of scenarios in which this process may fail; they include the recovery of partial, distorted or smudged marks, poor quality of the image resulting from inadequacy of the enhancement technique applied, extensive scarring/abrasion of the fingertips or absence of suspect's fingerprint records in the database. In all of these instances it would be very desirable to have a technology able to provide additional information from a fingermark exploiting its endogenous and exogenous chemical content. This opportunity could potentially provide new investigative leads, especially when the fingermark comparison and match process fails. We have demonstrated that Matrix Assisted Laser Desorption Ionisation Mass Spectrometry and Mass Spectrometry Imaging (MALDI MSI) can provide multiple images of the same fingermark in one analysis simultaneous with additional intelligence. Here, a review on the pioneering use and development of MALDI MSI for the analysis of latent fingermarks is presented along with the latest achievements on the forensic intelligence retrievable.
C1 [Francese, S.; Bradshaw, R.; Ferguson, L. S.; Wolstenholme, R.; Clench, M. R.] Sheffield Hallam Univ, Biomed Res Ctr, Sheffield S1 1WB, S Yorkshire, England.
   [Bleay, S.] Home Off Sci, Ctr Appl Sci & Technol, St Albans AL4 9HQ, England.
C3 Sheffield Hallam University
RP Francese, S (corresponding author), Sheffield Hallam Univ, Biomed Res Ctr, Howard St, Sheffield S1 1WB, S Yorkshire, England.
EM s.francese@shu.ac.uk
RI Clench, Malcolm/AAA-4100-2020
OI Bleay, Stephen/0000-0002-9176-5337; Clench, Malcolm/0000-0002-0798-831X
CR Almog J, 2004, J FORENSIC SCI, V49, P981
   Archer NE, 2005, FORENSIC SCI INT, V154, P224, DOI 10.1016/j.forsciint.2004.09.120
   Arnaud M . J., 1982, THEOPHYLLINE OTHER M, P135
   Bailey MJ, 2010, NUCL INSTRUM METH B, V268, P1929, DOI 10.1016/j.nimb.2010.02.104
   Bandey H.L., 2004, 5404 POL SCI DEV BRA
   Benton M, 2010, SURF INTERFACE ANAL, V42, P378, DOI 10.1002/sia.3112
   Bond JW, 2008, J FORENSIC SCI, V53, P812, DOI 10.1111/j.1556-4029.2008.00738.x
   Bradshaw R, 2012, FORENSIC SCI INT, V222, P318, DOI 10.1016/j.forsciint.2012.07.009
   Bradshaw R, 2013, ANALYST, V138, P2546, DOI 10.1039/c3an00195d
   Bradshaw R, 2011, RAPID COMMUN MASS SP, V25, P415, DOI 10.1002/rcm.4858
   Bright NJ, 2012, ANAL CHEM, V84, P4083, DOI 10.1021/ac300185j
   Caprioli RM, 1997, ANAL CHEM, V69, P4751, DOI 10.1021/ac970888i
   Chang WC, 2007, ANAL CHIM ACTA, V582, P1, DOI 10.1016/j.aca.2006.08.062
   Croxton RS, 2010, FORENSIC SCI INT, V199, P93, DOI 10.1016/j.forsciint.2010.03.019
   Day JS, 2004, SPECTROCHIM ACTA A, V60, P1725, DOI 10.1016/j.saa.2003.09.013
   Dong XL, 2010, ANAL CHEM, V82, P6208, DOI 10.1021/ac101022m
   Emerson B, 2011, J FORENSIC SCI, V56, P381, DOI 10.1111/j.1556-4029.2010.01655.x
   Ferguson L, 2011, ANAL CHEM, V83, P5585, DOI 10.1021/ac200619f
   Ferguson LS, 2013, J MASS SPECTROM, V48, P677, DOI 10.1002/jms.3216
   Ferguson LS, 2012, ANALYST, V137, P4686, DOI 10.1039/c2an36074h
   Francese S, 2010, MASS SPECTROMETRY MI, P91
   Francese S, 2009, COMB CHEM HIGH T SCR, V12, P156, DOI 10.2174/138620709787315454
   Fuchs Beate, 2008, V49, P541, DOI 10.1007/978-1-4020-8831-5_21
   Hsu NY, 2007, RAPID COMMUN MASS SP, V21, P2137, DOI 10.1002/rcm.3072
   Ifa DR, 2008, SCIENCE, V321, P805, DOI 10.1126/science.1157199
   Jaber N, 2012, ANGEW CHEM INT EDIT, V51, P12224, DOI 10.1002/anie.201205259
   Kallback P, 2012, J PROTEOMICS, V75, P4941, DOI 10.1016/j.jprot.2012.07.034
   KARAS M, 1987, INT J MASS SPECTROM, V78, P53, DOI 10.1016/0168-1176(87)87041-6
   KARAS M, 1985, ANAL CHEM, V57, P2935, DOI 10.1021/ac00291a042
   Kent T., 1998, MANUAL FINGERPRINT D
   KNOWLES AM, 1978, J PHYS E SCI INSTRUM, V11, P713, DOI 10.1088/0022-3735/11/8/001
   Koeniger SL, 2011, RAPID COMMUN MASS SP, V25, P503, DOI 10.1002/rcm.4891
   KOICHI T, 1988, RAPID COMMUN MASS SP, V2, P151, DOI 10.1002/rcm.1290020802
   Lim AY, 2011, J CHROMATOGR B, V879, P2244, DOI 10.1016/j.jchromb.2011.06.009
   Lim AY, 2011, ANALYST, V136, P2775, DOI 10.1039/c1an15172j
   Locard E., 1912, BIOL REV SCI MED, V2, P357
   Michalski S, 2013, J FORENSIC SCI, V58, pS215, DOI 10.1111/1556-4029.12010
   Musah RA, 2012, RAPID COMMUN MASS SP, V26, P1039, DOI 10.1002/rcm.6198
   Nilsson A, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0011411
   Pan CS, 2005, J AM SOC MASS SPECTR, V16, P883, DOI 10.1016/j.jasms.2005.03.009
   Ramotowski RS, 2001, CRC FOR P S, P63
   Ricci C, 2007, ANAL CHEM, V79, P5771, DOI 10.1021/ac070580j
   Ricci C, 2007, APPL SPECTROSC, V61, P514, DOI 10.1366/000370207780807849
   Ricci C, 2010, SURF INTERFACE ANAL, V42, P386, DOI 10.1002/sia.3098
   Rieg S, 2004, BRIT J DERMATOL, V151, P534, DOI 10.1111/j.1365-2133.2004.06081.x
   Rowell F, 2012, FORENSIC SCI INT, V221, P84, DOI 10.1016/j.forsciint.2012.04.007
   Rowell F, 2009, ANALYST, V134, P701, DOI 10.1039/b813957c
   Schittek Birgit, 2008, Infectious Disorders - Drug Targets, V8, P135
   Soltzberg LJ, 2004, RAPID COMMUN MASS SP, V18, P1455, DOI 10.1002/rcm.1505
   Song DF, 2011, FORENSIC SCI INT, V204, P97, DOI 10.1016/j.forsciint.2010.05.008
   Spencer SE, 2011, FORENSIC SCI INT, V207, P19, DOI 10.1016/j.forsciint.2010.08.010
   STAHL B, 1991, ANAL CHEM, V63, P1463, DOI 10.1021/ac00014a022
   Tahtouh M, 2005, J FORENSIC SCI, V50, P64
   Tang HW, 2010, ANAL CHEM, V82, P1589, DOI 10.1021/ac9026077
   TANG K, 1994, RAPID COMMUN MASS SP, V8, P673, DOI 10.1002/rcm.1290080902
   Trim PJ, 2010, ANAL BIOANAL CHEM, V397, P3409, DOI 10.1007/s00216-010-3874-6
   Tripathi A, 2011, APPL SPECTROSC, V65, P611, DOI 10.1366/10-06214
   Vaidyanathan S, 2007, RAPID COMMUN MASS SP, V21, P2072, DOI 10.1002/rcm.3063
   Wentworth B., 1918, PERSONAL IDENTIFICAT
   West MJ, 2009, SPECTROCHIM ACTA A, V71, P1984, DOI 10.1016/j.saa.2008.07.024
   Weyermann C, 2011, J FORENSIC SCI, V56, P102, DOI 10.1111/j.1556-4029.2010.01523.x
   Widjaja E, 2009, ANALYST, V134, P769, DOI 10.1039/b808259f
   Wolstenholme R, 2009, RAPID COMMUN MASS SP, V23, P3031, DOI 10.1002/rcm.4218
   Wu KJ, 1998, ANAL CHEM, V70, p456A, DOI 10.1021/ac981910q
   Xu LR, 2012, ANGEW CHEM INT EDIT, V51, P8068, DOI 10.1002/anie.201203815
   Yukihira D, 2010, ANAL CHEM, V82, P4278, DOI 10.1021/ac100024w
   [No title captured]
   [No title captured]
   [No title captured]
NR 69
TC 72
Z9 72
U1 1
U2 92
PU ROYAL SOC CHEMISTRY
PI CAMBRIDGE
PA THOMAS GRAHAM HOUSE, SCIENCE PARK, MILTON RD, CAMBRIDGE CB4 0WF, CAMBS,
   ENGLAND
SN 0003-2654
EI 1364-5528
J9 ANALYST
JI Analyst
PY 2013
VL 138
IS 15
BP 4215
EP 4228
DI 10.1039/c3an36896c
PG 14
WC Chemistry, Analytical
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry
GA 174JU
UT WOS:000321152300001
PM 23658933
DA 2022-02-03
ER

PT C
AU Seshadri, K
   Savvides, M
AF Seshadri, Keshav
   Savvides, Marios
GP IEEE
TI Robust Modified Active Shape Model for Automatic Facial Landmark
   Annotation of Frontal Faces
SO 2009 IEEE 3RD INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY,
   APPLICATIONS AND SYSTEMS
LA English
DT Proceedings Paper
CT 3rd IEEE International Conference on Biometrics - Theory, Applications
   and Systems (BTAS 2009)
CY SEP 28-30, 2009
CL Washington, DC
AB In this paper we present an improved method for locating facial landmarks in images containing frontal faces using a modified Active Shape Model. Our main contributions include the use of an optimal number of facial landmark points, better profiling methods during the fitting stage and the development of a more suitable optimization metric to determine the best location of the landmarks compared to the simplistic minimum Mahalanobis distance criteria used to date. We build a subspace to model variations of appearance around each facial landmark and use this subspace to enhance the accuracy of the fitting process around each landmark. This enhancement provides a significant improvement in fitting and simultaneously determines which points were poorly fitted using reconstruction error, thus allowing for automatic correction or interpolation of any poorly fitted points. Our implementation, with the above mentioned improvements, leads to extremely accurate results even when dealing with faces with expressions, slight pose variations and in-plane rotations. Experiments conducted on test sets drawn from three databases (NIST Multiple Biometric Grand Challenge-2008 (MBGC-2008), CMU Multi-PIE and the Japanese Female Facial Expression (JAFFE) database) show that our proposed approach leads to far better performance compared to the classical Active Shape Model of Cootes et al. and other traditional methods and provides a robust automatic facial landmark annotation which is the first critical step in face registration, pose correction and face recognition.
C1 [Seshadri, Keshav; Savvides, Marios] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.
C3 Carnegie Mellon University
RP Seshadri, K (corresponding author), Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.
EM kseshadr@andrew.cmu.edu; msavvid@cs.cmu.edu
CR Cootes T. F., 1994, P BRIT MACH VIS C, P327
   Cootes T.F., 2004, STAT MODELS APPEARAN
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478
   Gross R., 2008, P 8 IEEE INT C AUT F
   *INT, 2007, OP SOURC COMP VIS LI
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Lu HC, 2005, PROC INT C TOOLS ART, P642
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Mahoor MH, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P144
   *MBGC, 2008, NAT I STAND TECHN NI
   Milborrow S, 2008, LECT NOTES COMPUT SC, V5305, P504, DOI 10.1007/978-3-540-88693-8_37
   PHILLIPS PJ, 2009, P 3 IAPR IEEE INT C
   Wang W, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P523, DOI 10.1109/ICMI.2002.1167050
NR 14
TC 12
Z9 12
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-4244-5019-0
PY 2009
BP 319
EP 326
PG 8
WC Computer Science, Interdisciplinary Applications; Engineering,
   Electrical & Electronic; Mathematical & Computational Biology;
   Statistics & Probability
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Mathematical & Computational Biology;
   Mathematics
GA BOC54
UT WOS:000276176000051
DA 2022-02-03
ER

PT C
AU Kulkarni, SS
   Thepade, SD
AF Kulkarni, Samira S.
   Thepade, Sudeep D.
GP IEEE
TI Performance Appraise of Haar Wavelet, Cosine Wavelet and Cosine-Haar
   Hybrid Wavelet Based Bimodal Iris Recognition using Thepade's Sorted
   Ternary Block Truncation Coding
SO 2016 INTERNATIONAL CONFERENCE ON AUTOMATIC CONTROL AND DYNAMIC
   OPTIMIZATION TECHNIQUES (ICACDOT)
LA English
DT Proceedings Paper
CT International Conference on Automatic Control and Dynamic Optimization
   Techniques (ICACDOT)
CY SEP 09-10, 2016
CL Int Inst Informat Technol, Pune, INDIA
HO Int Inst Informat Technol
DE TSTBTC; feature level fusion; decision level fusion; score level fusion;
   GAR; Mean Squared Error(MSE)
AB Multimodal Biometric systems have proved more secure as compared to unimodal systems. Multimodal fusion can be achieved by using three approaches which are Feature-level fusion, Score-level fusion and Decision-level fusion. This paper presents an approach which fuses left and right iris using feature level fusion using Haar wavelet, Cosine wavelet and Haar-Cosine Hybrid wavelet followed by Thepade's Sorted Ternary Block Truncation Coding(TSTBTC). As compared to the only consideration of individual iris images the fusion of Left iris (L) and Right iris (R) has lead to increase in the accuracy. The combinations are in proportion as: L+R, L+2R and 2L+R. The dataset used is Palacky dataset. Dataset consists of total 90 images, of which 45 are of left iris and 45 of right. Mean Squared error is used as a similarity measure. Genuine Acceptance Rate (GAR) is used for performance comparison. The proposed method gives more accuracy than that of only right or left iris. Better performance is observed by Haar wavelet and Cosine wavelet as compared to Cosine-Haar wavelet for the proportion L+2R at level 1 which is 93.33%.
C1 [Kulkarni, Samira S.] Pimpri Chinchwad Coll Engn, Dept Informat Technol, Pune, Maharashtra, India.
   [Thepade, Sudeep D.] Pimpri Chinchwad Coll Engn, Dept Comp Engn & Informat Technol, Pune, Maharashtra, India.
RP Kulkarni, SS (corresponding author), Pimpri Chinchwad Coll Engn, Dept Informat Technol, Pune, Maharashtra, India.
EM samirakulkarni95@gmail.com; sudeepthepade@gmail.com
RI THEPADE, SUDEEP D/P-9054-2015
OI THEPADE, SUDEEP D/0000-0001-7809-4148
CR Banday Shoiab, 2013, INT C ADV EL SYST 20
   Ganorkar S. R., INT J ELECT COMMUNIC, V4
   Gayathri R., 2012, IJCSI, V9
   Kekre H. B., PERFORMANCE SUPERIOR
   Ravi Dr. S., INT C CIRC POW COMP
   Richiardi Jonas, 2007, 15 EUR SIGN PROC C E
   Thepade Sudeep, INT C CONTR COMP OMM
   Thepade Sudeep, PERFORMANCE RISE CON
   Thepade Sudeep, INT C COMM INF COMP
NR 9
TC 0
Z9 0
U1 0
U2 3
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-5090-2080-5
PY 2016
BP 699
EP 704
PG 6
WC Automation & Control Systems
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems
GA BI1DA
UT WOS:000405573700137
DA 2022-02-03
ER

PT J
AU De Felice, E
   Pacioni, C
   Tardella, FM
   Dall'Aglio, C
   Palladino, A
   Scocco, P
AF De Felice, Elena
   Pacioni, Cesare
   Tardella, Federico Maria
   Dall'Aglio, Cecilia
   Palladino, Antonio
   Scocco, Paola
TI A Novel Method for Increasing the Numerousness of Biometrical Parameters
   Useful for Wildlife Management: Roe Deer Mandible as Bone Model
SO ANIMALS
LA English
DT Article
DE biometry; size analysis; shape analysis; wildlife management; roe deer
ID HIND FOOT LENGTH; CAPREOLUS-CAPREOLUS; BODY-MASS; CERVUS-ELAPHUS; SIZE;
   POPULATION; FEATURES; AGE
AB Simple Summary The wildlife expansion in the Italian Apennines caused a general development in hunting activities, together with the necessity of using biometry (size analysis) and geometric morphometry (shape analysis) as methods for monitoring the status of wildlife populations. Thus, in the last decades, study of the sizes and shapes of structures in wildlife populations has been extensively investigated. Biometric surveys and analysis of the resulting cranial and body data are now crucial in management decisions and new possibilities of improving datasets should be considered. Thus, we attempted to identify a conversion factor between shape and size analysis methods, using the mandible of adult roe deer as a bone model. The availability of this conversion factor enhances the numerousness of parameters into the classical biometric database, by means of the conversion of shape measures into size measures. Therefore, the relationship among biometric parameters, animal and environmental features can be better studied. The obtained data can be very useful to assess both wildlife population status and its management.
   Abstract Study of dimensions (biometry) and shapes (geometric morphometry) of bone structures in ungulates is of extreme importance in wildlife population management. Unlike classical biometry, which involves the use of a caliper for measurements, geometric morphometry acquires, through software, a series of reference points (landmarks) from digital photos, providing a series of linear measures. A method to convert values obtained from the GeoGebra software into biometric measures is described. We took photos of 25 mandibles of adult roe deer and at the same time measured mandible length and teeth row length using a caliper. After image processing using GeoGebra, we calculated the conversion factor as the mean ratio between measures taken using GeoGebra and the caliper. The series of measurements, taken with two different methods (direct measurement using the caliper and conversion from GeoGebra output), showed a good degree of agreement. We used the conversion factor to obtain, from the GeoGebra database, four additional parameters of 50 mandibles. The analysis of variance showed that one parameter was significantly different between sexes (p = 0.04), demonstrating the usefulness of the measurement conversion. The conversion factor is helpful to improve classical biometric databases to better clarify the relationship between environment and wildlife status.
C1 [De Felice, Elena; Pacioni, Cesare; Tardella, Federico Maria; Scocco, Paola] Univ Camerino, Sch Biosci & Vet Med, Via Pontoni 5, Camerino 62032, Italy.
   [Dall'Aglio, Cecilia] Univ Perugia, Dept Vet Med, Via San Costanzo 4, Perugia 06126, Italy.
   [Palladino, Antonio] Univ Naples Federico II, Ctr Metrol & Adv Technol Serv, CESMA, Cupa Nuova Cintia 21, Naples 80146, Italy.
C3 University of Camerino; University of Perugia; University of Naples
   Federico II
RP Dall'Aglio, C (corresponding author), Univ Perugia, Dept Vet Med, Via San Costanzo 4, Perugia 06126, Italy.
EM elena.defelice@unicam.it; cesarepacioni@hotmail.com;
   dtfederico.tardella@unicam.it; cecilia.dallaglio@unipg.it;
   a.palladino1986@gmail.com; paola.scocco@unicam.it
RI De+Felice, Elena/AAV-9562-2020; Scocco, Paola/AAF-8596-2020; Palladino,
   Antonio/AAA-7394-2021
OI Palladino, Antonio/0000-0003-2855-6708; TARDELLA, Federico
   Maria/0000-0002-4319-9131; scocco, paola/0000-0003-4506-967X;
   Dall'Aglio, Cecilia/0000-0001-6451-3751; De Felice,
   Elena/0000-0003-1012-902X
CR Aragon S, 1998, J MAMMAL, V79, P131, DOI 10.2307/1382847
   Avdic R., 2013, Veterinaria (Sarajevo), V62, P1
   Azorit C, 2003, ACTA THERIOL, V48, P221, DOI 10.1007/BF03194161
   Becciolini V, 2016, ITAL J ANIM SCI, V15, P461, DOI 10.1080/1828051X.2016.1186505
   Blagojevic M, 2011, MAMM BIOL, V76, P735, DOI 10.1016/j.mambio.2011.06.004
   Bland JM, 2010, INT J NURS STUD, V47, P931, DOI 10.1016/j.ijnurstu.2009.10.001
   BLAND JM, 1986, LANCET, V1, P307, DOI 10.1016/s0140-6736(86)90837-8
   Blant M., 2004, Game & Wildlife Science, V21, P21
   Carnevali L., 2009, BIOL CONS FAUNA, V117, P1
   De Felice E, 2020, EUR ZOOL J, V87, P82, DOI 10.1080/24750263.2020.1716866
   FANDOS P, 1993, J ZOOL, V231, P39
   Giavarina D, 2015, BIOCHEM MEDICA, V25, P141, DOI 10.11613/BM.2015.015
   Gorecki G, 2014, ACTA ZOOL ACAD SCI H, V60, P271
   Hanzal V, 2017, APPL ECOL ENV RES, V15, P1623, DOI 10.15666/aeer/1504_16231632
   Hewison AJM, 1996, J ZOOL, V239, P573, DOI 10.1111/j.1469-7998.1996.tb05943.x
   Horcajada-Sanchez F, 2016, POL J ECOL, V64, P113, DOI 10.3161/15052249PJE2016.64.1.010
   Langvatn R, 1977, CRITERIA PHYS CONDIT, P1
   Mattioli S, 2009, GUIDA RILEVAMENTO BI, V28, P1
   Moretti M, 2014, J MT ECOL, V3, P56
   NUGENT G, 1994, J APPL ECOL, V31, P253, DOI 10.2307/2404541
   Pettorelli N, 2002, P ROY SOC B-BIOL SCI, V269, P747, DOI 10.1098/rspb.2001.1791
   SUTTIE JM, 1983, J ZOOL, V200, P431, DOI 10.1111/j.1469-7998.1983.tb02321.x
   Toigo C, 2006, ECOGRAPHY, V29, P301, DOI 10.1111/j.2006.0906-7590.04394.x
   Zannese A, 2006, WILDLIFE SOC B, V34, P351, DOI 10.2193/0091-7648(2006)34[351:HFLAIF]2.0.CO;2
NR 24
TC 2
Z9 2
U1 0
U2 2
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 2076-2615
J9 ANIMALS-BASEL
JI Animals
PD MAR
PY 2020
VL 10
IS 3
AR 465
DI 10.3390/ani10030465
PG 8
WC Agriculture, Dairy & Animal Science; Veterinary Sciences; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture; Veterinary Sciences; Zoology
GA LI3IM
UT WOS:000529378800098
PM 32168772
OA gold, Green Published
DA 2022-02-03
ER

PT J
AU Rabil, BS
   Tliba, S
   Granger, E
   Sabourin, R
AF Rabil, Bassem S.
   Tliba, Safa
   Granger, Eric
   Sabourin, Robert
TI Securing high resolution grayscale facial captures using a blockwise
   coevolutionary GA
SO EXPERT SYSTEMS WITH APPLICATIONS
LA English
DT Article
DE Biometrics; Intelligent watermarking; Evolutionary computation;
   Cooperative coevolution; Genetic algorithms; Grayscale texture masks
ID COOPERATIVE COEVOLUTION; OPTIMIZATION
AB In biometric systems, reference facial images captured during enrollment are commonly secured using watermarking, where invisible watermark bits are embedded into these images. Evolutionary Computation (EC) is widely used to optimize embedding parameters in intelligent watermarking (IW) systems. Traditional IW methods represent all blocks of a cover image as candidate embedding solutions of EC algorithms, and suffer from premature convergence when dealing with high resolution grayscale facial images. For instance, the dimensionality of the optimization problem to process a 2048 x 1536 pixel grayscale facial image that embeds 1 bit per 8 x 8 pixel block involves 491( variables represented with 293k binary bits. Such Large-Scale Global Optimization problems cannot be decomposed into smaller independent ones because watermarking metrics are calculated for the entire image. In this paper, a Blockwise Coevolutionary Genetic Algorithm (BCGA) is proposed for high dimensional IW optimization of embedding parameters of high resolution images. BCGA is based on the cooperative coevolution between different candidate solutions at the block level, using a local Block Watermarking Metric (BWM). It is characterized by a novel elitism mechanism that is driven by local blockwise metrics, where the blocks with higher BWM values are selected to form higher global fitness candidate solutions. The crossover and mutation operators of BCGA are performed on block level. Experimental results on PUT face image database indicate a 17% improvement of fitness produced by BCGA compared to classical GA. Due to improved exploration capabilities, BCGA convergence is reached in fewer generations indicating an optimization speedup. (C) 2013 Elsevier Ltd. All rights reserved.
C1 [Rabil, Bassem S.; Tliba, Safa; Granger, Eric; Sabourin, Robert] Univ Quebec, Lab Imagerie Vis & Intelligence Artificielle, Ecole Technol Super, Montreal, PQ H3C 3P8, Canada.
C3 University of Quebec; Ecole de Technologie Superieure - Canada;
   University of Quebec Montreal
RP Rabil, BS (corresponding author), Univ Quebec, Lab Imagerie Vis & Intelligence Artificielle, Ecole Technol Super, Montreal, PQ H3C 3P8, Canada.
EM bguendy@livia.etsmtl.ca; stliba@livia.etsmtl.ca; Eric.Granger@etsmtl.ca;
   Robert.Sabourin@etsmtl.ca
FU Natural Sciences and Engineering Research Council of CanadaNatural
   Sciences and Engineering Research Council of Canada (NSERC)CGIAR;
   BancTec Canada Inc.
FX This work was supported by the Natural Sciences and Engineering Research
   Council of Canada and BancTec Canada Inc.
CR Baluja S., 1994, TECHNICAL REPORT
   Chen WX, 2010, LECT NOTES COMPUT SC, V6239, P300, DOI 10.1007/978-3-642-15871-1_31
   Collette Yann, 2008, Journal of Computing and Information Technology - CIT, V16, P1, DOI 10.2498/cit.1000809
   den Bergh F. V., 2002, THESIS U PRETORIA PR
   Diaz DS, 2005, GECCO 2005: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, VOLS 1 AND 2, P2219
   Harik GR, 1999, IEEE T EVOLUT COMPUT, V3, P287, DOI 10.1109/4235.797971
   Kasinski Andrzej, 2008, IMAGE PROCESSING COM, V13, P59
   Lee ZJ, 2008, APPL SOFT COMPUT, V8, P798, DOI 10.1016/j.asoc.2007.03.011
   Li XD, 2012, IEEE T EVOLUT COMPUT, V16, P210, DOI 10.1109/TEVC.2011.2112662
   Licks Vinicius, 2005, GEOMETRIC ATTACKS IM
   Pelikan M, 2002, COMPUT OPTIM APPL, V21, P5, DOI 10.1023/A:1013500812258
   Potter MA, 2000, EVOL COMPUT, V8, P1, DOI 10.1162/106365600568086
   Rabil B S, 2010, Proceedings of the 2010 Sixth International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIHMSP 2010), P131, DOI 10.1109/IIHMSP.2010.40
   Rabil BS, 2014, MACH VISION APPL, V25, P277, DOI 10.1007/s00138-013-0493-1
   Rosin CD, 1997, EVOL COMPUT, V5, P1, DOI 10.1162/evco.1997.5.1.1
   Shan SQ, 2010, STRUCT MULTIDISCIP O, V41, P219, DOI 10.1007/s00158-009-0420-2
   Shieh CS, 2004, PATTERN RECOGN, V37, P555, DOI 10.1016/j.patcog.2003.07.003
   Sofge D, 2002, IEEE C EVOL COMPUTAT, P413, DOI 10.1109/CEC.2002.1006270
   Tang K, 2009, TECHNICAL REPORT
   van den Bergh F, 2004, IEEE T EVOLUT COMPUT, V8, P225, DOI [10.1109/TEVC.2004.826069, 10.1109/tevc.2004.826069]
   Vellasques E., 2010, HDB PATTERN RECOGNIT, P1
   Vellasques E, 2012, PROCEEDINGS OF THE FOURTEENTH INTERNATIONAL CONFERENCE ON GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P73, DOI 10.1145/2330163.2330174
   VOLOSHYNOVSKIY S, 1999, P 3 INT WORKSH INF H, P211
   Wang ZQ, 2007, LECT NOTES COMPUT SC, V4688, P307
   Wu M., 2001, THESIS PRINCETON U
   Yang ZY, 2008, INFORM SCIENCES, V178, P2985, DOI 10.1016/j.ins.2008.02.017
   [No title captured]
NR 27
TC 4
Z9 4
U1 0
U2 7
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4174
EI 1873-6793
J9 EXPERT SYST APPL
JI Expert Syst. Appl.
PD DEC 1
PY 2013
VL 40
IS 17
BP 6693
EP 6706
DI 10.1016/j.eswa.2013.06.043
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Operations Research & Management Science
GA 242CJ
UT WOS:000326214700001
DA 2022-02-03
ER

PT C
AU Klasen, L
AF Klasen, L
BE Higgins, K
TI Faceless identification: A model for person identification using the
   3D-shape and 3D-motion as cues.
SO INVESTIGATION AND FORENSIC SCIENCE TECHNOLOGIES
SE PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
   (SPIE)
LA English
DT Proceedings Paper
CT Meeting on Investigation and Forensic Science Technologies
CY NOV 03-04, 1998
CL BOSTON, MA
DE person identification; 3D shape and 3D motion estimation; Extended
   Kalman filter; analysis by synthesis; 3D wire-frame body model
ID MOTION
AB Person identification by using biometric methods based on image sequences, or still images, often requires a controllable and cooperative environment during the image capturing stage. In the forensic case the situation is more likely to be the opposite. In this work we propose a method that makes use of the anthropometry of the human body and human actions as cues for identification. Image sequences from surveillance systems are used, which can be seen as monocular image sequences. A 3D deformable wireframe body model is used as a platform to handle the non-rigid information of the 3D shape and 3D motion of the human body from the image sequence. A recursive method for estimating global motion and local shape variations is presented, using two recursive feedback systems.
   Estimation of the 3D motion and the 3D structure of a body part is obtained in the first feedback loop, by using Extended Kalman Filter (EKF) applied on all of the body parts in the wire-frame model. As the 3D motion is constrained to rigid motion, the local shape variations have to be dealt with separately. The image intensity provides us with an indirect measurement of the local shape variations and for this purpose we use analysis-by-synthesis in the second feedback loop. The refined structure of the estimated shape is sent to the Kalman filtering loop for updating the next estimation of structure and motion. Thereby the 3D deformable model is used to integrate the statically anthropometrical measures from each image and to estimate the dynamic motion, whose parameters are useful for the process of identification.
C1 Natl Lab Forens Sci, S-58194 Linkoping, Sweden.
RP Klasen, L (corresponding author), Natl Lab Forens Sci, S-58194 Linkoping, Sweden.
OI Widin Klasen, Lena/0000-0001-5094-5844
CR AZARBAYEJANI A, 1995, IEEE T PATTERN ANAL, V17, P562, DOI 10.1109/34.387503
   Bruce V, 1998, EYE BEHOLDER SCI FAC
   BRUEGGE RV, 1996, AM DEF PREP ASS 12 A
   CAREY LC, 1993, J FORENSIC IDENTIFIC, V43, P585
   Champod C, 1993, J FORENSIC IDENTIFIC, V43, P604
   CHAN SD, 1990, MOL ENDOCRINOL, V4, P639
   CRIMINISI A, 1998, P INT SOC OPTICAL EN, V3576
   CRIMINISI A, 1998, P EUR C COMP VIS ECC
   DAUGMAN J, 1995, P EUR CONV SEC DET 1, V48, P244
   DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676
   EADWEARD M, 1955, HUMAN FIGURE MOTION
   GLOOR PA, 1980, J FORENSIC SCI SOC, V20, P99, DOI 10.1016/S0015-7368(80)71315-4
   Hatze H, 1983, J SPORT SCI, V1, P3, DOI 10.1080/02640418308729656
   Horn B.K.P., 1986, ROBOT VISION
   JUNG SK, 1997, IEEE P VIS IMAGE SIG, V144
   KEEREWEER I, 1998, INFORMATION B SHOEPR, V4, P129
   Kennedy RB, 1996, FORENSIC SCI INT, V82, P81, DOI 10.1016/0379-0738(96)01969-X
   KLASEN L, 1997, INVESTIGATIVE IMAGE, V2942, P163
   KLASEN L, 1998, FACE RECOGNITION THE, P513
   LENA K, 1998, P 12 INT FOR SCI S
   LI H, 1993, LINKOPING STUDIES SC
   LI H, 1997, P SWED S IM AN 1997
   LI HB, 1993, IEEE T PATTERN ANAL, V15, P545, DOI 10.1109/34.216724
   PEDERSINI F, 1997, P PICT COD S 1997 SE, P573
   SOMMERFIELD A, 1950, MECH DEFORMABLE BODI
   THURFJALL L, 1998, LECT NOTES VISUAL IN
   Van der Lugt C., 1998, INFORMATION B SHOEPR, V4, P69
   1998, P 3 INT C FAC GEST R
NR 28
TC 3
Z9 3
U1 0
U2 1
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
BN 0-8194-3042-0
J9 P SOC PHOTO-OPT INS
PY 1999
VL 3576
BP 216
EP 226
DI 10.1117/12.334533
PG 11
WC Instruments & Instrumentation; Optics; Imaging Science & Photographic
   Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Instruments & Instrumentation; Optics; Imaging Science & Photographic
   Technology
GA BM50A
UT WOS:000078902000027
DA 2022-02-03
ER

PT C
AU Canto, E
   Fons, M
   Lopez, M
   Ramos, R
AF Canto, Enrique
   Fons, Mariano
   Lopez, Mariano
   Ramos, Rafael
BE Danek, M
   Kadlec, J
TI ACCELERATION OF COMPLEX ALGORITHMS ON A FAST RECONFIGURABLE EMBEDDED
   SYSTEM ON SPARTAN-3
SO FPL: 2009 INTERNATIONAL CONFERENCE ON FIELD PROGRAMMABLE LOGIC AND
   APPLICATIONS
SE International Conference on Field Programmable and Logic Applications
LA English
DT Proceedings Paper
CT 19th International Conference on Field Programmable Logic and
   Applications
CY AUG 31-SEP 02, 2009
CL ASCR, Informat Theory & Automat, Prague, CZECH REPUBLIC
HO ASCR, Informat Theory & Automat
AB Complex algorithms usually require several computation stages. Many embedded microprocessors have not enough computational performance to resolve these algorithms in a reasonable time, so dedicated coprocessors accelerate them although the main drawback is the area devoted to them. A reconfigurable coprocessor can drastically reduce the area, since it accommodates a set of coprocessors whose execution is multiplexed on time, although the reconfiguration speed reduces the overall system performance. Although self-reconfigurable systems are possible on Spartan-3 FPGAs, it requires a hard design task clue to the lack of software and hardware support available on higher-cost families. This paper describes the architecture of a fast self-reconfigurable embedded system mapped on Spartan-3, used as computation platform to solve a complex algorithm, such as the image-processing carried out in a fingerprint biometric algorithm. In order to reduce the reconfiguration time, the system uses our custom-made memory and reconfiguration controllers. Moreover, the dynamic coprocessor can access directly to external memory through our memory controller to improve processing time.
C1 [Canto, Enrique; Fons, Mariano] Univ Rovira & Virgili, Dept DEEEA, Tarragona, Spain.
   [Lopez, Mariano; Ramos, Rafael] Tech Univ Catalonia, Dept Elect Engn, Barcelona, Spain.
C3 Universitat Rovira i Virgili; Polytechnic University of Catalonia
RP Canto, E (corresponding author), Univ Rovira & Virgili, Dept DEEEA, Tarragona, Spain.
EM ecanto@etse.urv.es; lopezg@eel.upc.edu
RI Ramos-Lara, Rafael/I-1783-2015
OI Ramos-Lara, Rafael/0000-0001-6363-7250
FU Ministerio de Educacion y Ciencia of SpainSpanish GovernmentEuropean
   Commission [TEC2006-12365]
FX Authors would like to thank financial support to Ministerio de Educacion
   y Ciencia of Spain, under grant TEC2006-12365
CR *AVNET, 2003, XIL SPART 3 DEV KIT
   BRAUN L, 2008, DATA PATH DRIVEN WAV, P607
   Canto E., 2008, 4 INT WORKSH REC COM, P117
   Dorairaj N., 2005, XCELL J
   FONS F, 2007, C PHD RES MICR EL SE
   Gonzalez I, 2007, IEEE MICRO, V27, P49, DOI 10.1109/MM.2007.72
   PAULSSON K, 2007, IMPLEMENTATION VIRTU
   *XIL, 2008, XAPP452 XIL
   ZAIDI I, 2008, EVALUATIONG DYNAMIC, P547
NR 9
TC 1
Z9 1
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1946-1488
BN 978-1-4244-3891-4
J9 I C FIELD PROG LOGIC
PY 2009
BP 429
EP +
DI 10.1109/FPL.2009.5272242
PG 2
WC Computer Science, Software Engineering; Engineering, Electrical &
   Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BOT08
UT WOS:000277506300066
OA Green Published
DA 2022-02-03
ER

PT J
AU Kaur, B
AF Kaur, Bineet
TI Iris spoofing detection using discrete orthogonal moments
SO MULTIMEDIA TOOLS AND APPLICATIONS
LA English
DT Article
DE Contact lens; Dual-Hahn moments; Iris spoofing; Krawtchouk moments;
   Orthogonal moments; Print attacks; Tchebichef moments
ID IMAGE-ANALYSIS; RECOGNITION; IDENTIFICATION; FINGERPRINT
AB Human iris being the most stable biometric modality suffers from presentation attacks like colored textured contact lenses and print attacks that obfuscate the natural iris texture. The paper presents discrete orthogonal moment-based invariant feature-set comprising of Tchebichef, Krawtchouk and Dual-Hahn moments which are extracted at localized iris regions to capture local intensity distributions of the iris texture. The orthogonal moment-based feature-set is made rotation, translation and scale-invariant in order to accommodate for geometric transformations when images are acquired in uncontrolled environment. The performance of the proposed techniques is evaluated using four publicly available iris spoofing databases: IIITD-Contact Lens Iris, IIITD Iris Spoofing, Clarkson LivDet 2015 and Warsaw LivDet 2015. The textured contact lens detection rate of 100% for IIITD-CLI and 99.48% for Clarkson datasets is achieved, respectively. Similarly, print+scan and print+capture attacks are detected with 99% and 98.93% accuracy for IIS datasets, respectively. The print attacks are detected with 99.63% and 98.89% accuracy for Clarkson and Warsaw datasets, respectively. The proposed techniques thus, prove to be effective in terms of contact lens and print attacks detection when acquired using multiple sensors.
C1 [Kaur, Bineet] Punjab Engn Coll, Dept Elect & Commun Engn, Chandigarh 160012, India.
C3 Punjab Engineering College (Deemed University)
RP Kaur, B (corresponding author), Punjab Engn Coll, Dept Elect & Commun Engn, Chandigarh 160012, India.
EM bineetkaur91@gmail.com
RI Maunder, Bineet Kaur/I-8506-2019
CR Arora S. S., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P336, DOI 10.1109/ICB.2012.6199829
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   Daugman J., 2003, INT J WAVELETS MULTI, V1, P1, DOI DOI 10.1142/S0219691303000025
   Doyle JS, 2013, INT CONF BIOMETR
   Flom L., 1987, US Patent, Patent No. [4641349, 4,641,349]
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Gragnaniello D, 2015, IEEE T INF FOREN SEC, V10, P849, DOI 10.1109/TIFS.2015.2404294
   Gragnaniello D, 2015, PATTERN RECOGN, V48, P1050, DOI 10.1016/j.patcog.2014.05.021
   Gupta P, 2014, INT C PATT RECOG, P1681, DOI 10.1109/ICPR.2014.296
   Han M.K, 2011, DATA MINING CONCEPTS
   He XF, 2007, LECT NOTES COMPUT SC, V4642, P540
   Hollingsworth K, 2009, COMPUT VIS IMAGE UND, V113, P150, DOI 10.1016/j.cviu.2008.08.001
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/TIT.1962.1057692
   Hu Y, 2016, PATTERN RECOGN LETT, V82, P242, DOI 10.1016/j.patrec.2015.10.010
   Hui Zhang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4279, DOI 10.1109/ICPR.2010.1040
   Jain A. K., 2011, INTRO BIOMETRICS
   Junying Gan, 2005, Advances in Biometrics. International Conference, ICB 2006. Proceedings (Lecture Notes in Computer Science Vol.3832), P443
   Kaur B, 2018, J ENG APPL SCI, V13, P2049
   Kaur B, 2019, INT J BIOMETRICS, V11, P160
   Kaur B, 2019, COMPUT ELECTR ENG, V73, P279, DOI 10.1016/j.compeleceng.2018.12.002
   Kaur B, 2018, INT J BIOMETRICS, V10, P352, DOI 10.1504/IJBM.2018.095293
   Kaur B, 2018, ARAB J SCI ENG, V43, P7209, DOI 10.1007/s13369-017-3057-2
   Kaur B, 2018, WIRELESS PERS COMMUN, V99, P799, DOI 10.1007/s11277-017-5153-8
   Kaur B, 2017, WIRELESS PERS COMMUN, V95, P4823, DOI 10.1007/s11277-017-4126-2
   Kaur B, 2017, IMAGING SCI J, V65, P171, DOI 10.1080/13682199.2017.1311524
   Kaur B, 2016, ADV HUM-COMPUT INTER, V2016, DOI 10.1155/2016/6727806
   Kaur B, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTING, COMMUNICATION & AUTOMATION (ICCCA), P1085, DOI 10.1109/CCAA.2015.7148567
   Kohli N., 2013, P INT C BIOM ICB MAD, P1, DOI [10.1109/icb.2013.6613021, DOI 10.1109/ICB.2013.6613021]
   Ma L, 2003, IEEE T PATTERN ANAL, V25, P1519, DOI 10.1109/TPAMI.2003.1251145
   Masek Libor, 2003, RECOGNITION HUMAN IR, P1
   Menotti D, 2015, IEEE T INF FOREN SEC, V10, P864, DOI 10.1109/TIFS.2015.2398817
   Mukundan R, 2001, IEEE T IMAGE PROCESS, V10, P1357, DOI 10.1109/83.941859
   Nalla PR, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2616281
   Nichols JJ, 2012, ANN REPORT CONTACT L
   Pala F, 2017, IEEE IMAGE PROC, P116, DOI 10.1109/ICIP.2017.8296254
   Pillai JK, 2014, IEEE T PATTERN ANAL, V36, P73, DOI 10.1109/TPAMI.2013.98
   Priyal SP, 2013, PATTERN RECOGN, V46, P2202, DOI 10.1016/j.patcog.2013.01.033
   Raghavendra R, 2017, IEEE WINT CONF APPL, P1160, DOI 10.1109/WACV.2017.134
   Raghavendra R, 2014, 2 INT WORKSH BIOM FO, p[1, 24]
   Rahman SMM, 2016, PATTERN RECOGN, V54, P83, DOI 10.1016/j.patcog.2016.01.003
   Sequeira AF, 2014, IEEE IJCNN, P3002, DOI 10.1109/IJCNN.2014.6889816
   Silva P, 2015, SIBGRAPI, P157, DOI 10.1109/SIBGRAPI.2015.16
   Tan CW, 2013, P IEEE 6 INT C BIOM, P1
   TEAGUE MR, 1980, J OPT SOC AM, V70, P920, DOI 10.1364/JOSA.70.000920
   Tsougenis ED, 2012, J SYST SOFTWARE, V85, P1864, DOI 10.1016/j.jss.2012.02.045
   UID Authority of India, 2012, ROL BIOM TECHN AADH
   Wei ZH, 2008, GLOB TELECOMM CONF, DOI 10.1109/GLOCOM.2008.ECP.428
   Yadav D, 2014, IEEE T INF FOREN SEC, V9, P851, DOI 10.1109/TIFS.2014.2313025
   Yambay D, 2017, IEEE INT C ID SEC BE, P1
   Yap PT, 2007, IEEE T PATTERN ANAL, V29, P2057, DOI 10.1109/TPAMI.2007.70709
   Yap PT, 2003, IEEE T IMAGE PROCESS, V12, P1367, DOI 10.1109/TIP.2003.818019
   Zhang HY, 2016, IEEE IMAGE PROC, P1, DOI [10.1109/ICIP.2016.7532307, 10.1109/ICC.2016.7511620]
   Zitova, 2016, 2D 3D IMAGE ANAL MOM, DOI [10.1002/9781119039402, DOI 10.1002/9781119039402]
NR 53
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1380-7501
EI 1573-7721
J9 MULTIMED TOOLS APPL
JI Multimed. Tools Appl.
PD MAR
PY 2020
VL 79
IS 9-10
BP 6623
EP 6647
DI 10.1007/s11042-019-08281-x
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA KU8NV
UT WOS:000519969900053
DA 2022-02-03
ER

PT J
AU Liu, WW
AF Liu, Weiwei
TI Video Face Detection Based on Deep Learning
SO WIRELESS PERSONAL COMMUNICATIONS
LA English
DT Article
DE Living face detection; Convolution neural network; Deep learning; Face
   authentication system
ID RECOGNITION
AB In recent years, with the development of Internet plus concept, online identity has become a major problem based on the continuous expansion of network applications. The online authentication technology based on biometric features can maintain the consistency of human digital identity and physical identity, so people pay more attention to it. This paper studies the problem of human face detection. The main tasks are as follows: an active body detection algorithm for convolution neural networks based on dynamic feature is proposed. First, the Pyramid LK optical flow method is used to track the video, and the dynamic information of the image is obtained. Then, the information of the optical flow is analysed, and the horizontal and vertical displacement are calculated. According to the two displacements, the displacement amplitude diagram is calculated, that is, the dynamic feature graph. The dynamic feature graph is used as the input of the convolution neural network. Finally, the feature extraction and the living detection are carried out. A face authentication system with living face detection function is designed. The system includes the registration phase and the authentication phase. The registration phase includes face image detection and feature extraction module. The authentication phase includes face detection, human face discrimination, feature extraction and similarity calculation module.
C1 [Liu, Weiwei] Shenyang Sport Univ, Sch Management & Journalism & Commun, Shenyang 110102, Liaoning, Peoples R China.
C3 Shenyang Sport University
RP Liu, WW (corresponding author), Shenyang Sport Univ, Sch Management & Journalism & Commun, Shenyang 110102, Liaoning, Peoples R China.
EM weiweiliuedu@sina.com
CR Abdel-Qadir H, 2015, HEART, V101, P1554, DOI 10.1136/heartjnl-2015-307815
   Anthimopoulos M, 2016, IEEE T MED IMAGING, V35, P1207, DOI 10.1109/TMI.2016.2535865
   Chen YH, 2017, IEEE J SOLID-ST CIRC, V52, P127, DOI 10.1109/JSSC.2016.2616357
   Galea C, 2017, IEEE SIGNAL PROC LET, V24, P1586, DOI 10.1109/LSP.2017.2749266
   Huang ZW, 2015, PATTERN RECOGN, V48, P3113, DOI 10.1016/j.patcog.2015.03.011
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z
   Kumari S, 2014, INT J COMMUN SYST, V27, P3939, DOI 10.1002/dac.2590
   Myhre OF, 2017, J ACOUST SOC AM, V141, P1170, DOI 10.1121/1.4976096
   Schubert J, 2015, ARCH VIROL, V160, P1761, DOI 10.1007/s00705-015-2422-2
   Sudars K, 2017, AUTOM CONTROL COMPUT, V51, P50, DOI 10.3103/S0146411617010072
   [吴迪 Wu Di], 2016, [光电子·激光, Journal of Optoelectronics·Laser], V27, P655
NR 11
TC 3
Z9 4
U1 1
U2 15
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0929-6212
EI 1572-834X
J9 WIRELESS PERS COMMUN
JI Wirel. Pers. Commun.
PD OCT
PY 2018
VL 102
IS 4
BP 2853
EP 2868
DI 10.1007/s11277-018-5311-7
PG 16
WC Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Telecommunications
GA HA9EB
UT WOS:000450597900032
DA 2022-02-03
ER

PT J
AU Kajendran, K
   Pravin, A
AF Kajendran, K.
   Pravin, A.
TI Enhancement of Bio Metric Security of Automated Teller Machine through
   Integration of Bank Account with AADHAR Account and Using One Time
   Password to Avoid Fraudulent Transaction
SO RESEARCH JOURNAL OF PHARMACEUTICAL BIOLOGICAL AND CHEMICAL SCIENCES
LA English
DT Article
DE Palm Pattern Recognition; Bio Metric Security; ATM Security
AB Debit cards with traditional PIN authentications were more prone to security breach that helps fraudster to carry out financial fraud. To overcome this, bio metric based authentication can be used, which is more secure than PIN authentication. But, the drawback of bio metric authentication is that it restricts only the account holder to withdraw money, not by family members. Second drawback is that Even though it is highly secured, there are advanced technologies through which bio metric information such as pattern of finger, iris, and retina can be stolen and misused by culprit to withdraw money in an unauthorized way. This paper suggests an alternate solution to enhance ATM security using palm vein with more user friendly way by allowing their family members also to withdraw money from ATM machine using their palm vein image as mode of authentication. To make bio metric based authentication highly secured and more user friendly, this paper proposes AADHAR linked biometric authentication in the ATM machine, since AADHAR database consist palm vein images of all family member. To make this authentication more secure, generation one time password( OTP) is integrated with bio metric authentication
C1 [Kajendran, K.; Pravin, A.] Sathyabama Univ, Dept Comp Sci & Engn, Madras, Tamil Nadu, India.
C3 Sathyabama Institute of Science & Technology
RP Kajendran, K (corresponding author), Sathyabama Univ, Dept Comp Sci & Engn, Madras, Tamil Nadu, India.
RI , pravin/Q-5234-2019
OI , pravin/0000-0003-2988-7090
CR ADEPOJU AS, 2010, J INTERNET BANKING C, V0015, P00001
   Aru OE., 2013, AM J ENG RES AJER, V2, P188
   Aru Okereke Eze, AM J ENG RES AJER, V02, P188
   Bhosale S.T., 2012, INT J ADV TECHNOLOGY, V2
   Biswas S., 2012, INT J ADV RES COMPUT, V2
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Goud D. Shelkar, 2012, GLOBAL J ADV ENG TEC
   Jobin J., 2012, INT J ADV RES ELECT, V1
   Muhammad-Bello B.L., 2015, INT J COMPUTER APPL, V111
   Oko Selina, 2012, IJCSI INT J COMPUTER, V9
   Pittalia Prashant P., 2015, INT J ADV RES COMPUT, V5
   Rajendran Rathishala, ATM SECURITY USING F
   Saeed Khalid, ENHANCED METHODS COM, P185
   Spinella Edmund, 2003, BIOMETRIC SCANNING T
NR 14
TC 2
Z9 2
U1 0
U2 1
PU RJPBCS RESEARCH JOURNAL PHARMACEUTICAL, BIOLOGICAL & CHEMICAL SCIENCES
PI PRODDATUR
PA RJPBCS RESEARCH JOURNAL PHARMACEUTICAL, BIOLOGICAL & CHEMICAL SCIENCES,
   PRODDATUR, 00000, INDIA
SN 0975-8585
J9 RES J PHARM BIOL CHE
JI Res. J. Pharm. Biol. Chem. Sci.
PD JUL-AUG
PY 2017
VL 8
IS 4
BP 317
EP 321
PG 5
WC Multidisciplinary Sciences
WE Emerging Sources Citation Index (ESCI)
SC Science & Technology - Other Topics
GA FG8JF
UT WOS:000410676300041
DA 2022-02-03
ER

PT J
AU Wu, JC
   Wilson, CL
AF Wu, Jin Chu
   Wilson, Charles L.
TI Nonparametric analysis of fingerprint data on large data sets
SO PATTERN RECOGNITION
LA English
DT Article
DE fingerprint matching; nonparametric analysis; receiver operating
   characteristic (ROC) curve; Mann-Whitney statistic; significance test
ID OPERATING CHARACTERISTIC CURVES; AREAS
AB By executing different fingerprint-image matching algorithms on large data sets, it reveals that the match and non-match similarity scores have no specific underlying distribution function. Thus, it requires a nonparametric analysis for fingerprint-image matching algorithms on large data sets without any assumption about such irregularly discrete distribution functions. A precise receiver operating characteristic (ROC) curve based on the true accept rate (TAR) of the match similarity scores and the false accept rate (FAR) of the non-match similarity scores can be constructed. The area under such an ROC curve computed using the trapezoidal rule is equivalent to the Mann-Whitney statistic directly formed from the match and non-match similarity scores. Thereafter, the Z statistic formulated using the areas under ROC curves along with their variances and the correlation coefficient is applied to test the significance of the difference between two ROC curves. Four examples from the extensive testing of commercial fingerprint systems at the National Institute of Standards and Technology are provided. The Donparametric approach presented in this article can also be employed in the analysis of other large biometric data sets. (c) 2006 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.
C1 Natl Inst Stand & Technol, Image Grp, Informat Access Div, Informat Technol Lab, Gaithersburg, MD 20899 USA.
C3 National Institute of Standards & Technology (NIST) - USA
RP Wu, JC (corresponding author), Natl Inst Stand & Technol, Image Grp, Informat Access Div, Informat Technol Lab, Gaithersburg, MD 20899 USA.
EM jinchu.wu@nist.gov
CR BAMBER D, 1975, J MATH PSYCHOL, V12, P387, DOI 10.1016/0022-2496(75)90001-2
   DELONG ER, 1988, BIOMETRICS, V44, P837, DOI 10.2307/2531595
   Green D. M., 1966, SIGNAL DETECTION THE, P45
   HANLEY JA, 1983, RADIOLOGY, V148, P839, DOI 10.1148/radiology.148.3.6878708
   HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747
   LE CT, 1995, BIOMETRICAL J, V37, P869, DOI 10.1002/bimj.4710370709
   Maio D, 2002, IEEE T PATTERN ANAL, V24, P402, DOI 10.1109/34.990140
   MCCLISH DK, 1987, MED DECIS MAKING, V7, P149, DOI 10.1177/0272989X8700700305
   NOETHER GE, 1967, ELEMENTS NONPARAMETR, P31
   van der Waerden B. L., 1969, MATH STAT, P274
   WATSON C, 2004, 7119 NISTIR
   WIEAND S, 1989, BIOMETRIKA, V76, P585, DOI 10.1093/biomet/76.3.585
   Wilson C., 2004, 7123 NISTIR
   Zou KH, 2001, ACAD RADIOL, V8, P225, DOI 10.1016/S1076-6332(03)80531-7
NR 14
TC 16
Z9 16
U1 0
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0031-3203
J9 PATTERN RECOGN
JI Pattern Recognit.
PD SEP
PY 2007
VL 40
IS 9
BP 2574
EP 2584
DI 10.1016/j.patcog.2006.11.021
PG 11
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA 174HG
UT WOS:000246932200018
DA 2022-02-03
ER

PT J
AU Teshima, TL
   Patel, V
   Mainprize, JG
   Edwards, G
   Antonyshyn, OM
AF Teshima, Tara Lynn
   Patel, Vaibhav
   Mainprize, James G.
   Edwards, Glenn
   Antonyshyn, Oleh M.
TI A Three-Dimensional Statistical Average Skull: Application of Biometric
   Morphing in Generating Missing Anatomy
SO JOURNAL OF CRANIOFACIAL SURGERY
LA English
DT Article
DE Average skull; biometric morphing; cranioplasty; facial surgery;
   geometric morphometrics
ID GEOMETRIC MORPHOMETRICS
AB Purpose: The utilization of three-dimensional modeling technology in craniomaxillofacial surgery has grown exponentially during the last decade. Future development, however, is hindered by the lack of a normative three-dimensional anatomic dataset and a statistical mean three-dimensional virtual model. The purpose of this study is to develop and validate a protocol to generate a statistical three-dimensional virtual model based on a normative dataset of adult skulls.
   Method: Two hundred adult skull CT images were reviewed. The average three-dimensional skull was computed by processing each CT image in the series using thin-plate spline geometric morphometric protocol. Our statistical average three-dimensional skull was validated by reconstructing patient-specific topography in cranial defects. The experiment was repeated 4 times. In each case, computer-generated cranioplasties were compared directly to the original intact skull. The errors describing the difference between the prediction and the original were calculated.
   Results: A normative database of 33 adult human skulls was collected. Using 21 anthropometric landmark points, a protocol for three-dimensional skull landmarking and data reduction was developed and a statistical average three-dimensional skull was generated. Our results show the root mean square error (RMSE) for restoration of a known defect using the native best match skull, our statistical average skull, and worst match skull was 0.58, 0.74, and 4.4 mm, respectively.
   Conclusions: The ability to statistically average craniofacial surface topography will be a valuable instrument for deriving missing anatomy in complex craniofacial defects and deficiencies as well as in evaluating morphologic results of surgery.
C1 [Teshima, Tara Lynn; Edwards, Glenn; Antonyshyn, Oleh M.] Univ Toronto, Sunnybrook Hlth Sci Ctr, Div Plast Surg, Toronto, ON M4N 3M5, Canada.
   [Patel, Vaibhav; Mainprize, James G.; Edwards, Glenn] Sunnybrook Res Inst, Phys Sci, Med Imaging Res, Toronto, ON, Canada.
C3 University of Toronto; Sunnybrook Health Science Center; Sunnybrook
   Research Institute; University of Toronto; Sunnybrook Health Science
   Center; Sunnybrook Research Institute
RP Teshima, TL (corresponding author), Univ Toronto, Sunnybrook Hlth Sci Ctr M1 502, Dept Surg, Div Plast & Reconstruct Surg, Toronto, ON M4N 3M5, Canada.
EM taralynnteshima@gmail.com
RI Mainprize, James Gordon/J-4756-2019; Antonyshyn, Oleh/AAL-2421-2020
OI Mainprize, James Gordon/0000-0001-5479-1280; 
FU AO CranioMaxilloFacial Clinical Priority Program
FX This work was supported by the AO CranioMaxilloFacial Clinical Priority
   Program.
CR Adams DC, 2004, ITAL J ZOOL, V71, P5, DOI 10.1080/11250000409356545
   Bookstein F., 1991, MORPHOMETRIC TOOLS L
   BOOKSTEIN FL, 1991, LECT NOTES COMPUT SC, V511, P326
   Farkas L.G., 1994, ANTHROPOMETRY HEAD F, P405
   Gunz P, 2005, DEV PRIMATOL-PROG PR, P73, DOI 10.1007/0-387-27614-9_3
   Gunz P, 2009, J HUM EVOL, V57, P48, DOI 10.1016/j.jhevol.2009.04.004
   Hennessy RJ, 2005, J ANAT, V207, P283, DOI 10.1111/j.1469-7580.2005.00444.x
   Metzger MC, 2007, INT J ORAL MAX SURG, V36, P45, DOI 10.1016/j.ijom.2006.07.013
   Pahuta MA, 2009, ANN PLAS SURG, V62, P48, DOI 10.1097/SAP.0b013e3181743386
   Sena Khemachit, 2008, American Journal of Engineering and Applied Sciences, V1, P168
   Stindel E, 2002, Comput Aided Surg, V7, P156, DOI 10.3109/10929080209146026
   Tiddeman B, 2000, COMPUT METH PROG BIO, V63, P9, DOI 10.1016/S0169-2607(00)00072-9
   Wiley DF, 2005, P IEEE VIS
   Zhao LP, 2012, ARCH PLAST SURG-APS, V39, P309, DOI 10.5999/aps.2012.39.4.309
NR 14
TC 5
Z9 6
U1 0
U2 5
PU LIPPINCOTT WILLIAMS & WILKINS
PI PHILADELPHIA
PA TWO COMMERCE SQ, 2001 MARKET ST, PHILADELPHIA, PA 19103 USA
SN 1049-2275
EI 1536-3732
J9 J CRANIOFAC SURG
JI J. Craniofac. Surg.
PD JUL
PY 2015
VL 26
IS 5
BP 1634
EP 1638
DI 10.1097/SCS.0000000000001869
PG 5
WC Surgery
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Surgery
GA DD0LH
UT WOS:000369611000087
PM 26114514
DA 2022-02-03
ER

PT J
AU Lavanya, B
   Inbarani, HH
AF Lavanya, B.
   Inbarani, H. Hannah
TI A novel hybrid approach based on principal component analysis and
   tolerance rough similarity for face identification
SO NEURAL COMPUTING & APPLICATIONS
LA English
DT Article
DE Face biometric; PCA; Tolerance similarity; Tolerance rough set
ID SETS
AB Face identification plays one of the most important roles in biometrics to recognize a person. However, face identification is very difficult because of variations in size, orientations, different illuminations, and face expressions. In this paper, a hybrid approach is proposed based on principal component analysis (PCA) and tolerance rough similarity (TRS) for face identification. This paper comprises of three steps. First, PCA has been used to extract the feature vector from face images (eigenvectors). Second, the tolerance rough set-based similarity is applied for face matching and finally, the test image is compared with lower and upper approximation of similarity values that were found using TRS. The proposed hybrid approach gives a better recognition rate compared to other standard techniques like Euclidean distance and cosine similarity. The proposed work is evaluated on three face databases namely OUR databases and ORL databases and Yale databases. The experimental result of the proposed PCA-TRS approach is compared with other standard classification techniques like support vector machine (SVM), multilayer perceptron (MLP), back propagation network (BPN) and simple decision tree (CART) to conclude that proposed approach is better for face identification because of high accuracy and minimum error rate.
C1 [Lavanya, B.; Inbarani, H. Hannah] Periyar Univ, Dept Comp Sci, Salem, Tamil Nadu, India.
C3 Periyar University
RP Lavanya, B (corresponding author), Periyar Univ, Dept Comp Sci, Salem, Tamil Nadu, India.
EM hhinba@periyaruniversity.ac.in; hhinba@gmail.com
RI Balu, Lavanya/ABE-6743-2020; inbarani, hannah/ABB-1453-2020; ,
   lavanya/B-3555-2017
OI inbarani, hannah/0000-0002-2956-3507; , lavanya/0000-0003-0236-9541
FU Periyar University, Salem, Tamilnadu
FX The authors would like to thank Robotics lab, Olivetti University and
   Yale University for providing face databases. The first author extremely
   thanks the partial financial assistance under University Research
   Fellow, Periyar University, Salem, Tamilnadu.
CR Abdullah M., 2012, INT J ARTIFICIAL INT, V3, P23
   Azeem A, 2014, INT ARAB J INF TECHN, V11, P1
   Bazan JG, 2000, STUD FUZZ SOFT COMP, V56, P49
   Cevikalp H, 2010, PROC CVPR IEEE, P2567, DOI 10.1109/CVPR.2010.5539965
   Chen XG, 2011, INT J INTELL SYST, V26, P499, DOI 10.1002/int.20481
   Chen XG, 2010, LECT NOTES ARTIF INT, V6086, P356, DOI 10.1007/978-3-642-13529-3_38
   Dabbaghchian S, 2010, PATTERN RECOGN, V43, P1431, DOI 10.1016/j.patcog.2009.11.001
   Dai B, 2009, INT C PHOT IM AGR EN
   Hiremath P. S., 2007, MODELING UNCERTAINTY
   Hu YC, 2016, NEUROCOMPUTING, V179, P144, DOI 10.1016/j.neucom.2015.11.066
   Hu YC, 2015, APPL SOFT COMPUT, V27, P322, DOI 10.1016/j.asoc.2014.11.021
   Hu YC, 2013, APPL MATH MODEL, V37, P7330, DOI 10.1016/j.apm.2013.03.007
   Huang J, 2004, EURASIP J ADV SIG PR, V2004, P1
   Jensen R., 2007, 2007 IEEE INT FUZZ S, P1
   Jensen R., 2007, ROUGH COMPUTING O, P70
   Jesorsky O, 2001, LECT NOTES COMPUT SC, V2091, P90
   Kathirvalavakumar Thangairulappan, 2013, J INTELLIGENT LEARNI, V5, P115
   Kim D, 2000, IEEE T PATTERN ANAL, V22, P923, DOI 10.1109/34.877516
   Kim D, 2001, PATTERN RECOGN, V34, P1613, DOI 10.1016/S0031-3203(00)00057-1
   KIRBY M, 1990, IEEE T PATTERN ANAL, V12, P103, DOI 10.1109/34.41390
   Kumar D. Raj, 2014, INT J ADV RES COMPUT, V3
   Lai JH, 2001, PATTERN RECOGN, V34, P95, DOI 10.1016/S0031-3203(99)00200-9
   Li XL, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P1948
   LU X, 2003, IMAGE ANAL FACE RECO
   Mac Parthalain N, 2009, PATTERN RECOGN, V42, P655, DOI 10.1016/j.patcog.2008.08.029
   Mane A. V., 2010, INT J COMPUT SCI APP, P62
   PAUL LC, 2012, INT J ADV RES COMPUT, V1, P135
   PAWLAK Z, 1982, INT J COMPUT INF SCI, V11, P341, DOI 10.1007/BF01001956
   Pawlak Z., 2002, J TELECOMMUN INF TEC, V3, P7
   Pokowski L, 2002, ROUGH SETS MATH FDN
   Selamat MH, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTER, CONTROL, INFORMATICS AND ITS APPLICATIONS (IC3INA), P159, DOI 10.1109/IC3INA.2015.7377765
   Sharif M, 2012, INT ARAB J INF TECHN, V9, P562
   Skowron A., 1996, Fundamenta Informaticae, V27, P245
   So-In C, 2016, INT ARAB J INF TECHN, V13, P59
   Solunke V, 2014, INT J EMERGING RES M, V3, P38
   Sundaram Mala, 2016, FACE RECOGNITION DEM, P75
   Swiniarski R., 2000, INT C ROUGH SETS CUR, P561
   Thakur S, 2008, 2008 1st International Conference on Emerging Trends in Engineering and Technology (ICETET), P695, DOI 10.1109/ICETET.2008.104
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Xie XH, 2011, IEEE T IMAGE PROCESS, V20, P1807, DOI 10.1109/TIP.2010.2097270
   Yuen PC, 2002, PATTERN RECOGN, V35, P1247, DOI 10.1016/S0031-3203(01)00101-7
NR 41
TC 9
Z9 9
U1 1
U2 12
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 0941-0643
EI 1433-3058
J9 NEURAL COMPUT APPL
JI Neural Comput. Appl.
PD APR
PY 2018
VL 29
IS 8
BP 289
EP 299
DI 10.1007/s00521-017-2994-8
PG 11
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FZ7RZ
UT WOS:000427799900021
DA 2022-02-03
ER

PT C
AU Nagrani, A
   Albanie, S
   Zisserman, A
AF Nagrani, Arsha
   Albanie, Samuel
   Zisserman, Andrew
GP IEEE
TI Seeing Voices and Hearing Faces: Cross-modal biometric matching
SO 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
LA English
DT Proceedings Paper
CT 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 18-23, 2018
CL Salt Lake City, UT
ID LIP-MOTION; IDENTITY
AB We introduce a seemingly impossible task: given only an audio clip of someone speaking, decide which of two face images is the speaker. In this paper we study this, and a number of related cross-modal tasks, aimed at answering the question: how much can we infer from the voice about the face and vice versa?
   We study this task "in the wild", employing the datasets that are now publicly available Jar face recognition from static images (VGGFace) and speaker identification from audio (VoxCeleb). These provide training and testing scenarios for both static and dynamic testing of cross-modal matching. We make the fallowing contributions: (i) we introduce CNN architectures for both binary and multi-way cross-modal face and audio matching: (ii) we compare dynamic testing (where video information is available, but the audio is not from the same video) with static testing (where only a single still image is available): and (iii) we use human testing as a baseline to calibrate the difficulty of the task. We show that a CNN can indeed be trained to solve this task in both the static and dynamic scenarios, and is even well above chance on 10-way classification of the face given the voice. The CNN matches human performance on easy examples (e.g. different gender across faces) but exceeds hutnan performance on more challenging examples (e.g. faces with the same gender, age and nationality)1.
C1 [Nagrani, Arsha; Albanie, Samuel; Zisserman, Andrew] Univ Oxford, Dept Engn Sci, VGG, Oxford, England.
C3 University of Oxford
RP Nagrani, A (corresponding author), Univ Oxford, Dept Engn Sci, VGG, Oxford, England.
EM arsha@robots.ox.ac.uk; albanie@robots.ox.ac.uk; az@robots.ox.ac.uk
RI Albanie, Samuel/AAC-9729-2020
OI Albanie, Samuel/0000-0003-1732-9198
FU EPSRC CDT AIMSUK Research & Innovation (UKRI)Engineering & Physical
   Sciences Research Council (EPSRC) [EP/L015897/1, Seebibyte
   EP/M013774/1]; EPSRCUK Research & Innovation (UKRI)Engineering &
   Physical Sciences Research Council (EPSRC) [EP/M013774/1] Funding
   Source: UKRI
FX The authors gratefully acknowledge the support of EPSRC CDT AIMS
   EP/L015897/1 and the Programme Grant Seebibyte EP/M013774/1. The authors
   would also like to thank Erika Lu for help with the AMT study, Hakan
   Bilen and Joe Levy for useful discussions, and Joon Son Chung for being
   a living legend.
CR Antol S., 2015, P ICCV
   Arandjelovic R., 2017, P ICCV
   Aytar Y., 2017, IEEE PAMI
   Aytar Y., 2016, P 30 INT C NEUR INF, P892
   Bilen H., 2017, TPAMI
   Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331
   BRUNELLI R, 1995, IEEE T PATTERN ANAL, V17, P955, DOI 10.1109/34.464560
   Cetingul HE, 2006, SIGNAL PROCESS, V86, P3549, DOI 10.1016/j.sigpro.2006.02.045
   Cetingul HE, 2005, INT CONF ACOUST SPEE, P509
   Chatfield K., 2011, P BMVC
   Chung J. S., 2016, WORKSH MULT LIP READ
   Chung Joon Son, 2017, P CVPR
   Cvejic E, 2012, COGNITION, V122, P442, DOI 10.1016/j.cognition.2011.11.013
   Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307
   Fernando B., 2016, ARXIV161106646
   Frome A., 2013, ADV NEURAL INFORM PR, V2, P2121
   HOLLIEN H, 1960, J SPEECH HEAR RES, V3, P157, DOI 10.1044/jshr.0302.157
   JI SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI DOI 10.1109/TPAMI.2012.59
   Kamachi M, 2003, CURR BIOL, V13, P1709, DOI 10.1016/j.cub.2003.09.005
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kemelmacher-Shlizerman I, 2016, PROC CVPR IEEE, P4873, DOI 10.1109/CVPR.2016.527
   Khoury E, 2014, IMAGE VISION COMPUT, V32, P1147, DOI 10.1016/j.imavis.2013.10.001
   Kiros R., 2014, ARXIV14112539, P1
   Krauss RM, 2002, J EXP SOC PSYCHOL, V38, P618, DOI 10.1016/S0022-1031(02)00510-3
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Lachs L, 2004, J ACOUST SOC AM, V116, P507, DOI 10.1121/1.1757454
   Lander K, 2007, J EXP PSYCHOL HUMAN, V33, P905, DOI 10.1037/0096-1523.33.4.905
   Le N., 2017, ARXIV170702749
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Malinowski Mateusz, 2014, ADV NEURAL INFORM PR, P1682
   Mallat S, 2016, PHILOS T R SOC A, V374, DOI 10.1098/rsta.2015.0203
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Ouyang H., 2006, P 2005 NICTA HCSNET, V57, P33
   Owens A., 2016, P ECCV
   Parkhi O. M, 2015, P BMVC
   Rosenblum LD, 2006, PERCEPT PSYCHOPHYS, V68, P84, DOI 10.3758/BF03193658
   Roy A., 2010, BIOM THEOR APPL SYST, P1
   Saon G, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P55, DOI 10.1109/ASRU.2013.6707705
   Sheffert SM, 2004, PERCEPT PSYCHOPHYS, V66, P352, DOI 10.3758/BF03194884
   Simonyan K., 2014, ADV NEURAL INFORM PR, p568 576, DOI DOI 10.1002/14651858.CD001941.PUB3
   Simonyan K., 2015, P ICLR
   Smith HMJ, 2016, EVOL PSYCHOL-US, V14, DOI 10.1177/1474704916630317
   Smith HMJ, 2016, ATTEN PERCEPT PSYCHO, V78, P868, DOI 10.3758/s13414-015-1045-8
   Snyder D, 2017, INTERSPEECH, P999, DOI 10.21437/Interspeech.2017-620
   Sun Yi, 2014, ADV NEURAL INFORM PR
   Taigman Y., 2014, IEEE CVPR
   Thornhill R, 1997, BIOL REV, V72, P497, DOI 10.1017/S0006323197005082
   Tin X, 2015, PROC CVPR IEEE, P2984, DOI 10.1109/CVPR.2015.7298917
   Venugopalan S., 2014, ARXIV14124729
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang J., 2009, P BMVC
   Wells T, 2013, ARCH SEX BEHAV, V42, P805, DOI 10.1007/s10508-012-0054-0
   Yehia H, 1998, SPEECH COMMUN, V26, P23, DOI 10.1016/S0167-6393(98)00048-X
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang JM, 2016, LECT NOTES COMPUT SC, V9908, P543, DOI 10.1007/978-3-319-46493-0_33
   Zhao GY, 2013, VIROL J, V10, DOI 10.1186/1743-422X-10-266
   Zitnick CL, 2013, IEEE I CONF COMP VIS, P1681, DOI 10.1109/ICCV.2013.211
NR 57
TC 42
Z9 42
U1 2
U2 8
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1063-6919
BN 978-1-5386-6420-9
J9 PROC CVPR IEEE
PY 2018
BP 8427
EP 8436
DI 10.1109/CVPR.2018.00879
PG 10
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BL9NZ
UT WOS:000457843608062
OA Green Submitted
DA 2022-02-03
ER

PT J
AU Manas-Viniegra, L
   Veloso, AI
   Cuesta, U
AF Manas-Viniegra, Luis
   Veloso, Ana-Isabel
   Cuesta, Ubaldo
TI Fashion Promotion on Instagram with Eye Tracking: Curvy Girl Influencers
   Versus Fashion Brands in Spain and Portugal
SO SUSTAINABILITY
LA English
DT Article
DE Instagram; influencer; eye tracking; brand management; identity
   construction; curvy girls; body image; fashion
ID WOMENS BODY-IMAGE; SOCIAL MEDIA; ANOREXIA-NERVOSA; ATTENTION; NUMBER;
   IMPACT; SATISFACTION; PICTURE; INFORMATION; RECOGNITION
AB The rise of Instagram, as the fastest growing social network in Spain and Portugal, and its incorporation into the communication strategies of beauty and fashion brands have posed some risks for younger followers in relation to the development of identity and self-esteem. A physical appearance acceptance movement has also begun, based on interaction with images, on which the social network is also based. The purpose of this research was to determine how attention is paid to fashion promotion and to the awareness of physical appearance acceptance by curvy influencers in comparison with communications by fashion brands on Instagram. The quantitative and qualitative methodology is based on the use of a biometric eye tracking technique applied to a sample of 120 participants from Spain and Portugal, matching the profile of the main users of Instagram: urban university women under 25 years old with an interest in fashion, and a self-perception as a curvy woman. The results point to more attention focused on the imperfections for which curvy influencers are raising awareness than on the fashion they promote when these awareness factors are more visible, as well as more attention focused on the fashion accessories worn by curvy brand models than those worn by the influencers, with specific and significant differences between Spanish and Portuguese audiences.
C1 [Manas-Viniegra, Luis] Univ Complutense Madrid, Dept Appl Commun Sci, Madrid 28040, Spain.
   [Veloso, Ana-Isabel] Univ Aveiro, Dept Commun & Art, P-3810193 Aveiro, Portugal.
   [Cuesta, Ubaldo] Univ Complutense Madrid, Dept Theories & Anal Commun, Madrid 28040, Spain.
C3 Complutense University of Madrid; Universidade de Aveiro; Complutense
   University of Madrid
RP Manas-Viniegra, L (corresponding author), Univ Complutense Madrid, Dept Appl Commun Sci, Madrid 28040, Spain.
EM lmanas@ucm.es
RI Veloso, Ana Isabel/AAA-2458-2020; Manas-Viniegra, Luis/C-4069-2016;
   cuesta, ubaldo/H-5730-2015
OI Veloso, Ana Isabel/0000-0002-5070-0756; Manas-Viniegra,
   Luis/0000-0001-9129-5673; cuesta, ubaldo/0000-0001-7023-7132
CR Aagerup U, 2018, J FASH MARK MANAG, V22, P557, DOI 10.1108/JFMM-07-2017-0065
   Abidin C, 2016, MEDIA INT AUST, V161, P86, DOI 10.1177/1329878X16665177
   Ahadzadeh AS, 2017, COMPUT HUM BEHAV, V68, P8, DOI 10.1016/j.chb.2016.11.011
   Amatulli C, 2016, FASH THEORY, V20, P341, DOI 10.1080/1362704X.2015.1082294
   Ananos E, 2015, COMUNICAR, V23, P75, DOI 10.3916/C45-2015-08
   Ariely D, 2010, NAT REV NEUROSCI, V11, P284, DOI 10.1038/nrn2795
   Baker N, 2019, CYBERPSYCH BEH SOC N, V22, P277, DOI 10.1089/cyber.2018.0420
   Bauer A, 2017, J ABNORM CHILD PSYCH, V45, P1647, DOI 10.1007/s10802-017-0263-z
   Betz DE, 2017, BODY IMAGE, V22, P18, DOI 10.1016/j.bodyim.2017.04.004
   Blechert J, 2010, J ABNORM PSYCHOL, V119, P575, DOI 10.1037/a0019531
   BORNSTEIN RF, 1992, J PERS SOC PSYCHOL, V63, P545, DOI 10.1037//0022-3514.63.4.545
   Brandwatch. com, INST STAT 2016
   Brown Z, 2016, BODY IMAGE, V19, P37, DOI 10.1016/j.bodyim.2016.08.007
   Casalo LV, 2017, CYBERPSYCH BEH SOC N, V20, P369, DOI 10.1089/cyber.2016.0360
   Clayton RB, 2017, COMMUN MONOGR, V84, P406, DOI 10.1080/03637751.2017.1332770
   ComScore. com, 2017, US MOB APP REP
   Cornelissen KK, 2016, INT J EAT DISORDER, V49, P507, DOI 10.1002/eat.22505
   Cuesta-Cambra U, 2019, PROF INFORM, V28, DOI 10.3145/epi.2019.mar.17
   Cuesta-Cambra U, 2017, COMUNICAR, V25, P41, DOI 10.3916/C52-2017-04
   De Veirman M, 2017, INT J ADVERT, V36, P798, DOI 10.1080/02650487.2017.1348035
   Djafarova E, 2017, COMPUT HUM BEHAV, V68, P1, DOI 10.1016/j.chb.2016.11.009
   Duchowski A T, 2007, EYE TRACKING METHODO
   Etgar S, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.00842
   Fardouly J, 2018, NEW MEDIA SOC, V20, P4311, DOI 10.1177/1461444818771083
   Felix R, 2017, J BUS RES, V70, P118, DOI 10.1016/j.jbusres.2016.05.001
   FREEMAN R, 1991, INT J EAT DISORDER, V10, P709, DOI 10.1002/1098-108X(199111)10:6&lt;709::AID-EAT2260100609&gt;3.0.CO;2-N
   Goodrich K, 2011, PSYCHOL MARKET, V28, P417, DOI 10.1002/mar.20371
   Griggs L, 2017, AUST J COMPETIT CONS, V25, P113
   Halliwell E, 2015, BODY IMAGE, V14, P177, DOI 10.1016/j.bodyim.2015.03.003
   Halliwell E, 2014, HEALTH PSYCHOL, V33, P201, DOI 10.1037/a0032585
   Halliwell E, 2013, BODY IMAGE, V10, P509, DOI 10.1016/j.bodyim.2013.07.004
   Hendrickse J, 2017, COMPUT HUM BEHAV, V74, P92, DOI 10.1016/j.chb.2017.04.027
   Horndasch S, 2012, PSYCHIAT RES, V198, P321, DOI 10.1016/j.psychres.2011.12.029
   IABspain. es, 2017, ANN REP SOC MED
   Instagram. com Instagram. com, 700 MILL
   INTRAUB H, 1979, J EXP PSYCHOL-HUM L, V5, P78, DOI 10.1037/0278-7393.5.2.78
   Kerr-Gaffney J, 2019, INT J EAT DISORDER, V52, P3, DOI 10.1002/eat.22998
   Kleemans M, 2018, MEDIA PSYCHOL, V21, P93, DOI 10.1080/15213269.2016.1257392
   LOFTUS GR, 1979, J EXP PSYCHOL-HUM L, V5, P197, DOI 10.1037/0278-7393.5.3.197
   Lu DY, 2017, IEEE T MULTIMEDIA, V19, P1299, DOI 10.1109/TMM.2016.2646181
   Lup K, 2015, CYBERPSYCH BEH SOC N, V18, P247, DOI 10.1089/cyber.2014.0560
   Madan CR, 2010, EUREKA, V1, P34
   Marcus SR, 2016, CYBERPSYCHOLOGY, V10, DOI 10.5817/CP2016-2-5
   Marktest. com, 2017, PORT SOC MED
   Martensen A, 2018, J FASH MARK MANAG, V22, P335, DOI 10.1108/JFMM-09-2017-0095
   Mulgrew KE, 2015, SEX ROLES, V72, P127, DOI 10.1007/s11199-014-0440-2
   Phillipou A, 2016, EUR EAT DISORD REV, V24, P131, DOI 10.1002/erv.2423
   Pieters R, 2002, MANAGE SCI, V48, P765, DOI 10.1287/mnsc.48.6.765.192
   Pittman M, 2016, COMPUT HUM BEHAV, V62, P155, DOI 10.1016/j.chb.2016.03.084
   Primack BA, 2017, AM J PREV MED, V53, P1, DOI 10.1016/j.amepre.2017.01.010
   Reece AG, 2017, EPJ DATA SCI, V6, DOI 10.1140/epjds/s13688-017-0110-z
   Ribeiro-Cardoso P, 2016, REV MEDITERR COMUN, V7, P101, DOI 10.14198/MEDCOM2016.7.2.12
   Rodgers RF, 2016, CLIN PSYCHOL REV, V46, P1, DOI 10.1016/j.cpr.2016.04.006
   Roelens I, 2016, DECIS SUPPORT SYST, V91, P25, DOI 10.1016/j.dss.2016.07.005
   Rosental S., 2017, INTERNET, P17, DOI [10.1145/3014164, DOI 10.1145/3014164]
   Sheldon P, 2017, COMPUT HUM BEHAV, V75, P643, DOI 10.1016/j.chb.2017.06.009
   Sheldon P, 2016, COMPUT HUM BEHAV, V58, P89, DOI 10.1016/j.chb.2015.12.059
   Sherman LE, 2018, CHILD DEV, V89, P37, DOI 10.1111/cdev.12838
   Shumaker C, 2017, FASH STYLE POP CULT, V4, P365, DOI 10.1386/fspc.4.3.365_1
   Slater A, 2017, BODY IMAGE, V22, P87, DOI 10.1016/j.bodyim.2017.06.004
   Svaldi J, 2011, PSYCHOTHER PSYCHOSOM, V80, P186, DOI 10.1159/000317538
   Svaldi J, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0154462
   Targhi AT, 2017, INT J NONLINEAR ANAL, V8, P291, DOI 10.22075/ijnaa.2017.10744.1522
   The-cocktail. com, OBS SOC MED 8 STAG
   Tiggemann M, 2018, J HEALTH PSYCHOL, V23, P1003, DOI 10.1177/1359105316639436
   Turner PG, 2017, EAT WEIGHT DISORD-ST, V22, P277, DOI 10.1007/s40519-017-0364-2
   Tuschen-Caffier B, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0145886
   von Wietersheim J, 2012, PSYCHOSOM MED, V74, P107, DOI 10.1097/PSY.0b013e31823ba787
   Webb JB, 2017, BODY IMAGE, V22, P53, DOI 10.1016/j.bodyim.2017.05.003
   Whyte C, 2016, J SOC CLIN PSYCHOL, V35, P822, DOI 10.1521/jscp.2016.35.10.822
   ZAJONC RB, 1968, J PERS SOC PSYCHOL, V9, P1, DOI 10.1037/h0025848
   Zhang XB, 2018, IEEE ACCESS, V6, P49056, DOI 10.1109/ACCESS.2018.2865754
NR 72
TC 7
Z9 7
U1 4
U2 23
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2071-1050
J9 SUSTAINABILITY-BASEL
JI Sustainability
PD JUL 2
PY 2019
VL 11
IS 14
AR 3977
DI 10.3390/su11143977
PG 18
WC Green & Sustainable Science & Technology; Environmental Sciences;
   Environmental Studies
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Science & Technology - Other Topics; Environmental Sciences & Ecology
GA IS6KX
UT WOS:000482261800221
OA Green Submitted, Green Accepted, gold
DA 2022-02-03
ER

PT J
AU Hart, S
AF Hart, Sydney
TI Between Security and Spectatorship: The Media of Transnational Mobility
   at Canadian Airports
SO INTERMEDIALITES
LA English
DT Article
AB As crucial nodes for networks of globalization and border security, Canada's major international airports include a wide range of media for the visual representation of human mobility, including artistic and cultural displays, biometric imaging, and x-ray scans. Does the visuality of media at airport security bear any epistemological relation to the visuality of art and cultural displays at airports? This cent paper analyzes the cultural patterns that come into focus when processes of security and spectatorship are examined through forms of power such as pastoral power and governmentality. Furthermore, Canada's major airports order mobility and security through modes of symbolic representation, thereby playing significant educational roles. Visual media at these airports thereby educate travelers on ways of navigating space, while obscuring the movements of those most negatively impacted by differential mobility.
C1 [Hart, Sydney] Queens Univ, Cultural Studies, Kingston, ON, Canada.
   [Hart, Sydney] Queens Univ, Film & Media, Kingston, ON, Canada.
C3 Queens University - Canada; Queens University - Canada
RP Hart, S (corresponding author), Queens Univ, Cultural Studies, Kingston, ON, Canada.; Hart, S (corresponding author), Queens Univ, Film & Media, Kingston, ON, Canada.
CR Amoore L, 2010, CULT GEOGR, V17, P299, DOI 10.1177/1474474010368604
   Amoore L, 2009, CITIZENSHIP STUD, V13, P17, DOI 10.1080/13621020802586628
   [Anonymous], 2019, SYNOPTIQUE, V8
   [Anonymous], 2017, REV COMMPOSITE, V20, P44
   Auge Marc, 1995, INTRO ANTHR SUPERMOD, P96
   Bonkers Borders Are, 2016, BORDERS ARE BONKERS
   BURLEY J, 2015, DARK MATTERS SURVEIL, P139
   Canadian Press, 2018, CBC NEWS
   Castells Manuel, 2010, RISE NETWORK SOC, V1, P453
   Chak Tings, 2014, UNDOCUMENTED ARCHITE
   Cowen Deborah, 2014, DEADLY LIFE LOGISTIC, P74
   Crary Jonathan, 1999, SUSPENSIONS PERCEPTI
   de Certeau Michel, 1984, PRACTICE EVERYDAY LI, P80
   Foucault M., 1977, SECURITY TERRITORY P
   Galloway Alexander, 2012, INTERFACE EFFECT, P98
   Greater Toronto Airports Authority (GTAA), 2005, VIS VIS ART ARCH AIR
   Hubregtse M, 2016, INTERIORS, V7, P155, DOI 10.1080/20419112.2016.1215678
   Jameson Fredric, 1991, POSTMODERNISM CULTUR, P399
   Kinkle Jeff, 2015, CARTOGRAPHIES ABSOLU, P7
   Laurence Robin, 2015, SENSE PLACE ART VANC, P33
   Lloyd Justine, 2003, SPACE CULT, V6, P94
   Lynch K, 1960, IMAGE CITY
   Massey Doreen, 1993, MAPPING FUTURES LOCA, P65
   Mattern S., 2018, PLACES J
   Mirzoeff N, 2011, SOC RES, V78, P1185
   Moscardo G, 2008, BUILDING COMMUNITY CAPACITY FOR TOURISM DEVELOPMENT, pIX
   Munoz Leslie, 2016, THESIS, P73
   Neilson B, 2008, BORDER METHOD MULTIP
   ODoherty Brian, 1976, INSIDE WHITE CUBE ID
   Parks L, 2007, J VIS CULT, V6, P183, DOI 10.1177/1470412907078559
   Salter Mark B., 2010, MAPPING TRANSATLANTI, P60
   Salter MarkB., 2005, GLOBAL SURVEILLANCE, P36
   Statistics Canada, 2016, AIR CARRIER TRAFFIC
   Transport Canada, 2017, TRANSP CAN COMPR REP
   Zureik Elia, 2014, STUDIES POLITICAL EC, V73, P117
NR 35
TC 0
Z9 0
U1 0
U2 0
PU CENTRE RECHERCHE INTERMEDIALITE
PI MONTREAL
PA PAVILLON LIONEL-GROULX, UNIV MONTREAL, MONTREAL, QC H3C 3J7, CANADA
SN 1705-8546
EI 1920-3136
J9 INTERMEDIALITES
JI Intermedialites
PD FAL
PY 2019
IS 34
PG 22
WC Humanities, Multidisciplinary
WE Emerging Sources Citation Index (ESCI)
SC Arts & Humanities - Other Topics
GA OA5VZ
UT WOS:000577853800005
DA 2022-02-03
ER

PT C
AU Truong, T
   Bhatt, A
   Queiroz, L
   Lai, K
   Yanushkevich, S
AF Truong, Thomas
   Bhatt, Aakash
   Queiroz, Leonardo
   Lai, Kenneth
   Yanushkevich, Svetlana
GP IEEE
TI Instance Segmentation of Personal Protective Equipment using a
   Multi-stage Transfer Learning Process
SO 2020 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS
   (SMC)
SE IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings
LA English
DT Proceedings Paper
CT IEEE International Conference on Systems, Man, and Cybernetics (SMC)
CY OCT 11-14, 2020
CL ELECTR NETWORK
DE instance segmentation; PPE detection; personal protective equipment;
   object detection
AB This paper focuses on the instance segmentation of soft attributes on humans such as clothing and personal protective equipment at a hazardous workplace. We propose the use of soft biometric object classes from the Open Images V5 and DeepFashion2 datasets to pre-train a mask segmentation network to detect and segment personal protective equipment in the workplace. Preliminary results of our proposed model achieves a mean average precision, mAP 50, of 61.7% with minimal optimization, resulting in very good segmentation of construction helmets, high visibility vests, welding masks, and ear protection in the workplace. Applications of the results from this paper include improving workplace safety in hazardous industries by providing a tool to ensure proper personal protective equipment usage while maintaining worker anonymity.
C1 [Truong, Thomas; Bhatt, Aakash; Queiroz, Leonardo; Lai, Kenneth; Yanushkevich, Svetlana] Univ Calgary, Biometr Technol Lab, Dept Elect & Comp Engn, Calgary, AB, Canada.
C3 University of Calgary
RP Truong, T (corresponding author), Univ Calgary, Biometr Technol Lab, Dept Elect & Comp Engn, Calgary, AB, Canada.
EM thomas.truong@ucalgary.ca; aakash.bhatt@ucalgary.ca;
   leonardo.queiroz@ucalgary.ca; kelai@ucalgary.ca; syanshk@ucalgary.ca
RI Yanushkevich, Svetlana/AAS-1052-2021
OI Yanushkevich, Svetlana/0000-0003-4794-9849
FU Natural Sciences and Engineering Research Council of Canada
   (NSERC)Natural Sciences and Engineering Research Council of Canada
   (NSERC)
FX This project was partially supported by the Natural Sciences and
   Engineering Research Council of Canada (NSERC) through a Strategic
   Project Grant "Biometric-enabled Identity management and Risk Assessment
   for Smart Cities".
CR Abdulla Waleed, 2017, MASK R CNN OBJECT DE
   Barro-Torres S, 2012, COMPUT COMMUN, V36, P42, DOI 10.1016/j.comcom.2012.01.005
   Benenson R., 2019, CVPR
   Di Benedetto M., 2019, CONTENT BASED MEDIA
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fang WL, 2018, ADV ENG INFORM, V37, P139, DOI 10.1016/j.aei.2018.05.003
   Ge Yuying, 2019, CVPR
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z
   Mneymneh BE, 2017, PROCEDIA ENGINEER, V196, P895, DOI 10.1016/j.proeng.2017.08.022
   Nath ND, 2020, AUTOMAT CONSTR, V112, DOI 10.1016/j.autcon.2020.103085
   Redmon Joseph, 2018, ARXIV
   Ren SQ, 2015, ADV NEUR IN, V28
   Rubaiyat AM, 2016, 2016 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE WORKSHOPS (WIW 2016), P135, DOI [10.1109/WIW.2016.10, 10.1109/WIW.2016.045]
NR 14
TC 2
Z9 2
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1062-922X
BN 978-1-7281-8526-2
J9 IEEE SYS MAN CYBERN
PY 2020
BP 1181
EP 1186
PG 6
WC Computer Science, Cybernetics; Computer Science, Information Systems
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BS1DD
UT WOS:000687430601032
DA 2022-02-03
ER

PT C
AU Tan, ZC
   Yang, J
   Shang, ZF
   Shi, GS
   Chang, SJ
AF Tan, Zechao
   Yang, Jie
   Shang, Zifeng
   Shi, Guangshun
   Chang, Shengjiang
BE Zhou, SM
   Wang, W
TI Minutiae-Based Offline Palmprint Identification System
SO PROCEEDINGS OF THE 2009 WRI GLOBAL CONGRESS ON INTELLIGENT SYSTEMS, VOL
   IV
LA English
DT Proceedings Paper
CT 1st WRI Global Congress on Intelligent Systems (GCIS 2009)
CY MAY 19-21, 2009
CL Xiamen, PEOPLES R CHINA
AB As a noteworthy biometric technology, offline palmprint identification plays a very important role in the application of social security Given a palmprint scanned by a digital instrument, offline palmprint identification needs to perform robust feature matching because of the rotation and distortion existed in input. In our paper we design and implement a minutiae-based offline palmprint identification system, which can realize the unrestricted matching between the complete palmprint and misshapen palmprint, therefore being able to identify of the owner of the palmprint. The paper presents the location, image enhancement, thinning, minutia feature extraction and feature matching in detail. The system framework and processing approach can overcome various issues caused by the limitation of condition in palmprint acquisition and achieve stable results The experiment based on the true date test proves the effectiveness and efficiency of the work in this article.
C1 [Tan, Zechao; Yang, Jie; Shang, Zifeng; Shi, Guangshun; Chang, Shengjiang] Nankai Univ, Inst Machine Intelligence, Tianjin, Peoples R China.
C3 Nankai University
RP Tan, ZC (corresponding author), Nankai Univ, Inst Machine Intelligence, Tianjin, Peoples R China.
EM gsshi@nankai.edu.cn
CR Cappelli R, 2000, INT C PATT RECOG, P471, DOI 10.1109/ICPR.2000.903586
   Hennings P, 2005, LECT NOTES COMPUT SC, V3656, P1081
   Li J., 2007, NEW POINT PATTERN MA, P302
   Li WX, 2005, IEEE T MULTIMEDIA, V7, P891, DOI 10.1109/TMM.2005.854380
   LIU YF, 2007, RES SKELETONIZATION, P633
   Noh JS, 2005, FOURTH ANNUAL ACIS INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE, PROCEEDINGS, P94
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   RUTOVITZ D, 1966, J R STAT SOC SER A-G, V129, P504, DOI 10.2307/2982255
   WANG JG, 2007, FUSION PALMPRINT PAL, P1
   Zhang D, 2005, J COMPUT SCI TECH-CH, V20, P70, DOI 10.1007/s11390-005-0008-2
   Zhang D, 2003, IEEE T PATTERN ANAL, V25, P1041, DOI 10.1109/TPAMI.2003.1227981
   ZHANG L, 2007, P IEEE INT C IM PROC, V2, P417
   ZHENG Y, 2007, SIGNAL IMAGE TECHNOL, P751
   ZHENG Y, 2007, RES OFFLINE PALMPRIN, P541
   ZHENG Y, 2007, PALMPRINT IMAGE QUAL, P140
NR 15
TC 3
Z9 3
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
BN 978-0-7695-3571-5
PY 2009
BP 466
EP 471
DI 10.1109/GCIS.2009.431
PG 6
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BNX33
UT WOS:000275817400094
DA 2022-02-03
ER

PT J
AU Al-Humaidan, NA
   Prince, M
AF Al-Humaidan, Norah A.
   Prince, Master
TI A Classification of Arab Ethnicity Based on Face Image Using Deep
   Learning Approach
SO IEEE ACCESS
LA English
DT Article
DE Databases; Faces; Feature extraction; Face recognition; Support vector
   machines; Deep learning; Labeling; Arab; convolutional neural network
   (CNN); deep learning; deep clustering; ethnicity
AB Human face and facial features gain a lot of attention from researchers and are considered as one of the most popular topics recently. Features and information extracted from a person are known as soft biometric, they have been used to improve the recognition performance and enhance the search engine for face images, which can be further applied in various fields such as law enforcement, surveillance videos, advertisement, and social media profiling. By observing relevant studies in the field, we noted a lack of mention of the Arab world and an absence of Arab dataset as well. Therefore, our aim in this paper is to create an Arab dataset with proper labeling of Arab sub-ethnic groups, then classify these labels using deep learning approaches. Arab image dataset that was created consists of three labels: Gulf Cooperation Council countries (GCC), the Levant, and Egyptian. Two types of learning were used to solve the problem. The first type is supervised deep learning (classification); a Convolutional Neural Network (CNN) pre-trained model has been used as CNN models achieved state of art results in computer vision classification problems. The second type is unsupervised deep learning (deep clustering). The aim of using unsupervised learning is to explore the ability of such models in classifying ethnicities. To our knowledge, this is the first time deep clustering is used for ethnicity classification problems. For this, three methods were chosen. The best result of training a pre-trained CNN on the full Arab dataset then evaluating on a different dataset was 56.97%, and 52.12% when Arab dataset labels were balanced. The methods of deep clustering were applied on different datasets, showed an ACC from 32% to 59%, and NMI and ARI result from zero to 0.2714 and 0.2543 respectively.
C1 [Al-Humaidan, Norah A.; Prince, Master] Qassim Univ, Dept Comp Sci, Mulaydha 51452, Saudi Arabia.
C3 Qassim University
RP Al-Humaidan, NA (corresponding author), Qassim Univ, Dept Comp Sci, Mulaydha 51452, Saudi Arabia.
EM noura.ah1493@gmail.com
RI Prince, Master/AAY-6786-2021; , Prince/AAP-5396-2021
OI , Prince/0000-0002-2703-4580
FU Qassim University, Saudi Arabia
FX This work was supported by the Qassim University, Saudi Arabia, to
   complete Master Thesis under the course M.S. in computer science.
CR Ahmed M., 2018, DATA ENG INTELLIGENT, P217
   Anwar I, 2017, CYBERN INF TECHNOL, V17, P152, DOI 10.1515/cait-2017-0036
   Barz B, 2020, IEEE WINT CONF APPL, P1360, DOI 10.1109/WACV45572.2020.9093286
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen H., 2016, TECH REP
   Dalal N., 2010, IEEE COMP SOC C COMP, P886
   Dantcheva A, 2016, IEEE T INF FOREN SEC, V11, P441, DOI 10.1109/TIFS.2015.2480381
   Ding H., 2013, IEEE INT SYMPOS ROBO, V2013, P1, DOI [10.1109/ISR.2013.6695707, DOI 10.1109/ISR.2013.6695707]
   Dobies, SIMPLE IMAGE DOWNLOA
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Github.io, UTKFACE
   Grira N., 2004, REV MACHINE LEARNING, V1, P9
   Gudi A., 2016, ARXIV151200743
   Guo X., 2018, P 10 ASIAN C MACH LE, P550
   Guo Xifeng, 2017, P 26 INT JOINT C ART, DOI [10.24963/ijcai.2017/243, DOI 10.24963/IJCAI.2017/243]
   He K., 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Heng Z, 2018, P IEEE INT S CIRCUIT, P1
   Hu YX, 2010, LECT NOTES COMPUT SC, V5916, P66
   Jain AK, 2004, LECT NOTES COMPUT SC, V3072, P731
   Jewell E. J., 2001, NEW OXFORD AM DICT
   Krizhevsky A., 2012, INT C NEUR INF PROC, V25, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
   Kuhn HW., 1955, NORDIC J COMPUTING, V2, P83, DOI 10.1002/nav.3800020109
   Kumar N, 2008, LECT NOTES COMPUT SC, V5305, P340, DOI 10.1007/978-3-540-88693-8_25
   Labatut V., 2015, INT J SOC NETW MIN, V2, P44, DOI DOI 10.1504/IJSNM.2015.069776
   MeiWang andWeihong, 2020, P IEEE CVF C COMP VI
   Min EX, 2018, IEEE ACCESS, V6, P39501, DOI 10.1109/ACCESS.2018.2855437
   Mrabah N, 2020, NEURAL NETWORKS, V130, P206, DOI 10.1016/j.neunet.2020.07.005
   Muhammad G, 2012, INT J ARTIF INTELL T, V21, DOI 10.1142/S0218213012500194
   Narang N, 2016, INT CONF BIOMETR
   Niinuma K, 2010, IEEE T INF FOREN SEC, V5, P771, DOI 10.1109/TIFS.2010.2075927
   Paperswithcode.com, PAPERS CODE DEEP CLU
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X
   Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI [10.1162/NECO_a_00990, 10.1162/neco_a_00990]
   Rezaei M, 2016, IEEE T KNOWL DATA EN, V28, P2173, DOI 10.1109/TKDE.2016.2551240
   Santos Diego Junior da Silva, 2010, Dental Press J. Orthod., V15, P121
   Scikit-learn.org, 23 CLUSTERING SCIKIT
   Sein M. M., 2011, P INT C ADV COMP ENG, P1
   Srinivas N, 2017, IEEE INT CONF AUTOMA, P953, DOI 10.1109/FG.2017.118
   Veropoulos K, 2005, LECT NOTES COMPUT SC, V3804, P207
   Wang CR, 2018, MULTIMED TOOLS APPL, V77, P30311, DOI 10.1007/s11042-018-6018-1
   Wang M, 2019, IEEE I CONF COMP VIS, P692, DOI 10.1109/ICCV.2019.00078
   Wang W, 2016, LECT NOTES COMPUT SC, V9967, P176, DOI 10.1007/978-3-319-46654-5_20
   Xie JY, 2016, PR MACH LEARN RES, V48
   YU C, 2014, P CHIN C BIOMETRIC R, P136
NR 46
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2021
VL 9
BP 50755
EP 50766
DI 10.1109/ACCESS.2021.3069022
PG 12
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Telecommunications
GA RK6FI
UT WOS:000638389300001
OA gold
DA 2022-02-03
ER

PT J
AU GOUGH, JJ
   KENT, JT
   OHIGGINS, P
   ELLISON, LT
AF GOUGH, JJ
   KENT, JT
   OHIGGINS, P
   ELLISON, LT
TI VARIOGRAM METHODS FOR THE ANALYSIS OF BONY TRABECULAR SHADOWS IN PLAIN
   RADIOGRAPHS
SO INTERNATIONAL JOURNAL OF BIO-MEDICAL COMPUTING
LA English
DT Article
DE CANCELLOUS BONE; TRABECULAE; RADIOGRAPHS; VARIOGRAMS; AGE CHANGES; SEX
   DIFFERENCES
ID MINERAL DENSITY; WOMEN
AB Conventionally, trabecular morphology is described from thin sections on the basis of visual inspection and basic biometric parameters such as mean trabecular plate thickness, bone density, etc. In the clinical situation non-invasive studies of mineralised tissues may be undertaken using densitometric measurements. A little-explored non-invasive alternative is to study the organisation of shadows of trabeculae in plain radiographs. In radiographic images cancellous bone appears as a texture made up of the superimposed shadows of trabeculae from many planes. In this paper some early attempts to adapt modern image processing techniques to the analysis of trabecular architecture are reviewed. A new statistical technique based on variograms is applied to the investigation of anisotropy in trabecular radiographs and to changes in anisotropy with age. The results of regression analyses of variograms indicate that they can clearly demonstrate age-related changes in trabecular architecture and discriminate between the pattern of ageing in males and females. This approach shows promise as a useful research and investigative tool in a variety of contexts.
C1 UNIV WESTERN AUSTRALIA, DEPT ANAT & HUMAN BIOL, PERTH, WA 6009, AUSTRALIA.
   NATL RIVERS AUTHOR, RIVERS HOUSE, 21 PK SQ S, LEEDS LS1 2QG, ENGLAND.
   UNIV LEEDS, SCH MATH, DEPT STAT, LEEDS LS2 9JT, W YORKSHIRE, ENGLAND.
C3 University of Western Australia; University of Leeds
CR AARON J E, 1989, Clinical Rheumatology, V8, P84, DOI 10.1007/BF02207240
   AARON JE, 1987, CLIN ORTHOP RELAT R, P260
   AARON JE, 1989, CLIN ORTHOP RELAT R, V243, P295
   ATKINSON P. J., 1967, CALCIFIED TISSUE RES, V1, P24, DOI 10.1007/BF02008071
   BARQUERO LD, 1992, BONE MINER, V18, P159
   BEDDOE AH, 1978, CALC TISS RES, V25, P273, DOI 10.1007/BF02010781
   BEDDOE AH, 1976, PHYS MED BIOL, V21, P589, DOI 10.1088/0031-9155/21/4/010
   BIRKENHAGERFRENKEL DH, 1988, BONE MINER, V4, P197
   COHN SH, 1977, METABOLISM, V26, P171, DOI 10.1016/0026-0495(77)90052-X
   DURAND EP, 1991, J COMPUT ASSIST TOMO, V15, P133, DOI 10.1097/00004728-199101000-00021
   GERAETS WGM, 1990, J BONE MINER RES, V5, P227, DOI 10.1002/jbmr.5650050305
   HARALICK RM, 1979, P IEEE, V67, P786, DOI 10.1109/PROC.1979.11328
   LYNCH JA, 1991, MED INFORM, V16, P241, DOI 10.3109/14639239109012130
   Murray P.D.F., 1936, BONES STUDY DEV STRU
   NORDIN BEC, 1987, CALCIFIED TISSUE INT, V40, P57, DOI 10.1007/BF02555705
   OXNARD CE, 1981, S ZOOL SOC LOND, V46, P127
   Oxnard Charles E., 1973, FORM PATTERN HUMAN E
   PARFITT AM, 1987, J BONE MINER RES, V2, P595, DOI 10.1002/jbmr.5650020617
   REID IR, 1992, J BONE MINER RES, V7, P1221
   ROCKOFF SD, 1970, PROGR METHODS BONE M, P331
   Rosenfeld A., 1982, DIGITAL PICTURE PROC
   Serra J., 1982, IMAGE ANAL MATH MORP
   SOLOMON L, 1979, LANCET, V2, P1326
   STEIGER P, 1992, J BONE MINER RES, V7, P625
   VANDERSTELT PF, 1979, PERIAPICAL BONE LESI
   WAGSTAFFE WW, 1974, ST THOMAS HOSP REP, V5, P192
   WAKAMATU.E, 1969, CALC TISS RES, V4, P147, DOI 10.1007/BF02279116
   YANG HCL, 1978, THESIS U CHICAGO CHI
   [No title captured]
NR 29
TC 4
Z9 4
U1 0
U2 0
PU ELSEVIER IRELAND LTD
PI CLARE
PA ELSEVIER HOUSE, BROOKVALE PLAZA, EAST PARK SHANNON, CO, CLARE, 00000,
   IRELAND
SN 0020-7101
J9 INT J BIOMED COMPUT
JI Int. J. Bio-Med. Comput.
PD MAR
PY 1994
VL 35
IS 2
BP 141
EP 153
DI 10.1016/0020-7101(94)90063-9
PG 13
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Theory & Methods; Engineering, Biomedical; Medical Informatics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Medical Informatics
GA ND371
UT WOS:A1994ND37100007
PM 8194865
DA 2022-02-03
ER

PT J
AU Freitag, GP
   de Lima, LGF
   Jacomini, JA
   Kozicki, LE
   Ribeiro, LB
AF Freitag, Giovanna Polo
   de Lima, Luis Gustavo Freitag
   Jacomini, Julia Aparecida
   Kozicki, Luiz Ernandes
   Ribeiro, Leonir Bueno
TI An Accurate Image Analysis Method for Estimating Body Measurements in
   Horses
SO JOURNAL OF EQUINE VETERINARY SCIENCE
LA English
DT Article
DE Biometrics; Equine; Conformation; Morphological features
ID EQUINE CONFORMATION; PERFORMANCE; TRAITS; COHORT
AB This study proposes a standardized image analysis method for assessing horse conformation. A total of 34 adult American Quarter Horses (mean age = 6.7 +/- 2.9 years, body weight = 489 +/- 44 kg) were used. Reference angular measurements were performed using a digital goniometer, and reference linear measurements were determined using a standard measuring tape. For image analysis, photographs from the animal's left profile were taken at a distance of 3 m using eight markers to facilitate visualization. Images were processed using ImageJ software. Reference and estimated values showed great accuracy, with v 2 < 1%, Pearson's correlation coefficient > 0.999 ( P < . 001), and coefficient of variation < 2.70. Image analysis and reference results were similar ( P > . 05) for most variables. The highest coefficient of variation (3.05%) was observed for coxae-femur angle, where the software obtained an underestimation of 2.8 degrees compared with the reference method. Body length resulted in an overestimation of 3.4 cm in relation to reference values, but coefficient of variation was low (1.54%). Discrepancies between estimated and reference results can be attributed to possible errors when collecting biometric data using portable measuring tools, angles and long measurements being more difficult to obtain. Software analysis of standardized horse photographs is an accurate and precise method for obtaining body measurements and is therefore recommended for future studies on horse conformation.
   (c) 2021 Elsevier Inc. All rights reserved.
C1 [Freitag, Giovanna Polo; Jacomini, Julia Aparecida; Ribeiro, Leonir Bueno] Univ Estadual Maringa, Dept Anim Sci, BR-87020900 Maringa, Parana, Brazil.
   [Freitag, Giovanna Polo; Kozicki, Luiz Ernandes] Pontifical Catholic Univ, Dept Agr Sci & Vet Med, Curitiba, Parana, Brazil.
C3 Universidade Estadual de Maringa; Pontificia Universidade Catolica do
   Parana
RP Freitag, GP (corresponding author), Univ Estadual Maringa, Dept Anim Sci, BR-87020900 Maringa, Parana, Brazil.
EM giovanna_w7@hotmail.com
CR Anderson TM, 2004, EQUINE VET J, V36, P571, DOI 10.2746/0425164044864462
   Anderson TM, 2004, EQUINE VET J, V36, P563, DOI 10.2746/0425164044864507
   Cervantes I, 2009, LIVEST SCI, V125, P43, DOI 10.1016/j.livsci.2009.03.006
   Costa MD, 2016, ARQ BRAS MED VET ZOO, V68, P1629, DOI 10.1590/1678-4162-8884
   dos Santos MR, 2017, LIVEST SCI, V206, P24, DOI 10.1016/j.livsci.2017.10.009
   Salazar-Vidal DF, 2012, VET ZOOTEC-COLOMBIA, V6, P66
   Gmel AI, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0202931
   Janczarek I, 2017, PFERDEHEILKUNDE, V33, P139, DOI 10.21836/PEM20170205
   Mariz T. M. de A., 2015, Acta Veterinaria Brasilica, V9, P362
   Perez-Ruiz M, 2020, COMPUT ELECTRON AGR, V174, DOI 10.1016/j.compag.2020.105510
   POLY J, 1967, ANN ZOOTECH, V16, P183, DOI 10.1051/animres:19670204
   Smith AM, 2006, J EQUINE VET SCI, V26, P212, DOI 10.1016/j.jevs.2006.03.002
   Suontama M, 2013, J ANIM BREED GENET, V130, P178, DOI 10.1111/j.1439-0388.2012.01011.x
   van Weeren PR, 2006, EQUINE VET J, V38, P591, DOI 10.2746/042516406X159007
   Weller R, 2006, EQUINE VET J, V38, P622, DOI 10.2746/042516406X159034
   Weller R, 2006, EQUINE VET J, V38, P616, DOI 10.2746/042516406X150394
   Weller R, 2006, EQUINE VET J, V38, P610, DOI 10.2746/042516406X150367
NR 17
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER SCIENCE INC
PI NEW YORK
PA STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA
SN 0737-0806
EI 1542-7412
J9 J EQUINE VET SCI
JI J. Equine Vet. Sci.
PD JUN
PY 2021
VL 101
AR 103418
DI 10.1016/j.jevs.2021.103418
PG 7
WC Veterinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Veterinary Sciences
GA SD8EK
UT WOS:000651608100001
PM 33993939
DA 2022-02-03
ER

PT C
AU Laibacher, T
   Weyde, T
   Jalali, S
AF Laibacher, Tim
   Weyde, Tillman
   Jalali, Sepehr
GP IEEE
TI M2U-Net: Effective and Efficient Retinal Vessel Segmentation for
   Real-World Applications
SO 2019 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   WORKSHOPS (CVPRW 2019)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
LA English
DT Proceedings Paper
CT 32nd IEEE/CVF Conference on Computer Vision and Pattern Recognition
   (CVPR)
CY JUN 16-20, 2019
CL Long Beach, CA
ID ACCURATE; IMAGES
AB In this paper, we present a novel neural network architecture for retinal vessel segmentation that improves over the state of the art on two benchmark datasets, is the first to run in real time on high resolution images, and its small memory and processing requirements make it deployable in mobile and embedded systems.
   The M2U-Net has a new encoder-decoder architecture that is inspired by the U-Net. It adds pretrained components of MobileNetV2 in the encoder part and novel contractive bottleneck blocks in the decoder part that, combined with bilinear upsampling, drastically reduce the parameter count to 0.55M compared to 31.03M in the original U-Net.
   We have evaluated its performance against a wide body ofpreviously published results on three public datasets. On two of them, the M2 U-Net achieves new state-of-the-art performance by a considerable margin. When implemented on a GPU, our method is the first to achieve real-time inference speeds on high -resolution fundus images. We also implemented our proposed network on an ARM-based embedded system where it segments images in between 0.6 and 15 sec, depending on the resolution. Thus, the M2U-Net enables a number of applications of retinal vessel structure extraction, such as early diagnosis of eye diseases, retinal biometric authentication systems, and robot assisted microsurgery.
C1 [Laibacher, Tim; Weyde, Tillman] City Univ London, London, England.
   [Jalali, Sepehr] UCL, London, England.
C3 City University London; University of London; University College London
RP Laibacher, T (corresponding author), City Univ London, London, England.
EM tim.laibacher@city.ac.uk; t.e.weyde@city.ac.uk; s.jalali@ucl.ac.uk
FU Sir Michael Uren Foundation [R170010A]
FX This research is carried out in collaboration with The London Project to
   Cure Blindness, ORBIT, Institute of Ophthalmology, University College
   London (UCL), London, UK., NIHR Biomedical Research Centre at Moorfields
   Eye Hospital NHS Foundation Trust, UCL Institute of Ophthalmology,
   London, UK., Moorfields Eye Hospital NHS Foundation Trust, London, UK.
   and is partly funded by The Sir Michael Uren Foundation R170010A.
CR Abramoff Michael D, 2010, IEEE Rev Biomed Eng, V3, P169, DOI 10.1109/RBME.2010.2084567
   Ahmed MI, 2014, INT J MACH LEARN CYB, V5, P933, DOI 10.1007/s13042-013-0179-z
   Akram M. U., 2011, 2011 6th International Conference for Internet Technology and Secured Transactions (ICITST), P180
   Annunziata R, 2016, IEEE J BIOMED HEALTH, V20, P1129, DOI 10.1109/JBHI.2015.2440091
   [Anonymous], 2018, TENS DYN NEUR NETW P
   Argello F., 2014, J REAL-TIME IMAGE PR, V14, P773
   Azzopardi G, 2015, MED IMAGE ANAL, V19, P46, DOI 10.1016/j.media.2014.08.002
   Bendaoudi H, 2018, J REAL-TIME IMAGE PR, V15, P31, DOI 10.1007/s11554-016-0661-4
   Bibiloni P., 2018, J REAL TIME IMAGE PR
   Bloice M. D., 2017, ARXIV E PRINTS
   Braun D., 2018, INT J MED ROBOT, V14, P1, DOI DOI 10.3389/FNAGI.2018
   Budai A., 2013, ROBUST VESSEL SEGMEN
   Ching T, 2018, J R SOC INTERFACE, V15, DOI 10.1098/rsif.2017.0387
   De Fauw J, 2018, NAT MED, P1
   Dukhan M., 2018, ACCELERATION PACKAGE
   Figueiredo IN, 2016, COMPUT BIOL MED, V79, P130, DOI 10.1016/j.compbiomed.2016.09.019
   Fraz MM, 2012, IEEE T BIO-MED ENG, V59, P2538, DOI 10.1109/TBME.2012.2205687
   Huazhu Fu, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P132, DOI 10.1007/978-3-319-46723-8_16
   Hubbard LD, 1999, OPHTHALMOLOGY, V106, P2269, DOI 10.1016/S0161-6420(99)90525-0
   Iglovikov V., 2018, TERNAUSNETV2 FULLY C, P233
   Orlando JI, 2017, IEEE T BIO-MED ENG, V64, P16, DOI 10.1109/TBME.2016.2535311
   Jiang ZX, 2017, BIOCYBERN BIOMED ENG, V37, P412, DOI 10.1016/j.bbe.2017.04.001
   Kohler T, 2013, 2013 IEEE 26TH INTERNATIONAL SYMPOSIUM ON COMPUTER-BASED MEDICAL SYSTEMS (CBMS), P95, DOI 10.1109/CBMS.2013.6627771
   Koukounis D, 2014, INTEGRATION, V47, P377, DOI 10.1016/j.vlsi.2013.11.005
   Li QL, 2016, IEEE T MED IMAGING, V35, P109, DOI 10.1109/TMI.2015.2457891
   Liskowski P, 2016, IEEE T MED IMAGING, V35, P2369, DOI 10.1109/TMI.2016.2546227
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Loshchilov I, 2018, FIXING WEIGHT DECAY
   Maninis K. K., 2016, MED IMAGE COMPUTING
   Marin D, 2011, IEEE T MED IMAGING, V30, P146, DOI 10.1109/TMI.2010.2064333
   Marino C, 2006, PATTERN ANAL APPL, V9, P21, DOI 10.1007/s10044-005-0022-6
   Odstrcilik J, 2013, IET IMAGE PROCESS, V7, P373, DOI 10.1049/iet-ipr.2012.0455
   Ortega M, 2009, J VISUAL LANG COMPUT, V20, P80, DOI 10.1016/j.jvlc.2009.01.006
   Pont-Tuset J, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2481406
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roychowdhury S, 2015, IEEE J BIOMED HEALTH, V19, P1118, DOI 10.1109/JBHI.2014.2335617
   Sandler M., 2018, INVERTED RESIDUALS L
   Sifre L., 2014, THESIS ECOLE POLYTEC
   Simon, 1935, NEW YORK STATE J MED, V35, P901
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/CVPR.2015.7298594
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Tianqi Chen, 2018, SYSML 2018
   Xu XY, 2016, SCI REP-UK, V6, DOI 10.1038/srep34603
   Yan ZQ, 2018, IEEE T BIO-MED ENG, V65, P1912, DOI 10.1109/TBME.2018.2828137
   Zhang J, 2016, IEEE T MED IMAGING, V35, P2631, DOI 10.1109/TMI.2016.2587062
NR 46
TC 4
Z9 4
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2160-7508
BN 978-1-7281-2506-0
J9 IEEE COMPUT SOC CONF
PY 2019
BP 115
EP 124
DI 10.1109/CVPRW.2019.00020
PG 10
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BP9NT
UT WOS:000569983600014
OA Green Accepted
DA 2022-02-03
ER

PT C
AU Aruleba, I
   Viriri, S
AF Aruleba, Idowu
   Viriri, Serestina
BE Rojas, I
   Joya, G
   Catala, A
TI Enhanced Convolutional Neural Network for Age Estimation
SO ADVANCES IN COMPUTATIONAL INTELLIGENCE, IWANN 2021, PT I
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 16th International Work-Conference on Artificial Neural Networks (IWANN)
CY JUN 16-18, 2021
CL ELECTR NETWORK
DE Age estimation; Pre-processing; Feature extraction; Convolutional Neural
   Network
ID CLASSIFICATION
AB The human face constitute various biometric features that could be used to estimate an important detail such as age. Variations in facial landmarks and appearances have presented challenges to automated age estimation. This account for limitations attributed to conventional approaches such as the traditional hand-crafted method, which cannot efficiently and adequately estimate age. In this study, a six layered Convolutional Neural Network (CNN) were proposed, which extract features from facial images taken in an uncontrolled environment, and classifies them into appropriate classes. Since a huge datasets is needed to obtain good accuracy from the trained model and minimize overfitting, data augmentation was performed on the datasets to balance the number of images in each class. The UTKFace dataset was used to train the model while validation was carried out on FGNET dataset. With the proposed novel method, an accuracy of 89.75% was recorded on the UTKFace dataset, which is a significant improvement over existing state-of-the-art methods previously implemented on the UTKFace dataset.
C1 [Aruleba, Idowu; Viriri, Serestina] Univ KwaZulu Natal, Sch Math Stat & Comp Sci, Durban, South Africa.
C3 University of Kwazulu Natal
RP Viriri, S (corresponding author), Univ KwaZulu Natal, Sch Math Stat & Comp Sci, Durban, South Africa.
EM viriris@ukzn.ac.za
CR Agbo-Ajala O., 2020, SCI WORLD J, V2020
   Agbo-Ajala O, 2019, LECT NOTES ARTIF INT, V11683, P316, DOI 10.1007/978-3-030-28377-3_26
   Anand A., 2017, 2017 IEEE S SER COMP, DOI 10.1109/ssci.2017.8285381
   Angulu R, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0278-6
   Das A, 2019, LECT NOTES COMPUT SC, V11129, P573, DOI 10.1007/978-3-030-11009-3_35
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Huang J, 2017, MULTIMED TOOLS APPL, V76, P20231, DOI 10.1007/s11042-017-4646-5
   Huerta I, 2015, PATTERN RECOGN LETT, V68, P239, DOI 10.1016/j.patrec.2015.06.006
   Kwon YH, 1999, COMPUT VIS IMAGE UND, V74, P1, DOI 10.1006/cviu.1997.0549
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   Liu H, 2019, IEEE T CIRC SYST VID, V29, P486, DOI 10.1109/TCSVT.2017.2782709
   Mualla N, 2018, INT J ADV COMPUT SC, V9, P152
   Niu ZX, 2016, PROC CVPR IEEE, P4920, DOI 10.1109/CVPR.2016.532
   Oloko-Oba Mustapha, 2020, Image and Signal Processing. 9th International Conference, ICISP 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12119), P151, DOI 10.1007/978-3-030-51935-3_16
   Qiu J., 2016, CONVOLUTIONAL NEURAL
   Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3
   Savchenko AV, 2019, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.197
   Sendik O, 2019, SIGNAL PROCESS-IMAGE, V78, P368, DOI 10.1016/j.image.2019.08.003
   Sithungu S, 2019, LECT NOTES BUS INF P, V354, P245, DOI 10.1007/978-3-030-20482-2_20
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Yi D, 2015, LECT NOTES COMPUT SC, V9005, P144, DOI 10.1007/978-3-319-16811-1_10
   Zeng JF, 2018, IEEE ACCESS, V6, P66715, DOI 10.1109/ACCESS.2018.2877706
   Zhang Z., 2016, 4 INT C MACH MAT INF, P2017
NR 23
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-030-85030-2; 978-3-030-85029-6
J9 LECT NOTES COMPUT SC
PY 2021
VL 12861
BP 385
EP 394
DI 10.1007/978-3-030-85030-2_32
PG 10
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BS1TP
UT WOS:000696173400032
DA 2022-02-03
ER

PT J
AU LOMBARTE, A
   MORALESNIN, B
AF LOMBARTE, A
   MORALESNIN, B
TI MORPHOLOGY AND ULTRASTRUCTURE OF SACCULAR OTOLITHS FROM 5 SPECIES OF THE
   GENUS COELORINCHUS (GADIFORMES, MACROURIDAE) FROM THE SOUTHEAST ATLANTIC
SO JOURNAL OF MORPHOLOGY
LA English
DT Article
ID FUNDULUS-HETEROCLITUS; ORGANIC MATRIX; FISH OTOLITHS; GROWTH; DEPTH;
   AGE; TEMPERATURE; MERLUCCIUS; CAPENSIS; PISCES
AB The morphology of the saccular otolith (sagitta) of five species of the genus Coelorinchus (grenadiers) from the southeastern Atlantic (C. coelorinchus, C. fasciatus, C. occa, C. braueri, and C. acanthiger) was analyzed macro- and ultrastructurally by digital image processing and scanning electron microscopy (SEM). Interspecies variations have been noted in the morphology and ultrastructure of the sulcus acusticus and are linked to differences in form of the entire otolith. This relationship may indicate a genetic regulation in the growth of the sulcus. Standard canonical analysis of biometric measurements of the otolith and acoustic sulcus has proved to be a good way to distinguish otoliths of the five species of Coelorinchus. (C) 1995 Wiley-Liss, Inc.
C1 CSIC,INST ESTUDIS AVANCATS ILLES BALEARS,E-07071 PALMA DE MALLORCA,SPAIN.
C3 Consejo Superior de Investigaciones Cientificas (CSIC)
RP LOMBARTE, A (corresponding author), CSIC,INST CIENCIES MAR,PLACA MAR S-N,E-08039 BARCELONA,SPAIN.
RI Lombarte, Antoni/D-3142-2013
OI Morales-Nin, Beatriz/0000-0002-7264-0918; Lombarte,
   Antoni/0000-0001-5215-4587
CR BORI C, 1986, Investigacion Pesquera (Barcelona), V50, P247
   Botha L., 1971, INV REP DIV SEA FISH, V97, P1
   CAMPANA SE, 1985, CAN J FISH AQUAT SCI, V42, P1014, DOI 10.1139/f85-127
   Cohen D.M., 1990, FAO FISHERIES SYNOPS, V10
   CUADRAS CM, 1991, METODOS ANAL MULTIVA
   DEGENS ET, 1969, MAR BIOL, V2, P105, DOI 10.1007/BF00347005
   DUNKELBERGER DG, 1980, J MORPHOL, V163, P367, DOI 10.1002/jmor.1051630309
   FERSON S, 1985, SYST ZOOL, V34, P59, DOI 10.2307/2413345
   GAEMERS PAM, 1976, WEKGROEP TERT KWART, V131, P3
   GAGO FJ, 1993, B MAR SCI, V52, P949
   GAULDIE RW, 1993, J MORPHOL, V216, P271, DOI 10.1002/jmor.1052160304
   GAULDIE RW, 1990, COMP BIOCHEM PHYS A, V97, P449, DOI 10.1016/0300-9629(90)90111-5
   GAULDIE RW, 1990, COMP BIOCHEM PHYS A, V97, P119, DOI 10.1016/0300-9629(90)90159-P
   GAULDIE RW, 1991, ACTA ZOOL-STOCKHOLM, V72, P159, DOI 10.1111/j.1463-6395.1991.tb00943.x
   KALISH JM, 1989, J EXP MAR BIOL ECOL, V132, P151, DOI 10.1016/0022-0981(89)90126-3
   LEFEBVRE J, 1976, INTRO AUX ANAL STATI
   Lloris D, 1986, MONOGR ZOOL MAR, V1, P9
   LOMBARTE A, 1991, CAN J ZOOL, V69, P2442, DOI 10.1139/z91-343
   LOMBARTE A, 1991, Scientia Marina, V55, P413
   LOMBARTE A, 1993, ENVIRON BIOL FISH, V37, P297, DOI 10.1007/BF00004637
   LOMBARTE A, 1992, J MORPHOL, V214, P97, DOI 10.1002/jmor.1052140107
   LOMBARTE A, 1989, COLL SCI PAP INT COM, V16, P191
   Morales-Nin B., 1987, P331
   Morales-Nin B., 1992, SCI MARINA, V57, P95
   MORALESNIN B, 1980, INVEST PESQ, V44, P305
   MORALESNIN B, 1985, INVEST PESQ, V49, P379
   MORALESNIN B, 1986, S AFR J MARINE SCI, V4, P3, DOI DOI 10.2989/025776186784461639
   MORALESNIN B, 1984, MONOGRAFIAS U BARCEL
   MORALESNIN B, 1993, STUDY ASSESSMENT M 2, P56
   Mugiya Y., 1964, Bulletin of the Japanese Society of Scientific Fisheries, V30, P955
   MUGIYA Y, 1987, FISH B-NOAA, V85, P395
   Nolf D., 1985, HDB PALEOICHTHYOLOGY, V10
   NOLF D, 1989, SCI SER NAT HIST MUS, V32, P89
   Platt C., 1981, HEARING SOUND COMMUN, P3
   RADTKE RL, 1989, CAN J FISH AQUAT SCI, V46, P1884, DOI 10.1139/f89-237
   RADTKE RL, 1984, POLAR BIOL, V3, P203, DOI 10.1007/BF00292624
   RADTKE RL, 1990, J FISH BIOL UK, V35, P485
   ROHLF FJ, 1984, SYST ZOOL, V33, P302, DOI 10.2307/2413076
   ROHLF FJ, 1983, NATO ASI SERIES G, V1, P583
   Schmidt W., 1969, P393
   Schwarzhans W., 1980, Berliner Geowissenschaftliche Abhandlungen Reihe A Geologie und Palaeontologie, V26, P1
   Smith M.M., 1986, SMITHS SEA FISHES
   Sneath P. H., 1973, NUMERICAL TAXONOMY
   SOKAL ROBERT R., 1962, TAXON, V11, P33, DOI 10.2307/1217208
   SOKAL ROBERT R., 1958, UNIV KANSAS SCI BULL, V38, P1409
   TURON JM, 1986, DATOS INFORMATIVOS I, V17, P1
   WATABE N, 1982, J EXP MAR BIOL ECOL, V58, P127, DOI 10.1016/0022-0981(82)90100-9
   WATABE N, 1981, PROG CRYST GROWTH CH, V4, P99, DOI 10.1016/0146-3535(81)90050-2
   WILSON RR, 1985, COPEIA, P1011
   WOODHEAD PM, 1968, J MAR BIOL ASSOC UK, V48, P81, DOI 10.1017/S0025315400032434
   ZHANG Y, 1992, J MORPHOL, V211, P1
NR 51
TC 20
Z9 20
U1 0
U2 8
PU WILEY-LISS
PI NEW YORK
PA DIV JOHN WILEY & SONS INC 605 THIRD AVE, NEW YORK, NY 10158-0012
SN 0362-2525
J9 J MORPHOL
JI J. Morphol.
PD AUG
PY 1995
VL 225
IS 2
BP 179
EP 192
DI 10.1002/jmor.1052250204
PG 14
WC Anatomy & Morphology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Anatomy & Morphology
GA RN351
UT WOS:A1995RN35100003
PM 29865328
DA 2022-02-03
ER

PT C
AU Barni, M
   Bianchi, T
   Catalano, D
   Di Raimondo, M
   Labati, RD
   Failla, P
AF Barni, Mauro
   Bianchi, Tiziano
   Catalano, Dario
   Di Raimondo, Mario
   Labati, Ruggero Donida
   Failla, Pierluigi
GP ACM
TI Privacy-Preserving Fingercode Authentication
SO MM&SEC 2010: 2010 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP,
   PROCEEDINGS
LA English
DT Proceedings Paper
CT 12th ACM Multimedia Security Workshop
CY SEP 09-10, 2010
CL Univ Studi Roma TRE, Roma, ITALY
HO Univ Studi Roma TRE
ID FINGERPRINT; EFFICIENT
AB We present a privacy preserving protocol for fingerprint-based authentication. We consider a scenario where a client equipped with a fingerprint reader is interested into learning if the acquired fingerprint belongs to the database of authorized entities managed by a server. For security, it is required that the client does not learn anything on the database and the server should not get any information about the requested biometry and the outcome of the matching process. The proposed protocol follows a multi-party computation approach and makes extensive use of homomorphic encryption as underlying cryptographic primitive. To keep the protocol complexity as low as possible, a particular representation of fingerprint images, named Fingercode, is adopted. Although the previous works on privacy-preserving biometric identification focus on selecting the best matching identity in the database, our main solution is a generic identification protocol and it allows to select and report all the enrolled identities whose distance to the user's fingercode is under a given threshold. Variants for simple authentication purposes are provided. Our protocols gain a notable bandwidth saving (about 25 - 39%) if compared with the best, previous work [1] and its computational complexity is still low and suitable for practical applications. Moreover, even if such protocols are presented in the context of a fingerprint-based system, they can be generalized to any biometric system that shares the same matching methodology, namely distance computation and thresholding.
C1 [Barni, Mauro; Failla, Pierluigi] Univ Siena, Dipartimento Ingn Informaz, I-53100 Siena, Italy.
C3 University of Siena
RP Barni, M (corresponding author), Univ Siena, Dipartimento Ingn Informaz, I-53100 Siena, Italy.
EM barni@dii.unisi.it; tiziano.bianchi@unifi.it; catalano@dmi.unict.it;
   diraimondo@dmi.unict.it; ruggero.donida@unimi.it;
   pierluigi.failla@gmail.com
RI Scotti, Fabio/C-7405-2009; Bianchi, Tiziano/E-4796-2011
OI Scotti, Fabio/0000-0002-4277-3701; Bianchi, Tiziano/0000-0002-3965-3522
CR Bolle R, 1998, BIOMETRICS PERSONAL
   *CERT RES, 2000, STAND EFF CRYPT SEC2
   *CERT RES, 2000, STAND EFF CRYPT SEC1
   COHEN J, 1985, 26 FOCS
   Damgard I, 2007, LECT NOTES COMPUT SC, V4586, P416
   ELGAMAL T, 1985, IEEE T INFORM THEORY, V31, P469, DOI 10.1109/TIT.1985.1057074
   Erkin Z, 2009, LECT NOTES COMPUT SC, V5672, P235, DOI 10.1007/978-3-642-03168-7_14
   Jain AK, 1999, IEEE T PATTERN ANAL, V21, P348, DOI 10.1109/34.761265
   Jain AK, 1997, P IEEE, V85, P1365, DOI 10.1109/5.628674
   Jain AK, 2000, IEEE T IMAGE PROCESS, V9, P846, DOI 10.1109/83.841531
   Kolesnikov V, 2008, LECT NOTES COMPUT SC, V5126, P486, DOI 10.1007/978-3-540-70583-3_40
   Lee CJ, 1999, ELECTRON LETT, V35, P288, DOI 10.1049/el:19990213
   Malkhi D., 2004, USENIX SECURITY USEN
   Maltoni D., 2009, HDB FINGERPRINT RECO, V2nd
   Naor M, 2001, SIAM PROC S, P448
   NIST, 2005, NIST SPEC PUBL
   Paillier P, 1999, LECT NOTES COMPUT SC, V1592, P223
   Pinkas B, 2009, LECT NOTES COMPUT SC, V5912, P250, DOI 10.1007/978-3-642-10366-7_15
   Sadeghi A., 2009, LNCS, V5984, P235
   SUN HW, 2006, OTM WORKSH, P469
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   Wilson CL, 2000, PATTERN RECOGN, V33, P317, DOI 10.1016/S0031-3203(99)00052-7
NR 22
TC 73
Z9 73
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
BN 978-1-4503-0286-9
PY 2010
BP 231
EP 240
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BTG40
UT WOS:000286902900031
DA 2022-02-03
ER

PT C
AU Anusuya, S
   Jayarin, PJ
AF Anusuya, S.
   Jayarin, P. Jesu
GP IEEE
TI SURVEY ON MULTIMODEL BIOMETRICS ON HUMAN IDENTIFICATION USING REAL TIME
   FILTERING ALGORITHM
SO 2017 THIRD INTERNATIONAL CONFERENCE ON SCIENCE TECHNOLOGY ENGINEERING &
   MANAGEMENT (ICONSTEM)
LA English
DT Proceedings Paper
CT 3rd IEEE International Conference on Science Technology Engineering &
   Management (ICONSTEM)
CY MAR 23-24, 2017
CL Chennai, INDIA
DE Image Processing(MATLAB); HMM; Eigen vector algorithm
AB The Authentication framework utilizing Gait, Facial, Palm and voice acknowledgment on biometric elements is utilized to give a secret word free and physical gadget free validation. It keeps away from the bedlam because of the accessible frameworks of validation like passwords, card check which can are effortlessly be fashioned and hacked by the aggressors. The proposed framework takes the benefit of the human's one of a kind personality that is given by the biometric highlight of each person. The framework guarantees to give secure get to just to the legitimate clients without requiring them to recollect their check subtle elements or convey anything in abundance. It gives a dynamic and quick calculation for approving the clients from their innate normal qualities. The usage incorporates recording the step highlight of a man, which depicts his/her strolling style and the qualities removed from them is extraordinary for every one of the people. The step highlight joined with other remarkable components of people like palm print and facial elements will give powerful and difficult to manufacture secure validation framework. The other key element in this usage is, all the procedure is finished with the assistance of only three cameras and it is finished by picture handling innovation. The usage utilizes calculation which is dynamic, quick and plays out the proposed assignment adequately.
C1 [Anusuya, S.; Jayarin, P. Jesu] Jeppiaar Engn Coll, Dept Comp Sci & Engn, Chennai, Tamil Nadu, India.
RP Anusuya, S (corresponding author), Jeppiaar Engn Coll, Dept Comp Sci & Engn, Chennai, Tamil Nadu, India.
EM anudharsuya@gmail.com; jjayarin@gmail.com
CR Ali Hyder, 2011, INT J SIGNAL PROCESS, V4
   [Anonymous], TRACK PEOPL THEIR GA
   Badrinath, 2009, PALMPRINT BASED VERI
   Boyapathi Hari Krishna, 2015, INT C RES COMP INT C
   Fitzhgerald Michael, 2009, HUMAN IDENTIFICATION
   Jain Vipin Kumar, 2012, INT J ENG RES APPL I, V2
   Maheswari A., 2016, INT C SCI TECHN ENG
   Orr R. J., SMART FLOOR MECH NAT
   Senanayake M., 2010, INT C MULT FUS INT I
   Shakhnarovich G., INTEGRATED FACE GAIT
   Wang J W J, 2010, REV VISION BASED GAI
   Warma Swati, 2012, INT J ELECT ELECT EN, V1
   Yi Huang, 2010, INT C CIRC SYST VID
   Yoo Jang-Hee, 2011, ETRI J, V33
   Zhang Zhaoxiang, 2010, SUREY ADV BIOMETRIC
NR 15
TC 0
Z9 0
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-5090-4855-7
PY 2017
BP 142
EP 147
PG 6
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BJ5GH
UT WOS:000425882000024
DA 2022-02-03
ER

PT J
AU Liu, W
   Zhang, C
   Ma, HD
   Li, SQ
AF Liu, Wu
   Zhang, Cheng
   Ma, Huadong
   Li, Shuangqun
TI Learning Efficient Spatial-Temporal Gait Features with Deep Learning for
   Human Identification
SO NEUROINFORMATICS
LA English
DT Article
DE Gait recognition; Siamese neural network; Spatio-temporal features;
   Metric learning; Human identification
ID PARALLEL FRAMEWORK; PERSON RECOGNITION; PERFORMANCE; DISEASES; DATASET;
   MODEL; IMAGE
AB The integration of the latest breakthroughs in bioinformatics technology from one side and artificial intelligence from another side, enables remarkable advances in the fields of intelligent security guard computational biology, healthcare, and so on. Among them, biometrics based automatic human identification is one of the most fundamental and significant research topic. Human gait, which is a biometric features with the unique capability, has gained significant attentions as the remarkable characteristics of remote accessed, robust and security in the biometrics based human identification. However, the existed methods cannot well handle the indistinctive inter-class differences and large intra-class variations of human gait in real-world situation. In this paper, we have developed an efficient spatial-temporal gait features with deep learning for human identification. First of all, we proposed a gait energy image (GEI) based Siamese neural network to automatically extract robust and discriminative spatial gait features for human identification. Furthermore, we exploit the deep 3-dimensional convolutional networks to learn the human gait convolutional 3D (C3D) as the temporal gait features. Finally, the GEI and C3D gait features are embedded into the null space by the Null Foley-Sammon Transform (NFST). In the new space, the spatial-temporal features are sufficiently combined with distance metric learning to drive the similarity metric to be small for pairs of gait from the same person, and large for pairs from different persons. Consequently, the experiments on the world's largest gait database show our framework impressively outperforms state-of-the-art methods.
C1 [Liu, Wu; Ma, Huadong; Li, Shuangqun] Beijing Univ Posts & Telecommun, Beijing Key Lab Intelligent Telecommun Software, Beijing 100876, Peoples R China.
   [Zhang, Cheng] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
C3 Beijing University of Posts & Telecommunications; Ohio State University
RP Liu, W (corresponding author), Beijing Univ Posts & Telecommun, Beijing Key Lab Intelligent Telecommun Software, Beijing 100876, Peoples R China.
EM liuwu@bupt.edu.cn; zhang.7804@osu.edu; mhd@bupt.edu.cn;
   shuangqunli@bupt.edu.cn
RI Liu, Wu/AAG-3615-2019
OI Liu, Wu/0000-0003-1633-7575
FU Funds for International Cooperation and Exchange of the National Natural
   Science Foundation of ChinaNational Natural Science Foundation of China
   (NSFC) [61720106007]; NSFC-Guangdong Joint Fund [U1501254]; National
   Natural Science Foundation of ChinaNational Natural Science Foundation
   of China (NSFC) [61602049]; Cosponsored Project of Beijing Committee of
   Education
FX This work is partially supported by the Funds for International
   Cooperation and Exchange of the National Natural Science Foundation of
   China (No. 61720106007), the NSFC-Guangdong Joint Fund (No. U1501254),
   the National Natural Science Foundation of China (No. 61602049), and the
   Cosponsored Project of Beijing Committee of Education.
CR Ariyanto G., 2011, 2011 INT JOINT C BIO, P1, DOI DOI 10.1109/IJCB.2011.6117582
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Cao C., 2017, CORR
   Castro F. M., 2016, CORR
   Chen ZN, 2014, J COMPUT SCI TECH-CH, V29, P785, DOI 10.1007/s11390-014-1468-z
   Chen Z, 2017, MARIT POLICY MANAG, V44, P537, DOI 10.1080/03088839.2017.1327726
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feng Y, 2016, INT C PATT RECOG, P325, DOI 10.1109/ICPR.2016.7899654
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Gao J., 2017, CORR
   Guo YF, 2006, PATTERN RECOGN, V39, P2248, DOI 10.1016/j.patcog.2006.05.009
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hou R., 2017, CORR
   Hu MD, 2013, IEEE T CYBERNETICS, V43, P77, DOI 10.1109/TSMCB.2012.2199310
   Hu MD, 2011, IEEE T SYST MAN CY B, V41, P1429, DOI 10.1109/TSMCB.2011.2149518
   Iwama H, 2012, IEEE T INF FOREN SEC, V7, P1511, DOI 10.1109/TIFS.2012.2204253
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Johnson A. Y., 2001, P 2001 IEEE COMPUTER, V1, pI
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A., 2012, INT C NEUR INF PROC, V25, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
   Kusakunniran W, 2014, IMAGE VISION COMPUT, V32, P1117, DOI 10.1016/j.imavis.2014.10.004
   Lam THW, 2011, PATTERN RECOGN, V44, P973, DOI 10.1016/j.patcog.2010.10.011
   Liu W, 2015, PROC CVPR IEEE, P3707, DOI 10.1109/CVPR.2015.7298994
   Liu W, 2014, IEEE T MULTIMEDIA, V16, P2242, DOI 10.1109/TMM.2014.2359332
   Liu W, 2013, IEEE T CYBERNETICS, V43, P1442, DOI 10.1109/TCYB.2013.2272636
   Lombardi S, 2013, IEEE I CONF COMP VIS, P1041, DOI 10.1109/ICCV.2013.133
   Ma HD, 2018, IEEE MULTIMEDIA, V25, P76, DOI 10.1109/MMUL.2017.265091429
   Ma HD, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2089094.2089107
   Makihara Y, 2006, LECT NOTES COMPUT SC, V3953, P151
   Makihara Y, 2012, IEEE SYS MAN CYBERN, P1309, DOI 10.1109/ICSMC.2012.6377914
   MANNINI A, 2016, SENSORS BASEL, V16
   Martin-Felez R, 2012, LECT NOTES COMPUT SC, V7572, P328, DOI 10.1007/978-3-642-33718-5_24
   Muja M., 2012, 2012 Canadian Conference on Computer and Robot Vision, P404, DOI 10.1109/CRV.2012.60
   Muramatsu D, 2015, IEEE T IMAGE PROCESS, V24, P140, DOI 10.1109/TIP.2014.2371335
   Nie BX, 2015, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2015.7298734
   Oliva N, 2017, INT EL DEVICES MEET
   Ren P, 2017, IEEE T BIO-MED ENG, V64, P52, DOI 10.1109/TBME.2016.2536438
   Sama A, 2017, COMPUT BIOL MED, V84, P114, DOI 10.1016/j.compbiomed.2017.03.020
   Sarkar S, 2005, IEEE T PATTERN ANAL, V27, P162, DOI 10.1109/TPAMI.2005.39
   Shiraga K., 2016, INT C BIOM, P1, DOI [DOI 10.1109/ICB.2016.7550060, 10.1109/ICB.2016.7550060]
   Sigal L, 2012, INT J COMPUT VISION, V98, P15, DOI 10.1007/s11263-011-0493-4
   Sivapalan S, 2013, IEEE COMPUT SOC CONF, P125, DOI 10.1109/CVPRW.2013.26
   Sutskever I., 2014, ADV NEURAL INFORM PR, P3104
   Tao DC, 2007, IEEE T PATTERN ANAL, V29, P1700, DOI 10.1109/TPAMI.2007.1096
   Urtasun R, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P17, DOI 10.1109/AFGR.2004.1301503
   Varol G., 2016, CORR
   Wang B., 2015, INN SMART GRID TECHN, P1
   Wang C, 2010, LECT NOTES COMPUT SC, V6311, P257, DOI 10.1007/978-3-642-15549-9_19
   Wang CY, 2014, PROC CVPR IEEE, P2369, DOI 10.1109/CVPR.2014.303
   Wang L, 2004, IEEE T CIRC SYST VID, V14, P149, DOI 10.1109/TCSVT.2003.821972
   Wang L, 2003, IEEE T PATTERN ANAL, V25, P1505, DOI 10.1109/TPAMI.2003.1251144
   Wu ZF, 2017, IEEE T PATTERN ANAL, V39, P209, DOI 10.1109/TPAMI.2016.2545669
   Wu ZF, 2015, IEEE T MULTIMEDIA, V17, P1960, DOI 10.1109/TMM.2015.2477681
   Xia Y, 2015, BIOMED SIGNAL PROCES, V18, P254, DOI 10.1016/j.bspc.2015.02.002
   Xu H., 2017, CORR
   Yam CY, 2004, PATTERN RECOGN, V37, P1057, DOI 10.1016/j.patcog.2003.09.012
   Yan CG, 2014, IEEE T CIRC SYST VID, V24, P2077, DOI 10.1109/TCSVT.2014.2335852
   Yan CG, 2014, IEEE SIGNAL PROC LET, V21, P573, DOI 10.1109/LSP.2014.2310494
   Yu SQ, 2006, INT C PATT RECOG, P441
   Yuan X, 2006, IEEE IMAGE PROC, P2905, DOI 10.1109/ICIP.2006.313037
   Zha Z. J., 2007, P INT WORKSH MULT IN, p[227, 2007]
   Zhang C, 2016, INT CONF ACOUST SPEE, P2832, DOI 10.1109/ICASSP.2016.7472194
   Zhang D, 2015, IEEE I CONF COMP VIS, P2012, DOI 10.1109/ICCV.2015.233
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zolfaghari Mohammadreza, 2017, CORR
NR 69
TC 26
Z9 26
U1 3
U2 50
PU HUMANA PRESS INC
PI TOTOWA
PA 999 RIVERVIEW DRIVE SUITE 208, TOTOWA, NJ 07512 USA
SN 1539-2791
EI 1559-0089
J9 NEUROINFORMATICS
JI Neuroinformatics
PD OCT
PY 2018
VL 16
IS 3-4
SI SI
BP 457
EP 471
DI 10.1007/s12021-018-9362-4
PG 15
WC Computer Science, Interdisciplinary Applications; Neurosciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Neurosciences & Neurology
GA GQ3TU
UT WOS:000441590600017
PM 29404933
DA 2022-02-03
ER

PT J
AU Pang, M
   Wang, BH
   Cheung, YM
   Chen, YR
   Wen, BH
AF Pang, Meng
   Wang, Binghui
   Cheung, Yiu-ming
   Chen, Yiran
   Wen, Bihan
TI VD-GAN: A Unified Framework for Joint Prototype and Representation
   Learning From Contaminated Single Sample per Person
SO IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY
LA English
DT Article
DE Single sample per person; prototype learning; representation learning;
   generative adversarial network
ID FACE-RECOGNITION; PROJECTION; IMAGE
AB Single sample per person (SSPP) face recognition with a contaminated biometric enrolment database (SSPP-ce FR) is an emerging practical FR problem, where the SSPP in the enrolment database is no longer standard but contaminated by nuisance facial variations such as expression, lighting, pose, and disguise. In this case, the conventional SSPP FR methods, including the patch-based and generic learning methods, will suffer from serious performance degradation. Few recent methods were proposed to tackle SSPP-ce FR by either performing prototype learning on the contaminated enrolment database or learning discriminative representations that are robust against variation. Despite that, most of these approaches can only handle a specified single variation, e.g., pose, but cannot be extended to multiple variations. To address these two limitations, we propose a novel Variation Disentangling Generative Adversarial Network (VD-GAN) to jointly perform prototype learning and representation learning in a unified framework. The proposed VD-GAN consists of an encoder-decoder structural generator and a multi-task discriminator to handle universal variations including single, multiple, and even mixed variations in practice. The generator and discriminator play an adversarial game such that the generator learns a discriminative identity representation and generates an identity-preserved prototype for each face image, while the discriminator aims to predict face identity label, distinguish real vs. fake prototype, and disentangle target variations from the learned representations. Qualitative and quantitative evaluations on various real-world face datasets containing single/ multiple and mixed variations demonstrate the effectiveness of VD-GAN.
C1 [Pang, Meng; Wen, Bihan] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Wang, Binghui; Chen, Yiran] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
   [Cheung, Yiu-ming] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Nanyang Technological University & National Institute of Education (NIE)
   Singapore; Nanyang Technological University; Duke University; Hong Kong
   Baptist University
RP Wen, BH (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM meng.pang@ntu.edu.sg; binghui.wang@duke.edu; ymc@comp.hkbu.edu.hk;
   yiran.chen@duke.edu; bihan.wen@ntu.edu.sg
RI Wen, Bihan/B-3123-2017; Chen, Yiran/L-4812-2017; Cheung,
   Yiu-ming/E-2050-2015
OI Wen, Bihan/0000-0002-6874-6453; Chen, Yiran/0000-0002-1486-8412; Pang,
   Meng/0000-0001-7184-2043; Wang, Binghui/0000-0001-5616-060X; Cheung,
   Yiu-ming/0000-0001-7629-4648
FU NSFCNational Natural Science Foundation of China (NSFC) [61672444]; Hong
   Kong Baptist University (HKBU) [RC-FNRA-IG/18-19/SCI/03,
   RC-IRCMs/18-19/SCI/01]; Innovation and Technology Fund (ITF) of the
   Innovation and Technology Commission (ITC) of the Government of the Hong
   Kong, SAR [ITS/339/18]; Ministry of Education, Republic of
   SingaporeMinistry of Education, Singapore; National Research Foundation
   (NRF), Singapore, through the Singapore Cybersecurity Consortium (SGCSC)
   Grant Office [2019-S01]
FX The work of Yiu-ming Cheung was supported in part by the NSFC under
   Grant 61672444, in part by the Hong Kong Baptist University (HKBU) under
   Grant RC-FNRA-IG/18-19/SCI/03 and Grant RC-IRCMs/18-19/SCI/01, and in
   part by the Innovation and Technology Fund (ITF) of the Innovation and
   Technology Commission (ITC) of the Government of the Hong Kong, SAR,
   under Project ITS/339/18. The work of Bihan Wen was supported in part by
   the Ministry of Education, Republic of Singapore, under the Start-Up
   Grant; and in part by the National Research Foundation (NRF), Singapore,
   through the Singapore Cybersecurity Consortium (SGCSC) Grant Office,
   under Grant SGCSC_Grant_2019-S01.
CR Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Best-Rowden L, 2014, IEEE T INF FOREN SEC, V9, P2144, DOI 10.1109/TIFS.2014.2359577
   Cai D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P714
   Chen YA, 2017, IEEE IMAGE PROC, P1202, DOI 10.1109/ICIP.2017.8296472
   Cozzolino D, 2020, IEEE T INF FOREN SEC, V15, P144, DOI 10.1109/TIFS.2019.2916364
   Cuculo V, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19010146
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng WH, 2018, IEEE T PATTERN ANAL, V40, P2513, DOI 10.1109/TPAMI.2017.2757923
   Deng WH, 2013, PROC CVPR IEEE, P399, DOI 10.1109/CVPR.2013.58
   Fang XZ, 2016, IEEE T CYBERNETICS, V46, P1828, DOI 10.1109/TCYB.2015.2454521
   Gao SH, 2015, IEEE T INF FOREN SEC, V10, P2108, DOI 10.1109/TIFS.2015.2446438
   Gao SH, 2015, INT J COMPUT VISION, V111, P365, DOI 10.1007/s11263-014-0750-4
   Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557
   Gao Y, 2017, IEEE T IMAGE PROCESS, V26, P2545, DOI 10.1109/TIP.2017.2675341
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Huang Gary B, 2008, WORKSH FAC REAL LIF
   Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267
   Ji HK, 2017, PATTERN RECOGN, V62, P125, DOI 10.1016/j.patcog.2016.08.007
   King DB, 2015, ACS SYM SER, V1214, P1
   Kingma D. P., 2013, ARXIV13126114
   Kokiopoulou E, 2007, IEEE T PATTERN ANAL, V29, P2143, DOI 10.1109/TPAMI.2007.1131
   Kulkarni Tejas D, 2015, ADV NEURAL INFORM PR, P2539, DOI DOI 10.1063/1.4914407
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Li ZM, 2016, IEEE T INF FOREN SEC, V11, P2203, DOI 10.1109/TIFS.2016.2567318
   Lu JW, 2013, IEEE T PATTERN ANAL, V35, P39, DOI 10.1109/TPAMI.2012.70
   Ma W, 2018, INT C PATT RECOG, P2558, DOI 10.1109/ICPR.2018.8545434
   Martinez A., 1998, 24 COMP VIS CTR
   Mokhayeri F, 2019, IEEE T INF FOREN SEC, V14, P757, DOI 10.1109/TIFS.2018.2866295
   Pang M, 2021, IEEE T NEUR NET LEAR, V32, P1560, DOI 10.1109/TNNLS.2020.2985099
   Pang M, 2019, IEEE T CIRC SYST VID, V29, P3184, DOI 10.1109/TCSVT.2018.2879833
   Pang M, 2020, IEEE T INF FOREN SEC, V15, P195, DOI 10.1109/TIFS.2019.2919950
   Pang M, 2019, PATTERN RECOGN, V89, P91, DOI 10.1016/j.patcog.2019.01.005
   Pang M, 2017, IEEE ACCESS, V5, P13978, DOI 10.1109/ACCESS.2017.2730281
   Pei TW, 2017, PATTERN RECOGN, V64, P305, DOI 10.1016/j.patcog.2016.11.016
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Radford A., 2016, ARXIV151106434
   Robertson S., 2008, P 31 ANN INT ACM SIG, P689, DOI [10.1145/1390334.1390453, DOI 10.1145/1390334.1390453]
   Sengupta S, 2016, IEEE WINT CONF APPL
   Sharma P, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901319
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Song LX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P627, DOI 10.1145/3240508.3240612
   Tan XY, 2006, PATTERN RECOGN, V39, P1725, DOI 10.1016/j.patcog.2006.03.013
   Tran L, 2019, IEEE T PATTERN ANAL, V41, P3007, DOI 10.1109/TPAMI.2018.2868350
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   TURK MA, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P586
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang XG, 2004, INT C PATT RECOG, P142, DOI 10.1109/ICPR.2004.1333724
   Wei CP, 2015, IEEE T IMAGE PROCESS, V24, P1722, DOI 10.1109/TIP.2015.2409738
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Yang M, 2020, IEEE T INF FOREN SEC, V15, P2469, DOI 10.1109/TIFS.2020.2965301
   Yang M, 2017, PATTERN RECOGN, V66, P117, DOI 10.1016/j.patcog.2016.12.028
   Yang M, 2013, IEEE I CONF COMP VIS, P689, DOI 10.1109/ICCV.2013.91
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P2655, DOI 10.1109/TIFS.2020.2970590
   Yi D, 2014, ARXIV14117923
   Yin X, 2017, IEEE I CONF COMP VIS, P4010, DOI 10.1109/ICCV.2017.430
   Zhang FZ, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2644828
   Zhang GQ, 2016, PATTERN RECOGN, V60, P613, DOI 10.1016/j.patcog.2016.06.012
   Zhang GQ, 2016, NEUROCOMPUTING, V171, P1193, DOI 10.1016/j.neucom.2015.07.048
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang PY, 2016, PATTERN RECOGN, V52, P249, DOI 10.1016/j.patcog.2015.09.024
   Zhang Z, 2017, IEEE T IMAGE PROCESS, V26, P1607, DOI 10.1109/TIP.2017.2654163
   Zhao K, 2019, PROC CVPR IEEE, P1136, DOI 10.1109/CVPR.2019.00123
   Zhao Y., 2018, ARXIV180708772
   Zhou Y, 2017, IEEE T CYBERNETICS, V47, P830, DOI 10.1109/TCYB.2016.2529299
   Zhu PF, 2012, LECT NOTES COMPUT SC, V7572, P822, DOI 10.1007/978-3-642-33718-5_59
NR 71
TC 1
Z9 1
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1556-6013
EI 1556-6021
J9 IEEE T INF FOREN SEC
JI IEEE Trans. Inf. Forensic Secur.
PY 2021
VL 16
BP 2246
EP 2259
DI 10.1109/TIFS.2021.3050055
PG 14
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA QG0ZH
UT WOS:000617315800006
DA 2022-02-03
ER

PT C
AU Oliveira, JS
   Souza, GB
   Rocha, AR
   Deus, FE
   Marana, AN
AF Oliveira, Johnatan S.
   Souza, Gustavo B.
   Rocha, Anderson R.
   Deus, Flavio E.
   Marana, Aparecido N.
BE Teran, L
   Pincay, J
   Portmann, E
TI Cross-Domain Deep Face Matching for Real Banking Security Systems
SO 2020 SEVENTH INTERNATIONAL CONFERENCE ON EDEMOCRACY & EGOVERNMENT
   (ICEDEG)
SE International Conference on eDemocracy and eGovernment ICEDEG
LA English
DT Proceedings Paper
CT 7th International Conference on eDemocracy and eGovernment (ICEDEG)
CY APR 22-24, 2020
CL Buenos Aires, ARGENTINA
AB Ensuring the security of transactions is currently one of the major challenges that banking systems deal with. The usage of face for biometric authentication of users is attracting large investments from banks worldwide due to its convenience and acceptability by people, especially in cross-domain scenarios, in which facial images from ID documents are compared with digital self-portraits (selfies) for the automated opening of new checking accounts, e.g, or financial transactions authorization. Actually, the comparison of selfies and IDs has also been applied in another wide variety of tasks nowadays, such as automated immigration control. The major difficulty in such process consists in attenuating the differences between the facial images compared given their different domains. In this work, in addition to collecting a large cross-domain face dataset, with 27,002 real facial images of selfies and ID documents (13,501 subjects) captured from the databases of the major public Brazilian bank, we propose a novel architecture for such cross-domain matching problem based on deep features extracted by two well-referenced Convolutional Neural Networks (CNN). Results obtained on the dataset collected, called FaceBank, with accuracy rates higher than 93 %, demonstrate the robustness of the proposed approach to the cross-domain face matching problem and its feasible application in real banking security systems.
C1 [Oliveira, Johnatan S.; Deus, Flavio E.] Univ Brasilia UnB, Dept Elect Engn, Brasilia, DF, Brazil.
   [Souza, Gustavo B.] Fed Univ Sao Carlos UFSCar, Dept Comp, Sao Carlos, Brazil.
   [Rocha, Anderson R.] Univ Campinas Unicamp, Inst Comp, Campinas, Brazil.
   [Marana, Aparecido N.] Sao Paulo State Univ Unesp, Dept Comp, Bauru, SP, Brazil.
C3 Universidade de Brasilia; Universidade Federal de Sao Carlos;
   Universidade Estadual de Campinas; Universidade Estadual Paulista
RP Oliveira, JS (corresponding author), Univ Brasilia UnB, Dept Elect Engn, Brasilia, DF, Brazil.
EM jow@gmail.com; gustavo.botelho@gmail.com; anderson.rocha@ic.unicamp.br;
   flavioelias@unb.br; nilceu@fc.unesp.br
RI ; Marana, Aparecido Nilceu/A-6334-2008
OI Deus, Flavio Elias de/0000-0001-7953-6227; Marana, Aparecido
   Nilceu/0000-0003-4861-7061
CR Amos Brandon, 2016, CMUCS16118
   Blanco-Gonzalo R, 2015, HUM-CENT COMPUT INFO, V5, DOI 10.1186/s13673-015-0043-0
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Button M., 2019, FINANCIAL COST FRAUD
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Du S, 2017, CS231626 STANF U
   Experian Information Solutions, 2020, 2020 GLOB ID FRAUD R
   FEBRABAN, 2018, REL ANUAL 2018
   Federal Financial Institutions Examination Council (FFIE), 2005, LETTFIL1032005 FFIE
   Folego G., 2016, P WORKSH COMP VIS WV
   Fosse G., 2017, FEBRABAN SURVEY BANK
   Grm K, 2018, IET BIOMETRICS, V7, P81, DOI 10.1049/iet-bmt.2017.0083
   Ho HT, 2014, INT J COMPUT VISION, V109, P110, DOI 10.1007/s11263-014-0720-x
   Jain A. K., 2011, INTRO BIOMETRICS
   Javelin Strategy & Research, 2019, 2019 ID FRAUD STUD F
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Krizhevsky A., 2012, INT C NEUR INF PROC, V25, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   METZ CE, 1978, SEMIN NUCL MED, V8, P283, DOI 10.1016/S0001-2998(78)80014-2
   Nielsen M.A., 2015, NEURAL NETWORKS DEEP, VVolume 25
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Raschka S., 2017, Mlxtend-rasbt/mlxtend: Version 0.10.0, DOI 10.5281/zenodo.1127706
   Rizzi A, 2003, PATTERN RECOGN LETT, V24, P1663, DOI 10.1016/S0167-8655(02)00323-9
   Rodriguez C., COMBATE SEM TREGUAAS
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Srinivasan R, 2015, INT CONF BIOMETR THE
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wu JX, 2015, IEEE T NEUR NET LEAR, V26, P2357, DOI 10.1109/TNNLS.2014.2382123
   Wu JX, 2012, PROC CVPR IEEE, P2344, DOI 10.1109/CVPR.2012.6247946
NR 32
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2573-2005
EI 2573-1998
BN 978-1-7281-5882-2
J9 INT CONF EDEMOC EGOV
PY 2020
BP 21
EP 28
PG 8
WC Computer Science, Interdisciplinary Applications; Political Science
WE Conference Proceedings Citation Index - Science (CPCI-S); Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)
SC Computer Science; Government & Law
GA BS2MT
UT WOS:000703889300003
OA Green Submitted
DA 2022-02-03
ER

PT J
AU Ding, IJ
   Tsai, CY
   Yen, CY
AF Ding, Ing-, Jr.
   Tsai, Cheng-Yang
   Yen, Cheng-Yu
TI A design on recommendations of sensor development platforms with
   different sensor modalities for making gesture biometrics-based service
   applications of the specific group
SO MICROSYSTEM TECHNOLOGIES-MICRO-AND NANOSYSTEMS-INFORMATION STORAGE AND
   PROCESSING SYSTEMS
LA English
DT Article
ID AG NANOPARTICLES; ZNO NANORODS
AB Gesture biometrics sensors will be extremely helpful for constructing a gesture-based application system. Compared with traditional acoustic sensor-based speech recognition applications and RGB image sensor-based surveillance applications, proper utilizations of the gesture biometrics sensor will absolutely be able to create an innovative and practical application and speed up the maturity of gesture recognition. Currently, three well-known gesture biometrics sensor platforms are Kinect, Leap Motion, and Myo. Kinect and Leap motion sensor devices belong to the categorization of 3D image sensors, containing both RGB and depth sensors. Myo armband is categorized into the type of wearable sensor devices. Nowadays, related studies on application system establishments using Kinect, Leap Motion, or Kinect have been frequently seen in the recent years. However, most of all these related gesture-based application systems are designed for end users. Extremely few investigations using these popular gesture biometrics sensors are done specifically for the system developer of a gesture-based application system. In fact, a correct and proper adoption of the gesture biometric sensor platform will speed up the development of the desired system and increase the practicality of the designed system. In this paper, a recommendation scheme of Kinect, Leap Motion, and Myo sensor platforms is presented for supporting developments of a mid-air gesture application. Compared with the conventional application system design procedure, the presented sensor recommendation scheme incorporated between two stages of functional operation and system design specifications will have a great help to the system developer to establish his desired system. In addition, a case study on system developments of efficient web-based streaming media control by disabled persons using the presented sensor platform recommendation scheme is also reported in this paper.
C1 [Ding, Ing-, Jr.; Tsai, Cheng-Yang; Yen, Cheng-Yu] Natl Formosa Univ, Dept Elect Engn, Huwei Township, Yunlin 632, Taiwan.
C3 National Formosa University
RP Ding, IJ (corresponding author), Natl Formosa Univ, Dept Elect Engn, Huwei Township, Yunlin 632, Taiwan.
EM ingjr@nfu.edu.tw
FU Ministry of Science and Technology (MOST) in TaiwanMinistry of Science
   and Technology, Taiwan [MOST 107-2221-E-150-039]
FX This research is partially supported by the Ministry of Science and
   Technology (MOST) in Taiwan under Grant MOST 107-2221-E-150-039.
CR Ameur Safa, 2016, 2016 7th International Conference on Sciences of Electronics, Technologies of Information and Telecommunications (SETIT), P514, DOI 10.1109/SETIT.2016.7939924
   Boyali A, 2015, 2015 IEEE 4TH GLOBAL CONFERENCE ON CONSUMER ELECTRONICS (GCCE), P200, DOI 10.1109/GCCE.2015.7398619
   Chen YM, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION, P1419, DOI 10.1109/ICInfA.2015.7279509
   Demircioglu B, 2016, 2016 24TH SIGNAL PROCESSING AND COMMUNICATION APPLICATION CONFERENCE (SIU), P589, DOI 10.1109/SIU.2016.7495809
   Ding IJ, 2017, P ISERD 79 INT C HON, P42
   Ding IJ, 2017, NEUROCOMPUTING, V262, P108, DOI 10.1016/j.neucom.2016.11.089
   Ding IJ, 2016, MULTIMED TOOLS APPL, V75, P9669, DOI 10.1007/s11042-015-2782-3
   Hosoya R, 2016, Proceedings NICOGRAPH International 2016, P150, DOI 10.1109/NicoInt.2016.43
   Lai LT, 2018, IEEE ELECTR DEVICE L, V39, P1932, DOI 10.1109/LED.2018.2872343
   Mapari RB, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON RESEARCH IN COMPUTATIONAL INTELLIGENCE AND COMMUNICATION NETWORKS (ICRCICN), P323, DOI 10.1109/ICRCICN.2015.7434258
   McCartney R, 2015, P INT C IM PROC COMP, P3
   Mulling T, 2015, BRITISH HCI 2015, P283, DOI 10.1145/2783446.2783612
   Rawat S, 2016, PROCEEDINGS OF THE 5TH INTERNATIONAL CONFERENCE ON SYSTEM MODELING & ADVANCEMENT IN RESEARCH TRENDS (SMART-2016), P115, DOI 10.1109/SYSMART.2016.7894501
   Schwarz A, 2017, IEEE COMPUT SOC CONF, P1165, DOI 10.1109/CVPRW.2017.155
   Young SJ, 2018, J ELECTROCHEM SOC, V165, pB3043, DOI 10.1149/2.0061808jes
   Soltani F., 2012, 2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems (CISIS), P491, DOI 10.1109/CISIS.2012.55
   Vokorokos L, 2016, IEEE INT CONF INTELL, P293, DOI 10.1109/INES.2016.7555139
   Wang QH, 2017, CHIN CONTR CONF, P10823, DOI 10.23919/ChiCC.2017.8029083
   Young SJ, 2018, MICROSYST TECHNOL, V24, P4207, DOI 10.1007/s00542-018-3712-x
   Young SJ, 2018, IEEE T NANOTECHNOL, V17, P1063, DOI 10.1109/TNANO.2018.2864294
   Young SJ, 2018, ACS OMEGA, V3, P8135, DOI 10.1021/acsomega.8b01041
   Young SJ, 2018, SENSOR ACTUAT A-PHYS, V269, P363, DOI 10.1016/j.sna.2017.11.044
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
NR 23
TC 2
Z9 2
U1 0
U2 0
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 0946-7076
EI 1432-1858
J9 MICROSYST TECHNOL
JI Microsyst. Technol.
PD JAN
PY 2022
VL 28
IS 1
SI SI
BP 153
EP 166
DI 10.1007/s00542-019-04503-2
PG 14
WC Engineering, Electrical & Electronic; Nanoscience & Nanotechnology;
   Materials Science, Multidisciplinary; Physics, Applied
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering; Science & Technology - Other Topics; Materials Science;
   Physics
GA YK2UZ
UT WOS:000745075900018
DA 2022-02-03
ER

PT J
AU Nongpiur, ME
   Ku, JYF
   Aung, T
AF Nongpiur, Monisha E.
   Ku, Judy Y. F.
   Aung, Tin
TI Angle closure glaucoma: a mechanistic review
SO CURRENT OPINION IN OPHTHALMOLOGY
LA English
DT Review
DE angle closure mechanisms; glaucoma; imaging; ultrasound biomicroscopy
ID ULTRASOUND BIOMICROSCOPY; BIOMETRIC PARAMETERS; OCCLUDABLE ANGLES; LASER
   IRIDOTOMY; UVEAL EFFUSION; PLATEAU IRIS; PREVALENCE; CHINESE; EYES;
   ASSOCIATION
AB Purpose of review
   With recent advances in imaging techniques such as anterior segment optical coherence tomography and ultrasound biomicroscopy, there is a better understanding of nonpupil block mechanisms and novel risk factors contributing to the pathogenesis of angle closure glaucoma.
   Recent findings
   Recent studies suggest that multiple anatomical and physiological factors interplay in the pathogenesis of angle closure glaucoma. The association of greater iris convexity, area and thickness with narrow angles could result in a more anterior bowing and crowding of the peripheral iris. Other novel anatomic parameters such as greater lens vault, smaller anterior chamber width, area and volume, independently increase the risk of having angle closure. Dynamic increase or lesser reduction in iris volume during dilation supports the theory of physiological predisposition to the disease process. Choroidal expansion has been demonstrated in untreated and treated, acute and chronic primary angle closure eyes. It remains unknown whether this finding is a cause or effect in this condition.
   Summary
   With a wider availability of imaging tools and a better understanding of risk factors and mechanisms, clinicians maybe able to more accurately identify those at greater risk of developing angle closure disease and tailor their treatment according to the predominant factor(s) involved.
C1 [Aung, Tin] Natl Univ Singapore, Singapore Natl Eye Ctr, Glaucoma Serv, Singapore 168751, Singapore.
   [Nongpiur, Monisha E.; Ku, Judy Y. F.; Aung, Tin] Natl Univ Singapore, Singapore Eye Res Inst, Singapore 168751, Singapore.
   [Aung, Tin] Natl Univ Singapore, Yong Loo Lin Sch Med, Singapore 168751, Singapore.
C3 National University of Singapore; Singapore National Eye Center;
   National University of Singapore; Singapore National Eye Center;
   National University of Singapore
RP Aung, T (corresponding author), Natl Univ Singapore, Singapore Natl Eye Ctr, Glaucoma Serv, 11 3rd Hosp Ave, Singapore 168751, Singapore.
EM tin11@pacific.net.sg
CR Alsbirk P H, 1994, Ugeskr Laeger, V156, P5117
   Aptel F, 2010, OPHTHALMOLOGY, V117, pc, DOI 10.1016/j.ophtha.2009.10.030
   CONGDON N, 1992, SURV OPHTHALMOL, V36, P411, DOI 10.1016/S0039-6257(05)80022-0
   Congdon NG, 1997, OPHTHALMOLOGY, V104, P1489, DOI 10.1016/S0161-6420(97)30112-2
   EDWARDS RS, 1982, BRIT J OPHTHALMOL, V66, P576, DOI 10.1136/bjo.66.9.576
   Foster PJ, 2000, BRIT J OPHTHALMOL, V84, P186, DOI 10.1136/bjo.84.2.186
   Foster PJ, 2001, BRIT J OPHTHALMOL, V85, P1277, DOI 10.1136/bjo.85.11.1277
   Gazzard G, 2003, OPHTHALMOLOGY, V110, P630, DOI 10.1016/S0161-6420(02)01893-6
   George R, 2003, BRIT J OPHTHALMOL, V87, P399, DOI 10.1136/bjo.87.4.399
   He M, 2006, EYE, V20, P3, DOI 10.1038/sj.eye.6701797
   He MG, 2008, J GLAUCOMA, V17, P386, DOI 10.1097/IJG.0b013e31815c5f69
   He MG, 2007, OPHTHALMOLOGY, V114, P1513, DOI 10.1016/j.ophtha.2006.11.032
   Hu Z, 1989, CHIN J OPHTHALMOL, V25, P115
   KLEIN BEK, 1992, OPHTHALMOLOGY, V99, P1499
   Kumar RS, 2008, OPHTHALMOLOGY, V115, P430, DOI 10.1016/j.ophtha.2007.07.026
   Kumar RS, 2009, ARCH OPHTHALMOL-CHIC, V127, P1269, DOI 10.1001/archophthalmol.2009.241
   Kumar RS, 2008, ARCH OPHTHALMOL-CHIC, V126, P1647, DOI 10.1001/archophthalmol.2008.514
   Lavanya R, 2008, ARCH OPHTHALMOL-CHIC, V126, P686, DOI 10.1001/archopht.126.5.686
   LOWE RF, 1970, BRIT J OPHTHALMOL, V54, P161, DOI 10.1136/bjo.54.3.161
   Marchini G, 1998, OPHTHALMOLOGY, V105, P2091, DOI 10.1016/S0161-6420(98)91132-0
   Nolan WP, 2000, BRIT J OPHTHALMOL, V84, P1255, DOI 10.1136/bjo.84.11.1255
   Nongpiur ME, 2010, OPHTHALMOLOGY
   Nongpiur ME, 2010, OPHTHALMOLOGY, V117, P1967, DOI 10.1016/j.ophtha.2010.02.007
   PAVLIN CJ, 1992, AM J OPHTHALMOL, V113, P390, DOI 10.1016/S0002-9394(14)76160-4
   Quigley HA, 2003, J GLAUCOMA, V12, P167, DOI 10.1097/00061198-200304000-00013
   Quigley HA, 2009, J GLAUCOMA, V18, P173, DOI 10.1097/IJG.0b013e31818624ce
   RITCH R, 1996, GLAUCOMAS, V2, P801
   Sakai H, 2005, OPHTHALMOLOGY, V112, P413, DOI 10.1016/j.ophtha.2004.08.026
   Seah SKL, 1997, ARCH OPHTHALMOL-CHIC, V115, P1436, DOI 10.1001/archopht.1997.01100160606014
   Sihota R, 2008, EYE, V22, P521, DOI 10.1038/sj.eye.6702687
   Sihota R, 2005, J GLAUCOMA, V14, P387, DOI 10.1097/01.ijg.0000176934.14229.32
   SNOW J T, 1977, Transactions of the Ophthalmological Societies of the United Kingdom, V97, P189
   Thomas R, 1999, Ophthalmic Surg Lasers, V30, P547
   TIELSCH JM, 1991, JAMA-J AM MED ASSOC, V266, P369, DOI 10.1001/jama.266.3.369
   Wang BS, 2011, BRIT J OPHTHALMOL, V95, P46, DOI 10.1136/bjo.2009.178129
   Wang BS, 2010, OPHTHALMOLOGY, V117, P11, DOI 10.1016/j.ophtha.2009.06.017
   Wang NL, 2002, CHINESE MED J-PEKING, V115, P1706
   WU RY, ARCH OPHTHA IN PRESS
   Yang M, 2005, BRIT J OPHTHALMOL, V89, P288, DOI 10.1136/bjo.2004.048686
NR 39
TC 108
Z9 122
U1 0
U2 20
PU LIPPINCOTT WILLIAMS & WILKINS
PI PHILADELPHIA
PA TWO COMMERCE SQ, 2001 MARKET ST, PHILADELPHIA, PA 19103 USA
SN 1040-8738
EI 1531-7021
J9 CURR OPIN OPHTHALMOL
JI Curr. Opin. Ophthalmol.
PD MAR
PY 2011
VL 22
IS 2
BP 96
EP 101
DI 10.1097/ICU.0b013e32834372b9
PG 6
WC Ophthalmology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Ophthalmology
GA 719FK
UT WOS:000287186600004
PM 21252671
DA 2022-02-03
ER

PT C
AU Doi, J
   Yamanaka, M
   Kajita, H
AF Doi, J
   Yamanaka, M
   Kajita, H
GP IEEE
TI Discrete finger and palmar feature extraction for personal
   authentication
SO 2003 IEEE INTERNATIONAL SYMPOSIUM ON INTELLIGENT SIGNAL PROCESSING,
   PROCEEDINGS: FROM CLASSICAL MEASUREMENT TO COMPUTING WITH PERCEPTIONS
SE International Symposium on Intelligent Signal Processing-WISP
LA English
DT Proceedings Paper
CT 3rd IEEE International Symposium on Intelligent Signal Processing
CY SEP 04-06, 2003
CL BUDAPEST, HUNGARY
DE personal authentication; biometric authentication; point wise feature
   extraction; finger crease feature extraction; palm crease feature
   extraction
AB A new method of a reliable and real-time authentication is proposed Finger geometry and feature extraction of the palmar flexion creases are integrated in discrete points of characteristics. A video image of either palm, palm placed freely facing toward a video camera in front of a low-reflective board, is acquired Fingers are brought together without any constraints. Discrete feature point extraction for each of the four fingers involves: intersection points of the three digital (finger) flexion creases on the finger skeletal line; skeletal lengths of the finger segments between the three creases; distances between the intersection points and the corresponding points of the adjacent fingers. Discrete feature extraction for the palm involves: intersection points of the major palmar flexion creases on tire extended finger skeletal line; orientation of the crease at each point of the intersection. These metrics define the feature vectors for matching. Matching results are perfect for 50 subjects so far. This point wise integration of the finger and palmar feature extraction, extracting enough feature from non contacting video image, requiring no time-consumptive palm print image analysis, and requiring less than one second processing time, will contribute to a real-time and reliable authentication.
C1 Chiba Inst Technol, Dept Comp Sci, Narashino, Chiba 2758588, Japan.
C3 Chiba Institute of Technology
RP Doi, J (corresponding author), Chiba Inst Technol, Dept Comp Sci, 2-17-1 Tsudanuma, Narashino, Chiba 2758588, Japan.
CR Arun R., 1999, C AUD VID BAS BIOM P, P166
   ASHBAUGH DR, 1999, CRC SER PR CRIM, P1
   *BIOM CONS, INTRO BIOM
   Duta N, 2002, PATTERN RECOGN LETT, V23, P477, DOI 10.1016/S0167-8655(01)00179-9
   Halici U, 1999, INT SER COMPUTAT INT, P1
   Jain A., 1999, BIOMETRICS PERSONAL, V1
   JAIN AK, 1999, P IEEE INT C IM PROC
   JAIN AK, 2002, P INT C IM PROC ICIP
   JONES GW, 2000, INTRO FINGERPRINT CO
   KROEKER KL, 2002, IEEE COMPUTER GRAPHI
   Li WX, 2002, INT J PATTERN RECOGN, V16, P417, DOI 10.1142/S0218001402001757
   LIU S, 2001, IT PROFESSIONAL
   Nanavati S., 2002, BIOMETRICS
   Pankanti S, 2000, COMPUTER, V33, P46, DOI 10.1109/2.820038
   PODIO FL, BIOMETRICS TECHNOLOG
   Ross A, 2001, LECT NOTES COMPUT SC, V2091, P354
   ROSS A, BIOMETRICS RES
   YAMANAKA M, 2003, P SPIE 8 ANN INT S N
NR 18
TC 3
Z9 3
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 0-7803-7864-4
J9 I S INTELL SIG PR
PY 2003
BP 37
EP 42
PG 6
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BY48R
UT WOS:000189347300008
DA 2022-02-03
ER

PT C
AU Aggarwal, JK
AF Aggarwal, J. K.
BE Bhanu, B
   Ravishankar, CV
   RoyChowdhury, AK
   Aghajan, H
   Terzopoulos, D
TI Motion Analysis: Past, Present and Future
SO DISTRIBUTED VIDEO SENSOR NETWORKS
LA English
DT Proceedings Paper
CT Workshop on Distributed Video Sensor Networks - Research Challenges and
   Future Directions
CY MAY 11-12, 2009
CL Univ California, Riverside, CA
HO Univ California
DE Motion; Tracking; Human activities; Past; Present; Future
ID RECOGNITION; REPRESENTATION; MOVEMENT
AB The subject of motion has been the center of interdisciplinary studies since the time when Zeno posed his paradox circa 500BC. However, computer vision, the use of a camera and a computer to recognize objects, people and/or events automatically, is a relatively young field of research. Its development began in the early 1960s; however, it has matured fairly quickly. Today, it is contributing to the solutions of some of the most serious societal problems. Motion analysis of a sequence of images is an important part of computer vision. This chapter briefly presents the contributions to motion analysis from other fields followed by the computer vision-based analysis of motion from a sequence of images. Analysis and understanding of images based on both feature tracking and optical flow estimation are presented. Early works focused on the computation of structure from motion of objects from a sequence of images via point features. This was followed by the computation of optical flow to characterize motion. Applications today focus on the monitoring of traffic, providing guidance to a motorist in terms of his/her position relative to traffic lanes and traffic ahead, and inspection of complicated three-dimensional industrial parts, to mention a few. Research focus has shifted from inanimate objects to people, for example monitoring people and their activities in public places or monitoring activities from an unmanned aerial vehicle. These applications are dominating the research scene through the belief that computer vision/motion analysis can contribute to the solution of societal surveillance and biometric problems. The chapter ends with a discussion of the future directions of research in motion analysis and possible applications.
C1 [Aggarwal, J. K.] Univ Texas Austin, Austin, TX 78712 USA.
C3 University of Texas System; University of Texas Austin
RP Aggarwal, JK (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.
EM aggarwaljk@mail.utexas.edu
FU Texas Higher Education Coordinating Board [003658-0140-2007]
FX It is a pleasure to acknowledge the help and comments of Dr. Michael
   Ryoo, Professors Bill Geisler and Amar Mitiche and Mr. Birgi Tamersoy
   and Mr. Chia-Chih Chen. Also, my sincere thanks go to Ms. Selina Keilani
   for editing the manuscript. The research was supported in part by Texas
   Higher Education Coordinating Board award # 003658-0140-2007.
CR Addams R., 1834, LONDON EDINBURGH PHI, V5, P373, DOI DOI 10.1080/14786443408648481
   ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   Aggarwal J. K., 1985, P 3 WORKSH COMP VIS
   Aggarwal J. K., 2010, ACM COMPUT IN PRESS
   Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744
   AGGARWAL JK, 1975, IEEE T COMPUT, V24, P966, DOI 10.1109/T-C.1975.224102
   Badler N. I, 1979, WORKSH COMP AN TIM V
   Cai Q, 1999, IEEE T PATTERN ANAL, V21, P1241, DOI 10.1109/34.809119
   CAMPBELL LW, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P624, DOI 10.1109/ICCV.1995.466880
   CEDRAS C, 1995, IMAGE VISION COMPUT, V13, P129, DOI 10.1016/0262-8856(95)93154-K
   Deriche R., 1995, P 2 AS C COMP VIS SI, P71
   Derrington AM, 2004, ANNU REV PSYCHOL, V55, P181, DOI 10.1146/annurev.psych.55.090902.141903
   Fleet D., 2006, HDB MATH MODELS COMP
   Gao CY, 2010, COMPUT VIS IMAGE UND, V114, P168, DOI 10.1016/j.cviu.2009.03.005
   Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716
   GAVRILA DM, 1995, P INT WORKSH AUT FAC, P272
   Haritaoglu I, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P222, DOI 10.1109/AFGR.1998.670952
   Haritaoglu I, 1999, SECOND IEEE WORKSHOP ON VISUAL SURVEILLANCE (VS'99), PROCEEDINGS, P6, DOI 10.1109/VS.1999.780263
   Horn B.K.P., 1986, ROBOT VISION
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Huang T. S., 1983, NATO ASI SERIES
   JOHANSSON G, 1975, SCI AM, V232, P76, DOI 10.1038/scientificamerican0675-76
   Krishnan G., 2008, IEEE WORKSH APPL COM
   Lucas B.D., 1981, P INT JOINT C ART IN
   MARTIN W, 1988, MOTION UNDERSTANDING
   Mumford D. S., 1985, P CVPR
   Ogale AS, 2007, INT J COMPUT VISION, V72, P9, DOI 10.1007/s11263-006-8890-9
   Oliver N., 1999, Computer Vision Systems. First International Conference, ICVS'99. Proceedings, P255
   Park S, 2004, MULTIMEDIA SYST, V10, P164, DOI 10.1007/s00530-004-0148-1
   ROACH JW, 1980, IEEE T PATTERN ANAL, V2, P554, DOI 10.1109/TPAMI.1980.6447703
   Russell B., 1970, ZENOS PARADOXES, P51
   Ryoo MS, 2009, INT J COMPUT VISION, V82, P1, DOI 10.1007/s11263-008-0181-1
   Starner T., 1995, Proceedings International Symposium on Computer Vision (Cat. No.95TB100006), P265, DOI 10.1109/ISCV.1995.477012
   Tomasi C., 1991, DETECTION TRACKING P
   Turaga P, 2008, IEEE T CIRC SYST VID, V18, P1473, DOI 10.1109/TCSVT.2008.2005594
   ULLMAN S, 1979, PROC R SOC SER B-BIO, V203, P405, DOI 10.1098/rspb.1979.0006
   ULLMAN S, 1979, INTERPRETATION VISUA
   Vazquez C, 2006, IEEE T PATTERN ANAL, V28, P782, DOI 10.1109/TPAMI.2006.97
   WALLACH H, 1953, J EXP PSYCHOL, V45, P205, DOI 10.1037/h0056880
   WATSON AB, 1985, J OPT SOC AM A, V2, P322, DOI 10.1364/JOSAA.2.000322
   WEBB JA, 1982, ARTIF INTELL, V19, P107, DOI 10.1016/0004-3702(82)90023-6
   Yamato J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P379, DOI 10.1109/CVPR.1992.223161
   ZEKI S, 1992, SCI AM, V267, P69, DOI 10.1038/scientificamerican0992-68
NR 43
TC 7
Z9 7
U1 1
U2 1
PU SPRINGER-VERLAG LONDON LTD
PI GODALMING
PA SWEETAPPLE HOUSE CATTESHALL RD FARNCOMBE, GODALMING GU7 1NH, SURREY,
   ENGLAND
BN 978-0-85729-126-4
PY 2011
BP 27
EP 39
DI 10.1007/978-0-85729-127-1_2
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Imaging Science & Photographic Technology
GA BH0HC
UT WOS:000394929600002
DA 2022-02-03
ER

PT C
AU Jiang, WY
   Xiang, J
   Liu, LM
   Zha, DR
   Wang, L
AF Jiang, Weiyu
   Xiang, Ji
   Liu, Limin
   Zha, Daren
   Wang, Lei
GP IEEE
TI From Mini House Game to Hobby-driven Behavioral Biometrics-based
   Password
SO 2013 12TH IEEE INTERNATIONAL CONFERENCE ON TRUST, SECURITY AND PRIVACY
   IN COMPUTING AND COMMUNICATIONS (TRUSTCOM 2013)
SE IEEE International Conference on Trust Security and Privacy in Computing
   and Communications
LA English
DT Proceedings Paper
CT 12th IEEE International Conference on Trust, Security and Privacy in
   Computing and Communications (TrustCom)
CY JUL 16-18, 2013
CL Melbourne, AUSTRALIA
DE Hobby-driven; Habitual Behavior; Authentication; Similarity
ID UNCERTAINTY
AB It is believed that one of the major problems bothering people is that the password is prone to be forgotten when signing in services on the Internet. Although both recognition-based graphical password schemes and behavioral biometrics-based password schemes have been widely known in the field of information security because of their relatively high security level and being easy to memorize, the remaining security problems on graphical password schemes and accuracy issues upon behavioral biometrics-based password schemes are still great bottlenecks, which prevents them from being populated. In this paper a comprehensive study on habitual behaviors driven by hobbies will be presented firstly. Then a novel authentication system, which reaches a reasonable high security standard and accuracy level by combining the hobby-driven behavioral biometric technique and recognition-based graphical password techniques, is introduced. The key characteristic of the system lies in behavioral features related to style, color, position and habitual operating order of the object images for authentication. These metrics are relatively unique from person to person but steady because such a repeated behavior in daily life is a hobby-driven habitual behavior. And finally, the experiments' result analysis is available, which shows that the novel authentication system can maintain robust when facing traditional attacks while stay easy to remember at the same time.
C1 [Liu, Limin; Zha, Daren; Wang, Lei] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Jiang, Weiyu; Xiang, Ji; Liu, Limin; Zha, Daren; Wang, Lei] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Information Engineering,
   CAS
RP Zha, DR (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.; Zha, DR (corresponding author), Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing, Peoples R China.
EM wyjiang@lois.cn; jixiang@lois.cn; lmliu@lois.cn; zdr@lois.cn;
   lwang@lois.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [70890084/G021102, 61003274]; Strategy Pilot
   Project of Chinese Academy of Sciences [XDA06010702]; National High
   Technology ReResearch and Development Program of China (863
   Program)National High Technology Research and Development Program of
   China [2013AA01A214, 2012AA013104]
FX This work is supported by National Natural Science Foundation of China
   grant 70890084/G021102 and 61003274, Strategy Pilot Project of Chinese
   Academy of Sciences subproject XDA06010702, and National High Technology
   ReResearch and Development Program of China (863 Program, No.
   2013AA01A214 and 2012AA013104).
CR Ahmed AAE, 2005, Proceedings from the Sixth Annual IEEE Systems, Man and Cybernetics Information Assurance Workshop, P452, DOI 10.1109/IAW.2005.1495997
   Ahmed AAE, 2007, IEEE T DEPEND SECURE, V4, P165, DOI 10.1109/TDSC.2007.70207
   ATWOOD G, 1971, COGNITIVE PSYCHOL, V2, P290, DOI 10.1016/0010-0285(71)90015-6
   Biddle R., 2009, TR0909 CARL U SCH CO
   Gamboa H, 2004, P SOC PHOTO-OPT INS, V5404, P381, DOI 10.1117/12.542625
   HIGASHI M, 1982, INT J GEN SYST, V9, P43, DOI 10.1080/03081078208960799
   Jermyn I, 1999, USENIX ASSOCIATION PROCEEDINGS OF THE EIGHTH USENIX SECURITY SYMPOSIUM (SECURITY '99), P1
   Kauffman J., 2003, GRIP PATTERN RECOGNI
   KLIR GJ, 1987, FUZZY SET SYST, V24, P197, DOI 10.1016/0165-0114(87)90090-X
   MADIGAN S, 1974, AM J PSYCHOL, V87, P151, DOI 10.2307/1422009
   Marquardt P, 2011, PROCEEDINGS OF THE 18TH ACM CONFERENCE ON COMPUTER & COMMUNICATIONS SECURITY (CCS 11), P551
   Mihajlov M., 2011, INTERACTING COMPUTER
   Monrose F., 2002, International Journal of Information Security, V1, P69, DOI 10.1007/s102070100006
   Paivio A., 1968, PSYCHONOMIC SCI
   Pusara M, 2004, P 2004 ACM WORKSH VI, P1
   SHEPARD RN, 1967, J VERB LEARN VERB BE, V6, P156, DOI 10.1016/S0022-5371(67)80067-7
   Verplanken B, 1997, EUR J SOC PSYCHOL, V27, P539, DOI 10.1002/(SICI)1099-0992(199709/10)27:5<539::AID-EJSP831>3.3.CO;2-1
   Westeyn T, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P717, DOI 10.1109/AFGR.2004.1301619
   Westeyn T., 2005, P HCI INT 05
   Yampolskiy RV, 2008, INT J BIOMETRICS, V1, P81, DOI 10.1504/IJBM.2008.018665
   Zheng N, 2011, PROCEEDINGS OF THE 18TH ACM CONFERENCE ON COMPUTER & COMMUNICATIONS SECURITY (CCS 11), P139
NR 21
TC 6
Z9 6
U1 0
U2 4
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2324-898X
BN 978-0-7695-5022-0
J9 IEEE INT CONF TRUST
PY 2013
BP 712
EP 719
DI 10.1109/TrustCom.2013.86
PG 8
WC Computer Science, Hardware & Architecture; Computer Science, Theory &
   Methods; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA BA1QE
UT WOS:000332856700091
DA 2022-02-03
ER

PT C
AU Huerta, I
   Fernandez, C
   Prati, A
AF Huerta, Ivan
   Fernandez, Carles
   Prati, Andrea
BE Agapito, L
   Bronstein, MM
   Rother, C
TI Facial Age Estimation Through the Fusion of Texture and Local Appearance
   Descriptors
SO COMPUTER VISION - ECCV 2014 WORKSHOPS, PT II
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 13th European Conference on Computer Vision (ECCV)
CY SEP 06-12, 2014
CL Zurich, SWITZERLAND
DE Age estimation; CCA; HOG; LBP; SURF
ID RECOGNITION
AB Automatic extraction of soft biometric characteristics from face images is a very prolific field of research. Among these soft biometrics, age estimation can be very useful for several applications, such as advanced video surveillance [5,12], demographic statistics collection, business intelligence and customer profiling, and search optimization in large databases. However, estimating age from uncontrollable environments, with insufficient and incomplete training data, dealing with strong person-specificity, and high within-range variance, can be very challenging. These difficulties have been addressed in the past with complex and strongly hand-crafted descriptors, which make it difficult to replicate and compare the validity of posterior classification schemes. This paper presents a simple yet effective approach which fuses and exploits texture-and local appearance-based descriptors to achieve faster and more accurate results. A series of local descriptors and their combinations have been evaluated under a diversity of settings, and the extensive experiments carried out on two large databases (MORPH and FRGC) demonstrate state-of-the-art results over previous work.
C1 [Huerta, Ivan; Prati, Andrea] Univ IUAV, DPDCE, I-30135 Venice, Italy.
   [Fernandez, Carles] Herta Secur, Barcelona 08037, Spain.
C3 IUAV University Venice
RP Huerta, I (corresponding author), Univ IUAV, DPDCE, Santa Croce 1957, I-30135 Venice, Italy.
EM huertacasado@iuav.it; carles.fernandez@hertasecurity.com; aprati@iuav.it
RI Prati, Andrea/B-7440-2014
OI Prati, Andrea/0000-0002-1211-529X; Huerta, Ivan/0000-0002-4189-4034
CR Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Chang KY, 2011, PROC CVPR IEEE, P585, DOI 10.1109/CVPR.2011.5995437
   Fernandez C., 2014, FFER CONJUN IN PRESS
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   Gallagher AC, 2009, PROC CVPR IEEE, P256, DOI 10.1109/CVPRW.2009.5206828
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Guo G., 2014, IMAGE VISION COMPUTI
   Guo GD, 2013, IEEE INT CONF AUTOMA
   Guo GD, 2011, PROC CVPR IEEE, P657, DOI 10.1109/CVPR.2011.5995404
   Guo GD, 2009, PROC CVPR IEEE, P112, DOI 10.1109/CVPRW.2009.5206681
   Han H., 2013, INT C BIOM ICB
   Lanitis A, 2004, IEEE T SYST MAN CY B, V34, P621, DOI 10.1109/TSMCB.2003.817091
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   Minear M, 2004, BEHAV RES METH INS C, V36, P630, DOI 10.3758/BF03206543
   Montilla A, 2009, 2009 16TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOLS 1-6, P2465, DOI 10.1109/ICIP.2009.5414103
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oro D., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P530, DOI 10.1109/ICCVW.2011.6130288
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Triggs B., 2005, PROC CVPR IEEE, V1, P886, DOI DOI 10.1109/CVPR.2005.177
   Weng R., 2013, AFGR
NR 24
TC 8
Z9 8
U1 0
U2 4
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-319-16181-5; 978-3-319-16180-8
J9 LECT NOTES COMPUT SC
PY 2015
VL 8926
BP 667
EP 681
DI 10.1007/978-3-319-16181-5_51
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods; Robotics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Robotics
GA BD6QQ
UT WOS:000362495500051
DA 2022-02-03
ER

PT J
AU Proenca, H
   Neves, JC
AF Proenca, Hugo
   Neves, Joao C.
TI Soft Biometrics: Globally Coherent Solutions for Hair Segmentation and
   Style Recognition Based on Hierarchical MRFs
SO IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY
LA English
DT Article
DE Soft biometrics; visual surveillance; homeland security
ID COLOR; ALGORITHM
AB Markov Random Fields (MRFs) are a popular tool in many computer vision problems and faithfully model a broad range of local dependencies. However, rooted in the Hammersley-Clifford theorem, they face serious difficulties in enforcing the global coherence of the solutions without using too high order cliques that reduce the computational effectiveness of the inference phase. Having this problem in mind, we describe a multi-layered (hierarchical) architecture for MRFs that is based exclusively in pairwise connections and typically produces globally coherent solutions, with 1) one layer working at the local (pixel) level, modeling the interactions between adjacent image patches; and 2) a complementary layer working at the object (hypothesis) level pushing toward globally consistent solutions. During optimization, both layers interact into an equilibrium state that not only segments the data, but also classifies it. The proposed MRF architecture is particularly suitable for problems that deal with biological data (e.g., biometrics), where the reasonability of the solutions can be objectively measured. As test case, we considered the problem of hair / facial hair segmentation and labeling, which are soft biometric labels useful for human recognition in-the-wild. We observed performance levels close to the state-of-the-art at a much lower computational cost, both in the segmentation and classification (labeling) tasks.
C1 [Proenca, Hugo; Neves, Joao C.] Univ Beira Interior, Dept Comp Sci, IT Inst Telecomunicacoes, P-6201001 Covilha, Portugal.
C3 Universidade da Beira Interior
RP Proenca, H (corresponding author), Univ Beira Interior, Dept Comp Sci, IT Inst Telecomunicacoes, P-6201001 Covilha, Portugal.
EM hugomcp@di.ubi.pt; jcneves@di.ubi.pt
RI Proenca, Hugo/F-9499-2010; Neves, Joao/G-6477-2016
OI Proenca, Hugo/0000-0003-2551-8570; Neves, Joao/0000-0003-0139-2213
FU FCTPortuguese Foundation for Science and TechnologyEuropean Commission
   [UID/EEA/50008/2013]
FX This work was supported by the FCT Project under Grant
   UID/EEA/50008/2013. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Matti Pietikainen.
CR Albiol A, 2001, IEEE IMAGE PROC, P122, DOI 10.1109/ICIP.2001.958968
   Besbes A, 2009, PROC CVPR IEEE, P1295, DOI 10.1109/CVPRW.2009.5206649
   Byrd RH, 1999, SIAM J OPTIMIZ, V9, P877, DOI 10.1137/S1052623497325107
   Caputo B, 2002, INT C PATT RECOG, P565, DOI 10.1109/ICPR.2002.1048002
   Dass J., 2013, P 4 NAT C COMP VIS P, P1
   Efraty B., 2011, P INT JOINT C BIOM O, P1
   Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4
   Glocker B, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: FROM NANO TO MACRO, VOLS 1 AND 2, P422, DOI 10.1109/ISBI.2009.5193074
   Huang G. B., 2007, 0749 U MASS AMH
   Jaiswal S, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P370, DOI 10.1109/ICCVW.2013.56
   Julian P, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4617, DOI 10.1109/ICPR.2010.1134
   Kae A, 2013, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2013.263
   Kato Z, 2006, IMAGE VISION COMPUT, V24, P1103, DOI 10.1016/j.imavis.2006.03.005
   Koestinger Martin, 2011, P IEEE INT C COMP VI, P2144, DOI DOI 10.1109/ICCVW.2011.6130513
   Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200
   Krupka A., 2014, P 6 INT C ADV MUL, P102
   Lee K.-c., 2008, 2008 8 IEEE INT C AU 2008 IEEE INT C AUT, P1
   Lipowezky U, 2008, IEEE CONV EL ELECT I, P51, DOI 10.1109/EEEI.2008.4736632
   Proenca H, 2016, IEEE T PATTERN ANAL, V38, P2444, DOI 10.1109/TPAMI.2016.2522441
   Proenca H, 2014, 2014 IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2014)
   Rapp V, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P265, DOI 10.1109/FG.2011.5771409
   Rousset C, 2008, IEEE IMAGE PROC, P2276, DOI 10.1109/ICIP.2008.4712245
   Shen YH, 2014, SCI WORLD J, DOI 10.1155/2014/748634
   Tome P, 2014, IEEE T INF FOREN SEC, V9, P464, DOI 10.1109/TIFS.2014.2299975
   Ugurlu Y, 2012, INT C PATT RECOG, P1
   Wang CH, 2009, IEEE I CONF COMP VIS, P747
   Wang D., 2013, 2013 10 IEEE INT C W, P1
   Wang N, 2011, 2011 FIRST ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P110, DOI 10.1109/ACPR.2011.6166682
   Wang Y, 2014, INT C PATT RECOG, P450, DOI 10.1109/ICPR.2014.86
   Yacoob Y, 2006, IEEE T PATTERN ANAL, V28, P1164, DOI 10.1109/TPAMI.2006.139
   Young J., 1993, DOTFAAAM9310
   Yuksel C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618512
   Zhang Z, 2009, 2009 16TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOLS 1-6, P1137, DOI 10.1109/ICIP.2009.5413535
   Zhang Z, 2008, IEEE IMAGE PROC, P1644, DOI 10.1109/ICIP.2008.4712087
NR 34
TC 10
Z9 10
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1556-6013
EI 1556-6021
J9 IEEE T INF FOREN SEC
JI IEEE Trans. Inf. Forensic Secur.
PD JUL
PY 2017
VL 12
IS 7
BP 1637
EP 1645
DI 10.1109/TIFS.2017.2680246
PG 9
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA EU3HL
UT WOS:000400919700011
OA Green Published
DA 2022-02-03
ER

PT J
AU Pacifici, A
   Gargari, M
   Pacifici, L
AF Pacifici, A.
   Gargari, M.
   Pacifici, L.
TI 3D SOFTWARE SCANNING, PROCESSING AND ARCHIVING PALATAL RUGAE: "IDENTITY
   BASE" TECHNOLOGY
SO JOURNAL OF BIOLOGICAL REGULATORS AND HOMEOSTATIC AGENTS
LA English
DT Article
DE big-data; palatal rugae; forensic odontology; intraoral scanner; oral
   pathology
ID EXPRESSION
AB The palatal rugae, which are anatomically described as folds or wrinkles of the palate, are located on the anterior third of the palate on each side of the palatal raphe and behind the incisive papilla, The use of palatal rugae for personal identification was suggested several years ago, and attracted interest from different researchers which created different classifications, still used in scientific literature. The "identity base" (IB) system has as its object a complex information system and a personal identification protocol by means of three-dimensional palatal scans in digital format. The usefulness of this system is based on the management needs of big data. For example, in the field of forensic odontology, IB can be useful in the identification of a living or cadaver subject; and can estimate the age of a human subject. Moreover, IB stores its associated biometric data. The IB system demonstrated to overcome the issues shown by other similar systems of digital image storage. Furthermore, its high accuracy in the identification process makes IB a reliable tool for institutions in the management of immigrants, as well as in the archiving of people under restrictive measures. Finally, IB is also a system for sharing and processing clinical images, useful in dental prosthetics to reduce the number of steps from the first visit to dental prosthesis. The next generation of big-data archiving will speak the same language as IB: the route has been already set out.
C1 [Pacifici, A.; Pacifici, L.] Sapienza Univ Rome, Dept Oral & Maxillofacial Sci, Rome, Italy.
   [Gargari, M.] Univ Roma Tor Vergata, Dept Clin Sci & Translat Med, Rome, Italy.
C3 Sapienza University Rome; University of Rome Tor Vergata
RP Pacifici, A (corresponding author), Sapienza Univ Rome, Dept Oral & Maxillofacial Sci, Rome, Italy.
EM dott.andreapacifici@libero.it
OI Pacifici, Andrea/0000-0001-9234-5636
FU company "ASSI Applicazioni, Soluzioni e Servizi per l'Informatica"
FX The Authors wish to acknowledge Mr. Alessandro Zurli, Mr. Emanuele
   Bellini and Mr. Luca Barbieri and the company "ASSI Applicazioni,
   Soluzioni e Servizi per l'Informatica" for their support in the
   management of this project and for having improved the results of this
   pilot study.
CR Aulino P, 2015, INT J MED SCI, V12, P336, DOI 10.7150/ijms.10761
   Marrelli M, 2016, INT J IMMUNOPATH PH, V29, P778, DOI 10.1177/0394632016646121
   Marrelli M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0112444
   Marrelli M, 2012, INT J MED SCI, V9, P47, DOI 10.7150/ijms.9.47
   Ohtani M, 2008, FORENSIC SCI INT, V176, P178, DOI 10.1016/j.forsciint.2007.09.002
   Paduano F, 2017, INT J MOL SCI, V18, DOI 10.3390/ijms18102140
   Paduano F, 2016, STEM CELL REV REP, V12, P592, DOI 10.1007/s12015-016-9674-4
   Perniconi B, 2014, FRONT PHYSIOL, V5, DOI 10.3389/fphys.2014.00354
   Tatullo M, 2015, J BIOL REG HOMEOS AG, V29, P713
   Tatullo M, 2016, MEDICINE, V95, DOI 10.1097/MD.0000000000005589
   Tatullo M, 2016, SCI REP-UK, V6, DOI 10.1038/srep36042
   Tatullo M, 2015, J CANCER, V6, P976, DOI 10.7150/jca.11936
NR 12
TC 2
Z9 2
U1 0
U2 1
PU BIOLIFE SAS
PI SILVA MARINA (TE)
PA VIA S STEFANO 39 BIS, 64029 SILVA MARINA (TE), ITALY
SN 0393-974X
EI 1724-6083
J9 J BIOL REG HOMEOS AG
JI J. Biol. Regul. Homeost. Agents
PD SEP-OCT
PY 2018
VL 32
IS 5
BP 1291
EP 1294
PG 4
WC Endocrinology & Metabolism; Immunology; Medicine, Research &
   Experimental; Physiology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Endocrinology & Metabolism; Immunology; Research & Experimental
   Medicine; Physiology
GA GZ2KV
UT WOS:000449211700033
PM 30334428
DA 2022-02-03
ER

PT J
AU Lee, J
   Phan, CB
   Koo, S
AF Lee, Jungbin
   Phan, Cong-Bo
   Koo, Seungbum
TI Predicting Three-Dimensional Gait Parameters with a Single Camera Video
   Sequence
SO INTERNATIONAL JOURNAL OF PRECISION ENGINEERING AND MANUFACTURING
LA English
DT Article
DE Gait recognition; Segment length; Three-dimensional pose estimation;
   Video surveillance
ID 3D RECONSTRUCTION; RECOGNITION; MOTION; SHAPE
AB Human gait reflects biomedical conditions and thus can potentially be used for identification. With the increasing utility of CCTVs for surveillance, there have been various attempts to recognize persons using gait image sequences from a single camera. We investigated the accuracy of estimating body segment lengths and joint angles during gait calculated from a video sequence using a gait database. We recruited 30 subjects and collected motion capture data during walking and extracted the trajectories of 17 body points. Principal component analysis (PCA) was applied to the collected gait. We implemented full gait cycle-based (FGC) PCA and gait-phase-specific (GPS) PCA. Three-dimensional poses were estimated from gait event frames using FGC-PCA and GPS-PCA. The estimated poses in discrete gait event frames were interpolated to estimate motion during a full gait cycle. The body pose from GPSPCA was less sensitive to camera angles and smaller errors compared to FGC-PCA. The segment lengths of the upper arm (r=0.79), lower arm (r=0.63), upper leg (r=0.86), and lower leg (r=0.81) were highly correlated with the lengths obtained from the motion capture data. Three-dimensionally reconstructed human motion can reveal personal biometric information and has the potential to be used for human identification.
C1 [Lee, Jungbin; Phan, Cong-Bo; Koo, Seungbum] Chung Ang Univ, Sch Mech Engn, 84 Heukseok Ro, Seoul 06974, South Korea.
C3 Chung Ang University
RP Koo, S (corresponding author), Chung Ang Univ, Sch Mech Engn, 84 Heukseok Ro, Seoul 06974, South Korea.
EM skoo@cau.ac.kr
RI Koo, Seungbum/Q-7705-2018
OI Koo, Seungbum/0000-0002-4843-1083
FU Basic Science Research Program through the NRF [NRF-2017R1A2B2010763];
   Ministry of Science and ICT of Republic of Korea [PA-C000001]; Chung-Ang
   University Graduate Research Scholarship
FX This work was supported by the Basic Science Research Program through
   the NRF (NRF-2017R1A2B2010763) and Projects for Research and Development
   of Police Science and Technology through CRDPST and KNPA (PA-C000001)
   funded by the Ministry of Science and ICT of Republic of Korea. It was
   also supported by the Chung-Ang University Graduate Research Scholarship
   in 2016.
CR [Anonymous], 1995, GAIT POSTURE
   Bobick AF, 2001, PROC CVPR IEEE, P423
   Bouchrika I, 2011, J FORENSIC SCI, V56, P882, DOI 10.1111/j.1556-4029.2011.01793.x
   Collins RT, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P366, DOI 10.1109/AFGR.2002.1004181
   Cunado D, 2003, COMPUT VIS IMAGE UND, V90, P1, DOI 10.1010/SI077-3142(03)00008-0
   GE QJ, 1994, J MECH DESIGN, V116, P749, DOI 10.1115/1.2919446
   Goffredo M., 2008, P 2 IEEE INT C BIOM, P1, DOI DOI 10.1109/AFGR.2008.4813366.
   Gotardo P. F. U., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3065, DOI 10.1109/CVPR.2011.5995560
   Han J, 2004, PROC CVPR IEEE, P842
   Jain AK, 2004, IEEE T CIRC SYST VID, V14, P4, DOI 10.1109/TCSVT.2003.818349
   Jyoung Jy-Young, 2018, [Journal of the Korean Society for Precision Engineering, 한국정밀공학회지], V35, P33
   Kale A, 2004, IEEE T IMAGE PROCESS, V13, P1163, DOI 10.1109/TIP.2004.832865
   Kim CY, 2016, INT J PRECIS ENG MAN, V17, P1209, DOI 10.1007/s12541-016-0145-2
   Ko CY, 2016, INT J PRECIS ENG MAN, V17, P957, DOI 10.1007/s12541-016-0117-6
   Lee L, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P155, DOI 10.1109/AFGR.2002.1004148
   Liu ZY, 2004, INT C PATT RECOG, P211, DOI 10.1109/ICPR.2004.1333741
   Lynnerup N, 2005, J FORENSIC SCI, V50, P112
   Meredith M., 2001, DEP COMPUT SCI, V211, P241
   김지원, 2018, [Journal of the Korean Society for Precision Engineering, 한국정밀공학회지], V35, P47
   Park HS, 2010, LECT NOTES COMPUT SC, V6313, P158
   Purevsuren T, 2016, INT J PRECIS ENG MAN, V17, P1365, DOI 10.1007/s12541-016-0162-1
   Ramakrishna V, 2012, LECT NOTES COMPUT SC, V7575, P573, DOI 10.1007/978-3-642-33765-9_41
   Sarkar S, 2005, IEEE T PATTERN ANAL, V27, P162, DOI 10.1109/TPAMI.2005.39
   Shoemaker K., 1985, Computer Graphics, V19, P245
   Simo-Serra E, 2012, PROC CVPR IEEE, P2673, DOI 10.1109/CVPR.2012.6247988
   Tanawongsuwan R, 2001, PROC CVPR IEEE, P726
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Wandt B, 2016, IEEE T PATTERN ANAL, V38, P1505, DOI 10.1109/TPAMI.2016.2553028
   Wang CY, 2014, PROC CVPR IEEE, P2369, DOI 10.1109/CVPR.2014.303
   Wang L, 2003, IEEE T PATTERN ANAL, V25, P1505, DOI 10.1109/TPAMI.2003.1251144
   Wang L, 2003, IEEE T IMAGE PROCESS, V12, P1120, DOI 10.1109/TIP.2003.815251
   Yen-Lin Chen, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P71
NR 32
TC 3
Z9 3
U1 1
U2 3
PU KOREAN SOC PRECISION ENG
PI SEOUL
PA RM 306, KWANGMYUNG BLDG, 5-4 NONHYUN-DONG, KANGNAM-GU, SEOUL, 135-010,
   SOUTH KOREA
SN 2234-7593
EI 2005-4602
J9 INT J PRECIS ENG MAN
JI Int. J. Precis. Eng. Manuf.
PD MAY
PY 2018
VL 19
IS 5
BP 753
EP 759
DI 10.1007/s12541-018-0090-3
PG 7
WC Engineering, Manufacturing; Engineering, Mechanical
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering
GA GG7XO
UT WOS:000432912000013
DA 2022-02-03
ER

PT J
AU Beli, ILK
   Guo, CS
AF Beli, Idelette Laure Kambi
   Guo, Chunsheng
TI Enhancing Face Identification Using Local Binary Patterns and K-Nearest
   Neighbors
SO JOURNAL OF IMAGING
LA English
DT Article
DE face recognition; face identification; local binary pattern (LBP);
   k-nearest neighbor (K-NN)
AB The human face plays an important role in our social interaction, conveying people's identity. Using the human face as a key to security, biometric passwords technology has received significant attention in the past several years due to its potential for a wide variety of applications. Faces can have many variations in appearance (aging, facial expression, illumination, inaccurate alignment and pose) which continue to cause poor ability to recognize identity. The purpose of our research work is to provide an approach that contributes to resolve face identification issues with large variations of parameters such as pose, illumination, and expression. For provable outcomes, we combined two algorithms: (a) robustness local binary pattern (LBP), used for facial feature extractions; (b) k-nearest neighbor (K-NN) for image classifications. Our experiment has been conducted on the CMU PIE (Carnegie Mellon University Pose, Illumination, and Expression) face database and the LFW (Labeled Faces in the Wild) dataset. The proposed identification system shows higher performance, and also provides successful face similarity measures focus on feature extractions.
C1 [Beli, Idelette Laure Kambi; Guo, Chunsheng] Hangzhou Dianzi Univ, Sch Commun Engn, Hangzhou 310018, Zhejiang, Peoples R China.
C3 Hangzhou Dianzi University
RP Beli, ILK (corresponding author), Hangzhou Dianzi Univ, Sch Commun Engn, Hangzhou 310018, Zhejiang, Peoples R China.
EM kblaure@yahoo.fr; guo.chsh@gmail.com
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [61372157]
FX This work was supported by the National Natural Science Foundation of
   China (No. 61372157)
CR Abate A. F., 2005, ONE MANY 3D FACE REC, P5
   AHONEN T, 2004, FACE RECOGNITION LOC
   Ali A., 2012, BAHRIA U J INF COMMU, V5, P46
   Ameur B., 2016, P 2 INT C ADV TECHN
   Ebrahimpour H., 2007, P INT C SIGN PROC CO
   Hu G., 2015, P IEEE INT C COMP VI
   Jones Michael J, 2009, Transactions of the Institute of Electrical Engineers of Japan, Part C, V129, P770, DOI 10.1541/ieejeiss.129.770
   Kaur M., 2012, INT J COMPUT APPL, V60, P13
   Khorsheed J. A., 2016, P SIGN PROC COMM APP
   Mau S., 2012, P 2012 INT C DIG IM
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Panchal P, 2015, NIRMA UNIV INT CONF
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Rahim A., 2013, GLOB J COMPUT SIENCE, V13, P469
   Schroff F., 2011, P 2011 INT C COMP VI
   Sim T, 2002, P 5 IEEE INT C AUT F
   Sultana M., 2014, P 2014 INT C COMP VI
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Wen Y., 2016, P EUR C COMP VIS AMS
NR 19
TC 9
Z9 9
U1 1
U2 3
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2313-433X
J9 J IMAGING
JI J. Imaging
PD SEP
PY 2017
VL 3
IS 3
AR 37
DI 10.3390/jimaging3030037
PG 12
WC Imaging Science & Photographic Technology
WE Emerging Sources Citation Index (ESCI)
SC Imaging Science & Photographic Technology
GA FV2PL
UT WOS:000424410300001
OA Green Submitted, gold
DA 2022-02-03
ER

PT C
AU Severin, F
   Baradarani, A
   Taylor, J
   Zhelnakov, S
   Maev, R
AF Severin, Fedar
   Baradarani, Aryaz
   Taylor, Jason
   Zhelnakov, Serge
   Maev, Roman
GP IEEE
TI Auto-adjustment of Image Produced by Multi-transducer Ultrasonic System
SO 2014 IEEE INTERNATIONAL ULTRASONICS SYMPOSIUM (IUS)
SE IEEE International Ultrasonics Symposium
LA English
DT Proceedings Paper
CT IEEE International Ultrasonics Symposium (IUS)
CY SEP 03-06, 2014
CL Chicago, IL
DE Acoustical microscopy; array transducer; image processing
AB Acoustic microscopy is characterized by relatively long scanning time, which is required for the motion of the transducer over the entire scanning area. This time may be reduced by using a multi-channel acoustical system which has several identical transducers arranged as an array and is mounted on a mechanical scanner so that each transducer scans only a fraction of the total area. The resulting image is formed as a combination of all acquired partial data sets. The mechanical instability of the scanner, as well as the difference in parameters of the individual transducers causes a misalignment of the image fractures. This distortion may be partially compensated for by the introduction of constant or dynamical signal leveling and data shift procedures. However, a reduction of the random instability component requires more advanced algorithms, including auto-adjustment of processing parameters.
   The described procedure was implemented into the prototype of an ultrasonic fingerprint reading system. The specialized cylindrical scanner provides a helical spiral lens trajectory which eliminates repeatable acceleration, reduces vibration and allows constant data flow on maximal rate. It is equipped with an array of four spherically focused 50 MHz acoustic lenses operating in pulse-echo mode. Each transducer is connected to a separate channel including pulser, receiver and digitizer. The output 3D data volume contains interlaced B-scans coming from each channel. Afterward, data processing includes pre-determined procedures of constant layer shift in order to compensate for the transducer displacement, phase shift and amplitude leveling for compensation of variation in transducer characteristics. Analysis of statistical parameters of individual scans allows adaptive eliminating of the axial misalignment and mechanical vibrations. Further 2D correlation of overlapping partial C-scans will realize an interpolative adjustment which essentially improves the output image.
   Implementation of this adaptive algorithm into a data processing sequence allows us to significantly reduce misreading due to hardware noise and finger motion during scanning. The system provides a high quality acoustic image of the fingerprint including different levels of information: fingerprint pattern, sweat porous locations, internal dermis structures. These additional features can effectively facilitate fingerprint based identification.
   The developed principles and algorithm implementations allow improved quality, stability and reliability of acoustical data obtained with the mechanical scanner, accommodating several transducers. General principles developed during this work can be applied to other configurations of advanced ultrasonic systems designed for various biomedical and NDE applications. The data processing algorithm, developed for a specific biometric task, can be adapted for the compensation of mechanical imperfections of the other devices.
C1 [Severin, Fedar; Baradarani, Aryaz; Taylor, Jason; Zhelnakov, Serge; Maev, Roman] Inst Diagnost Imaging Res, Windsor, ON, Canada.
RP Severin, F (corresponding author), Inst Diagnost Imaging Res, Windsor, ON, Canada.
EM seviazy@uwindsor.ca
RI Seviaryn, Fedar/AAU-6033-2020; Maev, Roman/AAT-8808-2020
CR Maev R. Gr., 2002, ULTRASONIC IMAGING, V25, P157
   Maev RG, 2012, PROC SPIE, V8546, DOI 10.1117/12.976344
   Titov SA, 2009, INSTRUM EXP TECH+, V52, P721, DOI 10.1134/S0020441209050145
   Tuma T, 2012, P AMER CONTR CONF, P3791
   Zheng F., 2011, P SOC PHOTO-OPT INS, V7983
NR 5
TC 0
Z9 0
U1 0
U2 5
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1948-5719
BN 978-1-4799-7049-0
J9 IEEE INT ULTRA SYM
PY 2014
BP 1944
EP 1947
DI 10.1109/ULTSYM.2014.0483
PG 4
WC Acoustics; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Acoustics; Engineering
GA BC4NN
UT WOS:000352792500481
DA 2022-02-03
ER

PT C
AU Sequeira, AF
   Murari, J
   Cardoso, JS
AF Sequeira, Ana F.
   Murari, Juliano
   Cardoso, Jaime S.
GP IEEE
BE Battiato, S
   Braz, J
TI Iris Liveness Detection Methods in Mobile Applications
SO PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION,
   THEORY AND APPLICATIONS (VISAPP 2014), VOL 3
LA English
DT Proceedings Paper
CT 9th International Conference on Computer Vision Theory and Applications
   (VISAPP)
CY JAN 05-08, 2014
CL Lisbon, PORTUGAL
DE Biometrics; Iris; Liveness Detection; Fake Database; Handheld Device
ID FEATURE-SELECTION; RECOGNITION; IMAGE
AB Biometric systems are vulnerable to different kinds of attacks. Particularly, the systems based on iris are vulnerable to direct attacks consisting on the presentation of a fake iris to the sensor trying to access the system as it was from a legitimate user. The analysis of some countermeasures against this type of attacking scheme is the problem addressed in the present paper. Several state-of-the-art methods were implemented and included in a feature selection framework so as to determine the best cardinality and the best subset that conducts to the highest classification rate. Three different classifiers were used: Discriminant analysis, K nearest neighbours and Support Vector Machines. The implemented methods were tested in existing databases for iris liveness purposes (Biosec and Clarkson) and in a new fake database which was constructed for evaluation of iris liveness detection methods in the mobile scenario. The results suggest that this new database is more challenging than the others. Therefore, improvements are required in this line of research to achieve good performance in real world mobile applications.
C1 [Sequeira, Ana F.; Cardoso, Jaime S.] INESC TEC, INESC Porto, Oporto, Portugal.
   [Sequeira, Ana F.; Cardoso, Jaime S.] Univ Porto, Fac Engn, Oporto, Portugal.
   [Murari, Juliano] Univ Fed S Paulo, Sao Paulo, Brazil.
C3 INESC; Universidade do Porto; Universidade do Porto
RP Sequeira, AF (corresponding author), INESC TEC, INESC Porto, Oporto, Portugal.
EM ana.filipa.sequeira@fe.up.pt; juliano.murari@usp.dc.mu.edu;
   jaime.cardoso@fe.up.pt
RI Sequeira, Ana F./AAF-6339-2020; Cardoso, Jaime/I-3286-2013
OI Sequeira, Ana F./0000-0002-6685-2033; Cardoso, Jaime/0000-0002-3760-2473
FU Fundacao para a Ciencia e Tecnologia (FCT) - PortugalPortuguese
   Foundation for Science and Technology [SFRH/BD/74263/2010]
FX The first author would like to thank the Fundacao para a Ciencia e
   Tecnologia (FCT) - Portugal for the financial support for the PhD grant
   with reference SFRH/BD/74263/2010. The second author would like to thank
   the National Council for Scientific and Technological Development (CNPq)
   - Brazil.
CR Abhyankar A, 2009, PATTERN RECOGN, V42, P1878, DOI 10.1016/j.patcog.2009.01.004
   Blind Ref B. R., 2013, REFERENCE REMOVED BL
   Clarkson University N. D. U. and of Technology W. U., 2013, LIV DET IRIS COMP 20
   Daugman J, 2002, IEEE IMAGE PROC, P33
   Daugman J., 1998, INFORM SECURITY TECH, V3, P33
   Daugman J., 2004, 7 INT BIOM C
   Fierrez J, 2007, PATTERN RECOGN, V40, P1389, DOI 10.1016/j.patcog.2006.10.014
   Galbally J., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P271, DOI 10.1109/ICB.2012.6199819
   Galbally J., 2007, DATABASE, V1, P4
   Galbally J, 2012, FUTURE GENER COMP SY, V28, P311, DOI 10.1016/j.future.2010.11.024
   GIMP G., 2008, USER MANUAL EDGE DET
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   He XF, 2007, LECT NOTES COMPUT SC, V4642, P540
   He XF, 2009, LECT NOTES COMPUT SC, V5558, P1132
   Jain A, 1997, IEEE T PATTERN ANAL, V19, P153, DOI 10.1109/34.574797
   Kanematsu Masashi, 2007, SICE '07. 46th SICE Annual Conference, P361
   Lee EC, 2006, LECT NOTES COMPUT SC, V3832, P397
   Li JW, 2004, P SOC PHOTO-OPT INS, V5404, P296, DOI 10.1117/12.541955
   Ma L, 2003, IEEE T PATTERN ANAL, V25, P1519, DOI 10.1109/TPAMI.2003.1251145
   Monteiro Joao C., 2013, International Conference on Computer Vision Theory and Applications (VISAPP 2013). Proceedings, P180
   Monteiro J. C., 2014, CCIS COMMUNICATIONS
   PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9
   Ratha NK, 2001, LECT NOTES COMPUT SC, V2091, P223
   Ruiz-Albacete V, 2008, LECT NOTES COMPUT SC, V5372, P181, DOI 10.1007/978-3-540-89991-4_19
   Schuckers S., 2013, LIVINESS DETECTION I
   Stearns S. D., 1976, 3rd International Joint Conference on Pattern Recognition, P71
   Une M., 2006, IPSJ MAGAZINE, V47, P605
   Wei ZH, 2008, GLOB TELECOMM CONF, DOI 10.1109/GLOCOM.2008.ECP.428
   WHITNEY AW, 1971, IEEE T COMPUT, VC 20, P1100, DOI 10.1109/T-C.1971.223410
NR 29
TC 17
Z9 17
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-9-8975-8133-5
PY 2014
BP 22
EP 33
PG 12
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BI5RF
UT WOS:000412740600003
DA 2022-02-03
ER

PT J
AU Xue, MF
   He, C
   Wang, J
   Liu, WQ
AF Xue, Mingfu
   He, Can
   Wang, Jian
   Liu, Weiqiang
TI Backdoors hidden in facial features: a novel invisible backdoor attack
   against face recognition systems
SO PEER-TO-PEER NETWORKING AND APPLICATIONS
LA English
DT Article
DE Artificial intelligence security; Backdoor attacks; Deep learning; Face
   recognition systems; Biometric feature recognition
ID LEARNING FRAMEWORK
AB Deep neural network (DNN) based face recognition system has become one of the most popular modalities for user identity authentication. However, some recent studies have indicated that, the malicious attackers can inject specific backdoors into the DNN model of a face recognition system, which is known as backdoor attack. As a result, the attacker can trigger the backdoors and impersonate someone else to log into the system, while not affecting the normal usage of the legitimate users. Existing studies use the accessories (such as purple sunglasses or bandanna) as the triggers of their backdoor attacks, which are visually conspicuous and can be easily perceptible by humans, thus result in the failure of backdoor attacks. In this paper, for the first time, we exploit the facial features as the carriers to embed the backdoors, and propose a novel backdoor attack method, named BHF2 (Backdoor Hidden in Facial Features). The BHF2 constructs the masks with the shapes of facial features (eyebrows and beard), and then injects the backdoors into the masks to ensure the visual stealthiness. Further, to make the backdoors look more natural, we propose BHF2N (Backdoor Hidden in Facial Features Naturally) method, which exploits the artificial intelligence (AI) based tool to auto-embed the natural backdoors. The generated backdoors are visually stealthy, which can guarantee the concealment of the backdoor attacks. The proposed methods (BHF2 and BHF2N) can be applied for those black-box attack scenarios, in which a malicious adversary has no knowledge of the target face recognition system. Moreover, the proposed attack methods are feasible for those strict identity authentication scenarios where the accessories are not permitted. Experimental results on two state-of-the-art face recognition models show that, the maximum success rate of the proposed attack method reaches 100% on DeepID1 and VGGFace models, while the accuracy degradation of target recognition models are as low as 0.01% (DeepID1) and 0.02% (VGGFace), respectively. Meantime, the generated backdoors can achieve visual stealthiness, where the pixel change rate of a backdoor instance relative to its clean face image is as low as 0.16%, and their structural and dHash similarity score are high up to 98.82% and 98.19%, respectively.
C1 [Xue, Mingfu; He, Can; Wang, Jian] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Peoples R China.
   [Liu, Weiqiang] Nanjing Univ Aeronaut & Astronaut, Coll Elect & Informat Engn, Nanjing, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Nanjing University of
   Aeronautics & Astronautics
RP Xue, MF (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Peoples R China.
EM mingfu.xue@nuaa.edu.cn; hecan@nuaa.edu.cn; wangjian@nuaa.edu.cn;
   liuweiqiang@nuaa.edu.cn
OI Xue, Mingfu/0000-0003-2408-503X
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [61602241]
FX This work is supported by the National Natural Science Foundation of
   China (No. 61602241).
CR [Anonymous], 2015, INSIDER VERSUS OUTSI
   [Anonymous], 2017, INSIDER THREATS MAIN
   Arsenovic M, 2017, I S INTELL SYST INFO, P53, DOI 10.1109/SISY.2017.8080587
   Can He, 2020, TURC'20: Proceedings of the Turing Celebration Conference - China, P231, DOI 10.1145/3393527.3393567
   Chen B, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P1, DOI [10.1111/pbi.13183, 10.1145/3304109.3306234]
   Chen X., 2017, ARXIV171205526
   Choi K, 2011, PATTERN RECOGN, V44, P386, DOI 10.1016/j.patcog.2010.08.009
   Dang Q, 2019, TSINGHUA SCI TECHNOL, V24, P663, DOI 10.26599/TST.2018.9010100
   Doan B. G., 2019, ARXIV190803369
   Gao YS, 2019, 35TH ANNUAL COMPUTER SECURITY APPLICATIONS CONFERENCE (ACSA), P113, DOI 10.1145/3359789.3359790
   Gu T., 2017, ARXIV170806733
   Haoti Zhong, 2020, CODASPY'20. Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy, P97, DOI 10.1145/3374664.3375751
   Jiang WW, 2019, TSINGHUA SCI TECHNOL, V24, P52, DOI 10.26599/TST.2018.9010033
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Li S., 2019, ARXIV190902742
   Liu K, 2018, LECT NOTES COMPUT SC, V11050, P273, DOI 10.1007/978-3-030-00470-5_13
   Liu Y. X., 2018, IEEE J SEL TOP QUANT, V25, P1, DOI DOI 10.1109/jstqe.2018.2846046
   Pagano C., 2012, IJCNN, P1, DOI DOI 10.1109/IJCNN.2012.6252659
   Parkhi OM, 2015, P BR MACH VIS, P1, DOI DOI 10.5244/C.29.41
   Rakin AS, 2020, P 11 INT POW EL DRIV, P1
   Sarkar E., 2020, ARXIV200611623
   Soyata T, 2012, 2012 IEEE SYMPOSIUM ON COMPUTERS AND COMMUNICATIONS (ISCC), P59, DOI 10.1109/ISCC.2012.6249269
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Udeshi S, 2020, ARXIV200611623
   Wang BL, 2019, P IEEE S SECUR PRIV, P707, DOI 10.1109/SP.2019.00031
   Wang S, 2020, ARXIV200103274
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen Z, 2011, P INT C GEN EV COMP, P709
   Wenger E, 2020, ARXIV200614580
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Xi GH, 2019, TSINGHUA SCI TECHNOL, V24, P226, DOI 10.26599/TST.2018.9010114
   Xue M., 2020, IEEE T DEPEND SECURE, P1
   Xue MF, 2020, IEEE ACCESS, V8, P74720, DOI 10.1109/ACCESS.2020.2987435
   Yao YS, 2019, PROCEEDINGS OF THE 2019 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'19), P2041, DOI 10.1145/3319535.3354209
   Zou M., 2018, ARXIV180203043
NR 35
TC 1
Z9 1
U1 3
U2 10
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 1936-6442
EI 1936-6450
J9 PEER PEER NETW APPL
JI Peer Peer Netw. Appl.
PD MAY
PY 2021
VL 14
IS 3
SI SI
BP 1458
EP 1474
DI 10.1007/s12083-020-01031-z
EA JAN 2021
PG 17
WC Computer Science, Information Systems; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RN3EV
UT WOS:000606159100003
DA 2022-02-03
ER

PT J
AU Meraoumia, A
   Samai, D
   Chitroub, S
AF Meraoumia, Abdallah
   Samai, Djamel
   Chitroub, Salim
TI Can finger knuckle patterns help strengthen the e-banking security?
SO INTERNATIONAL JOURNAL OF EMBEDDED SYSTEMS
LA English
DT Article
DE information security; cryptography; fuzzy commitment; biometrics;
   feature extraction; finger-knuckle-print; FKP; local binary pattern;
   LBP; data fusion
ID CLASSIFICATION; RECOGNITION; ENCRYPTION; IMAGES; FUSION
AB Communication via the internet has become vital for any kind of information exchange private, public, commercial, or military. Banks are the first that have used the internet for financial transactions (e-banking). However, the safe use of e-banking implies that all precautions have been considered to identify legitimate users and thus avoid economic and social damage that may be caused by any possible fraud. In this context, we propose in this paper a secure biometric system dedicated to e-banking for reducing the fraud risk and strengthening the customer confidence. The fuzzy commitment concept associated with the fmger-knuckle-print (FKP) is the core of our proposed system. However, such a system will only be efficient if the FKP features are accurately extracted. For this, we have developed a new method of feature extraction called adaptive extended binary pattern (AELBP). The obtained experimental results have been judged promising for a high security of e-banking with guaranteed trust from costumers.
C1 [Meraoumia, Abdallah] Univ Tebessa, Lab Math Informat & Syst LAMIS, Tebessa 12002, Algeria.
   [Samai, Djamel] Univ Ouargla, Fac Nouvelles Technol Informat & Commun, Lab Genie Elect, Ouargla 30000, Algeria.
   [Chitroub, Salim] Univ Houari Boumediene, Lab Intelligent & Commun Syst Engn LIS, Algiers 16111, Algeria.
C3 Universite de Tebessa; Universite Kasdi Merbah Ouargla; University
   Science & Technology Houari Boumediene
RP Meraoumia, A (corresponding author), Univ Tebessa, Lab Math Informat & Syst LAMIS, Tebessa 12002, Algeria.
EM ameraoumia@gmail.com; samai.djamel@gmail.com; s_chitroub@hotmail.com
RI Samai, Djamel/AAD-4320-2019
OI SAMAI, Djamel/0000-0002-6198-9620
CR Abreu R., 2015, 10 IB C INF SYST TEC, P1
   Bavarsad B., 2015, 9 INT C E COMM DEV C, P1
   Bhat NN, 2016, INT J ADV MANUF TECH, V83, P1487, DOI 10.1007/s00170-015-7441-3
   Farmanbar M, 2016, SIGNAL IMAGE VIDEO P, V10, P951, DOI 10.1007/s11760-015-0845-6
   Goh K. H., 2015, INT J BUSINESS EXCEL, V8, P378
   Guo FC, 2016, IEEE T INF FOREN SEC, V11, P247, DOI 10.1109/TIFS.2015.2489179
   Inamdar SR, 2016, INT J BIOMETRICS, V8, P33, DOI 10.1504/IJBM.2016.077136
   JADID MA, 2016, WIREL TELECOMM SYMP, P184
   Jena Debasish, 2011, International Journal of Information and Communication Technology, V3, P32, DOI 10.1504/IJICT.2011.039521
   Jolfaei A, 2015, INT J ELECTRON SECUR, V7, P258, DOI 10.1504/IJESDF.2015.070389
   Kadri F, 2016, 2016 INT C INF TECHN, P1
   Mahesh VGV, 2015, INT J BIOMETRICS, V7, P286
   Nigam A, 2015, 2015 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P341, DOI 10.1109/SSCI.2015.58
   Singh PK, 2015, INT J APPL PATTERN R, V2, P1, DOI 10.1504/IJAPR.2015.068929
   Soufi Basil, 2013, Human Interface and the Management of Information. Information and Interaction Design. 15th International Conference, HCI International 2013. Proceedings: LNCS 8016, P375, DOI 10.1007/978-3-642-39209-2_43
   Sun B, 2016, J MULTIMODAL USER IN, V10, P125, DOI 10.1007/s12193-015-0203-6
   Suruliandi A, 2015, INT J SIGNAL IMAGING, V8, P260, DOI 10.1504/IJSISE.2015.070546
   The Hong Kong Polytechnic University (PolyU), 2009, FING KNUCKL PRINT DA
   Wang W, 2016, WIRELESS COMMUNICATION AND SENSOR NETWORK, P711
   Zhang L., 2009, IEEE INT C IM PROC I, P1961
   Zheng Y., 2015, INT C DES MAN MECH I, P482
NR 21
TC 0
Z9 0
U1 0
U2 3
PU INDERSCIENCE ENTERPRISES LTD
PI GENEVA
PA WORLD TRADE CENTER BLDG, 29 ROUTE DE PRE-BOIS, CASE POSTALE 856, CH-1215
   GENEVA, SWITZERLAND
SN 1741-1068
EI 1741-1076
J9 INT J EMBED SYST
JI Int. J. Embed. Syst.
PY 2019
VL 11
IS 3
SI SI
BP 325
EP 339
DI 10.1504/IJES.2019.099401
PG 15
WC Computer Science, Hardware & Architecture
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA HW8SH
UT WOS:000466959700009
DA 2022-02-03
ER

PT C
AU Kiertscher, T
   Fischer, R
   Vielhauer, C
AF Kiertscher, Tobias
   Fischer, Robert
   Vielhauer, Claus
GP ACM
TI Latent Fingerprint Detection using a Spectral Texture Feature
SO MM&SEC 11: PROCEEDINGS OF THE 2011 ACM SIGMM MULTIMEDIA AND SECURITY
   WORKSHOP
LA English
DT Proceedings Paper
CT 13th ACM Multimedia Security Workshop
CY SEP 29-30, 2011
CL Buffalo, NY
AB Technologies for advancing and supporting criminalistic forensic are an upcoming challenge with rising importance within the domain of multimedia security and forensic. For example the acquisition and automated analysis of latent fingerprints, using high resolution 3D scanners, appears to be a promising area of research. Within the paper we will give a detailed demarcation of biometric and forensic fingerprint analysis. We will introduce the aim of a partly automated process for forensic fingerprint acquisition, detection, and processing. Based on the idea of splitting the overall scan process into a fast, low resolution coarse scan and a high resolution detailed scan, different approaches and algorithms has to be evaluated, whether they are feasible for detecting latent fingerprints. Referring to the subject of biometrics and emerging applications, this work will show an approach of a novel Fourier-based spectral texture feature for detecting latent fingerprints in low resolution grey-scale images. Our work is based on data that is acquired using a chromatic white light sensor (OWL), which provides intensity and topographic information of the scanned surface areas. While using a very limited set of different surfaces so far, our first experiments have shown good results on flat and non structured surfaces. Using the presented feature, it is possible to detect latent fingerprints on these surfaces. Even on slightly structured surfaces, like wood imitation, the application of the spectral density feature yields promising results. However the first evaluation of the spectral density feature has also shown some serious limitations of its use on low resolution images.
C1 [Kiertscher, Tobias; Fischer, Robert; Vielhauer, Claus] Brandenburg Univ Appl Sci, D-14770 Brandenburg, Germany.
RP Kiertscher, T (corresponding author), Brandenburg Univ Appl Sci, Magdeburger Str 50, D-14770 Brandenburg, Germany.
EM tobias.kiertscher@fh-brandenburg.de; robert.fischer@fh-brandenburg.de;
   claus.vielhauer@fh-brandenburg.de
CR Bracewell R. N., 1986, FOURIER TRANSFORM IT
   Briggs W. L., 1987, DFT OWNERS MANUAL DI
   Brigham E O., 1988, FAST FOURIER TRANSFO
   Hildebrandt M., 2011, SPRINGER LECT NOTES, V6583, P2011
   ISO/IEC, 2007, 247412007E ISOIEC TR
   Leich M., 2011, P SPIE, V7864
   Leich M., 2011, P SPIE, V7880
   Steinert U., 2008, DAKTYLOSKOPIE
   Steinert U., 2008, GRUNDLAGEN KRIMINALT
NR 9
TC 6
Z9 6
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
BN 978-1-4503-0806-9
PY 2011
BP 27
EP 32
PG 6
WC Computer Science, Software Engineering; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BAA40
UT WOS:000303493400005
DA 2022-02-03
ER

PT J
AU Revathi, A
   Sasikaladevi, N
   Jeyalakshmi, C
AF Revathi, A.
   Sasikaladevi, N.
   Jeyalakshmi, C.
TI Digital speech watermarking to enhance the security using speech as a
   biometric for person authentication
SO INTERNATIONAL JOURNAL OF SPEECH TECHNOLOGY
LA English
DT Article
DE Discrete wavelet transform (DWT); Variance; Speech watermarking;
   Watermark; Security; Speaker authentication; Copyright protection; Peak
   signal to noise ratio (PSNR); Bit error rate (BER); Recognition accuracy
   (RA)
AB This work presents the modules for enhancing the security of speaker authentication by embedding the watermark in a speech signal. Speaker is authenticated by speech as well as the extracted watermark from the watermarked speech. Firstly, the speech signal is converted into frames, and discrete wavelet transform is applied to each frame, and it is preferable to embed the watermark in detail coefficients. The segment for embedding the watermark is appropriately chosen based on the energy calculations. The approximation and the modified detail coefficients are used to generate the watermarked speech by inverse discrete wavelet transform. Imperceptibility of the watermark in a watermarked speech is purely depending on the embedding of the watermark. In the receiver, the watermarked speech will undergo wavelet decomposition, and the watermark bits are extracted from the detail coefficients and appropriately transformed into watermark speech/image. The performance the work is evaluated by using the metrics such as Peak signal to noise ratio (PSNR) between original watermark and extracted watermark, PSNR between original speech and watermarked speech and Bit error rate (BER) and Perceptual evaluation speech quality (PESQ). Speaker identification system is assessed by using extraction of the perceptual features and application of features to develop the models for the set of utterances about the speaker during the training phase of the work. Testing is done by applying the original and watermarked speech utterances to the feature extraction phase, followed by we have the testing phase which is used for computing the accuracy. Accuracy is 98.2% for the speaker identification with the set of original test utterances and 98.1% with watermarked set of test utterances and it is observed that there is the marginal difference in accuracy for using speech as a watermark. It is 97.85% for using the image as a watermark. Cover speech signals and watermark speech used in our work are continuous speech utterances chosen from "TIMIT" speech database. Image watermark is the Quick response (QR) code for the LOGO. This work also emphasizes the effectiveness of the algorithm in providing robustness for copyright protection to ownership of the data and authenticating persons using speech as a biometric.
C1 [Revathi, A.] SASTRA Deemed Univ, Dept ECE SEEE, Thanjavur, India.
   [Sasikaladevi, N.] SASTRA Deemed Univ, Dept CSE SoC, Thanjavur, India.
   [Jeyalakshmi, C.] KRamakrishnan Coll Engn, Samayapuram, Trichy, India.
C3 Shanmugha Arts, Science, Technology & Research Academy (SASTRA);
   Shanmugha Arts, Science, Technology & Research Academy (SASTRA)
RP Sasikaladevi, N (corresponding author), SASTRA Deemed Univ, Dept CSE SoC, Thanjavur, India.
EM sasikalade@gmail.com
RI , Sasikaladevi/AAF-7847-2019; chelliah, jeyalakshmi/R-7723-2019;
   chelliah, lakshmi/AAI-8622-2020
OI Arunachalam, Revathi/0000-0001-9515-3592; , Sasikaladevi
   N/0000-0002-0841-502X
CR Das RK, 2017, J SIGNAL PROCESS SYS, V88, P259, DOI 10.1007/s11265-016-1148-z
   Desai N, 2016, 2016 INTERNATIONAL CONFERENCE ON MICRO-ELECTRONICS AND TELECOMMUNICATION ENGINEERING (ICMETE), P105, DOI 10.1109/ICMETE.2016.13
   Garofolo J. S, 1993, TIMIT ACOUSTIC PHONE
   Hermansky H., 1986, ICASSP 86 Proceedings. IEEE-IECEJ-ASJ International Conference on Acoustics, Speech and Signal Processing (Cat. No.86CH2243-4), P1971
   HERMANSKY H, 1991, CONFERENCE RECORD OF THE TWENTY-FIFTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P800, DOI 10.1109/ACSSC.1991.186557
   Hermansky H, 1994, IEEE T SPEECH AUDI P, V2, P578, DOI 10.1109/89.326616
   Nematollahi MA, 2014, IEEE REGION 10 SYMP, P476, DOI 10.1109/TENCONSpring.2014.6863080
   Nematollahi MA, 2017, MULTIMED TOOLS APPL, V76, P7251, DOI 10.1007/s11042-016-3350-1
   Nematollahi MA, 2015, MATH PROBL ENG, V2015, DOI 10.1155/2015/372398
   Rabiner L. R., 1993, FUNDAMENTALS SPEECH
   Rani R., 2016, GENETIC ALGORITHM US
   Revathi A, 2011, 2011 International Conference on Communications and Signal Processing (ICCSP), P198, DOI 10.1109/ICCSP.2011.5739300
   Safavi S, 2016, INT CONF DAT MIN WOR, P1074, DOI [10.1109/ICDMW.2016.0155, 10.1109/ICDMW.2016.115]
   Sarria-Paja M, 2015, CAN CON EL COMP EN, P1254, DOI 10.1109/CCECE.2015.7129458
   Tahilramani N. V., 2016, INT J INNOVATIVE RES, V4, P152
   [No title captured]
NR 16
TC 4
Z9 4
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 1381-2416
EI 1572-8110
J9 INT J SPEECH TECHNOL
JI Int. J. Speech Technol.
PD DEC
PY 2018
VL 21
IS 4
BP 1021
EP 1031
DI 10.1007/s10772-018-09563-9
PG 11
WC Engineering, Electrical & Electronic
WE Emerging Sources Citation Index (ESCI)
SC Engineering
GA HD9WD
UT WOS:000452913900024
DA 2022-02-03
ER

PT C
AU Yu, DQ
   Zou, YC
   Xu, XR
   Shi, AH
   Yang, XB
   Xiao, ZY
AF Yu, Daquan
   Zou, Yichao
   Xu, Xirui
   Shi, Aihua
   Yang, Xiaobing
   Xiao, Zhiyi
GP IEEE
TI Development of 3D WLCSP with Black Shielding for Optical Finger Print
   Sensor for the Application of Full Screen Smart Phone
SO 2019 IEEE 69TH ELECTRONIC COMPONENTS AND TECHNOLOGY CONFERENCE (ECTC)
SE Electronic Components and Technology Conference
LA English
DT Proceedings Paper
CT 69th IEEE Electronic Components and Technology Conference (ECTC)
CY MAY 29-31, 2019
CL Las Vegas, NV
DE Finger Print Sensor; CMOS image sensor, WLP; TSVs; Biometric Technique
ID SILICON
AB As an important biometric technique, fingerprint verification is widely used for smartphone, IoT and payment due to the small size and high security. To meet full screen trend of smartphone, optical finger print sensor (FPS) using CIS received great interest and gradually replaced capacitive FPS. For such a device, a thin package size with better image performance is required. To meet these critical requirements, 3D wafer-level chip scale package (WLCSP) with via last TSV (through silicon via) was the best solution for CIS.
   In this paper, 3D WLCSP for FPS based on CIS with via last TSV interconnects was developed. For the optical FPS, the package size is 920x656 mu m with 580 mu m height, which includes 200 mu m thick glass, similar to 100 mu m thick optical layer, similar to 100 mu m thick CIS chip and BGAs of 80 mu m height. Electrical interconnections are implemented by via last TSVs, where big silicon trenches were formed on the backside of the two-row pads on the adjacent chips, then the small TSVs were etched for contacting with the pads of the backside. Dry etch was used to remove the oxide of the pads and PVD, electroplating and electroless plating were used to form redistribution layer (RDL).The light reflection and scattering at the sidewalls of the CIS chip and optical layer will affect the sensor performance greatly. Therefore, the coating of black materials on the sidewalls was developed to improve the performance of the optical FPS for high-end applications. The precut on dicing street was used to form the deep trench on CIS wafer and optical layer for black shielding. To form a void-free filling, the wetting property of the black glue is very important. Three different materials with required optical shielding properties were used. The process of dispensing of black glue into the trenches was developed. The droplet diameter of 50 mu m with a jetting speed of 70mm/s was used for black material filling. The results showed that one of the materials with 15 mu m thickness on the sidewalls with 99.9% optical shielding from 430-560nm light wavelength and void-free filling was obtained which is good enough for the application. Reliability of the 3D WLCSP with black shielding was characterized by thermal cycling (TC) and highly accelerated stress test temperature storage (HAST) tests and the results show good package level reliability. The results indicate that the 3D WLCSP can provide a low cost and reliable solution for optical FPS.
C1 [Yu, Daquan; Zou, Yichao; Xu, Xirui; Shi, Aihua; Yang, Xiaobing; Xiao, Zhiyi] Huantian Technol Kunshan Elect Co Ltd, 112 LongTeng RD, Kunshan, Jiangsu, Peoples R China.
RP Yu, DQ (corresponding author), Huantian Technol Kunshan Elect Co Ltd, 112 LongTeng RD, Kunshan, Jiangsu, Peoples R China.
EM daquan.yu_ks@ht-tech.com
FU National Science and Technology Major Project [2017ZX02519]
FX The authors gratefully acknowledge the support from Panacol and Henkel
   for material development. The support from H&H Technology Co., Ltd for
   jetting process and tool demo is highly appreciated. Present study was
   financially supported by National Science and Technology Major Project
   No. 2017ZX02519.
CR Banijamali B, 2011, ELEC COMP C, P285, DOI 10.1109/ECTC.2011.5898527
   Liu L., 2017, SECUR COMMUN NETW, P1
   Motoyoshi M, 2009, P IEEE, V97, P43, DOI 10.1109/JPROC.2008.2007462
   Wang HJ, 2016, MICROSYST TECHNOL, V22, P337, DOI 10.1007/s00542-014-2390-6
   Xiao ZY, 2016, ELEC COMP C, P302, DOI 10.1109/ECTC.2016.55
   Yao MJ, 2018, J ELECTRON MATER, V47, P7544, DOI 10.1007/s11664-018-6701-z
   Zhao S., IEEE T COMPONENTS PA
NR 7
TC 0
Z9 0
U1 5
U2 10
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 0569-5503
EI 2377-5726
BN 978-1-7281-1498-9
J9 ELEC COMP C
PY 2019
BP 884
EP 889
DI 10.1109/ECTC.2019.00138
PG 6
WC Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering
GA BO1YO
UT WOS:000503261500131
DA 2022-02-03
ER

PT C
AU Kumar, R
   Singh, JP
   Srivastava, G
AF Kumar, Ram
   Singh, Jasvinder Pal
   Srivastava, Gaurav
BE Babu, BV
   Nagar, A
   Deep, K
   Pant, M
   Bansal, JC
   Ray, K
   Gupta, U
TI Altered Fingerprint Identification and Classification Using SP Detection
   and Fuzzy Classification
SO PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON SOFT COMPUTING FOR
   PROBLEM SOLVING (SOCPROS 2012)
SE Advances in Intelligent Systems and Computing
LA English
DT Proceedings Paper
CT 2nd International Conference on Soft Computing for Problem Solving
   (SocProS)
CY DEC 28-30, 2012
CL JK Lakshmipat Univ, Jaipur, INDIA
HO JK Lakshmipat Univ
DE Fingerprints; Alteration; Image enhancement; Reliability; Singular
   points
AB Fingerprint recognition is one of the most commonly used biometric technology. Even if fingerprint temporarily changes (cuts, bruises) it reappears after the finger heals. Criminals started to be aware of this and try to fool the identification systems applying methods from ingenious to very cruel. It is possible to remove, alter, or even fake fingerprints (made of glue, latex, silicone), by burning the fingertip skin (fire, acid, other corrosive material), by using plastic surgery (changing the skin completely, causing change in pattern-portions of skin are removed from a finger and grafted back in different positions, like rotation or "Z" cuts, transplantations of an area from other parts of the body like other fingers, palms, toes, and soles). This paper presents a new algorithm for altered fingerprints detection based on fingerprint orientation field reliability. The map of the orientation field reliability has peaks in the singular point locations. These peaks are used to analyze altered fingerprints because, due to alteration, more peaks as singular points appear with lower amplitudes.
C1 [Kumar, Ram; Singh, Jasvinder Pal; Srivastava, Gaurav] RKDF Inst Technol & Sci, Bhopal, India.
RP Kumar, R (corresponding author), RKDF Inst Technol & Sci, Bhopal, India.
EM hr.coet@gmail.com; jasvinder162@gmail.com; gashr83@gmail.com
RI Shrivastava, Gourav/P-2897-2016; Singh, Jasvinder Pal/AAV-3427-2020;
   Kumar, Ram/AAS-1617-2020; Singh, Jaikaran/O-5363-2015
OI Shrivastava, Gourav/0000-0002-1777-0533; Singh,
   Jaikaran/0000-0002-9273-6765; SINGH, JASVINDER PAL/0000-0002-5550-2724
CR Cummins H, 1935, J CRIM LAW CRIM, V25, P982, DOI 10.2307/1134845
   Jain A. K., 2009, SURG ALTERED FINGERP
   Singh K., 2008, ALTERED FINGERPRINTS
   Yoon S, 2012, IEEE T PATTERN ANAL, V34, P451, DOI 10.1109/TPAMI.2011.161
NR 4
TC 0
Z9 0
U1 0
U2 2
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 2194-5357
EI 2194-5365
BN 978-81-322-1602-5; 978-81-322-1601-8
J9 ADV INTELL SYST
PY 2014
VL 236
BP 1343
EP 1349
DI 10.1007/978-81-322-1602-5_139
PG 7
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BA7VR
UT WOS:000337801800139
DA 2022-02-03
ER

PT J
AU Spagnolo, GS
   Cozzella, L
   Simonetti, C
AF Spagnolo, Giuseppe Schirripa
   Cozzella, Lorenzo
   Simonetti, Carla
TI Banknote security using a biometric-like technique: a hylemetric
   approach
SO MEASUREMENT SCIENCE AND TECHNOLOGY
LA English
DT Article
DE banknote check; optical detection of random features; biometry; digital
   image processing
AB Banknote security is an issue that has led in the last decades to insert, inside the banknote itself, a very high number of controlling methods with the aim of verifying possible tampering attempts. In order to distinguish the false banknotes, sophisticated means (i.e. watermark, feel of the paper, raised print, metallic threads, quality of the printing, holograms, ultraviolet features, micro-lettering, etc) are often used. The purpose of this paper is to show a new approach and related method to protect banknotes and to verify their originality, based on the idea of hylemetry (methodology conceptually similar to biometry) applied to banknotes. Specifically, the hylemetric feature used in this paper is the random distribution pattern of the metallic security fibers set into the paper pulp. The outcome of the proposed solution is to identify an original banknote using a binary sequence derived from the banknote itself.
C1 [Spagnolo, Giuseppe Schirripa; Cozzella, Lorenzo; Simonetti, Carla] Univ Roma Tre, Dipartimento Ingn Elettron, I-00146 Rome, Italy.
C3 Roma Tre University
RP Spagnolo, GS (corresponding author), Univ Roma Tre, Dipartimento Ingn Elettron, Via Vasca Navale 84, I-00146 Rome, Italy.
EM schirrip@uniroma3.it
RI SPAGNOLO, Giuseppe SCHIRRIPA/AAQ-2962-2021
OI SPAGNOLO, Giuseppe SCHIRRIPA/0000-0003-0108-1335
CR Gonzalez R. C., 2001, DIGITAL IMAGE PROCES, V3rd
   Haist T, 1998, OPT COMMUN, V147, P173, DOI 10.1016/S0030-4018(97)00546-4
   Hardwick B, 2001, ADV MATER, V13, P980, DOI 10.1002/1521-4095(200107)13:12/13<980::AID-ADMA980>3.0.CO;2-F
   HOFER R, 2006, REAL FAKE MANUAL DET
   HUBY PM, 1974, CLASSICAL REV, V24, P44
   MALTONI D, 2003, HDB FINGERPRINT RECO
   Nanni L, 2009, EXPERT SYST APPL, V36, P10401, DOI 10.1016/j.eswa.2009.01.037
   RIVEST RL, 1978, COMMUN ACM, V21, P120, DOI 10.1145/359340.359342
   TISTARELLI M, 2002, BIOMETRIC AUTHENTICA
   Van Renesse, 1998, OPTICAL DOCUMENT SEC
   WOODWARD JD, 2003, BIOMETRICS
NR 11
TC 11
Z9 11
U1 1
U2 12
PU IOP PUBLISHING LTD
PI BRISTOL
PA TEMPLE CIRCUS, TEMPLE WAY, BRISTOL BS1 6BE, ENGLAND
SN 0957-0233
EI 1361-6501
J9 MEAS SCI TECHNOL
JI Meas. Sci. Technol.
PD MAY
PY 2010
VL 21
IS 5
AR 055501
DI 10.1088/0957-0233/21/5/055501
PG 8
WC Engineering, Multidisciplinary; Instruments & Instrumentation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering; Instruments & Instrumentation
GA 582WS
UT WOS:000276631600040
DA 2022-02-03
ER

PT J
AU Joseph, AA
   Pog, EIAP
   Chin, KL
   Liang, DBB
   Mat, DAA
   Song, NS
   Rulaningtyas, R
AF Joseph, Annie Anak
   Pog, Edward Ijau Anak Pelias
   Chin, Kho Lee
   Liang, David Bong Boon
   Mat, Dyg Azra Awang
   Song, Ngu Sze
   Rulaningtyas, Rilies
TI Online Person Identification based on Multitask Learning
SO INTERNATIONAL JOURNAL OF INTEGRATED ENGINEERING
LA English
DT Article
DE Feature extraction; biometrics; Linear Discriminant Analysis (LDA);
   incremental learning; multitask learning; person identification
ID LINEAR DISCRIMINANT-ANALYSIS; VISUAL TRACKING
AB In the digital world, everything is digitized and data are generated consecutively over the times. To deal with this situation, incremental learning plays an important role. One of the important applications that needs an incremental learning is person identification. On the other hand, password and code are no longer the only way to prevent the unauthorized person to access the information and it tends to be forgotten. Therefore, biometric characteristics system is introduced to solve the problems. However, recognition based on single biometric may not be effective, thus, multitask learning is needed. To solve the problems, incremental learning is applied for person identification based on multitask learning. Considering that the complete data is not possible to be collected at one time, online learning is adopted to update the system accordingly. Linear Discriminant Analysis (LDA) is used to create a feature space while Incremental LDA (ILDA) is adopted to update LDA. Through multitask learning, not only human faces are trained, but fingerprint images are trained in order to improve the performance. The performance of the system is evaluated by using 50 datasets which includes both male and female datasets. Experimental results demonstrate that the learning time of ILDA is faster than LDA. Apart from that, the learning accuracies are evaluated by using K-Nearest Neighbor (KNN) and achieve more than 80% for most of the simulation results. In the future, the system is suggested to be improved by using better sensor for all the biometrics. Other than that, incremental feature extraction is improved to deal with some other online learning problems.
C1 [Joseph, Annie Anak; Pog, Edward Ijau Anak Pelias; Chin, Kho Lee; Liang, David Bong Boon; Mat, Dyg Azra Awang; Song, Ngu Sze] Univ Malaysia Sarawak, Fac Engn, Dept Elect & Elect Engn, Sarawak 94300, Malaysia.
   [Rulaningtyas, Rilies] Univ Airlangga, Fac Sci & Technol, Phys Dept, Surabaya, Indonesia.
C3 University of Malaysia Sarawak; Airlangga University
RP Joseph, AA (corresponding author), Univ Malaysia Sarawak, Fac Engn, Dept Elect & Elect Engn, Sarawak 94300, Malaysia.
EM jannie@unimas.my
RI CHIN, Kho LEE/AAX-3400-2021
OI CHIN, Kho LEE/0000-0001-7708-6031
FU Universiti Malaysia Sarawak [F02/SpTDG/1772/2018]
FX This work is supported by Universiti Malaysia Sarawak through the
   provision of research grant F02/SpTDG/1772/2018.
CR Aydin AA, 2017, PROCEEDINGS OF THE 50TH ANNUAL HAWAII INTERNATIONAL CONFERENCE ON SYSTEM SCIENCES, P5911
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Carbone Paris, 2017, HDB BIG DATA TECHNOL, P219
   Verissimo JMC, 2016, J BUS RES, V69, P5456, DOI 10.1016/j.jbusres.2016.04.155
   Devi R, 2017, 2017 CONFERENCE ON EMERGING DEVICES AND SMART SYSTEMS (ICEDSS), P267, DOI 10.1109/ICEDSS.2017.8073691
   Doan T, 2016, 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2016), P697, DOI [10.1109/ICMLA.2016.0123, 10.1109/ICMLA.2016.158]
   Frincu M., 2016, 2016 IEEE PES INN SM, P1
   Fu CH, 2014, INT CONF UNMAN AIRCR, P649, DOI 10.1109/ICUAS.2014.6842309
   Guo CN, 2018, 2018 16TH IEEE INT CONF ON DEPENDABLE, AUTONOM AND SECURE COMP, 16TH IEEE INT CONF ON PERVAS INTELLIGENCE AND COMP, 4TH IEEE INT CONF ON BIG DATA INTELLIGENCE AND COMP, 3RD IEEE CYBER SCI AND TECHNOL CONGRESS (DASC/PICOM/DATACOM/CYBERSCITECH), P100, DOI 10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00030
   Hisada M, 2010, EVOL SYST-GER, V1, P17, DOI 10.1007/s12530-010-9000-3
   Ismail A, 2019, INT J INTEGR ENG, V11, P51
   Ji SW, 2008, IEEE T NEURAL NETWOR, V19, P1768, DOI 10.1109/TNN.2008.2002078
   Joseph A. A., 2014, T I SYSTEMS CONTROL, V27, P133
   Joseph AA, 2012, LECT NOTES COMPUT SC, V7664, P640, DOI 10.1007/978-3-642-34481-7_78
   Kaur M., 2012, INT J COMPUT APPL, V60, P13
   Khan A, 2018, 2018 15TH LEARNING AND TECHNOLOGY CONFERENCE (L&T), P1, DOI 10.1109/LT.2018.8368484
   Khan WM, 2018, IEEE T CONSUM ELECTR, V64, P145, DOI 10.1109/TCE.2018.2844729
   Ko T, 2006, 34TH APPLIED IMAGERY AND PATTERN RECOGNITION WORKSHOP, P218
   Kodali RK, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON COMPUTING, COMMUNICATION AND AUTOMATION (ICCCA), P1286, DOI 10.1109/CCAA.2016.7813916
   Liu CY, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2911, DOI 10.1109/IJCNN.2011.6033603
   Madkour DM, 2019, INT J INTEGR ENG, V11, P61
   Pang S, 2005, IEEE T SYST MAN CY B, V35, P905, DOI 10.1109/TSMCB.2005.847744
   Pawar KB, 2017, 2017 INTERNATIONAL CONFERENCE ON CURRENT TRENDS IN COMPUTER, ELECTRICAL, ELECTRONICS AND COMMUNICATION (CTCEEC), P822, DOI 10.1109/CTCEEC.2017.8454985
   Yu HB, 2019, 2019 IEEE 4TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING AND BIG DATA ANALYSIS (ICCCBDA), P701, DOI 10.1109/ICCCBDA.2019.8725689
   Yu J, 2015, INT CONF CONTR AUTO, P113, DOI 10.1109/ICCAIS.2015.7338643
   Zhang S, 2017, PROC INT CONF ANTI, P6, DOI 10.1109/ICASID.2017.8285733
NR 26
TC 0
Z9 0
U1 1
U2 1
PU UNIV TUN HUSSEIN ONN MALAYSIA
PI JOHOR
PA 86400 PARIT RAJA, BATU PAHAT, JOHOR, 00000, MALAYSIA
SN 2229-838X
J9 INT J INTEGR ENG
JI Int. J. Integr. Eng.
PY 2021
VL 13
IS 2
BP 119
EP 126
DI 10.30880/ijie.2021.13.02.014
PG 8
WC Engineering, Multidisciplinary
WE Emerging Sources Citation Index (ESCI)
SC Engineering
GA QK4TH
UT WOS:000620379000014
DA 2022-02-03
ER

PT J
AU Rutty, GN
   Abbas, A
   Crossling, D
AF Rutty, GN
   Abbas, A
   Crossling, D
TI Could earprint identification be computerised? An illustrated proof of
   concept paper
SO INTERNATIONAL JOURNAL OF LEGAL MEDICINE
LA English
DT Article
DE ear; earprint; computer; biometric; centroids
ID EAR
AB To date, the ear remains an under-utilised part of the human body for use in forensic practice. Although the ear has been used since the nineteenth century as part of the process of human identification, in this particular function its use, to date, remains low and in the case of earprints, controversial. A limited number of publications exist related to methods used for the purpose of ear image identification and the growing field of ear biometrics but to date, a computerised system for earprint identification does not exist. This paper illustrates the concept of a computerised earprint identification system. To assist those considering similar developments we share the concept problems and possible solutions we have identified and encountered to date, and highlight the advantages for such a system over traditional manual methods used for earprint identification.
C1 Univ Leicester, Leicester Royal Infirm, Forens Pathol Unit, Leicester LE2 7LX, Leics, England.
C3 University of Leicester
RP Rutty, GN (corresponding author), Univ Leicester, Leicester Royal Infirm, Forens Pathol Unit, Robert Kilpatrick Bldg, Leicester LE2 7LX, Leics, England.
EM gnr3@le.ac.uk
CR Abbas A, 2003, J PATHOL, V201, p44A
   ABBAS A, 2003, THESIS U LEICESTER
   Abbas Ali, 2003, J Clin Forensic Med, V10, P129, DOI 10.1016/S1353-1131(02)00166-9
   Alexander M., 1968, ANTHROPOMETRY HUMAN
   BROWN T, 2005, IN PRESS ENCY FORENS
   Burge M, 1998, BIOMETRICS PERSONAL, P273
   Burge Mark, 1997, P 21 WORKSH AUSTR AS, P275
   Champod C, 2001, J FORENSIC SCI, V46, P1275
   Chang K, 2003, IEEE T PATTERN ANAL, V25, P1160, DOI 10.1109/TPAMI.2003.1227990
   CROSSING D, Patent No. 9728513
   FARKAS LG, 1992, CLEFT PALATE-CRAN J, V29, P324, DOI 10.1597/1545-1569(1992)029<0324:AGSOTE>2.3.CO;2
   FARKAS LG, 1974, S RECONSTRUCTION AUR, P24
   HURLEY D, 2000, P IEE C VIS BIOM
   Iannarelli A, 1989, EAR IDENTIFICATION F
   Kennedy RB, 1996, FORENSIC SCI INT, V82, P81, DOI 10.1016/0379-0738(96)01969-X
   KENNEDY RB, 1996, CAN SOC FORENS SCI J, V29, P233
   MAAT GJR, 1999, UNPUB EAR PRINT PROJ
   Meijerman L, 2004, FORENSIC SCI INT, V140, P91, DOI 10.1016/j.forsciint.2003.10.024
   RUTTY GN, 2004, P AM AC FOR SCI DALL
   RUTTY GN, 2001, THESIS U SHEFFIELD
   Smith DR, 2002, J FORENSIC SCI, V47, P937
   Swift B, 2003, J FORENSIC SCI, V48, P153
   VANDERLUGT C, 2001, EARPRINT IDENTIFICAT
   Victor B, 2002, INT C PATT RECOG, P429, DOI 10.1109/ICPR.2002.1044746
NR 24
TC 12
Z9 12
U1 0
U2 3
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0937-9827
EI 1437-1596
J9 INT J LEGAL MED
JI Int. J. Legal Med.
PD NOV
PY 2005
VL 119
IS 6
BP 335
EP 343
DI 10.1007/s00414-005-0527-y
PG 9
WC Medicine, Legal
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Legal Medicine
GA 980RO
UT WOS:000233036100003
PM 15703959
DA 2022-02-03
ER

PT C
AU Schuch, P
   Schulz, SD
   Busch, C
AF Schuch, Patrick
   Schulz, Simon-Daniel
   Busch, Christoph
GP IEEE
TI Deep Expectation for Estimation of Fingerprint Orientation Fields
SO 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB)
LA English
DT Proceedings Paper
CT IEEE International Joint Conference on Biometrics (IJCB)
CY OCT 01-04, 2017
CL Denver, CO
AB Estimation of the orientation field is one of the key challenges during biometric feature extraction from a fingerprint sample. Many important processing steps rely on an accurate and reliable estimation. This is especially challenging for samples of low quality, for which in turn accurate preprocessing is essential. Regressional Convolutional Neural Networks have shown their superiority for bad quality samples in the independent benchmark framework FVC-ongoing. This work proposes to incorporate Deep Expectation. Options for further improvements are evaluated in this challenging environment of low quality images and small amount of training data. The findings from the results improve the new algorithm called DEX-OF. Incorporating Deep Expectation, improved regularization, and slight model changes DEX-OF achieves an RMSE of 7.52 degrees on the bad quality dataset and 4.89 degrees at the good quality dataset at FVC-ongoing. These are the best reported error rates so far.
C1 [Schuch, Patrick; Busch, Christoph] NTNU, Norwegian Biometr Lab, Gjovik, Norway.
   [Schulz, Simon-Daniel] Dermalog Identificat Syst GmbH, Hamburg, Germany.
C3 Norwegian University of Science & Technology (NTNU)
RP Schuch, P (corresponding author), NTNU, Norwegian Biometr Lab, Gjovik, Norway.
EM patrick.schuch2@ntnu.no; simon.schulz@dermalog.com;
   christoph.busch@ntnu.no
RI Busch, Christoph/AAF-8176-2019
OI Busch, Christoph/0000-0002-9159-2923
CR Cao K, 2015, INT CONF BIOMETR, P349, DOI 10.1109/ICB.2015.7139060
   Dorizzi B, 2009, LECT NOTES COMPUT SC, V5558, P725, DOI 10.1007/978-3-642-01793-3_74
   Glorot X., 2010, P 13 INT C ART INT S, P249
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A.G., ARXIV13125402
   Hu, 2006, P 5 WSEAS INT C SIGN, P158
   Jia Y., 2014, ARXIV14085093
   Krizhevsky A., 2012, INT C NEUR INF PROC, V25, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
   Lin M, 2013, ARXIV PREPRINT ARXIV
   Maltoni D., 2009, HDB FINGERPRINT RECO, V2nd
   Mills DM, 2012, IEEE ENG MED BIO, P2318, DOI 10.1109/EMBC.2012.6346427
   RATHA NK, 1995, PATTERN RECOGN, V28, P1657, DOI 10.1016/0031-3203(95)00039-3
   Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41
   Sahasrabudhe M, 2013, 2013 SECOND IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR 2013), P351, DOI 10.1109/ACPR.2013.37
   Schuch P, 2017, LECT NOTES COMPUT SC, V10269, P325, DOI 10.1007/978-3-319-59126-1_27
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/CVPR.2015.7298594
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Turroni F, 2011, IEEE T INF FOREN SEC, V6, P1002, DOI 10.1109/TIFS.2011.2150216
   Wang Y, 2007, IEEE T PATTERN ANAL, V29, P573, DOI 10.1109/TPAMI.2007.1003
   Yang X, 2014, IEEE T PATTERN ANAL, V36, P955, DOI 10.1109/TPAMI.2013.184
   Zhang Chiyuan, 2016, ARXIV161103530
NR 21
TC 10
Z9 11
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-5386-1124-1
PY 2017
BP 185
EP 190
PG 6
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BJ6XF
UT WOS:000426973200023
DA 2022-02-03
ER

PT C
AU Repanovici, A
   Cotoros, D
   Haineala, M
   Nemet, C
   Dinu, E
AF Repanovici, A.
   Cotoros, D.
   Haineala, M.
   Nemet, C.
   Dinu, E.
GP IEEE
TI Systems for Monitoring Hands Hygiene of Medical Staff in Hospitals
SO 2020 INTERNATIONAL CONFERENCE ON E-HEALTH AND BIOENGINEERING (EHB)
SE E-Health and Bioengineering Conference
LA English
DT Proceedings Paper
CT 8th International Conference on E-Health and Bioengineering (EHB)
CY OCT 29-30, 2020
CL ELECTR NETWORK
DE hands washing technique; disinfection; hygiene; monitoring systems
AB One of the most worrying issues that clouds the activity in Romanian hospitals and around the world is the occurrence of nosocomial infections, in many cases due to the improper hygiene of the medical staff's hands. Also in the new context of the COVID-19 pandemic, the issue of hospitals hygiene takes new dimensions. For this reason, the research in the present paper aims at identifying new methods and devices to be used in monitoring the hands hygiene of the medical staff in an efficient and simple manner, using mainly image processing and biometric analysis.
   It was noticed that by thoroughly observing the problem areas in the attempts of properly sanitizing the hands of the medical staff there arise the possibility of increasing the efficiency of doing away with infections and reduce the risk of further transmitting dangerous diseases.
C1 [Repanovici, A.; Cotoros, D.; Haineala, M.] Univ Transilvania Brasov, Prod Design Mechatron & Environm, Brasov, Romania.
   [Nemet, C.; Dinu, E.] Transilvania Univ Brasov, Fac Med, Brasov, Romania.
C3 Transylvania University of Brasov; Transylvania University of Brasov
RP Repanovici, A (corresponding author), Univ Transilvania Brasov, Prod Design Mechatron & Environm, Brasov, Romania.
EM arepanovici@unitbv.ro; dcotoros@unitbv.ro; mihaelahaineala98@gmail.com;
   codruta_nemet@yahoo.com; dinueleonora@yahoo.com
CR BUCHRIESER O, 1996, HYG MED, V21, P670
   Chavali S, 2014, INDIAN J CRIT CARE M, V18, P689, DOI 10.4103/0972-5229.142179
   Dinu E., REV MEDICO CHURURGIC, V123, P335
   Drugus D, 2017, REV CERCET INTERV SO, V56, P79
   Ojanpera H, 2020, B WORLD HEALTH ORGAN, V98, P475, DOI 10.2471/BLT.19.247494
   Repanovici A, 2008, MATH COMPUT SCI ENG, P73
   Widmer AF, 2004, INFECT CONT HOSP EP, V25, P207, DOI 10.1086/502379
NR 7
TC 0
Z9 0
U1 2
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2575-5137
EI 2575-5145
BN 978-1-7281-8803-4
J9 E-HEALTH BIOENG CONF
PY 2020
PG 4
WC Health Care Sciences & Services; Engineering, Biomedical
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Health Care Sciences & Services; Engineering
GA BR3FU
UT WOS:000646194100017
DA 2022-02-03
ER

PT C
AU Parada, M
   Sanches, I
AF Parada, Marcelo
   Sanches, Ivandro
GP IEEE
TI Visual Voice Activity Detection Based on Motion Vectors of MPEG Encoded
   Video
SO UKSIM-AMSS 11TH EUROPEAN MODELLING SYMPOSIUM ON COMPUTER MODELLING AND
   SIMULATION (EMS 2017)
SE UKSim European Symposium on Computer Modeling and Simulation
LA English
DT Proceedings Paper
CT 11th UKSim-AMSS European Modelling Symposium on Computer Modelling and
   Simulation (EMS)
CY NOV 20-22, 2017
CL Manchester, ENGLAND
DE biometry; VVAD; VAD; liveness detection
AB This article presents a visual voice activity detector (VVAD) that relies on features extracted from an MPEG encoded video, e.g. MPEG-4 AVC (H.264) or MPEG-H part 2 HEVC (H.265). The technique uses the inter frames resulting from the motion compensation and estimation of the temporal redundancy analysis, in which blocks of the image comprises the representation of motion vectors. The proposed algorithm relies on this information already available in the encoded video to estimate the lip motion and thus reducing processing time and code complexity. A standard voice activity detector (VAD) based on the audio signal is also performed and similarities on the audio and video approaches can assure not only a more reliable voice activity detector, especially in noisy environments but can also be useful as liveness detection for biometric systems.
C1 [Parada, Marcelo; Sanches, Ivandro] Ctr Univ FEI, Dept Elect Engn, Sao Bernardo Do Campo, Brazil.
C3 Centro Universitario da FEI
RP Parada, M (corresponding author), Ctr Univ FEI, Dept Elect Engn, Sao Bernardo Do Campo, Brazil.
EM mparada@fei.edu.br; isanches@fei.edu.br
OI Sanches, Ivandro/0000-0001-9247-2301
FU Centro Universitario FEI
FX The authors would like to thank Centro Universitario FEI for the
   support.
CR Chang JH, 2006, IEEE T SIGNAL PROCES, V54, P1965, DOI 10.1109/TSP.2006.874403
   Dov D, 2015, IEEE-ACM T AUDIO SPE, V23, P732, DOI 10.1109/TASLP.2015.2405481
   Farkas L.G., 1994, ANTHROPOMETRY HEAD F, V2nd
   FFmpeg Developers, 2016, FFMPEG TOOLS SOFTW
   Kantorov V., 2014, P 2014 IEEE COMP SOC
   Marpe D., 2006, IEEE COMMUNICATIONS, V44
   Messer K., 1999, P 2 C AUD VID BAS BI
   Montazzolli S, 2015, IEEE IMAGE PROC, P3886, DOI 10.1109/ICIP.2015.7351533
   Shaikh A A, 2010, 2010 3rd International Congress on Image and Signal Processing (CISP 2010), P327, DOI 10.1109/CISP.2010.5646264
   Sodoyer D., 2006, IEEE INT C AC SPEECH, V1
   Sohn J, 1999, IEEE SIGNAL PROC LET, V6, P1, DOI 10.1109/97.736233
   Stillittano S, 2013, MACH VISION APPL, V24, P1, DOI 10.1007/s00138-012-0445-1
   Sullivan GJ, 2012, IEEE T CIRCUITS SYST, V12, P22
   Tiawongsombat P, 2012, PATTERN RECOGN, V45, P783, DOI 10.1016/j.patcog.2011.07.011
   Viola P, 2001, P 2001 IEEE COMP SOC, V1
NR 15
TC 1
Z9 1
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2473-3539
BN 978-1-5386-1410-5
J9 UKSIM EURO SYMP COMP
PY 2017
BP 89
EP 94
DI 10.1109/EMS.2017.26
PG 6
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BK3DU
UT WOS:000434813800015
DA 2022-02-03
ER

PT J
AU Mancini, AM
   Grelaud, M
   Ziveri, P
   Nallino, E
   Lozar, F
AF Mancini, A. M.
   Grelaud, M.
   Ziveri, P.
   Nallino, E.
   Lozar, F.
TI Calcareous Nannofossil Size and Abundance Response to the Messinian
   Salinity Crisis Onset and Paleoenvironmental Dynamics
SO PALEOCEANOGRAPHY AND PALEOCLIMATOLOGY
LA English
DT Article
DE biometry; coccolithophores; paleoenvironmental reconstruction; SYRACO;
   Mediterranean; micropaleontology
ID COCCOLITHOPHORID EMILIANIA-HUXLEYI; EOCENE THERMAL MAXIMUM; BASIN SE
   SPAIN; SURFACE-WATER; SORBAS BASIN; NANNOPLANKTON RESPONSE; CARBONATE
   CHEMISTRY; OCEAN ACIDIFICATION; SAPROPEL SEQUENCES; ORGANIC-CARBON
AB Dwarfism is a common feature affecting organisms across extreme events that characterized the Earth history, frequently referred as the result of "stressed conditions." To date, no study addressed the morphological and biometric changes across the Messinian Salinity Crisis (MSC), one of the most recent and impacting event occurred in the Mediterranean Sea, historically interpreted as characterized by hypersaline conditions. Here we focus on morpho/biometric changes affecting calcareous nannofossils (CN) toward the MSC onset in order to better constrain the paleoenvironmental dynamics and the loosely defined "stressed conditions" characterizing this interval. Size characterization and absolute abundance of selected CN taxa were performed in the Perales (Spain, Western Mediterranean) and in the Banengo and Pollenzo sections (Italy, Northern Mediterranean). We also tested whether size changes and orbital cyclicity were related through analyzing size and calcite mass of Reticulofenestra minuta using an automated image analysis system of CN recognition (SYRACO). We recorded a significant size reduction affecting the CN taxa involved in the MSC onset bioevent, related to the restriction of the Mediterranean Basin that resulted in increased productivity and enhanced environmental variability, stimulating CN growth rate and decreasing their platelet sizes. Reticulofenestra minuta size and mass correlate with the orbital cyclicity with minimum values during periods of enhanced environmental variability, coinciding with the diatomite deposition in the Sorbas Basin. Our finding reveals that the size change recorded across the MSC onset coincided with the instauration of a productive and highly variable environment, linked to the restriction of the paleo Gibraltar Strait.
C1 [Mancini, A. M.; Nallino, E.; Lozar, F.] Univ Torino, Dept Earth Sci, Turin, Italy.
   [Mancini, A. M.; Grelaud, M.; Ziveri, P.] Univ Autonoma Barcelona UAB, Inst Environm Sci & Technol ICTA, Bellaterra, Spain.
   [Ziveri, P.] Catalan Inst Res & Adv Studies ICREA, Barcelona, Spain.
C3 University of Turin; Autonomous University of Barcelona; ICREA
RP Mancini, AM (corresponding author), Univ Torino, Dept Earth Sci, Turin, Italy.; Mancini, AM (corresponding author), Univ Autonoma Barcelona UAB, Inst Environm Sci & Technol ICTA, Bellaterra, Spain.
EM alanmaria.mancini@unito.it
RI Ziveri, Patrizia/I-3856-2015
OI Ziveri, Patrizia/0000-0002-5576-0301; Grelaud,
   Michael/0000-0001-8649-9743; Mancini, Alan Maria/0000-0003-4009-9781
FU Universita degli Studi di Torino within the CRUI-CARE Agreement
FX The authors thank to the three anonymous reviewers that improved the
   overall quality of the manuscript. Open Access Funding provided by
   Universita degli Studi di Torino within the CRUI-CARE Agreement.
CR Aloisi G, 2015, BIOGEOSCIENCES, V12, P4665, DOI 10.5194/bg-12-4665-2015
   Baggley KA, 2000, PALAEONTOLOGY, V43, P1069, DOI 10.1111/1475-4983.00162
   Beaufort L, 2004, MAR MICROPALEONTOL, V51, P57, DOI 10.1016/j.marmicro.2003.09.003
   Beaufort L, 2011, NATURE, V476, P80, DOI 10.1038/nature10295
   Beaufort L, 2014, NAT PROTOC, V9, P633, DOI 10.1038/nprot.2014.028
   Blanc PL, 2006, PALAEOGEOGR PALAEOCL, V238, P349, DOI 10.1016/j.palaeo.2006.03.033
   Bollmann J, 2007, EARTH PLANET SC LETT, V255, P273, DOI 10.1016/j.epsl.2006.12.029
   Bolton CT, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms10284
   Bolton CT, 2013, NATURE, V500, P558, DOI 10.1038/nature12448
   Bourillot R, 2010, SEDIMENTOLOGY, V57, P477, DOI 10.1111/j.1365-3091.2009.01092.x
   Braga JC, 2006, SEDIMENT GEOL, V188, P131, DOI 10.1016/j.sedgeo.2006.03.002
   Browning EL, 2008, PALEOCEANOGRAPHY, V23, DOI 10.1029/2007PA001413
   Capella W, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-40208-2
   Cita MB., 1978, INITIAL REPORTS DEEP, P1003
   Clauzon G, 2015, MAR PETROL GEOL, V66, P71, DOI 10.1016/j.marpetgeo.2015.02.016
   Corbi H, 2020, GEO-MAR LETT, V40, P341, DOI 10.1007/s00367-020-00647-7
   Corbi H, 2016, MAR PETROL GEOL, V77, P1010, DOI 10.1016/j.marpetgeo.2016.08.004
   Corbi H, 2016, MAR GEOL, V379, P246, DOI 10.1016/j.margeo.2016.05.017
   D'Amario B, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-69519-5
   D'Amario B, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0201161
   De Bodt C, 2010, BIOGEOSCIENCES, V7, P1401, DOI 10.5194/bg-7-1401-2010
   Dedert M, 2014, MAR MICROPALEONTOL, V107, P18, DOI 10.1016/j.marmicro.2013.12.004
   Dela Pierre F, 2011, PALAEOGEOGR PALAEOCL, V310, P238, DOI 10.1016/j.palaeo.2011.07.017
   Engel A, 2005, LIMNOL OCEANOGR, V50, P493, DOI 10.4319/lo.2005.50.2.0493
   Erba E, 2019, B SOC PALEONTOL ITAL, V58, P51, DOI 10.4435/BSPI.2019.08
   Erba E, 2010, SCIENCE, V329, P428, DOI 10.1126/science.1188886
   Faucher G, 2020, CLIM PAST, V16, P1007, DOI 10.5194/cp-16-1007-2020
   Faucher G, 2017, BIOGEOSCIENCES, V14, P3603, DOI 10.5194/bg-14-3603-2017
   Faucher G, 2017, RIV ITAL PALEONTOL S, V123, P159
   Ferreira J, 2017, PALAEOGEOGR PALAEOCL, V465, P177, DOI 10.1016/j.palaeo.2016.10.029
   Filippelli GM, 2003, PALAEOGEOGR PALAEOCL, V190, P335, DOI 10.1016/S0031-0182(02)00613-2
   Flecker R, 2015, EARTH-SCI REV, V150, P365, DOI 10.1016/j.earscirev.2015.08.007
   Flores JA, 2005, MAR MICROPALEONTOL, V56, P50, DOI 10.1016/j.marmicro.2005.04.002
   Flores JA, 1997, MAR MICROPALEONTOL, V29, P351, DOI 10.1016/S0377-8398(96)00029-1
   Fortuin AR, 2003, SEDIMENT GEOL, V160, P213, DOI 10.1016/S0037-0738(02)00377-9
   Gennari R, 2018, NEWSL STRATIGR, V51, P33, DOI 10.1127/nos/2017/0350
   Gibbs S, 2004, MAR MICROPALEONTOL, V51, P39, DOI 10.1016/j.marmicro.2003.09.002
   Gibbs SJ, 2018, PHILOS T R SOC A, V376, DOI 10.1098/rsta.2017.0075
   Gibbs SJ, 2013, NAT GEOSCI, V6, P218, DOI 10.1038/ngeo1719
   Gibbs SJ, 2006, GEOLOGY, V34, P233, DOI 10.1130/G22381.1
   Grelaud M, 2012, PALEOCEANOGRAPHY, V27, DOI 10.1029/2012PA002288
   Hammer Oyvind, 2001, Palaeontologia Electronica, V4, pUnpaginated
   Henderiks J, 2007, BIOGEOSCIENCES, V4, P323, DOI 10.5194/bg-4-323-2007
   Henderiks J, 2008, EARTH PLANET SC LETT, V269, P575, DOI 10.1016/j.epsl.2008.03.016
   Herbert TD, 2016, NAT GEOSCI, V9, P843, DOI [10.1038/NGEO2813, 10.1038/ngeo2813]
   Herrmann S, 2012, PALAEOGEOGR PALAEOCL, V333, P92, DOI 10.1016/j.palaeo.2012.03.011
   Herrmann S, 2012, MAR MICROPALEONTOL, V82-83, P1, DOI 10.1016/j.marmicro.2011.09.006
   Hsu K.J., 1978, MEDITERRANEAN SEA, V42, DOI [10.2973/dsdp.proc.42-1.155.1978, DOI 10.2973/DSDP.PROC.42-1.155.1978]
   Iaccarino Silvia M., 1999, Proceedings of the Ocean Drilling Program Scientific Results, V161, P529
   Iglesias-Rodriguez MD, 2008, SCIENCE, V320, P336, DOI 10.1126/science.1154122
   Kaiho K, 2006, PALAEOGEOGR PALAEOCL, V237, P456, DOI 10.1016/j.palaeo.2005.12.017
   Karakitsios V, 2017, BASIN RES, V29, P203, DOI 10.1111/bre.12173
   Keller G, 2009, PALAEOGEOGR PALAEOCL, V284, P47, DOI 10.1016/j.palaeo.2009.08.029
   Kemp AES, 2013, PROG OCEANOGR, V119, P4, DOI 10.1016/j.pocean.2013.06.004
   Kouwenhoven TJ, 2006, MAR MICROPALEONTOL, V60, P17, DOI 10.1016/j.marmicro.2006.02.005
   Langer G, 2009, BIOGEOSCIENCES, V6, P2637, DOI 10.5194/bg-6-2637-2009
   Langer G, 2006, GEOCHEM GEOPHY GEOSY, V7, DOI 10.1029/2005GC001227
   Langer Gerald, 2011, Journal of Nannoplankton Research, V32, P29
   Laskar J, 2004, ASTRON ASTROPHYS, V428, P261, DOI 10.1051/0004-6361:20041335
   Lopez-Otalvaro GE, 2008, MAR MICROPALEONTOL, V69, P52, DOI 10.1016/j.marmicro.2007.11.009
   Lozar F, 2019, MAR MICROPALEONTOL, V151, DOI 10.1016/j.marmicro.2019.101752
   Lozar F, 2018, NEWSL STRATIGR, V51, P11, DOI 10.1127/nos/2017/0354
   Lubke N, 2016, CRETACEOUS RES, V61, P169, DOI 10.1016/j.cretres.2016.01.006
   Lubke N, 2015, MAR MICROPALEONTOL, V117, P25, DOI 10.1016/j.marmicro.2015.03.002
   Lugli S, 2010, PALAEOGEOGR PALAEOCL, V297, P83, DOI 10.1016/j.palaeo.2010.07.017
   Manzi V, 2013, TERRA NOVA, V25, P315, DOI 10.1111/ter.12038
   MARGALEF R, 1978, OCEANOL ACTA, V1, P493
   Mancini AM, 2020, PALAEOGEOGR PALAEOCL, V554, DOI 10.1016/j.palaeo.2020.109796
   Martin JM, 2004, J GEOL SOC LONDON, V161, P387, DOI 10.1144/0016-764903-044
   Mattioli E, 2004, MAR MICROPALEONTOL, V52, P5, DOI 10.1016/j.marmicro.2004.04.004
   Mattioli E, 2009, GLOBAL PLANET CHANGE, V65, P134, DOI 10.1016/j.gloplacha.2008.10.018
   MCKENZIE JA, 1979, PALAEOGEOGR PALAEOCL, V29, P125, DOI 10.1016/0031-0182(79)90077-4
   Meier KJS, 2014, BIOGEOSCIENCES, V11, P2857, DOI 10.5194/bg-11-2857-2014
   Meyer J, 2015, BIOGEOSCIENCES, V12, P1671, DOI 10.5194/bg-12-1671-2015
   Milner S, 2016, LIMNOL OCEANOGR, V61, P1322, DOI 10.1002/lno.10292
   Modestou S, 2017, PALEOCEANOGRAPHY, V32, P531, DOI 10.1002/2016PA003061
   Moissette Pierre, 1993, Paleontologia i Evolucio, V24-25, P245
   Natalicchio M, 2019, PALAEOGEOGR PALAEOCL, V518, P45, DOI 10.1016/j.palaeo.2019.01.009
   Negri A, 1999, MAR GEOL, V157, P89, DOI 10.1016/S0025-3227(98)00135-2
   Oviedo A, 2015, OCEAN SCI, V11, P13, DOI 10.5194/os-11-13-2015
   Paasche E, 2001, PHYCOLOGIA, V40, P503, DOI 10.2216/i0031-8884-40-6-503.1
   Perez-Folgado M, 2003, PALAEOGEOGR PALAEOCL, V190, P317, DOI 10.1016/S0031-0182(02)00612-0
   Poot-Delgado CA, 2015, REV BIOL MAR OCEANOG, V50, P465, DOI 10.4067/S0718-19572015000400006
   Reghizzi M, 2017, PALAEOGEOGR PALAEOCL, V469, P60, DOI 10.1016/j.palaeo.2016.12.039
   Riding R, 1998, MAR GEOL, V146, P1, DOI 10.1016/S0025-3227(97)00136-9
   Riebesell U, 2000, NATURE, V407, P364, DOI 10.1038/35030078
   Riebesell U, 2017, NAT GEOSCI, V10, P19, DOI [10.1038/ngeo2854, 10.1038/NGEO2854]
   Rost B, 2004, COCCOLITHOPHORES: FROM MOLECULAR PROCESSES TO GLOBAL IMPACT, P99
   Sabino M, 2020, PALAEOGEOGR PALAEOCL, V545, DOI 10.1016/j.palaeo.2020.109632
   Sakka A, 1999, AQUAT MICROB ECOL, V19, P149, DOI 10.3354/ame019149
   Salaviale C, 2018, PALAEOGEOGR PALAEOCL, V490, P240, DOI 10.1016/j.palaeo.2017.11.003
   Salter I, 2014, NAT GEOSCI, V7, P885, DOI 10.1038/NGEO2285
   Sheward RM, 2017, BIOGEOSCIENCES, V14, P1493, DOI 10.5194/bg-14-1493-2017
   Sierro FJ, 2003, PALAEOGEOGR PALAEOCL, V190, P289, DOI 10.1016/S0031-0182(02)00611-9
   Sierro FJ, 2001, PALAEOGEOGR PALAEOCL, V168, P141, DOI 10.1016/S0031-0182(00)00253-4
   Stoll HM, 2019, QUATERNARY SCI REV, V208, P1, DOI 10.1016/j.quascirev.2019.01.012
   Sucheras-Marx B, 2010, PALAEOGEOGR PALAEOCL, V295, P281, DOI 10.1016/j.palaeo.2010.06.006
   Supraha L, 2015, SCI REP-UK, V5, DOI 10.1038/srep16499
   Tremolada F, 2008, MAR MICROPALEONTOL, V67, P239, DOI 10.1016/j.marmicro.2008.01.010
   Triantaphyllou MV, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0200012
   Troelstra S.R., 1980, GEOL MEDITERR, V8, P115
   Urbanek Adam, 1993, Historical Biology, V7, P29
   van de Poel H.M., 1992, Scripta Geologica, V102, P1
   Vasiliev I, 2019, PALEOCEANOGR PALEOCL, V34, P182, DOI 10.1029/2018PA003438
   Violanti D, 2013, B SOC PALEONTOL ITAL, V52, P45
   WATABE N, 1966, LIMNOL OCEANOGR, V11, P567, DOI 10.4319/lo.1966.11.4.0567
   Yanez-Arancibia A., 2005, ECOSYSTEM FUNCTIONIN, P77, DOI [10.1016/j.ecoleng.2013.03.007, DOI 10.1016/J.ECOLENG.2013.03.007]
   Young J.R., 2017, NANNOTAX3 WEBSITE, DOI [10.1016/s0967-0645%2800%2900003-5, DOI 10.1016/S0967-0645%2800%2900003-5]
   Young JR, 2000, DEEP-SEA RES PT II, V47, P1679, DOI 10.1016/S0967-0645(00)00003-5
   Zeebe RE, 2016, NAT GEOSCI, V9, P325, DOI 10.1038/NGEO2681
   Ziveri P, 2007, DEEP-SEA RES PT II, V54, P659, DOI 10.1016/j.dsr2.2007.01.006
   Ziveri P, 2014, BIOL BULL-US, V226, P282, DOI 10.1086/BBLv226n3p282
   Zondervan I, 2007, DEEP-SEA RES PT II, V54, P521, DOI 10.1016/j.dsr2.2006.12.004
NR 113
TC 0
Z9 0
U1 1
U2 1
PU AMER GEOPHYSICAL UNION
PI WASHINGTON
PA 2000 FLORIDA AVE NW, WASHINGTON, DC 20009 USA
SN 2572-4517
EI 2572-4525
J9 PALEOCEANOGR PALEOCL
JI Paleoceanogr. Paleoclimatology
PD SEP
PY 2021
VL 36
IS 9
AR e2020PA004155
DI 10.1029/2020PA004155
PG 22
WC Geosciences, Multidisciplinary; Oceanography; Paleontology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Geology; Oceanography; Paleontology
GA WE9MW
UT WOS:000705942700001
OA hybrid, Green Published
DA 2022-02-03
ER

PT J
AU Jacobi, PC
AF Jacobi, PC
TI Primary lensectomy following acute primary angle closure glaucoma
SO OPHTHALMOLOGE
LA German
DT Article
DE pupillary block; iridectomy; iridotomy; primary phacoemulsification;
   iris-lens diaphragm
ID INTRAOCULAR-LENS IMPLANTATION; PRIMARY PHACOEMULSIFICATION;
   CATARACT-EXTRACTION; EYES
AB Recent developments and clinical studies indicate that primary phacoemulsification and intraocular lens implantation are safe and effective for the surgical treatment of primary angle closure glaucoma (ACG) compared to conventional iridectomy or laser-iridotomy. When compared to control eyes treated using standard peripheral iridectomy, the outcome in terms of intraocular pressure control, adjunct anti-glaucoma medication, visual acuity, and the necessity for successive surgical interventions favored primary phacoemulsification and intraocular lens implantation. Earlier biometric data underline the importance of the "lens factor" in the pathogenesis of relative pupillary block in ACG obtained by Scheimflug image processing and ultrasound biomicroscopy. The vast improvements in modern cataract surgery combined with our current understanding of the pathogenesis of relative pupillary block in ACG indicate that lens extraction is a better procedure in uncontrolled angle closure glaucoma than conventional iridectomy.
C1 VENIVIDI, Cologne, Germany.
RP Jacobi, PC (corresponding author), Aachenerstr 1006-1012, D-50858 Cologne, Germany.
EM jacobi@augen-venividi.com
CR Acton J, 1997, J CATARACT REFR SURG, V23, P930, DOI 10.1016/S0886-3350(97)80255-6
   Erie JC, 1997, ARCH OPHTHALMOL-CHIC, V115, P177, DOI 10.1001/archopht.1997.01100150179005
   Gunning FP, 1998, J CATARACT REFR SURG, V24, P1347, DOI 10.1016/S0886-3350(98)80227-7
   Jacobi PC, 2002, OPHTHALMOLOGY, V109, P1597, DOI 10.1016/S0161-6420(02)01123-5
   KAUFMANN PL, 1997, GLAUCOMAS, P307
   Kurimoto Y, 1997, AM J OPHTHALMOL, V124, P775, DOI 10.1016/S0002-9394(14)71694-0
   LOWE RF, 1970, BRIT J OPHTHALMOL, V54, P161, DOI 10.1136/bjo.54.3.161
   MARKOWITZ SN, 1985, AM J OPHTHALMOL, V99, P400, DOI 10.1016/0002-9394(85)90005-4
   PANEK WC, 1990, AM J OPHTHALMOL, V110, P185, DOI 10.1016/S0002-9394(14)76989-2
   Ritch R., 1992, J GLAUCOMA, V1, P206
   RITCH R, 1996, GLAUCOMAS, V3, P1521
   Roberts TV, 2000, J CATARACT REFR SURG, V26, P1012, DOI 10.1016/S0886-3350(00)00358-8
   Spaeth G L, 1971, Trans Ophthalmol Soc U K, V91, P709
   TOMLINSON A, 1975, AM J OPTOM PHYS OPT, V52, P817
   Yang CH, 1997, J CATARACT REFR SURG, V23, P1109, DOI 10.1016/S0886-3350(97)80089-2
NR 15
TC 3
Z9 5
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013 USA
SN 0941-293X
J9 OPHTHALMOLOGE
JI Ophthalmologe
PD DEC
PY 2005
VL 102
IS 12
BP 1207
EP 1211
DI 10.1007/s00347-005-1285-4
PG 5
WC Ophthalmology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Ophthalmology
GA 996NQ
UT WOS:000234181200013
PM 16283186
DA 2022-02-03
ER

PT J
AU Komeili, M
   Armanfard, N
   Hatzinakos, D
AF Komeili, Majid
   Armanfard, Narges
   Hatzinakos, Dimitrios
TI Multiview Feature Selection for Single-View Classification
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Feature extraction; Training; Dimensionality reduction; Correlation;
   Error analysis; Biomedical imaging; Feature selection; multiview;
   feature weighting; multiview training single view test; classification
ID MULTICLASS CLASSIFICATION; FRAMEWORK; INFORMATION; ALGORITHMS
AB In many real-world scenarios, data from multiple modalities (sources) are collected during a development phase. Such data are referred to as multiview data. While additional information from multiple views often improves the performance, collecting data from such additional views during the testing phase may not be desired due to the high costs associated with measuring such views or, unavailability of such additional views. Therefore, in many applications, despite having a multiview training data set, it is desired to do performance testing using data from only one view. In this paper, we present a multiview feature selection method that leverages the knowledge of all views and use it to guide the feature selection process in an individual view. We realize this via a multiview feature weighting scheme such that the local margins of samples in each view are maximized and similarities of samples to some reference points in different views are preserved. Also, the proposed formulation can be used for cross-view matching when the view-specific feature weights are pre-computed on an auxiliary data set. Promising results have been achieved on nine real-world data sets as well as three biometric recognition applications. On average, the proposed feature selection method has improved the classification error rate by 31 percent of the error rate of the state-of-the-art.
C1 [Komeili, Majid] Carleton Univ, Sch Comp Sci, Ottawa, ON K1S 5B6, Canada.
   [Armanfard, Narges] McGill Univ, Dept Elect & Comp Engn, Montreal, PQ H3A 0G4, Canada.
   [Hatzinakos, Dimitrios] Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 2E4, Canada.
C3 Carleton University; McGill University; University of Toronto
RP Komeili, M (corresponding author), Carleton Univ, Sch Comp Sci, Ottawa, ON K1S 5B6, Canada.
EM majid.komeili@carleton.ca; armanfn@mcmaster.ca;
   dimitris@comm.utoronto.ca
FU Natural Sciences and Engineering Research Council of Canada
   (NSERC)Natural Sciences and Engineering Research Council of Canada
   (NSERC)
FX This research was supported by the Natural Sciences and Engineering
   Research Council of Canada (NSERC).
CR Abrahamsen T. J., 2013, KERNEL METHODS MACHI
   Akata Z., 2015, PROC CVPR IEEE, P2927
   Alpaydin E, 2014, ADAPT COMPUT MACH LE, P1
   Ao M., 2009, P1
   Armanfard N, 2018, IEEE T NEUR NET LEAR, V29, P1396, DOI 10.1109/TNNLS.2017.2676101
   Armanfard N, 2016, IEEE T PATTERN ANAL, V38, P1217, DOI 10.1109/TPAMI.2015.2478471
   Banerjee M, 2015, IEEE T KNOWL DATA EN, V27, P3390, DOI 10.1109/TKDE.2015.2455509
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bilmes J., 2015, PR MACH LEARN RES, P1083
   Brown G, 2012, J MACH LEARN RES, V13, P27
   Chen B, 2009, IEEE T KNOWL DATA EN, V21, P1475, DOI 10.1109/TKDE.2008.238
   Cheng QA, 2011, IEEE T PATTERN ANAL, V33, P1217, DOI 10.1109/TPAMI.2010.195
   EL-Manzalawy Y, 2018, BMC MED GENOMICS, V11, DOI 10.1186/s12920-018-0388-0
   Eleftheriadis S, 2015, IEEE T IMAGE PROCESS, V24, P189, DOI 10.1109/TIP.2014.2375634
   Feng Y., 2012, P AS C COMP VIS, P343
   Gilad-Bachrach R., 2004, P 21 ACM INT C MACH
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hernandez JCH, 2007, LECT NOTES COMPUT SC, V4447, P90
   Hewett R, 2008, BMC GENOMICS, V9, DOI 10.1186/1471-2164-9-S2-S21
   Hou CP, 2014, IEEE T CYBERNETICS, V44, P793, DOI 10.1109/TCYB.2013.2272642
   Hu RY, 2017, NEUROCOMPUTING, V220, P130, DOI 10.1016/j.neucom.2016.05.081
   Jakulin A., 2005, THESIS
   John G., DNA DATASET STATLOG
   Kan MN, 2016, IEEE T PATTERN ANAL, V38, P188, DOI 10.1109/TPAMI.2015.2435740
   Kazemi V., 2014, PROC CVPR IEEE, P1867, DOI DOI 10.1109/CVPR.2014.241
   Kira K., 1992, MACHINE LEARNING, P249
   Kodirov E., 2015, IEEE I CONF COMP VIS, P2452, DOI DOI 10.1109/ICCV.2015.282
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lin DH, 2006, LECT NOTES COMPUT SC, V3951, P68
   Lin W.-Y., 2018, PROC CVPR IEEE, P5784, DOI DOI 10.1109/CVPR.2018.00606
   Liu B, 2013, PATTERN RECOGN, V46, P2798, DOI 10.1016/j.patcog.2013.02.012
   Liu H., 2016, IEEE DATA MINING, P281, DOI DOI 10.1109/ICDM.2016.37
   Liu ZQ, 2011, BIOINFORMATICS, V27, P3242, DOI 10.1093/bioinformatics/btr547
   Luo MN, 2018, IEEE T NEUR NET LEAR, V29, P944, DOI 10.1109/TNNLS.2017.2650978
   Ma Y., 2007, P 24 INT C MACH LEAR, P577, DOI DOI 10.1145/1273496.1273569
   Meyer PE, 2006, LECT NOTES COMPUT SC, V3907, P91
   Nag K, 2016, IEEE T CYBERNETICS, V46, P499, DOI 10.1109/TCYB.2015.2404806
   Nie F., 2016, 30 AAAI C ART INT, P1302
   Nie F, 2010, ADV NEURAL INFORM PR, P1813, DOI DOI 10.1007/978-3-319-10690-8_12
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Shao W., 2016, IEEE DATA MINING, P1203, DOI DOI 10.1109/ICDM.2016.134
   Sharma A., 2012, PROC CVPR IEEE, P2160
   Stiglic G, 2010, J BIOMED BIOTECHNOL, DOI 10.1155/2010/616358
   Sun SL, 2016, IEEE T CYBERNETICS, V46, P3272, DOI 10.1109/TCYB.2015.2502248
   Sun YJ, 2007, IEEE T PATTERN ANAL, V29, P1035, DOI 10.1109/TPAMI.2007.1093
   Sun YJ, 2010, IEEE T PATTERN ANAL, V32, P1610, DOI 10.1109/TPAMI.2009.190
   Tang J., 2013, P SIAM INT C DAT MIN, P270
   van't Veer LJ, 2002, NATURE, V415, P530, DOI 10.1038/415530a
   Wang H., 2004, IEEE IMAGE PROC, V2, P1397
   Wang L, 2008, IEEE T PATTERN ANAL, V30, P1534, DOI 10.1109/TPAMI.2007.70799
   Wang YX, 2005, LANCET, V365, P671, DOI 10.1016/S0140-6736(05)17947-1
   Wangila K. W., 2017, IEEE IMAGE PROC, P1930
   Xu X., 2017, PROC CVPR IEEE, P3798, DOI DOI 10.1109/CVPR.2017.217
   Yang HH, 2000, ADV NEUR IN, V12, P687
   Yang WQ, 2015, IEEE T NEUR NET LEAR, V26, P2801, DOI 10.1109/TNNLS.2015.2396937
   Zhai C., 2014, P 23 ACM INT C C INF, P1963
   Zhu P., 2016, 30 AAAI C ART INT, P2422
   Zhu PF, 2017, PATTERN RECOGN, V66, P364, DOI 10.1016/j.patcog.2017.01.016
   Zhu XF, 2018, IEEE T KNOWL DATA EN, V30, P517, DOI 10.1109/TKDE.2017.2763618
NR 60
TC 0
Z9 0
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD OCT 1
PY 2021
VL 43
IS 10
BP 3573
EP 3586
DI 10.1109/TPAMI.2020.2987013
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA UK8RG
UT WOS:000692232400023
PM 32305902
DA 2022-02-03
ER

PT J
AU Lee, BK
   Lee, YS
AF Lee, Byong Kwon
   Lee, Yang Sun
TI Distinction Between Real Faces and Photos by Analysis of Face Data
SO INTELLIGENT AUTOMATION AND SOFT COMPUTING
LA English
DT Article
DE Virtual Reality; Mobile HMD; Multi-Kinect
AB Biometric user authentication using the face has been applied mainly to access control systems. However, access is allowed even when a photo is presented instead of an actual face. This can facilitate illegal access including attending as a substitute or substitute authentication. An alternative approach has been implemented to solve this problem. The approach determines between a real face and a photo of a face using a UV sensor but this requires substantial cost and installation process because additional hardware (the UV sensor) is necessary. This paper proposes a three-step approach to identify between a real image and a photo. Step 1 determines authenticity using the background data and eliminating the face data. Step 2 determines authenticity using eyelid blinking on the face and facial gestures. Step 3 authorizes the user by extracting the feature points on the face.
C1 [Lee, Byong Kwon] Dongguk Univ, Coll Engn, Dept Multimedia Engn, Seoul 04620, South Korea.
   [Lee, Yang Sun] Mokwon Univ, Div Convergence Comp & Media, Daejeon 35349, South Korea.
C3 Dongguk University
RP Lee, YS (corresponding author), Mokwon Univ, Div Convergence Comp & Media, Daejeon 35349, South Korea.
EM yslee48@gmail.com
CR Akbar MA, 2016, IEEE SENS J, V16, P5734, DOI 10.1109/JSEN.2016.2565721
   Bartlett MS, 2002, IEEE T NEURAL NETWOR, V13, P1450, DOI 10.1109/TNN.2002.804287
   Chowdhury M., 2017, SECURECOMM 2016 LNIC, V198, P287, DOI [10.1007/978-3-319-59608-2_16, DOI 10.1007/978-3-319-59608-2_16]
   Fu LM, 2018, IEEE INT C INT ROBOT, P287, DOI 10.1109/ICRIS.2018.00080
   Hanna J., 2012, Proceedings of the 2012 6th IEEE International Conference Intelligent Systems (IS), P388, DOI 10.1109/IS.2012.6335247
   Li Bin, 2013, 2013 IEEE INT C CONS, P1255
   Marques I, 2012, LECT NOTES COMPUT SC, V7209, P436
   Noureddin B, 2012, IEEE T BIO-MED ENG, V59, P2103, DOI 10.1109/TBME.2011.2108295
   Saboori Arash, 2016, SIGN PROC COMM SYST, P1
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shigang Chen, 2011, 2011 International Conference on Multimedia and Signal Processing (CMSP), P251, DOI 10.1109/CMSP.2011.58
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Xia DS, 2007, SNPD 2007: EIGHTH ACIS INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, ARTIFICIAL INTELLIGENCE, NETWORKING, AND PARALLEL/DISTRIBUTED COMPUTING, VOL 1, PROCEEDINGS, P220, DOI 10.1109/SNPD.2007.237
NR 13
TC 7
Z9 7
U1 0
U2 2
PU TSI PRESS
PI SAN ANTONIO
PA 18015 BULLIS HILL, SAN ANTONIO, TX, UNITED STATES
SN 1079-8587
EI 2326-005X
J9 INTELL AUTOM SOFT CO
JI Intell. Autom. Soft Comput.
PD MAR
PY 2020
VL 26
IS 1
BP 133
EP 139
DI 10.31209/2019.100000134
PG 7
WC Automation & Control Systems; Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Automation & Control Systems; Computer Science
GA KP9LV
UT WOS:000516553300013
DA 2022-02-03
ER

PT C
AU Anitha, ML
   Rao, KAR
AF Anitha, M. L.
   Rao, K. A. Radhakrishna
GP IEEE
TI An Efficient Palmprint Identification System based on an Indexing
   approach
SO 2013 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS
   AND INFORMATICS (ICACCI)
LA English
DT Proceedings Paper
CT 2nd International Conference on Advances in Computing, Communications
   and Informatics (ICACCI)
CY AUG 22-25, 2013
CL Sri Jayachamarajendra Coll Engn, Mysore, INDIA
HO Sri Jayachamarajendra Coll Engn
DE Biometrics; Feature extraction; Indexing; Palm print; Person
   identification
ID FEATURE-EXTRACTION
AB Performance of a biometric based person identification system depends upon the search time and accuracy of identification. These two factors increase with the increase in the size of the database. One way to reduce the search time is to retrieve small number of candidate identities from the database against which matching is performed for person identification. In this paper we propose an approach for reducing the search space by indexing the database prior to identification process to improve the accuracy and search time. Experiments results on a palmprint database indicate 80% reduction in search space with 100% correct index power. Results reveal that the proposed method reduces the search time without compromising the accuracy of identification. The main objective is to look at suitability of an approach which searches and identifies a query image in reasonable time and with good accuracy.
C1 [Anitha, M. L.] PES Coll Engn, PET Res Fdn, Mandya, Karnataka, India.
   [Rao, K. A. Radhakrishna] PES Coll Engn, Dept Elect & Commun Engn, Mandya, Karnataka, India.
RP Anitha, ML (corresponding author), PES Coll Engn, PET Res Fdn, Mandya, Karnataka, India.
EM m_l_anitha@yahoo.co.in; karkrao@yahoo.com
CR Connie T, 2005, IMAGE VISION COMPUT, V23, P501, DOI 10.1016/j.imavis.2005.01.002
   Connie Tee, 2003, P IM VIS COMP NOV, P227
   Delac K, 2004, PROCEEDINGS ELMAR-2004: 46TH INTERNATIONAL SYMPOSIUM ELECTRONICS IN MARINE, P184
   Fang L, 2006, IEEE SYS MAN CYBERN, P2965, DOI 10.1109/ICSMC.2006.384569
   Gyaourova A., 2009, P IEEE COMP SOC WORK
   Huang DS, 2008, PATTERN RECOGN, V41, P1316, DOI 10.1016/j.patcog.2007.08.016
   Jain AK, 2004, IEEE T CIRC SYST VID, V14, P4, DOI 10.1109/TCSVT.2003.818349
   Jayaraman U, 2009, INT J BIOMETRICS, V1, P418, DOI 10.1504/IJBM.2009.027304
   Jia Y, 2010, PROC CVPR IEEE, P3392, DOI 10.1109/CVPR.2010.5540006
   Kong A, 2006, PATTERN RECOGN, V39, P478, DOI 10.1016/j.patcog.2005.08.014
   Kong AWK, 2004, INT C PATT RECOG, P520, DOI 10.1109/ICPR.2004.1334184
   Kong WK, 2003, PATTERN RECOGN, V36, P2339, DOI 10.1016/S0031-3203(03)00121-3
   Lu G., 2003, PATTERN RECOGN, V24, P1473
   Lu GM, 2002, PROC SPIE, V4875, P780, DOI 10.1117/12.477069
   Mhatre A, 2005, LECT NOTES COMPUT SC, V3546, P841
   Moore Andrew. W., 1991, 209 U CAMBR COMP LAB
   Negi Atul, 2006, 9 INT C INF TECHN
   Paliwal A, 2010, IEEE IMAGE PROC, P2377, DOI 10.1109/ICIP.2010.5652614
   Ross A., 2007, P 12 EUR SIGN PROC C, P1121
   Sakdanupab Mongkon, 2011, ECTIT T COMPUTER INF, V5, P118
   Wu XQ, 2004, PATTERN RECOGN, V37, P1987, DOI 10.1016/j.patcog.2004.02.015
   Zhang D, 2003, IEEE T PATTERN ANAL, V25, P1041, DOI 10.1109/TPAMI.2003.1227981
NR 22
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-4799-2432-5; 978-1-4799-2659-6
PY 2013
BP 688
EP 693
PG 6
WC Computer Science, Information Systems; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BB5FF
UT WOS:000343771500119
DA 2022-02-03
ER

PT C
AU Merkel, R
   Gruhn, S
   Dittmann, J
   Vielhauer, C
   Brautigam, A
AF Merkel, Ronny
   Gruhn, Stefan
   Dittmann, Jana
   Vielhauer, Claus
   Braeutigam, Anja
BE Baskurt, AM
   Sitnik, R
TI General Fusion Approaches for the Age Determination of Latent
   Fingerprint Traces: Results for 2D and 3D Binary Pixel Feature Fusion
SO THREE-DIMENSIONAL IMAGE PROCESSING (3DIP) AND APPLICATIONS II
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT Conference on Three-Dimensional Image Processing (3DIP) and Applications
   II
CY JAN 24-26, 2012
CL Burlingame, CA
DE latent fingerprint traces; crime scene forensics; age determination;
   binary pixel; non-invasive fingerprint lifting; fusion; CWL;
   spectroscopy; microscopy
ID ULTRAVIOLET
AB Determining the age of latent fingerprint traces found at crime scenes is an unresolved research issue since decades. Solving this issue could provide criminal investigators with the specific time a fingerprint trace was left on a surface, and therefore would enable them to link potential suspects to the time a crime took place as well as to reconstruct the sequence of events or eliminate irrelevant fingerprints to ensure privacy constraints. Transferring imaging techniques from different application areas, such as 3D image acquisition, surface measurement and chemical analysis to the domain of lifting latent biometric fingerprint traces is an upcoming trend in forensics. Such non-destructive sensor devices might help to solve the challenge of determining the age of a latent fingerprint trace, since it provides the opportunity to create time series and process them using pattern recognition techniques and statistical methods on digitized 2D, 3D and chemical data, rather than classical, contact-based capturing techniques, which alter the fingerprint trace and therefore make continuous scans impossible.
   In prior work, we have suggested to use a feature called binary pixel, which is a novel approach in the working field of fingerprint age determination. The feature uses a Chromatic White Light (CWL) image sensor to continuously scan a fingerprint trace over time and retrieves a characteristic logarithmic aging tendency for 2D-intensity as well as 3D-topographic images from the sensor. In this paper, we propose to combine such two characteristic aging features with other 2D and 3D features from the domains of surface measurement, microscopy, photography and spectroscopy, to achieve an increase in accuracy and reliability of a potential future age determination scheme.
   Discussing the feasibility of such variety of sensor devices and possible aging features, we propose a general fusion approach, which might combine promising features to a joint age determination scheme in future. We furthermore demonstrate the feasibility of the introduced approach by exemplary fusing the binary pixel features based on 2D-intensity and 3D-topographic images of the mentioned CWL sensor. We conclude that a formula based age determination approach requires very precise image data, which cannot be achieved at the moment, whereas a machine learning based classification approach seems to be feasible, if an adequate amount of features can be provided.
C1 [Merkel, Ronny; Dittmann, Jana] Otto von Guericke Univ, Res Grp Multimedia & Secur, Dept Comp Sci, Univ Pl 2, D-39106 Magdeburg, Germany.
   [Gruhn, Stefan; Vielhauer, Claus] Brandenburg Univ Appl Sci, Dept Informat & Media, D-14770 Brandenburg, Germany.
   [Braeutigam, Anja] State Criminal Police Off Saxony Anhalt, D-39124 Magdeburg, Germany.
C3 Otto von Guericke University
RP Merkel, R (corresponding author), Otto von Guericke Univ, Res Grp Multimedia & Secur, Dept Comp Sci, Univ Pl 2, D-39106 Magdeburg, Germany.
EM ronny.merkel@iti.cs.uni-magdeburg.de
FU German Federal Ministry of Education and Science (BMBF) through the
   Research ProgrammeFederal Ministry of Education & Research (BMBF)
   [13N10816, 13N10818, 13N10822]
FX The work in this paper has been funded in part by the German Federal
   Ministry of Education and Science (BMBF) through the Research Programme
   under Contract No. FKZ: 13N10816, FKZ: 13N10818 and FKZ: 13N10822. We
   also thank Miranda Mowbray for her mathematical support.
CR Aehnlich J., 2001, THESIS U HANNOVER
   [Anonymous], 2011, WEK MACH LEARN TOOLB
   Antoine KM, 2010, J FORENSIC SCI, V55, P513, DOI 10.1111/j.1556-4029.2009.01262.x
   Bhargava R, 2009, ANAL BIOANAL CHEM, V394, P2069, DOI 10.1007/s00216-009-2817-6
   Conchello JA, 2005, NAT METHODS, V2, P920, DOI 10.1038/NMETH815
   Connatser RM, 2010, J FORENSIC SCI, V55, P1462, DOI 10.1111/j.1556-4029.2010.01484.x
   Crane N. C., 2007, J FORENSIC SCI, V52
   Dubey SK, 2008, J OPT A-PURE APPL OP, V10, DOI 10.1088/1464-4258/10/01/015307
   Ekanayake-Mudiyanselage S, 2003, J INVEST DERMATOL, V120, P915, DOI 10.1046/j.1523-1747.2003.12233.x
   FRISBIE CD, 1994, SCIENCE, V265, P2071, DOI 10.1126/science.265.5181.2071
   Goddard AJ, 2010, J FORENSIC SCI, V55, P58, DOI 10.1111/j.1556-4029.2009.01217.x
   Grant A., 2005, APPL SPECTROSCOPY, V59
   GUTTERIDGE JMC, 1990, TRENDS BIOCHEM SCI, V15, P129, DOI 10.1016/0968-0004(90)90206-Q
   Kuivalainen K, 2010, APPL OPTICS, V49, P5081, DOI 10.1364/AO.49.005081
   Kung S. Y., 2005, BIOMETRIC AUTHENTICA
   Lin ACY, 2007, J FORENSIC SCI, V52, P1148, DOI 10.1111/j.1556-4029.2007.00502.x
   Lin SS, 2006, J OPT SOC AM A, V23, P2137, DOI 10.1364/JOSAA.23.002137
   Merkel R., 2011, P SPIE SECURITY DEFE
   Merkel R., 2011, P IEEE ISPA 2011
   Merkel R., FORENSIC SCI I UNPUB
   Merkel R., 2011, P IEEE INT WORKSH IN
   Merkel R, 2011, MM&SEC 11: PROCEEDINGS OF THE 2011 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, P41
   Merkel R, 2011, LECT NOTES COMPUT SC, V7025, P59, DOI 10.1007/978-3-642-24712-5_5
   Oermann A, 2006, LECT NOTES COMPUT SC, V4105, P546
   Richards A, REFLECTED ULTRAVIOLE
   Saitoh N, 2005, THESCIENTIFICWORLDJO, V5, P355, DOI 10.1100/tsw.2005.43
   Shafer G., 1976, MATH THEORY EVIDENCE
   Watson GS, 2007, J PHYS CONF SER, V61, P1251, DOI 10.1088/1742-6596/61/1/247
   Williams DK, 2011, FORENSIC SCI INT, V206, P161, DOI 10.1016/j.forsciint.2010.07.033
   Williams G, 2001, J FORENSIC SCI, V46, P1085
   Williams G, 2007, FORENSIC SCI INT, V167, P102, DOI 10.1016/j.forsciint.2006.08.018
NR 31
TC 6
Z9 6
U1 0
U2 4
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 978-0-8194-8937-1
J9 PROC SPIE
PY 2012
VL 8290
AR 82900Y
DI 10.1117/12.908637
PG 16
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Optics; Imaging Science & Photographic Technology
GA BAI79
UT WOS:000304302300033
DA 2022-02-03
ER

PT C
AU Bin Mansoor, A
   Mumtaz, M
   Masood, H
   Butt, MAA
   Khan, SA
AF Bin Mansoor, Atif
   Mumtaz, M.
   Masood, H.
   Butt, M. Asif A.
   Khan, Shoab A.
BE Bebis, G
   Boyle, R
   Parvin, B
   Koracin, D
   Remagnino, P
   Porikli, F
   Peters, J
   Klosowski, J
   Arns, L
   Chun, YK
   Rhyne, TM
   Monroe, L
TI Personal Identification Using Palmprint and Contourlet Transform
SO ADVANCES IN VISUAL COMPUTING, PT II, PROCEEDINGS
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 4th International Symposium on Visual Computing
CY DEC 01-03, 2008
CL Las Vegas, NV
ID AUTHENTICATION; EXTRACTION
AB Palmprint based personal verification has gained preference over other biometric modalities due to its ease of acquisition, high user acceptance and reliability. This paper presents a new palmprint based identification approach which uses the textural information available on the palmprint by employing the Contourlet Transform (CT). Center of the palm is calculated using the Distance Transform and by calculating the parameters for the best fitting ellipse, the alignment of hand 'theta' is found. Rotational invariance is achieved by cropping a square region of size 256 x 256 pixels around the center aligned at theta degrees. After establishing the region of interest (ROI), the two dimensional (2-D) spectrums is divided into fine slices, using iterated directional filterbanks. Next, directional energy components for cacti block of the decomposed subband outputs are computed. The proposed algorithm captures both local and global details in a palmprint as a compact fixed length palm code of the computed directional energies. Palmprint; matching is then performed using Normalized Euclidean Distance classifier. The proposed algorithm is tested on a total of 500 palm images of GPDS Hand database, acquired from University of Las Palmas de Gran Canaria. The experimental results demonstrated the feasibility of the proposed system by exhibiting Genuine Acceptance Rate of 98.2%, Decidability Index of 2.6212 and Equal Error Rate of 0.7082%.
C1 [Bin Mansoor, Atif; Mumtaz, M.; Masood, H.; Butt, M. Asif A.; Khan, Shoab A.] Natl Univ Sci & Technol, Rawalpindi, Pakistan.
C3 National University of Sciences & Technology - Pakistan
RP Bin Mansoor, A (corresponding author), Natl Univ Sci & Technol, Tamiz Ud Din Rd, Rawalpindi, Pakistan.
EM atif-cae@nust.edu.pk; mustafa672@hotmail.com; hassan13204@yahoo.com;
   asif-cae@nust.edu.pk; kshoab@yahoo.com
CR Candes E., 2000, P SPIE, V4119
   Candes E.J., 1999, CURVE SURFACE FITTIN
   Connie Tee, 2003, P IM VIS COMP NOV, P227
   DO MN, 2001, THESIS EPFL LAUSANNE
   DO MN, 2003, WAVELET
   EKINCI M, 2007, LNCS, V4571
   *GPDS, GPDS HAND DAT
   JIANG W, 2006, IEEE INT C DIG TEL I
   Kong AWK, 2006, IEEE T SYST MAN CY B, V36, P1201, DOI 10.1109/TSMCB.2006.876168
   KUMAR A, 2003, P 4 INT C AUD AND VI
   Kumar A, 2006, IEEE T IMAGE PROCESS, V15, P2454, DOI 10.1109/TIP.2006.875214
   KUMAR SHC, 2002, P INT C SYS CYB SCI
   Li WX, 2002, INT J PATTERN RECOGN, V16, P417, DOI 10.1142/S0218001402001757
   Lu G., 2003, PATTERN RECOGN, V24, P1473
   Masood H., 2008, P IEEE INT S BIOM SE
   Park CH, 2004, IEEE T CIRC SYST VID, V14, P74, DOI 10.1109/TCSVT.2003.818355
   SHANG L, 2006, LNCS, V3972
   TAO J, 2006, LNCS, V4088
   TRAVIESOGONZALE.CM, 2004, COMPLEX SYSTEMS INTE, P581
   Wu XQ, 2006, IEEE T SYST MAN CY A, V36, P978, DOI 10.1109/TSMCA.2006.871797
   YOU J, 2004, LNCS, V3072
   Zhang D, 2003, IEEE T PATTERN ANAL, V25, P1041, DOI 10.1109/TPAMI.2003.1227981
   ZHANG D, 2004, LNCS, V3338
   ZHANG D, 2005, LNCS, V3776
   Zhang Ren, 2005, Archaea, V1, P335, DOI 10.1155/2005/509646
   ZHOU X, 2006, LNCS, V4099
   CONTOURLET TOOLBOX
NR 27
TC 3
Z9 3
U1 0
U2 0
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-540-89645-6
J9 LECT NOTES COMPUT SC
PY 2008
VL 5359
BP 521
EP 530
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Imaging Science &
   Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BIT91
UT WOS:000262709700051
DA 2022-02-03
ER

PT C
AU Li, C
   Liu, YH
   Shih, HC
AF Li, Chieh
   Liu, Yu Hsiang
   Shih, Huang-Chia
GP IEEE
TI Adaptive Skin Color Tone Detection with Morphology-based Model
   Refinement
SO 2013 9TH INTERNATIONAL CONFERENCE ON INFORMATION, COMMUNICATIONS AND
   SIGNAL PROCESSING (ICICS)
LA English
DT Proceedings Paper
CT 9th International Conference on Information, Communications and Signal
   Processing (ICICS)
CY NOV 10-13, 2013
CL Tainan, TAIWAN
AB Nowadays, many applications use the biometric feature to deal with human-computer interaction. The skin color detection is the one of the most common approaches to locate human body part such as face, hands, and arms. It plays an important role of the following processes. Most of the researches are focus on specifying the particular range of the color space and using morphological operator without any restriction. Practically, the range of skin color tone is dynamic changes depending on the lighting condition and shadow interference. Without restrictions, the result image is usually under- or overestimated. Moreover, the difference of the ethnicities is usually results in the detection faults. This paper proposed an adaptive skin color tone detection using YCbCr color model. Based on the morphological closing and the constrained centroid-shifting algorithm to refine the skin color model. In experiments, it shows that our proposed method samples the target body part more compact and smooth.
C1 [Li, Chieh; Liu, Yu Hsiang; Shih, Huang-Chia] Yuan Ze Univ, Taoyuan, Taiwan.
C3 Yuan Ze University
RP Li, C (corresponding author), Yuan Ze Univ, Taoyuan, Taiwan.
EM acehilmmichael@gmail.com; klcs601@gmail.com; hcshih@saturn.yzu.edu.tw
RI Shih, Huang-Chia/AAH-4966-2021
CR Chai D, 1999, IEEE T CIRC SYST VID, V9, P551, DOI 10.1109/76.767122
   Dawod A. Y., 2010, 3 INT C ADV COMP THE, P562
   Huang C. C., 2004, P IEEE INT S COMM IN, V2, P1232
   Liu LY, 2011, IEEE T CONSUM ELECTR, V57, P1295, DOI 10.1109/TCE.2011.6018887
   Mutto C. D., 2012, TIME OF FLIGHT CAMER
   Plagemann V., 2010, IEEE CVPR, P755
NR 6
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-4799-0434-1
PY 2013
PG 4
WC Engineering, Electrical & Electronic; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Telecommunications
GA BC5JU
UT WOS:000353339000090
DA 2022-02-03
ER

PT J
AU Neto, LC
   Ramalho, GLB
   Neto, JFSR
   Veras, RMS
   Medeiros, FNS
AF Neto, Luiz Camara
   Ramalho, Geraldo L. B.
   Rocha Neto, Jeova F. S.
   Veras, Rodrigo M. S.
   Medeiros, Fatima N. S.
TI An unsupervised coarse-to-fine algorithm for blood vessel segmentation
   in fundus images
SO EXPERT SYSTEMS WITH APPLICATIONS
LA English
DT Article
DE Retinal vasculature; Local coarse segmentation; Balanced accuracy;
   Vessel refinement
ID RETINAL IMAGES; CLASSIFICATION; RECONSTRUCTION; EFFICIENT; FEATURES
AB Algorithms for retinal vessel segmentation are powerful tools in automatic tracking systems for early detection of ophthalmological and cardiovascular diseases, and for biometric identification. In order to create more robust and reliable systems, the algorithms need to be accurately evaluated to certify their ability to emulate specific human expertise. The main contribution of this paper is an unsupervised method to detect blood vessels in fundus images using a coarse-to-fine approach. Our methodology combines Gaussian smoothing, a morphological top-hat operator, and vessel contrast enhancement for background homogenization and noise reduction. Here, statistics of spatial dependency and probability are used to coarsely approximate the vessel map with an adaptive local thresholding scheme. The coarse segmentation is then refined through curvature analysis and morphological reconstruction to reduce pixel mislabeling and better estimate the retinal vessel tree. The method was evaluated in terms of its sensitivity, specificity and balanced accuracy. Extensive experiments have been conducted on DRIVE and STARE public retinal images databases. Comparisons with state-of-the-art methods revealed that our method outperformed most recent methods in terms of sensitivity and balanced accuracy with an average of 0.7819 and 0.8702, respectively. Also, the proposed method outperformed state-of-the-art methods when evaluating only pathological images that is a more challenging task. The method achieved for this set of images an average of 0.7842 and 0.8662 for sensitivity and balanced accuracy, respectively. Visual inspection also revealed that the proposed approach effectively addressed main image distortions by reducing mislabeling of central vessel reflex regions and false-positive detection of pathological patterns. These improvements indicate the ability of the method to accurately approximate the vessel tree with reduced visual interference of pathological patterns and vessel-like structures. Therefore, our method has the potential for supporting expert systems in screening, diagnosis and treatment of ophthalmological diseases, and furthermore for personal recognition based on retinal profile matching. (C) 2017 Elsevier Ltd. All rights reserved.
C1 [Neto, Luiz Camara; Rocha Neto, Jeova F. S.; Medeiros, Fatima N. S.] Univ Fed Ceara, Dept Engn Teleinformat, Campus Pici S-N,Bloco 725, BR-60455970 Fortaleza, CE, Brazil.
   [Ramalho, Geraldo L. B.] Inst Fed Educ Ciencia & Tecnol Ceara, Av 13 Maio 2081, BR-60040215 Fortaleza, CE, Brazil.
   [Veras, Rodrigo M. S.] Univ Fed Piaui, Dept Comp, Campus Univ Minist Petronio Portella, BR-64049550 Teresina, PI, Brazil.
C3 Universidade Federal do Ceara; Instituto Federal do Ceara (IFCE);
   Universidade Federal do Piaui
RP Medeiros, FNS (corresponding author), Univ Fed Ceara, Dept Engn Teleinformat, Campus Pici S-N,Bloco 725, BR-60455970 Fortaleza, CE, Brazil.
EM luizneto92@hotmail.com; gramalho@ifce.edu.br; jeovafarias@gmail.com;
   rveras@ufpi.edu.br; fsombra@ufc.br
RI Ramalho, Geraldo/N-3995-2019; de Medeiros, Fatima Nelsizeuma
   Sombra/E-1168-2011; Veras, Rodrigo M S/D-7358-2015
OI Ramalho, Geraldo/0000-0003-1872-3250; de Medeiros, Fatima Nelsizeuma
   Sombra/0000-0002-4143-1486; Veras, Rodrigo M S/0000-0001-8180-4032
FU CNPqConselho Nacional de Desenvolvimento Cientifico e Tecnologico
   (CNPQ); CAPESCoordenacao de Aperfeicoamento de Pessoal de Nivel Superior
   (CAPES) [444784/2014-4, 401442/2014-4]
FX We acknowledge that this research was partially supported by CNPq and
   CAPES (No. 444784/2014-4 and No. 401442/2014-4). We greatly appreciate
   the reviewers who helped to improve the paper.
CR Al-Diri B, 2009, IEEE T MED IMAGING, V28, P1488, DOI 10.1109/TMI.2009.2017941
   Azzopardi G, 2015, MED IMAGE ANAL, V19, P46, DOI 10.1016/j.media.2014.08.002
   Brodersen Kay H, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3121, DOI 10.1109/ICPR.2010.764
   Cheng EK, 2014, MACH VISION APPL, V25, P1779, DOI 10.1007/s00138-014-0638-x
   Cheng J, 2015, IEEE T BIO-MED ENG, V62, P1395, DOI 10.1109/TBME.2015.2389234
   Do Carmo M., 1976, DIFFERENTIAL GEOMETR
   Fraz MM, 2012, COMPUT METH PROG BIO, V108, P407, DOI 10.1016/j.cmpb.2012.03.009
   Fraz MM, 2012, IEEE T BIO-MED ENG, V59, P2538, DOI 10.1109/TBME.2012.2205687
   Gavet Y, 2014, MACH VISION APPL, V25, P1953, DOI 10.1007/s00138-014-0625-2
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hilal S, 2014, NEUROSCI LETT, V577, P95, DOI 10.1016/j.neulet.2014.06.024
   Hoover A, 2000, IEEE T MED IMAGING, V19, P203, DOI 10.1109/42.845178
   Joshi VS, 2012, THESIS
   Kanski J.J., 2007, CLIN OPHTHALMOLOGY
   Lam BSY, 2008, IEEE T MED IMAGING, V27, P237, DOI 10.1109/TMI.2007.909827
   Lazar I, 2015, COMPUT BIOL MED, V66, P209, DOI 10.1016/j.compbiomed.2015.09.008
   Li MH, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088738
   Li QL, 2016, IEEE T MED IMAGING, V35, P109, DOI 10.1109/TMI.2015.2457891
   Li Q, 2012, EXPERT SYST APPL, V39, P7600, DOI 10.1016/j.eswa.2011.12.046
   Marin D, 2011, IEEE T MED IMAGING, V30, P146, DOI 10.1109/TMI.2010.2064333
   Mendonca AM, 2006, IEEE T MED IMAGING, V25, P1200, DOI 10.1109/TMI.2006.879955
   Narasimha-Iyer H, 2008, IEEE T INF TECHNOL B, V12, P406, DOI 10.1109/TITB.2007.897782
   Rahebi J, 2014, J MED SYST, V38, DOI 10.1007/s10916-014-0085-2
   Salazar-Gonzalez A, 2014, IEEE J BIOMED HEALTH, V18, P1874, DOI 10.1109/JBHI.2014.2302749
   Serra J., 1983, IMAGE ANAL MATH MORP
   Soares JVB, 2006, IEEE T MED IMAGING, V25, P1214, DOI 10.1109/TMI.2006.879967
   Soille P., 2002, MORPHOLOGICAL IMAGE
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Villalobos-Castaldi FM, 2010, J VISUAL-JAPAN, V13, P263, DOI 10.1007/s12650-010-0037-y
   Vincent L, 1993, IEEE T IMAGE PROCESS, V2, P176, DOI 10.1109/83.217222
   Yau JWY, 2012, DIABETES CARE, V35, P556, DOI 10.2337/dc11-1909
   Yin BJ, 2015, MED IMAGE ANAL, V26, P232, DOI 10.1016/j.media.2015.09.002
   ZACK GW, 1977, J HISTOCHEM CYTOCHEM, V25, P741, DOI 10.1177/25.7.70454
   Zana F, 2001, IEEE T IMAGE PROCESS, V10, P1010, DOI 10.1109/83.931095
NR 34
TC 58
Z9 59
U1 0
U2 45
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4174
EI 1873-6793
J9 EXPERT SYST APPL
JI Expert Syst. Appl.
PD JUL 15
PY 2017
VL 78
BP 182
EP 192
DI 10.1016/j.eswa.2017.02.015
PG 11
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Operations Research & Management Science
GA ER5XK
UT WOS:000398877400014
DA 2022-02-03
ER

PT J
AU Xia, ZH
   Yuan, CS
   Lv, R
   Sun, XM
   Xiong, NN
   Shi, YQ
AF Xia, Zhihua
   Yuan, Chengsheng
   Lv, Rui
   Sun, Xingming
   Xiong, Neal N.
   Shi, Yun-Qing
TI A Novel Weber Local Binary Descriptor for Fingerprint Liveness Detection
SO IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS
LA English
DT Article
DE Feature extraction; Authentication; Information science; Histograms;
   Collaboration; Technological innovation; Software; Biometrics; digital
   forensics; fingerprint liveness detection (FLD); local binary pattern
   (LBP); Weber's law
ID PERSPIRATION; FEATURES; PATTERN
AB In recent years, fingerprint authentication systems have been extensively deployed in various applications, including attendance systems, authentications on smartphones, mobile payment authorizations, as well as various safety certifications. However, similar to the other biometric identification technologies, fingerprint recognition is vulnerable to artificial replicas made from cheap materials, such as silicon, gelatin, etc. Thus, it is especially necessary to distinguish whether a given fingerprint is a live or a spoof one prior to such authentication. In order to solve the problems above, a novel local descriptor named Weber local binary descriptor for fingerprint liveness detection (FLD) has been proposed in this paper. The method consists of two components: the local binary differential excitation component that extracts intensity-variance features and the local binary gradient orientation component that extracts orientation features. The co-occurrence probability of the two components is calculated to construct a discriminative feature vector, which is fed into support vector machine (SVM) classifiers. The effectiveness of the proposed method is intuitively analyzed on the image samples and numerically demonstrated by Mahalanobis distance. Experiments are performed on two public databases from FLD competitions from 2011 and 2013. The results have proved that the proposed method obtains the best detection accuracy among the existing image local descriptors in FLD.
C1 [Xia, Zhihua; Yuan, Chengsheng; Lv, Rui; Sun, Xingming] Nanjing Univ Informat Sci & Technol, Jiangsu Engn Ctr Network Monitoring, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Nanjing 210044, Peoples R China.
   [Xia, Zhihua] Sungkyunkwan Univ, Coll Informat & Commun Engn, Seoul 16419, South Korea.
   [Yuan, Chengsheng; Lv, Rui; Sun, Xingming] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Peoples R China.
   [Xiong, Neal N.] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Shi, Yun-Qing] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
C3 Nanjing University of Information Science & Technology; Sungkyunkwan
   University (SKKU); Nanjing University of Information Science &
   Technology; Tianjin University; New Jersey Institute of Technology
RP Xiong, NN (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
EM xia_zhihua@163.com; ycs_nuist@163.com; lrain_nuist@163.com;
   sunnudt@163.com; xiongnaixue@gmail.com; shi@njit.edu
RI xiong, naixue/M-4277-2019
OI xiong, naixue/0000-0002-0394-4635; Xia, Zhihua/0000-0001-6860-647X
FU Jiangsu Basic Research Programs-Natural Science Foundation [BK20181407,
   BK20150925, BK20151530]; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China (NSFC) [61502242,
   61702276, U1536206, U1405254, 61772283, 61602253, 61601236, 61572258,
   61672294]; Six Peak Talent Project of Jiangsu Province [R2016L13];
   National Key Research and Development Program of China [2018YFB1003205];
   NRF [2016R1D1A1B03933294]; Priority Academic Program Development of
   Jiangsu Higher Education Institutions Fund; Collaborative Innovation
   Center of Atmospheric Environment and Equipment Technology Fund, China;
   BK21+ Program from the Ministry of Education of South Korea
FX This work was supported in part by the Jiangsu Basic Research
   Programs-Natural Science Foundation under Grant BK20181407, Grant
   BK20150925, and Grant BK20151530; in part by the National Natural
   Science Foundation of China under Grant 61672294; in part by the Six
   Peak Talent Project of Jiangsu Province under Grant R2016L13; in part by
   the National Natural Science Foundation of China under Grant 61502242,
   Grant 61702276, Grant U1536206, Grant U1405254, Grant 61772283, Grant
   61602253, Grant 61601236, and Grant 61572258; in part by the National
   Key Research and Development Program of China under Grant
   2018YFB1003205; in part by the NRF under Grant 2016R1D1A1B03933294; in
   part by the Priority Academic Program Development of Jiangsu Higher
   Education Institutions Fund; and in part by the Collaborative Innovation
   Center of Atmospheric Environment and Equipment Technology Fund, China.
   The work of Z. Xia was supported by BK21+ Program from the Ministry of
   Education of South Korea. This paper was recommended by Associate Editor
   X. Wang.
CR Abhyankar A, 2006, IEEE IMAGE PROC, P321, DOI 10.1109/ICIP.2006.313158
   Abhyankar A, 2009, PATTERN RECOGN, V42, P452, DOI 10.1016/j.patcog.2008.06.012
   Akhtar Z, 2015, INT CONF BIOMETR, P305, DOI 10.1109/ICB.2015.7139054
   Alajlan M, 2013, 2013 INTERNATIONAL CONFERENCE ON INDIVIDUAL AND COLLECTIVE BEHAVIORS IN ROBOTICS (ICBR), P1, DOI 10.1109/ICBR.2013.6729271
   Antonelli A., 2005, Advances in Biometrics. International Conference, ICB 2006. Proceedings (Lecture Notes in Computer Science Vol.3832), P221
   Antonelli A, 2006, IEEE T INF FOREN SEC, V1, P360, DOI 10.1109/TIFS.2006.879289
   Bera A, 2020, IEEE T SYST MAN CY-S, V50, P747, DOI [10.1109/TSMC.2017.2744669, 10.1007/978-981-10-1678-3_1]
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen J, 2010, IEEE T PATTERN ANAL, V32, P1705, DOI 10.1109/TPAMI.2009.155
   Coli P, 2007, 2007 IEEE WORKSHOP ON AUTOMATIC IDENTIFICATION ADVANCED TECHNOLOGIES, PROCEEDINGS, P169, DOI 10.1109/AUTOID.2007.380614
   De Maesschalck R, 2000, CHEMOMETR INTELL LAB, V50, P1, DOI 10.1016/S0169-7439(99)00047-7
   Dubey RK, 2016, IEEE T INF FOREN SEC, V11, P1461, DOI 10.1109/TIFS.2016.2535899
   Egberts P, 2017, TRIBOL LUBR TECHNOL, V73, P58
   EKMAN G, 1959, J PSYCHOL, V47, P343, DOI 10.1080/00223980.1959.9916336
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Galbally J, 2012, FUTURE GENER COMP SY, V28, P311, DOI 10.1016/j.future.2010.11.024
   Ghiani L, 2017, IMAGE VISION COMPUT, V58, P110, DOI 10.1016/j.imavis.2016.07.002
   Ghiani L, 2013, INT CONF BIOMETR, DOI 10.1109/ICB.2013.6613027
   Ghiani L, 2012, INT C PATT RECOG, P537
   Ghiani L, 2012, LECT NOTES COMPUT SC, V7378, P210, DOI 10.1007/978-3-642-31567-1_21
   Gottschlich C, 2014, 2014 IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2014)
   Gragnaniello Diego, 2013, Proceedings of the 2013 IEEE Workshop on Biometric Measurements and Systems for Security and Medical Applications (BIOMS), P46, DOI 10.1109/BIOMS.2013.6656148
   Gragnaniello D, 2015, IEEE T INF FOREN SEC, V10, P849, DOI 10.1109/TIFS.2015.2404294
   Gragnaniello D, 2015, PATTERN RECOGN, V48, P1050, DOI 10.1016/j.patcog.2014.05.021
   Jia XF, 2014, INFORM SCIENCES, V268, P91, DOI 10.1016/j.ins.2013.06.041
   Jin C, 2011, LECT NOTES COMPUT SC, V6513, P281, DOI 10.1007/978-3-642-17955-6_21
   Jin Z, 2016, IEEE T SYST MAN CY-S, V46, P1415, DOI 10.1109/TSMC.2015.2499725
   Johnson-Laird PN, 2013, PSYCHOL LEARN MOTIV, V59, P1, DOI 10.1016/B978-0-12-407187-2.00001-0
   Kim H., 2016, P INT C BIOM SPEC IN, P1, DOI DOI 10.1109/WACV.2016.7477589
   Kim W, 2017, IEEE SIGNAL PROC LET, V24, P51, DOI 10.1109/LSP.2016.2636158
   Liu F, 2013, NEUROCOMPUTING, V120, P325, DOI 10.1016/j.neucom.2012.06.061
   Manivanan N, 2010, ELECTRON LETT, V46, P1268, DOI 10.1049/el.2010.1549
   Marasco E, 2012, PATTERN RECOGN LETT, V33, P1148, DOI 10.1016/j.patrec.2012.01.009
   Marcialis Gian Luca, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1289, DOI 10.1109/ICPR.2010.321
   Marcialis GL, 2009, LECT NOTES COMPUT SC, V5716, P12, DOI 10.1007/978-3-642-04146-4_4
   Menotti D, 2015, IEEE T INF FOREN SEC, V10, P864, DOI 10.1109/TIFS.2015.2398817
   Moon YS, 2005, ELECTRON LETT, V41, P1112, DOI 10.1049/el:20052577
   Mura V., 2015, PROC INT C BIOMETRIC, P1
   Mura V, 2018, INT CONF BIOMETR, P297, DOI 10.1109/ICB2018.2018.00052
   Nikam Shankar Bhausaheb, 2008, 2008 1st International Conference on Emerging Trends in Engineering and Technology (ICETET), P675, DOI 10.1109/ICETET.2008.134
   Nikam SB, 2008, I C COMP GRAPH IM VI, P217, DOI 10.1109/CGIV.2008.9
   Nikam SB, 2009, NEUROCOMPUTING, V72, P2491, DOI 10.1016/j.neucom.2008.11.003
   Nikam SB, 2008, LECT NOTES COMPUT SC, V5259, P1103, DOI 10.1007/978-3-540-88458-3_100
   Nogueira RF, 2016, IEEE T INF FOREN SEC, V11, P1206, DOI 10.1109/TIFS.2016.2520880
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Sousedik C, 2014, IET BIOMETRICS, V3, P219, DOI 10.1049/iet-bmt.2013.0020
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Tan BZ, 2010, PATTERN RECOGN, V43, P2845, DOI 10.1016/j.patcog.2010.01.023
   Xia Z., 2013, SIGNAL IMAGE VIDEO P, V11, P381
   Xia ZH, 2018, MULTIMED TOOLS APPL, V77, P18187, DOI 10.1007/s11042-017-5517-9
   Xia ZH, 2018, IEEE ACCESS, V6, P30392, DOI 10.1109/ACCESS.2018.2845456
   Yambay D., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P208, DOI 10.1109/ICB.2012.6199810
   Yang YX, 2017, IEEE T SYST MAN CY-S, V47, P950, DOI 10.1109/TSMC.2016.2523907
   Yuan CS, 2019, SOFT COMPUT, V23, P5157, DOI 10.1007/s00500-018-3182-1
   Yuan CS, 2016, CHINA COMMUN, V13, P60, DOI 10.1109/CC.2016.7559076
NR 55
TC 21
Z9 21
U1 5
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2168-2216
EI 2168-2232
J9 IEEE T SYST MAN CY-S
JI IEEE Trans. Syst. Man Cybern. -Syst.
PD APR
PY 2020
VL 50
IS 4
BP 1526
EP 1536
DI 10.1109/TSMC.2018.2874281
PG 11
WC Automation & Control Systems; Computer Science, Cybernetics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Automation & Control Systems; Computer Science
GA KX9WI
UT WOS:000522225200028
DA 2022-02-03
ER

PT J
AU Nair, SK
   Chinnappan, SK
   Dubey, AK
   Subburaj, A
   Subramaniam, S
   Balasubramaniam, V
   Sengan, S
AF Krishnan Nair, Sreekumar
   Chinnappan, Sathiya Kumar
   Dubey, Anil Kumar
   Subburaj, Arjun
   Subramaniam, Shanthi
   Balasubramaniam, Vivekanandam
   Sengan, Sudhakar
TI Prewitt Logistic Deep Recurrent Neural Learning for Face Log Detection
   by Extracting Features from Images
SO ARABIAN JOURNAL FOR SCIENCE AND ENGINEERING
LA English
DT Article; Early Access
DE Face log detection; Prewitt Logistic Deep Recurrent Neural Learning; Key
   frame extraction; Facial feature extraction; Input layer; Hidden layer;
   Prewitt edge detector; Logistic activation function
AB Face log detection (FLD) in the surveillance video extracts a new face image from the video sequences (VS). FLD utilizes biometric techniques for humans' recognition. To improve the precise FLD with less complexity of our proposed method is Prewitt Logistic Deep Recurrent Neural Learning (PLDRNL) used. The input VS was received from the video database. Next, the keyframes are extracted from the VS. This proposed deep recurrent neural learning method uses four hidden layers to remove the facial features such as the face, eyes, nose, and mouth in the form of an edge. The edges of each element are derived using the Prewitt edge detector through the horizontal and vertical mask. Finally, the relevant features are fed into the output layer. The PLDRNL uses a logistic activation function at the output layer for matching the extracted related elements with the pre-stored testing feature vector. If two features are matched, then the face in the given VF is detected. The error in the FD is minimized using gradient descent function at the output layer. Based on the results, the human face effectively identified with the minimum false-positive rate (FPR). Experimental evaluation is carried out using different factors such as FLD, FPR, and time complexity.
C1 [Krishnan Nair, Sreekumar] SRM Inst Sci & Technol, Sch Comp, Dept Comp Sci & Engn, Kattankulathur 603203, Tamil Nadu, India.
   [Chinnappan, Sathiya Kumar] Vellore Inst Technol, Sch Comp Sci & Engn, Dept Computat Intelligence, Vellore 632014, Tamil Nadu, India.
   [Dubey, Anil Kumar] ABES Engn Coll, Dept Comp Sci & Engn, Ghaziabad 201009, Uttar Pradesh, India.
   [Subburaj, Arjun] Tranxit Technol Solut Private Ltd, Chennai, Tamil Nadu, India.
   [Subramaniam, Shanthi] Kongu Engn Coll, Dept Comp Sci & Engn, Perundurai 638060, Tamil Nadu, India.
   [Balasubramaniam, Vivekanandam] Lincoln Univ Coll, Fac Comp Sci & Multimedia, Kota Baharu, Kelantan, Malaysia.
   [Sengan, Sudhakar] PSN Coll Engn & Technol, Dept Comp Sci & Engn, Tirunelveli 627152, Tamil Nadu, India.
C3 SRM Institute of Science & Technology Chennai; Vellore Institute of
   Technology; Kongu Engineering College; Lincoln University College
RP Sengan, S (corresponding author), PSN Coll Engn & Technol, Dept Comp Sci & Engn, Tirunelveli 627152, Tamil Nadu, India.
EM sreekumar.k@ktr.srmuniv.ac.in; sathiyakumar.c@vit.ac.in;
   anildudenish@gmail.com; aj@sentientlabs.io; shanthi.kongumca@gmail.com;
   vivekresearch2014@gmail.com; sudhasengan@gmail.com
RI Senagn, Sudhakar/AAH-6502-2020; S, Shanthi/AAL-2519-2020;
   Balasubramaniam, Vivekanandam/AAZ-1422-2021
OI Senagn, Sudhakar/0000-0003-4901-1432; S, Shanthi/0000-0001-8611-4196; 
CR An L, 2013, IEEE J EM SEL TOP C, V3, P155, DOI 10.1109/JETCAS.2013.2256752
   Angadi SA, 2017, PATTERN RECOGN, V71, P235, DOI 10.1016/j.patcog.2017.06.014
   Bashbaghi S, 2017, PATTERN RECOGN, V69, P61, DOI 10.1016/j.patcog.2017.04.014
   Bhatt HS, 2014, IEEE T INF FOREN SEC, V9, P1056, DOI 10.1109/TIFS.2014.2318433
   Chitaliya NG, 2010, PROCEDIA COMPUT SCI, V2, P52, DOI 10.1016/j.procs.2010.11.008
   Choi JY, 2012, IEEE T SYST MAN CY B, V42, P1270, DOI 10.1109/TSMCB.2012.2185693
   Dang LM, 2019, EXPERT SYST APPL, V129, P156, DOI 10.1016/j.eswa.2019.04.005
   De-la-Torre M, 2015, INFORM FUSION, V24, P31, DOI 10.1016/j.inffus.2014.05.006
   Dewan MAA, 2016, PATTERN RECOGN, V49, P129, DOI 10.1016/j.patcog.2015.08.002
   Ding CX, 2018, IEEE T PATTERN ANAL, V40, P1002, DOI 10.1109/TPAMI.2017.2700390
   Ding SH, 2015, IEEE T CIRC SYST VID, V25, P1586, DOI 10.1109/TCSVT.2014.2351094
   Dong ZL, 2020, IEEE ACCESS, V8, P63421, DOI 10.1109/ACCESS.2020.2982779
   Faseela T, 2016, PROC TECH, V24, P1285, DOI 10.1016/j.protcy.2016.05.118
   Gaikawad AD., 2016, INT J SCI ENG TECHNO, V5, P1245
   Gao G., 2016, PLOS ONE, V11, P1
   Guo X, 2018, J VIS COMMUN IMAGE R, V50, P65, DOI 10.1016/j.jvcir.2017.11.007
   He R, 2018, PATTERN RECOGN, V75, P4, DOI 10.1016/j.patcog.2017.02.005
   Jingxiao Zheng, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P194, DOI 10.1109/TBIOM.2020.2973504
   Ju-Yuan Hsiao, 2016, International Journal of Computers and Applications, V38, P1, DOI 10.1080/1206212X.2016.1188553
   Korshunov P, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2000486.2000488
   Lai ZF, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/2061516
   Liu Q, 2016, NEUROCOMPUTING, V194, P10, DOI 10.1016/j.neucom.2016.02.011
   Lu Z, 2018, SIGNAL PROCESS, V144, P296, DOI 10.1016/j.sigpro.2017.10.024
   Ma CF, 2015, NEUROCOMPUTING, V149, P1232, DOI 10.1016/j.neucom.2014.09.004
   Martinez-Diaz Y, 2018, J VIS COMMUN IMAGE R, V51, P155, DOI 10.1016/j.jvcir.2018.01.017
   Ng CJ, 2018, J VIS COMMUN IMAGE R, V55, P548, DOI 10.1016/j.jvcir.2018.07.002
   Niu G, 2018, J VIS COMMUN IMAGE R, V55, P457, DOI 10.1016/j.jvcir.2018.07.001
   Park BH, 2017, EXPERT SYST APPL, V89, P66, DOI 10.1016/j.eswa.2017.07.018
   Ranjan Rajeev, 2019, IEEE Transactions on Biometrics, Behavior, and Identity Science, V1, P82, DOI 10.1109/TBIOM.2019.2908436
   Rehman YAU, 2018, EXPERT SYST APPL, V108, P159, DOI 10.1016/j.eswa.2018.05.004
   Sanya liu, 2019, INFRARED PHYS TECHN
   Shieh MY, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/694321
   Vageeswaran P, 2013, IEEE T IMAGE PROCESS, V22, P1362, DOI 10.1109/TIP.2012.2228498
   Wu WQ, 2019, IEEE T CYBERNETICS, V49, P4017, DOI 10.1109/TCYB.2018.2859482
   Yaji GS, 2012, PROC TECH, V1, P475, DOI 10.1016/j.protcy.2012.10.057
   Yoo CH, 2017, J VIS COMMUN IMAGE R, V45, P11, DOI 10.1016/j.jvcir.2017.02.009
   Yu YF, 2017, PATTERN RECOGN, V67, P201, DOI 10.1016/j.patcog.2017.02.004
   Zhao SH, 2018, OPTIK, V168, P920, DOI 10.1016/j.ijleo.2018.05.013
NR 38
TC 0
Z9 0
U1 1
U2 2
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 2193-567X
EI 2191-4281
J9 ARAB J SCI ENG
JI Arab. J. Sci. Eng.
DI 10.1007/s13369-021-05609-4
EA APR 2021
PG 12
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA RM6AQ
UT WOS:000639740900010
DA 2022-02-03
ER

PT J
AU Mamta
   Hanmandlu, M
AF Mamta
   Hanmandlu, Madasu
TI Robust ear based authentication using Local Principal Independent
   Components
SO EXPERT SYSTEMS WITH APPLICATIONS
LA English
DT Article
DE PCA; LPIC; Ear biometric; Effective information (EI); Energy feature
   (EF); Sigmoid feature (SF); Multi Quadratic feature (MQD); Inner product
   classifier (IPC); Euclidean classifier (EC)
ID RECOGNITION; FACE
AB This paper presents the ear based authentication using Local Principal Independent Components (LPIC) an extension of PCA. As PCA is a global approach dealing with all pixel intensities, it is difficult to get finer details from the ear image. The concept of information sets is introduced in this paper so as to have leverage over the local information. These sets are based on the granularization of the ear image in the form of windows. The features based on these sets allow us to change the local information which goes into LPIC as the input. Thus LPIC not only uses this local information but also helps to reduce the dimensions of the deduced features far less than that can be achieved with PCA. For the extraction of sparse information from ear, features such as Effective information (EI), Energy feature (EF), Sigmoid feature (SF), Multi Quadratic feature (MQD) are derived and then LPIC is applied to get the reduced number of features. Inner product classifier (IPC) is developed for the classification of these features. The experiments carried out on constrained and unconstrained databases show that LPIC is effective not only under the ideal conditions but also under the unconstrained environment. (c) 2013 Elsevier Ltd. All rights reserved.
C1 [Mamta; Hanmandlu, Madasu] Indian Inst Technol Delhi, Dept Elect Engn, Delhi 110016, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Delhi
RP Mamta (corresponding author), Indian Inst Technol Delhi, Dept Elect Engn, Delhi 110016, India.
EM mamtabansal.iitd@gmail.com; mhmandlu@gmail.com
CR Abate AF, 2006, INT C PATT RECOG, P437
   Abdel-Mottaleb M., 2005, Advances in Biometrics. International Conference, ICB 2006. Proceedings (Lecture Notes in Computer Science Vol.3832), P786
   [Anonymous], 1998, BBC NEWS
   [Anonymous], 2007, IIT DELHI EAR DATABA
   Arbab-Zavar B, 2007, 2007 FIRST IEEE INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS, P80
   Arbab-Zavar B, 2007, LECT NOTES COMPUT SC, V4842, P549
   Bertillon A., 1896, SIGNALETIC INSTRUCTI
   Bolle RM, 2005, FOURTH IEEE WORKSHOP ON AUTOMATIC IDENTIFICATION ADVANCED TECHNOLOGIES, PROCEEDINGS, P15, DOI 10.1109/AUTOID.2005.48
   Burge M, 2000, INT C PATT RECOG, P822, DOI 10.1109/ICPR.2000.906202
   Burge M, 1998, BIOMETRICS PERSONAL, P273
   Bustard JD, 2010, IEEE T SYST MAN CY A, V40, P486, DOI 10.1109/TSMCA.2010.2041652
   Chan TS, 2012, PATTERN RECOGN LETT, V33, P1870, DOI 10.1016/j.patrec.2011.11.013
   Chang K, 2003, IEEE T PATTERN ANAL, V25, P1160, DOI 10.1109/TPAMI.2003.1227990
   Cummin A. H., 2010, P 4 IEEE INT C BIOM, P1
   Dallagher M., 2004, RELEASED NEWS ITEAM
   FIELDS C, 1960, OBSTET GYNECOL, V16, P98
   Hanmandlu M, 2011, DEFENCE SCI J, V61, P415, DOI 10.14429/dsj.61.1177
   Hanmandlu M, 2003, PATTERN RECOGN LETT, V24, P81, DOI 10.1016/S0167-8655(02)00191-5
   Hoogstrate AJ, 2001, SCI JUSTICE, V41, P167, DOI 10.1016/S1355-0306(01)71885-0
   Hurley DJ, 2005, COMPUT VIS IMAGE UND, V98, P491, DOI 10.1016/j.cviu.2004.11.001
   Hurley DJ, 2002, IMAGE VISION COMPUT, V20, P311, DOI 10.1016/S0262-8856(02)00003-3
   Iannarelli A., 1989, EAR IDENTIFICATION S
   Islam SMS, 2011, INT J COMPUT VISION, V95, P52, DOI 10.1007/s11263-011-0436-0
   Klement E. P., 2000, TRIANGULAR NORMS
   Kumar A, 2013, PATTERN RECOGN, V46, P73, DOI 10.1016/j.patcog.2012.06.020
   Kumar A, 2012, PATTERN RECOGN, V45, P956, DOI 10.1016/j.patcog.2011.06.005
   Li Yuan, 2012, Proceedings of the 2012 International Conference on System Science and Engineering (ICSSE), P349, DOI 10.1109/ICSSE.2012.6257205
   Nosrati MS, 2007, ICIAS 2007: INTERNATIONAL CONFERENCE ON INTELLIGENT & ADVANCED SYSTEMS, VOLS 1-3, PROCEEDINGS, P616
   Osterburgh J. W., 1989, CRIME LAB
   PAL NR, 1992, INFORM SCIENCES, V66, P119, DOI 10.1016/0020-0255(92)90090-U
   Perpinan C., 1995, THESIS TU MADRID
   Rahman M., 2007, INT J COMPUT INTERNE, V15, P1
   Ross A., 2011, HUMAN EAR RECOGNITIO
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Victor B, 2002, INT C PATT RECOG, P429, DOI 10.1109/ICPR.2002.1044746
   Yahagi, 2006, P COMP INT, P253
   Yuan L, 2006, INT C PATT RECOG, P501
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
   Zhang HJ, 2005, Proceedings of 2005 International Conference on Machine Learning and Cybernetics, Vols 1-9, P4511
   Zhou JD, 2012, IEEE T INF FOREN SEC, V7, P978, DOI 10.1109/TIFS.2012.2189005
NR 41
TC 43
Z9 43
U1 0
U2 9
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4174
EI 1873-6793
J9 EXPERT SYST APPL
JI Expert Syst. Appl.
PD NOV 15
PY 2013
VL 40
IS 16
BP 6478
EP 6490
DI 10.1016/j.eswa.2013.05.020
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Operations Research & Management Science
GA 197NF
UT WOS:000322857200025
DA 2022-02-03
ER

PT C
AU Poh, N
   Heusch, G
   Kittler, J
AF Poh, Norman
   Heusch, Guillaume
   Kittler, Josef
BE Haindl, M
   Kittler, J
   Roli, F
TI On combination of face authentication experts by a mixture of quality
   dependent fusion classifiers
SO MULTIPLE CLASSIFIER SYSTEMS, PROCEEDINGS
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 7th International Workshop on Multiple Classifier Systems
CY MAY 23-25, 2007
CL Prague, CZECH REPUBLIC
AB Face as a biometric is known to be sensitive to different factors, e.g., illumination condition and pose. The resultant degradation in face image quality affects the system performance. To counteract this problem, we investigate the merit of combining a set of face verification systems incorporating image-related quality measures. We propose a fusion paradigm where the quality measures are quantised into a finite set of discrete quality states, e.g., "good illumination vs. "bad illumination". For each quality state, we design a fusion classifier. The outputs of these fusion classifiers are then combined by a weighted averaging controlled by the a posteriori probability of a quality state given the observed quality measures. The use of quality states in fusion is compared to the direct use of quality measures where the density of scores and quality are jointly estimated. There are two advantages of using quality states. Firstly, much less training data is needed in the former since the relationship between base classifier output scores and quality measures is not learnt jointly but separately via the conditioning quality states. Secondly, the number of quality states provides an explicit control over the complexity of the resulting fusion classifier. In all our experiments involving XM2VTS well illuminated and dark face data sets, there is a systematic improvement in performance over the baseline method (without using quality information) and the direct use of quality in two types of applications: as a quality-dependent score normalisation procedure and as a quality-dependent fusion method (involving several systems).
C1 [Poh, Norman; Kittler, Josef] Univ Surrey, CVSSP, Surrey GU2 7XH, England.
   [Heusch, Guillaume] IDIAP, EPFL, CH1920 Martigny, Switzerland.
C3 University of Surrey
RP Poh, N (corresponding author), Univ Surrey, CVSSP, Surrey GU2 7XH, England.
EM normanpoh@ieee.org; guillaume.heusch@epfl.ch; j.kittler@surrey.ac.uk
RI Poh, Norman/Y-8261-2019
FU Swiss National Science FoundationSwiss National Science Foundation
   (SNSF)European Commission [PBEL2-114330]; Engineering and Physical
   Sciences Research Council (EPSRC) ResearchUK Research & Innovation
   (UKRI)Engineering & Physical Sciences Research Council (EPSRC)
   [GR/S46543]
FX This work was supported partially by the prospective researcher
   fellowship PBEL2-114330 of the Swiss National Science Foundation, by the
   BioSecure project (www.biosecure.info) and by Engineering and Physical
   Sciences Research Council (EPSRC) Research Grant GR/S46543. This
   publication only reflects the authors' view.
CR Bigun J, 2003, 12TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P2, DOI 10.1109/ICIAP.2003.1234017
   BISHOP CM, 1999, NEURAL NETWORKS PATT
   Cardinaux F, 2006, IEEE T SIGNAL PROCES, V54, P361, DOI 10.1109/TSP.2005.861075
   Dass SC, 2005, LECT NOTES COMPUT SC, V3546, P1049
   Fierrez-Aguilar J, 2004, PROC SPIE, V5404, P544, DOI 10.1117/12.542800
   GROSS R, 2003, 4 INT C AUD VID BAS, P10
   Hastie T., 2009, ELEMENTS STAT LEARNI, V1
   Heusch G, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P9
   Jensen FV, 1996, INTRO BAYESIAN NETWO, DOI [10.1007/978-3-642-54157-5_5, DOI 10.1007/978-3-642-54157-5_5]
   Kittler J., 2000, BRIT MACH VIS C BMVC
   KRYSZCZUK K, 2005, P 12 EUR C SIGN PROC
   Matas J, 2000, INT C PATT RECOG, P858
   Messer K, 2006, LECT NOTES COMPUT SC, V3832, P1
   Nandakumar K, 2006, INT C PATT RECOG, P473
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   Toh KA, 2004, LECT NOTES COMPUT SC, V3072, P678
NR 17
TC 22
Z9 22
U1 0
U2 2
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-540-72481-0
J9 LECT NOTES COMPUT SC
PY 2007
VL 4472
BP 344
EP +
PG 3
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BGG41
UT WOS:000246659200035
DA 2022-02-03
ER

PT C
AU Kamath, KMS
   Rajeev, S
   Panetta, K
   Agaian, SS
AF Kamath, Shreyas K. M.
   Rajeev, Srijith
   Panetta, Karen
   Agaian, Sos S.
GP IEEE
TI Fingerprint Authentication Using Geometric Features
SO 2017 IEEE INTERNATIONAL SYMPOSIUM ON TECHNOLOGIES FOR HOMELAND SECURITY
   (HST)
LA English
DT Proceedings Paper
CT IEEE International Symposium on Technologies for Homeland Security (HST)
CY APR 25-26, 2017
CL Waltham, MA
DE fingerprint authentication; fingerprint matching; geometric features;
   SIFT; detectors; descriptors
ID VERIFICATION
AB Biometric based authentication, particularly for fingerprint authentication systems play a vital role in identifying an individual. The existing fingerprint authentication systems depend on specific points known as minutiae for recognizing an individual. Designing a reliable automatic fingerprint authentication system is still very challenging, since not all fingerprint information is available. Further, the information obtained is not always accurate due to cuts, scars, sweat, distortion and various skin conditions. Moreover, the existing fingerprint authentication systems do not utilize other significant minutiae information, which can improve the accuracy. Various local feature detectors such as Difference-of-Gaussian, Hessian, Hessian Laplace, Harris Laplace, Multiscale Harris, and Multiscale Hessian have been extensively used for feature detection. However, these detectors have not been employed for detecting fingerprint image features. In this article, a versatile local feature fingerprint matching scheme is proposed. The local features are obtained by exploiting these local geometric detectors and SIFT descriptor. This scheme considers local characteristic features of the fingerprint image, thus eliminating the issues caused in existing fingerprint feature based matching techniques. Computer simulations of the proposed algorithm on specific databases show significant improvements when compared to existing fingerprint matchers, such as minutiae matcher, hierarchical matcher and graph based matcher. Computer simulations conducted on the Neurotechnology database demonstrates a very low Equal Error Rate (EER) of 0.8%. The proposed system a) improves the accuracy of the fingerprint authentication system, b) works when the minutiae information is sparse, and c) produces satisfactory matching accuracy in the case when minutiae information is unavailable. The proposed system can also be employed for partial fingerprint authentication.
C1 [Kamath, Shreyas K. M.; Rajeev, Srijith; Panetta, Karen] Tufts Univ, Dept Elect & Comp Engn, Medford, MA 02155 USA.
   [Agaian, Sos S.] Univ Texas San Antonio, Dept Elect & Comp Engn, San Antonio, TX USA.
C3 Tufts University; University of Texas System; University of Texas at San
   Antonio (UTSA)
RP Kamath, KMS (corresponding author), Tufts Univ, Dept Elect & Comp Engn, Medford, MA 02155 USA.
EM skamat04@tufts.edu; srajee02@tufts.edu; karen@ece.tufts.edu;
   Sos.Agaian@utsa.edu
RI Mohandas, Shreyas Kamath Kalasa/W-3823-2019; Panetta, Karen/C-1299-2011;
   Rajeev, Srijith/AAY-5809-2021
OI Mohandas, Shreyas Kamath Kalasa/0000-0001-7562-7471; Rajeev,
   Srijith/0000-0003-4537-8974
CR Agaian S., 2000, IASTED INT C SIGNAL, P19
   Agaian S.S., 2016, ELECTRON IMAGING, V2016, P1
   Agaian SS, 2010, IEEE T SYST MAN CY B, V40, P371, DOI 10.1109/TSMCB.2009.2024771
   Agaian SS, 2001, IEEE T IMAGE PROCESS, V10, P367, DOI 10.1109/83.908502
   [Anonymous], 2016, SEARCH PRACTICAL 201
   [Anonymous], 2016, RIDGE FEATURE BASED
   [Anonymous], 2016, FVC2004 3 INT FINGER
   [Anonymous], 2016, SIFT IMAGE FEATURES
   AYYANNA K., 2007, STUDY SOME FINGERPRI
   Bakhtiari S., 2012, SPIE DEFENSE SECURIT
   Chih-Jen Lee, 1999, 1999 IEEE Workshop on Signal Processing Systems. SiPS 99. Design and Implementation (Cat. No.99TH8461), P371, DOI 10.1109/SIPS.1999.822342
   Chikkerur S, 2006, LECT NOTES COMPUT SC, V3832, P309
   Derpanis K.G., 2010, IMAGE ROCHESTER NY, V4, P2
   Gamassi M., 2004, Proceedings of the 21st IEEE Instrumentation and Measurement Technology Conference (IEEE Cat. No.04CH37510), P510, DOI 10.1109/IMTC.2004.1351099
   Grauman K., 2011, SYNTHESIS LECT ARTIF, V5, P1, DOI 10.2200/S00332ED1V01Y201103AIM011
   Gu JW, 2006, IEEE T IMAGE PROCESS, V15, P1952, DOI 10.1109/TIP.2006.873443
   Jain AK, 1997, P IEEE, V85, P1365, DOI 10.1109/5.628674
   Li C, 2016, IEEE T INF FOREN SEC, V11, P543, DOI 10.1109/TIFS.2015.2505630
   Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo XP, 2000, INT C PATT RECOG, P833
   Malathi S., 2010, INT J COMPUTER SCI E, V02, P1411
   Maltoni D., 2009, HDB FINGERPRINT RECO, V2nd
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2002, LECT NOTES COMPUT SC, V2350, P128, DOI 10.1007/3-540-47969-4_9
   Mikolajczyk K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P525, DOI 10.1109/ICCV.2001.937561
   N Us Department of Commerce, 2015, NBIS
   Neurotechnology, 2016, DOWNL BIOM ALG DEM S
   Panetta KA, 2008, J COMPUT, V3, P11
   Park U, 2008, SPIE DEF SEC S
   Peralta D, 2015, INFORM SCIENCES, V315, P67, DOI 10.1016/j.ins.2015.04.013
   Rawat A., 2009, HIERARCHICAL FINGERP
   Sagai V. K., 1999, ICONIP'99. ANZIIS'99 & ANNES'99 & ACNN'99. 6th International Conference on Neural Information Processing. Proceedings (Cat. No.99EX378), P1138, DOI 10.1109/ICONIP.1999.844696
   Schmid C, 2000, INT J COMPUT VISION, V37, P151, DOI 10.1023/A:1008199403446
   T. C. S. Monitor, 2008, MOR EXT TOUR FING CO
   Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017
   Wharton E., 2006, DEF SEC S
   Wharton EJ, 2007, IEEE SYS MAN CYBERN, P1587
   Yager N, 2004, PATTERN ANAL APPL, V7, P94, DOI 10.1007/s10044-003-0201-2
   Yeegahng Song, 2004, Proceedings of 2004 International Symposium on Intelligent Signal Processing And Communication Systems ISPACS 2004 (IEEE Cat. No.04EX910), P524, DOI 10.1109/ISPACS.2004.1439111
   Zhang X., 2012, COMPUT SCI ENG, V2, P37, DOI DOI 10.5923/J.COMPUTER.20120203.06
   Zhou R, 2013, SENSORS-BASEL, V13, P3142, DOI 10.3390/s130303142
NR 44
TC 0
Z9 0
U1 0
U2 3
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-5090-6356-7
PY 2017
PG 7
WC Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering
GA BI2CH
UT WOS:000408273100010
DA 2022-02-03
ER

PT J
AU Sharma, D
   Selwal, A
AF Sharma, Deepika
   Selwal, Arvind
TI HyFiPAD: a hybrid approach for fingerprint presentation attack detection
   using local and adaptive image features
SO VISUAL COMPUTER
LA English
DT Article; Early Access
DE Fingerprint biometrics; Spoof attacks; Presentation attack detection;
   Image features; Sequential model
ID LIVENESS DETECTION; TEXTURE CLASSIFICATION; PERSPIRATION; DESCRIPTOR;
   NETWORK; SCALE
AB With the pervasiveness of secured biometric authentication applications, the fingerprint-based identification system has fascinated much attention recently. However, the major detriment is their recognition sensors are vulnerable to presentation or spoofing attacks from fake fingerprint artifacts. To resolve these issues, a viable anti-deception countermeasure known as presentation attack detection (PAD) mechanism is developed. As handcrafted feature-based classification techniques exhibit encouraging results in computer vision, they are widely employed in fingerprint spoof detection. Notably, the single-feature-based techniques do not perform uniformly over different spoofing and sensing technologies. In this research work, we expound a new hybrid fingerprint presentation attack detection approach (HyFiPAD) that discriminates live and fake fingerprints using majority voting ensemble build on three local and adaptive textural image features. We propose a new descriptor (i.e., a variant of LBP) which is termed as Local Adaptive Binary Pattern (LABP). Thus, the notion of proposed LABP is used to extract more detailed micro-textural features from the fingerprint images. Our LABP features are combined with an existing Complete Local Binary Pattern (CLBP) descriptor to learn two respective SVM classifiers and additionally a sequential model is trained with the manually extracted Binary Statistical Image Features (BSIF). The experiments are performed on benchmark anti-spoofing datasets namely; LivDet 2009, LivDet 2011, LivDet 2013, and LivDet 2015, where an average classification error rate (ACER) of 4.11, 3.19, 2.88, and 2.97% is, respectively, achieved. The overall experimental analysis of the HyFiPAD demonstrates superiority against majority of the state-of-the-art methods. In addition, the proposed technique yields a promising performance on cross-database and cross-sensor liveness detection tests, claiming good generalization capability.
C1 [Sharma, Deepika; Selwal, Arvind] Cent Univ Jammu, Dept Comp Sci & Informat Technol, Samba 181143, Jammu & Kashmir, India.
C3 Central University of Jammu
RP Sharma, D (corresponding author), Cent Univ Jammu, Dept Comp Sci & Informat Technol, Samba 181143, Jammu & Kashmir, India.
EM sharmadeepika749@gmail.com
CR Abhyankar A., 2010, INT J COMPUT ELECT E, V2, P1793
   Abhyankar A, 2009, PATTERN RECOGN, V42, P452, DOI 10.1016/j.patcog.2008.06.012
   Agarwal S, 2020, EXPERT SYST APPL, V146, DOI 10.1016/j.eswa.2019.113160
   Alshdadi AA, 2020, BIOMED SIGNAL PROCES, V61, DOI 10.1016/j.bspc.2020.102039
   Antonelli A, 2006, LECT NOTES COMPUT SC, V3832, P221
   Baldisserra D, 2006, LECT NOTES COMPUT SC, V3832, P265
   Bhanarkar A, 2013, 2013 IEEE SECOND INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P166, DOI 10.1109/ICIIP.2013.6707575
   Chugh T., 2018, IEEE T INF FORENS SE, V6013, P1
   Coli P, 2008, INT J IMAGE GRAPH, V8, P495, DOI 10.1142/S0219467808003209
   de Souza GB, 2019, J ARTIF INTELL SOFT, V9, P41, DOI 10.2478/jaiscr-2018-0023
   Drahansky M, 2006, 2006 IEEE INFORMATION ASSURANCE WORKSHOP, P42, DOI 10.1109/IAW.2006.1652075
   Dubey RK, 2016, IEEE T INF FOREN SEC, V11, P1461, DOI 10.1109/TIFS.2016.2535899
   Espinoza M, 2011, FORENSIC SCI INT, V204, P41, DOI 10.1016/j.forsciint.2010.05.002
   Galbally J., 2019, HDB BIOMETRIC ANTISP, DOI [10.1007/978-3-319-92627-8_1, DOI 10.1007/978-3-319-92627-8_1]
   Galbally J., 2009, 2009 1 IEEE INT C BI, P1, DOI [10.1109/BIDS.2009.5507534, DOI 10.1109/BIDS.2009.5507534]
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Galbally J, 2012, FUTURE GENER COMP SY, V28, P311, DOI 10.1016/j.future.2010.11.024
   Ghiani L., 2013, IEEE 6 INT C BIOM TH, P1
   Ghiani L, 2017, IET BIOMETRICS, V6, P224, DOI 10.1049/iet-bmt.2016.0007
   Ghiani L, 2013, INT CONF BIOMETR, DOI 10.1109/ICB.2013.6613027
   Ghiani L, 2012, INT C PATT RECOG, P537
   Gonzalez-Soler LJ, 2021, IEEE ACCESS, V9, P5806, DOI 10.1109/ACCESS.2020.3048756
   Gragnaniello D, 2014, ELECTRON LETT, V50, P439, DOI 10.1049/el.2013.4044
   Gragnaniello D., 2013, 2013 IEEE WORKSH BIO
   Gragnaniello D, 2015, PATTERN RECOGN, V48, P1050, DOI 10.1016/j.patcog.2014.05.021
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Jain AK, 2004, IEEE T CIRC SYST VID, V14, P4, DOI 10.1109/TCSVT.2003.818349
   Jia J, 2007, LECT NOTES COMPUT SC, V4642, P309
   Jia XF, 2014, INFORM SCIENCES, V268, P91, DOI 10.1016/j.ins.2013.06.041
   Jian W, 2021, IEEE ACCESS, V9, P2229, DOI 10.1109/ACCESS.2020.3047723
   Jiang YJ, 2018, J ELECTR COMPUT ENG, V2018, DOI 10.1155/2018/1539298
   Jung HY, 2018, ELECTRON LETT, V54, P564, DOI 10.1049/el.2018.0621
   Kannala J, 2012, INT C PATT RECOG, P1363
   Kim W, 2016, IEEE SIGNAL PROC LET, VLett1, P1
   Krizhevsky A., 2012, NIPS, P1
   Lazimul Limnd T. P., 2017, 2017 International Conference on Energy, Communication, Data Analytics and Soft Computing (ICECDS), P731, DOI 10.1109/ICECDS.2017.8389533
   Lee JE, 2008, 2008 BIOMETRICS SYMPOSIUM (BSYM), P1, DOI [10.1109/BSYM.2008.4655515, 10.1109/PLASMA.2008.4591032]
   Li QQ, 2014, INT C WAVEL ANAL PAT, P13, DOI 10.1109/ICWAPR.2014.6961283
   Lu MY, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATIONS (CSA), P77, DOI 10.1109/CSA.2015.79
   Marasco E., 2010 IEEE WORKSH BIO, P8, DOI [10.1109/BIOMS.2010.5610440, DOI 10.1109/BIOMS.2010.5610440]
   Marcialis GL, 2009, LECT NOTES COMPUT SC, V5716, P12, DOI 10.1007/978-3-642-04146-4_4
   Martinsen OG, 2007, IEEE T BIO-MED ENG, V54, P891, DOI 10.1109/TBME.2007.893472
   Memon S., 2011, 2011 19th Telecommunications Forum Telfor (TELFOR), P619, DOI 10.1109/TELFOR.2011.6143624
   Minaee S., BIOMETRIC RECOGNITIO
   Minaee S., 2019, ARXIV PREPRINT ARXIV
   Moon YS, 2005, ELECTRON LETT, V41, P1112, DOI 10.1049/el:20052577
   Mura V, 2015, INT CONF BIOMETR THE
   Nikam Shankar Bhausaheb, 2008, 2008 1st International Conference on Emerging Trends in Engineering and Technology (ICETET), P675, DOI 10.1109/ICETET.2008.134
   Nogueira RF, 2016, IEEE T INF FOREN SEC, V11, P1206, DOI 10.1109/TIFS.2016.2520880
   Nogueira RF, 2014, 2014 IEEE WORKSHOP ON BIOMETRIC MEASUREMENTS AND SYSTEMS FOR SECURITY AND MEDICAL APPLICATIONS (BIOMS) PROCEEDINGS, P22, DOI 10.1109/BIOMS.2014.6951531
   Ojala T, 2000, LECT NOTES COMPUT SC, V1842, P404
   Ojansivu V, 2008, LECT NOTES COMPUT SC, V5099, P236, DOI 10.1007/978-3-540-69905-7_27
   Park EY, 2019, IMMUNOPHARM IMMUNOT, V41, P477, DOI 10.1080/08923973.2019.1628044
   Park E, 2016, LECT NOTE INFORM, VP-260
   Parthasaradhi STV, 2005, IEEE T SYST MAN CY C, V35, P335, DOI 10.1109/TSMCC.2005.848192
   Pereira L.F.A., 2016, IEEE INT C IM PROC I, V49, P13
   Ratha NK, 2001, LECT NOTES COMPUT SC, V2091, P223
   Reddy PV, 2008, IEEE T BIOMED CIRC S, V2, P328, DOI 10.1109/TBCAS.2008.2003432
   Sharma RP, 2019, VISUAL COMPUT, V35, P1393, DOI 10.1007/s00371-018-01618-x
   Toosi A, 2017, P IJCCI, P158
   Uliyan DM, 2020, ENG SCI TECHNOL, V23, P264, DOI 10.1016/j.jestch.2019.06.005
   Vapnik V.N., 1963, Avtomatika i Telemekhanika, V24, P774
   Wang CG, 2015, LECT NOTES COMPUT SC, V9428, P241, DOI 10.1007/978-3-319-25417-3_29
   Xia ZH, 2020, IEEE T SYST MAN CY-S, V50, P1526, DOI 10.1109/TSMC.2018.2874281
   Xia ZH, 2017, SIGNAL IMAGE VIDEO P, V11, P381, DOI 10.1007/s11760-016-0936-z
   Yambay D., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P208, DOI 10.1109/ICB.2012.6199810
   Yuan CS, 2020, IEEE T COGN DEV SYST, V12, P461, DOI 10.1109/TCDS.2019.2920364
   Yuan CS, 2019, IEEE ACCESS, V7, P26953, DOI 10.1109/ACCESS.2019.2901235
   Yuan CS, 2018, J INTERNET TECHNOL, V19, P1499, DOI 10.3966/160792642018091905021
   Zhang YL, 2020, IEEE ACCESS, V8, P183391, DOI 10.1109/ACCESS.2020.3027846
   Zhang YL, 2020, IEEE ACCESS, V8, P84141, DOI 10.1109/ACCESS.2020.2990909
   Zhang YL, 2019, IEEE ACCESS, V7, P91476, DOI 10.1109/ACCESS.2019.2927357
   Zhang YL, 2014, LECT NOTES COMPUT SC, V8833, P191, DOI 10.1007/978-3-319-12484-1_21
NR 73
TC 1
Z9 1
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0178-2789
EI 1432-2315
J9 VISUAL COMPUT
JI Visual Comput.
DI 10.1007/s00371-021-02173-8
EA JUN 2021
PG 27
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SS0OW
UT WOS:000661442800001
DA 2022-02-03
ER

PT J
AU van Breemen, AJJM
   Ollearo, R
   Shanmugam, S
   Peeters, B
   Peters, LCJM
   van de Ketterij, RL
   Katsouras, I
   Akkerman, HB
   Frijters, CH
   Di Giacomo, F
   Veenstra, S
   Andriessen, R
   Janssen, RAJ
   Meulenkamp, EA
   Gelinck, GH
AF van Breemen, Albert J. J. M.
   Ollearo, Riccardo
   Shanmugam, Santhosh
   Peeters, Bart
   Peters, Laurens C. J. M.
   van de Ketterij, Richard L.
   Katsouras, Ilias
   Akkerman, Hylke B.
   Frijters, Corne H.
   Di Giacomo, Francesco
   Veenstra, Sjoerd
   Andriessen, Ronn
   Janssen, Rene A. J.
   Meulenkamp, Eric A.
   Gelinck, Gerwin H.
TI A thin and flexible scanner for fingerprints and documents based on
   metal halide perovskites
SO NATURE ELECTRONICS
LA English
DT Article
ID LIGHT-EMITTING-DIODES; SOLAR-CELLS; PHOTODETECTORS; ARRAYS
AB Solution-processed photodetectors could be of use in large-area light-sensing applications because they can be fabricated at low cost on plastic substrates and their absorption spectra can be tuned by chemical design. However, fabricating photodetectors with low dark currents and integrating them into high-resolution backplanes remains challenging. Here we show that solution-processed metal halide perovskite photodiodes on top of an amorphous indium gallium zinc oxide transistor backplane can be used to create a flexible image sensor that is similar to 100 mu m thick and has a resolution of 508 pixels per inch. We have developed a pixel edge cover layer for the system that reduces electrode current leakage and thus dark current density. The low noise current in combination with high external quantum efficiency results in high photodetectivity at wavelengths from 550 nm to 770 nm. We show that our imager can be used for document scanning and biometric fingerprinting and that it can be wrapped around objects with radii as small as 0.6 cm.
C1 [van Breemen, Albert J. J. M.; Shanmugam, Santhosh; Peeters, Bart; Peters, Laurens C. J. M.; van de Ketterij, Richard L.; Katsouras, Ilias; Akkerman, Hylke B.; Frijters, Corne H.; Meulenkamp, Eric A.; Gelinck, Gerwin H.] Dutch Org Appl Sci Res, TNO Holst Ctr, Eindhoven, Netherlands.
   [Ollearo, Riccardo; Janssen, Rene A. J.; Gelinck, Gerwin H.] Eindhoven Univ Technol, Dept Appl Phys, Eindhoven, Netherlands.
   [Di Giacomo, Francesco; Veenstra, Sjoerd; Andriessen, Ronn] TNO Partner Solliance, Eindhoven, Netherlands.
   [Katsouras, Ilias] ASML Netherlands BV, Eindhoven, Netherlands.
   [Frijters, Corne H.] SALDtech, Eindhoven, Netherlands.
   [Di Giacomo, Francesco] Univ Roma Tor Vergata, Dept Elect Engn, Ctr Hybrid & Organ Solar Energy CHOSE, Rome, Italy.
C3 Netherlands Organization Applied Science Research; Eindhoven University
   of Technology; ASML Holding; University of Rome Tor Vergata
RP van Breemen, AJJM; Gelinck, GH (corresponding author), Dutch Org Appl Sci Res, TNO Holst Ctr, Eindhoven, Netherlands.; Gelinck, GH (corresponding author), Eindhoven Univ Technol, Dept Appl Phys, Eindhoven, Netherlands.
EM albert.vanbreemen@tno.nl; gerwin.gelinck@tno.nl
RI Di Giacomo, Francesco/U-3316-2017; Janssen, Rene/W-2959-2017
OI Di Giacomo, Francesco/0000-0002-2489-5385; Janssen,
   Rene/0000-0002-1920-5124
FU Flexlines project within the Interreg V-programme Flanders-Netherlands,
   a cross-border cooperation programme; European Regional Development
   FundEuropean Commission; Province of Noord-Brabant,
   NetherlandsNetherlands Government
FX We thank the process engineers of Holst Centre's R&D TFT Pilot Line for
   the realization of the TFT backplanes, as well as the thin-film
   encapsulation on top of the PPD frontplane. We also acknowledge M.
   Fattori (Eindhoven University of Technology) for his help with the noise
   measurements. This work is partly financed through the Flexlines project
   within the Interreg V-programme Flanders-Netherlands, a cross-border
   cooperation programme with financial support from the European Regional
   Development Fund, and co-financed by the Province of Noord-Brabant,
   Netherlands.
CR Bas, 2016, MTR05B0016R9V15
   Burschka J, 2013, NATURE, V499, P316, DOI 10.1038/nature12340
   Chen B, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-15077-3
   Chen CW, 2015, J MATER CHEM A, V3, P9152, DOI 10.1039/c4ta05237d
   Cui D, 2016, J PHYS CHEM C, V120, P42, DOI 10.1021/acs.jpcc.5b09393
   Deng W, 2019, ADV OPT MATER, V7, DOI 10.1002/adom.201801521
   Dou LT, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms6404
   Fang YJ, 2019, NAT PHOTONICS, V13, P1, DOI 10.1038/s41566-018-0288-z
   Galagan Y, 2018, ADV ENERGY MATER, V8, DOI 10.1002/aenm.201801935
   Gil-Escrig L, 2018, ADV ENERGY MATER, V8, DOI 10.1002/aenm.201703506
   Glowienka D, 2020, ACS APPL ENERG MATER, V3, P8285, DOI 10.1021/acsaem.0c00767
   Gu LL, 2016, ADV MATER, V28, P9713, DOI 10.1002/adma.201601603
   Herz LM, 2017, ACS ENERGY LETT, V2, P1539, DOI 10.1021/acsenergylett.7b00276
   Hu HL, 2020, J MATER CHEM A, V8, P1578, DOI 10.1039/c9ta11245f
   Kronemeijer Auke Jisk, 2018, SID Symposium Digest of Technical Papers, V49, P1577, DOI 10.1002/sdtp.12311
   Lee W, 2017, ADV MATER, V29, DOI 10.1002/adma.201702902
   Li F. M., 2014, SID S DIG TECH PAP, V45, P431, DOI DOI 10.1002/J.2168-0159.2014.TB00116.X
   Liu Y, 2017, ACS APPL MATER INTER, V9, P11662, DOI 10.1021/acsami.7b01379
   Mulato M, 2003, J ELECTROCHEM SOC, V150, pG735, DOI 10.1149/1.1621416
   Nill, 2006, MTR060170R5
   Saliba M, 2016, SCIENCE, V354, P206, DOI 10.1126/science.aah5557
   Saliba M, 2016, ENERG ENVIRON SCI, V9, P1989, DOI 10.1039/c5ee03874j
   Stranks SD, 2015, NAT NANOTECHNOL, V10, P391, DOI [10.1038/nnano.2015.90, 10.1038/NNANO.2015.90]
   Stranks SD, 2013, SCIENCE, V342, P341, DOI 10.1126/science.1243982
   Tordera D, 2019, ADV MATER TECHNOL-US, V4, DOI 10.1002/admt.201900651
   van de Weijer P, 2019, ORG ELECTRON, V66, P43, DOI 10.1016/j.orgel.2018.12.003
   Veldhuis SA, 2016, ADV MATER, V28, P6804, DOI 10.1002/adma.201600669
   Wang H, 2017, CHEM SOC REV, V46, P5204, DOI 10.1039/c6cs00896h
   Wang Y, 2020, ADV MATER TECHNOL-US, V5, DOI 10.1002/admt.201900752
   Wang YK, 2017, ORG ELECTRON, V42, P203, DOI 10.1016/j.orgel.2016.12.042
   Wu WQ, 2019, ADV MATER, V31, DOI 10.1002/adma.201805913
   Xue J, 2018, NANO LETT, V18, P7628, DOI 10.1021/acs.nanolett.8b03209
   Zardetto V, 2017, SUSTAIN ENERG FUELS, V1, P30, DOI 10.1039/c6se00076b
   Zeng JP, 2019, ADV FUNCT MATER, V29, DOI 10.1002/adfm.201904461
   Zhang D, 2011, IEEE T INSTRUM MEAS, V60, P863, DOI 10.1109/TIM.2010.2062610
   Zhang M, 2017, ACS ENERGY LETT, V2, P438, DOI 10.1021/acsenergylett.6b00697
   Zhu HL, 2019, ACS NANO, V13, P11800, DOI 10.1021/acsnano.9b05774
   Zou TY, 2019, INT EL DEVICES MEET
NR 38
TC 2
Z9 2
U1 32
U2 32
PU NATURE PORTFOLIO
PI BERLIN
PA HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY
SN 2520-1131
J9 NAT ELECTRON
JI Nat. Electron.
PD NOV
PY 2021
VL 4
IS 11
BP 818
EP 826
DI 10.1038/s41928-021-00662-1
EA NOV 2021
PG 9
WC Engineering, Electrical & Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering
GA XD3SK
UT WOS:000713544000001
DA 2022-02-03
ER

PT C
AU Ahmed, KI
   Habaebi, MH
   Islam, MR
   Zainal, NAB
AF Ahmed, Kazi Istiaque
   Habaebi, Mohamed Hadi
   Islam, Md Rafiqul
   Zainal, Nur Aishah Bt
GP IEEE
TI Enhanced Vision Based Vein Detection System
SO 2017 IEEE 4TH INTERNATIONAL CONFERENCE ON SMART INSTRUMENTATION,
   MEASUREMENT AND APPLICATION (ICSIMA 2017)
LA English
DT Proceedings Paper
CT IEEE 4th International Conference on Smart Instrumentation, Measurement
   and Application (ICSIMA)
CY NOV 28-30, 2017
CL Putrajaya, MALAYSIA
DE Needle Infusion; Vision-Based Vein; Vein Detection; NIR; Video
AB The process of needle infusion is done daily endless times everywhere and even though it is simple procedure the sheer number of erroneous procedures that can be serious and fatal for the patient is attentive. To prevent or at least reduce the number of errors, this paper proposes an easy guiding method for nurse and doctors to be integrated into the procedure. A vision-based imaging technique, that gives a practitioner a new perspective for the needle infusion procedure, is introduced. The idea behind of this process is to use the IR camera to capture video sequences of the arm and then compute the effect of electromagnetic wave imprint on each pixel. Then trace the vein location by locating the NIR illumination that is absorbed by the blood in the vein and highlighting it in comparison to the surrounding tissue. Interestingly, this can lead to other applications for the developed system like locating abdominal bleeding, stroke inducing clots in veins near the skin surface and body part vein map-based individual identification biometric to name a few.
C1 [Ahmed, Kazi Istiaque; Habaebi, Mohamed Hadi; Islam, Md Rafiqul; Zainal, Nur Aishah Bt] Int Islamic Univ Malaysia, Kulliyyah Engn, Dept Elect & Comp Engn, Selangor, Malaysia.
C3 International Islamic University Malaysia
RP Ahmed, KI (corresponding author), Int Islamic Univ Malaysia, Kulliyyah Engn, Dept Elect & Comp Engn, Selangor, Malaysia.
EM istiahmed@gmail.com; habaebi@iium.edu.my; rafiq@iium.edu.my
RI Habaebi, Mohamed Hadi/P-2128-2017
OI Habaebi, Mohamed Hadi/0000-0002-2263-0850
FU International Islamic University Malaysia [RIGS16-362-0526]
FX This work is partially sponsored by International Islamic University
   Malaysia Grant RIGS16-362-0526.
CR Al Ghozali H. K, 2016 INT EL S IES, P122, DOI [10.1109/ELECSYM.2016.7860987, DOI 10.1109/ELECSYM.2016.7860987]
   Besra B, 2017, 2017 IEEE 3RD INTERNATIONAL CONFERENCE ON SENSING, SIGNAL PROCESSING AND SECURITY (ICSSS), P89, DOI 10.1109/SSPS.2017.8071571
   Christie Medical Holdings, INFR VEIN FIND VEIN
   Crisan S, 2017, MEASUREMENT, V108, P207, DOI 10.1016/j.measurement.2017.05.053
   D. DV, 2016, SATELLITE IMAGERY
   Dai X., UST 2013 2013 IEEE I, P146, DOI [10.1109/IST.2013.6729680, DOI 10.1109/IST.2013.6729680]
   Ficke BW, 2017, J HAND SURG-AM, V42, DOI 10.1016/j.jhsa.2017.03.039
   Fletcher R. R, 2014 P 4 IEEE GLOB H, P541, DOI [10.1109/GHTC.2014.6970336, DOI 10.1109/GHTC.2014.6970336]
   Gnee N. S, ICICS 2009 C P 7 INT, P5, DOI [10.1109/ICICS.2009.5397752, DOI 10.1109/ICICS.2009.5397752]
   Kacmaz S, 2017, INFRARED PHYS TECHN, V86, P120, DOI 10.1016/j.infrared.2017.09.005
   Kanzawa Y, 2010, IAPR C MACH VIS APPL, P503
   Marathe M., P INT C CIRC COMM CO, P277, DOI [10.1109/CIMCA.2014.7057805, DOI 10.1109/CIMCA.2014.7057805]
   Meng G. C., 2015 IEEE 11 INT C S, P112, DOI [10.1109/CSPA.2015.7225628, DOI 10.1109/CSPA.2015.7225628]
   Wadhwani M., 2015, INT J SCI ENG RES, V6, P780
   Zuiderveld K, 1994, CONTRAST LTD ADAPTIV, DOI [10.1016/B978-0-12-336156-1.50061-6, DOI 10.1016/B978-0-12-336156-1.50061-6]
NR 15
TC 0
Z9 1
U1 1
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-5386-3960-3
PY 2017
PG 6
WC Engineering, Electrical & Electronic; Instruments & Instrumentation
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Instruments & Instrumentation
GA BK0YP
UT WOS:000431392400018
DA 2022-02-03
ER

PT C
AU Lee, R
AF Lee, R
BE VanRenesse, RL
TI Micro mirror array nanostructures for anti-counterfeiting applications
SO OPTICAL SECURITY AND COUNTERFEIT DETERRENCE TECHNIQUES V
SE PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
   (SPIE)
LA English
DT Proceedings Paper
CT 5th Conference on Optical Security and Counterfeit Deterrence Techniques
CY JAN 20-22, 2004
CL San Jose, CA
ID CURVILINEAR DIFFRACTION GRATINGS
AB The optical characteristics of pixellated passive micro mirror arrays are derived and applied in the context of their use as reflective optically variable device (OVD) nanostructures for the protection of documents from counterefeiting. The traditional design variables of foil based diffractive OVDs are shown to be able to be mapped to a corresponding set of design parameters for reflective optical micro mirror array (OMMA) devices. The greatly increased depth characteristics of micro mirror array OVDs provides an opportunity for directly printing the OVD microstructure onto the security document in-line with the normal printing process. The micro mirror array OVD architecture therefore eliminates the need for hot stamping foil as the carrier of the OVD information, thereby reducing costs. The origination of micro mirror array devices via a palette based data format and a combination electron beam lithography and photolithography techniques is discussed via an artwork example and experimental tests. Finally the application of the technology to the design of a generic class of devices which have the interesting property of allowing for both application and customer specific OVD image encoding and data encoding at the end user stage of production is described. Because of the end user nature of the image and data encoding process these devices are particularly well suited to ID document applications and for this reason we refer this new OVD concept as biometric OVD technology.
C1 CSIRO, Mfg & Infrastruct Technol, Clayton, Vic 3168, Australia.
C3 Commonwealth Scientific & Industrial Research Organisation (CSIRO)
RP Lee, R (corresponding author), CSIRO, Mfg & Infrastruct Technol, Gate 4,Normanby Rd, Clayton, Vic 3168, Australia.
CR ABRAMS J, 1965, TENSOR CALCULUS DIFF
   BERRY MV, 1975, J PHYS A, V8, P556
   Chester C., 1957, P CAMB PHILOS SOC, V53, P599
   HILDEBRANDT A, 2003, 19 INT SEC PRINT C M
   LEE RA, 1983, OPT ACTA, V30, P449, DOI 10.1080/716099640
   Lee RA, 2002, MICROELECTRON ENG, V61-2, P105, DOI 10.1016/S0167-9317(02)00520-8
   LEE RA, 1983, OPT ACTA, V30, P267, DOI 10.1080/716099625a
   Lee RA, 2000, MICROELECTRON ENG, V53, P513, DOI 10.1016/S0167-9317(00)00367-1
   LEE RA, 1983, OPT ACTA, V30, P91
   LEE RA, 2001, 18 INT SEC PRINT C S
   LEE RA, Patent No. 5825547
   LEE RA, Patent No. 5428479
   Leech PW, 2003, MICROELECTRON ENG, V65, P439, DOI 10.1016/S0167-9317(03)00162-X
   5335113
   2001, Patent No. 4833
   2003903502
NR 16
TC 5
Z9 5
U1 1
U2 4
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
BN 0-8194-5213-0
J9 P SOC PHOTO-OPT INS
PY 2004
VL 5310
BP 350
EP 368
DI 10.1117/12.523919
PG 19
WC Optics; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Optics; Imaging Science & Photographic Technology
GA BAJ79
UT WOS:000222603200037
DA 2022-02-03
ER

PT J
AU Crunchant, AS
   Egerer, M
   Loos, A
   Burghardt, T
   Zuberbuhler, K
   Corogenes, K
   Leinert, V
   Kulik, L
   Kuhl, HS
AF Crunchant, Anne-Sophie
   Egerer, Monika
   Loos, Alexander
   Burghardt, Tilo
   Zuberbuhler, Klaus
   Corogenes, Katherine
   Leinert, Vera
   Kulik, Lars
   Kuehl, Hjalmar S.
TI Automated face detection for occurrence and occupancy estimation in
   chimpanzees
SO AMERICAN JOURNAL OF PRIMATOLOGY
LA English
DT Article
DE animal biometrics; apes; automated image recognition; camera placement;
   site use
ID ESTIMATING SITE OCCUPANCY; DENSITY-ESTIMATION; BUDONGO FOREST; GREAT
   APES; DECLINE; CONSERVATION; MODELS; DUNG
AB Surveying endangered species is necessary to evaluate conservation effectiveness. Camera trapping and biometric computer vision are recent technological advances. They have impacted on the methods applicable to field surveys and these methods have gained significant momentum over the last decade. Yet, most researchers inspect footage manually and few studies have used automated semantic processing of video trap data from the field. The particular aim of this study is to evaluate methods that incorporate automated face detection technology as an aid to estimate site use of two chimpanzee communities based on camera trapping. As a comparative baseline we employ traditional manual inspection of footage. Our analysis focuses specifically on the basic parameter of occurrence where we assess the performance and practical value of chimpanzee face detection software. We found that the semi-automated data processing required only 2-4% of the time compared to the purely manual analysis. This is a non-negligible increase in efficiency that is critical when assessing the feasibility of camera trap occupancy surveys. Our evaluations suggest that our methodology estimates the proportion of sites used relatively reliably. Chimpanzees are mostly detected when they are present and when videos are filmed in high-resolution: the highest recall rate was 77%, for a false alarm rate of 2.8% for videos containing only chimpanzee frontal face views. Certainly, our study is only a first step for transferring face detection software from the lab into field application. Our results are promising and indicate that the current limitation of detecting chimpanzees in camera trap footage due to lack of suitable face views can be easily overcome on the level of field data collection, that is, by the combined placement of multiple high-resolution cameras facing reverse directions. This will enable to routinely conduct chimpanzee occupancy surveys based on camera trapping and semi-automated processing of footage.
C1 [Crunchant, Anne-Sophie; Egerer, Monika; Corogenes, Katherine; Leinert, Vera; Kulik, Lars; Kuehl, Hjalmar S.] Max Planck Inst Evolutionary Anthropol, Deutsch Pl 6, D-04103 Leipzig, Germany.
   [Loos, Alexander] Fraunhofer Inst Digital Media Technol IDMT, Ilmenau, Germany.
   [Burghardt, Tilo] Univ Bristol, Dept Comp Sci, Bristol, Avon, England.
   [Zuberbuhler, Klaus] Univ Neuchatel, Dept Comparat Cognit, Neuchatel, Switzerland.
   [Zuberbuhler, Klaus] Univ St Andrews, Sch Psychol & Neurosci, St Andrews, Scotland.
   [Zuberbuhler, Klaus] Budongo Conservat Field Stn, Masindi, Uganda.
   [Kuehl, Hjalmar S.] German Ctr Integrat Biodivers Res IDiv Halle Leip, Leipzig, Germany.
C3 Max Planck Society; Fraunhofer Gesellschaft; University of Bristol;
   University of Neuchatel; University of St Andrews
RP Crunchant, AS (corresponding author), Max Planck Inst Evolutionary Anthropol, Deutsch Pl 6, D-04103 Leipzig, Germany.
EM as.crunchant@gmail.com
RI Egerer, Monika/AAV-6902-2021; Zuberbuhler, Klaus/A-9053-2011
OI Egerer, Monika/0000-0002-3304-0725; Zuberbuhler,
   Klaus/0000-0001-8378-088X
FU Max Planck Society Innovation Fund; Krekeler Foundation; Robert Bosch
   Foundation; Pact for Research and Innovation
FX Max Planck Society Innovation Fund; Krekeler Foundation; Robert Bosch
   Foundation; Pact for Research and Innovation
CR Andresen L, 2014, J ZOOL, V292, P212, DOI 10.1111/jzo.12098
   Araabi BN, 2000, ANN BIOMED ENG, V28, P1269, DOI 10.1114/1.1317532
   Arandjelovic M, 2010, BIOL CONSERV, V143, P1780, DOI 10.1016/j.biocon.2010.04.030
   Ardovini A, 2008, PATTERN RECOGN, V41, P1867, DOI 10.1016/j.patcog.2007.11.010
   Bengsen AJ, 2011, J WILDLIFE MANAGE, V75, P1222, DOI 10.1002/jwmg.132
   Bermejo M, 2006, SCIENCE, V314, P1564, DOI 10.1126/science.1133105
   Borchers DL, 2008, BIOMETRICS, V64, P377, DOI 10.1111/j.1541-0420.2007.00927.x
   Borchers D. L., 2002, ESTIMATING ANIMAL AB, P314
   Buckland S.T., 2001, pi
   Buckland ST, 2010, INT J PRIMATOL, V31, P833, DOI 10.1007/s10764-010-9431-5
   Campbell G, 2008, CURR BIOL, V18, pR903, DOI 10.1016/j.cub.2008.08.015
   Carlsen F., 2012, W CHIMPANZEE POPULAT, P124
   Dunn A., 2014, REVISED REGIONAL ACT, P49
   EGGELING WJ, 1947, J ECOL, V34, P20, DOI 10.2307/2256760
   Ernst Andreas, 2011, Proceedings of the 2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2011), P279, DOI 10.1109/AVSS.2011.6027337
   Fiske IJ, 2011, J STAT SOFTW, V43, P1
   Foster RJ, 2012, J WILDLIFE MANAGE, V76, P224, DOI 10.1002/jwmg.275
   Funwi-Gabga N., 2014, STATE APES 2013 EXTR, P252
   Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442
   Greengrass Elizabeth J., 2009, Primate Conservation, V24, P77
   Guschanski K, 2009, BIOL CONSERV, V142, P290, DOI 10.1016/j.biocon.2008.10.024
   Head JS, 2013, ECOL EVOL, V3, P2903, DOI 10.1002/ece3.670
   Hughes B., 2015, 26 BRIT MACH VIS C B
   IUCN & ICCN, 2012, BON PAN PAN CONS STR, P65
   Junker J, 2012, DIVERS DISTRIB, V18, P1077, DOI 10.1111/ddi.12005
   Kalman R. E., 1960, T ASME J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Kondgen S, 2008, CURR BIOL, V18, P260, DOI 10.1016/j.cub.2008.01.012
   Kormos R., 2003, REGIONAL ACTION PLAN, P24
   Kouakou CY, 2009, AM J PRIMATOL, V71, P447, DOI 10.1002/ajp.20673
   Kublbeck C, 2006, IMAGE VISION COMPUT, V24, P564, DOI 10.1016/j.imavis.2005.08.005
   Kuehl H. S., 2008, OCCASIONAL PAPERS IU, P32
   Kuehl HS, 2007, ECOL APPL, V17, P2403, DOI 10.1890/06-0934.1
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Kuehl HS, 2009, BIOL CONSERV, V142, P1500, DOI 10.1016/j.biocon.2009.02.032
   Lahiri M., 2011, P 1 ACM INT C MULT R
   Leendertz FH, 2006, BIOL CONSERV, V131, P325, DOI 10.1016/j.biocon.2006.05.002
   Leendertz FH, 2004, NATURE, V430, P451, DOI 10.1038/nature02722
   Loos A., 2016, THESIS, P279
   Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
   Loos A, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P116, DOI 10.1109/ISM.2012.30
   Mackenzie DI, 2005, J APPL ECOL, V42, P1105, DOI 10.1111/j.1365-2664.2005.01098.x
   MacKenzie DI, 2003, ECOLOGY, V84, P2200, DOI 10.1890/02-3090
   MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.2307/3072056
   MacKenzie DI., 2006, OCCUPANCY ESTIMATION, P324
   Macmillan N. A., 2004, DETECTION THEORY USE, P512
   Maldonado O., 2012, GRAUERS GORILLAS AND, P66
   Mathias M, 2014, LECT NOTES COMPUT SC, V8692, P720, DOI 10.1007/978-3-319-10593-2_47
   Morgan B, 2011, REGIONAL ACTION PLAN
   Murai M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0075024
   Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
   Noss AJ, 2012, ANIM CONSERV, V15, P527, DOI 10.1111/j.1469-1795.2012.00545.x
   O'Connell A. F., 2012, ANIMAL CONSERVATION, V15, P527
   O'Connell A. F., 2010, CAMERA TRAPS ANIMAL, P271
   Oates J., 2007, Regional action plan for the conservation of the Cross River Gorilla (Gorilla gorilla diehli)
   Oates JF, 1996, AUST J ECOL, V21, P1, DOI 10.1111/j.1442-9993.1996.tb00580.x
   Plumptre A. J., 2010, E CHIMPANZEE PAN TRO, P52
   Plumptre AJ, 2006, PRIMATES, V47, P65, DOI 10.1007/s10329-005-0146-8
   Plumptre AJ, 1996, FOREST ECOL MANAG, V89, P101, DOI 10.1016/S0378-1127(96)03854-6
   Plumptre AJ, 1996, INT J PRIMATOL, V17, P85, DOI 10.1007/BF02696160
   Robinson P.T., 1981, Zoonooz, V54, P7
   Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901
   Sherley Richard B., 2010, Endangered Species Research, V11, P101, DOI 10.3354/esr00267
   Todd AF, 2008, INT J PRIMATOL, V29, P549, DOI 10.1007/s10764-008-9247-8
   Tuyttens FAM, 2014, ANIM BEHAV, V90, P273, DOI 10.1016/j.anbehav.2014.02.007
   Tweh CG, 2015, ORYX, V49, P710, DOI 10.1017/S0030605313001191
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Walsh PD, 2003, NATURE, V422, P611, DOI 10.1038/nature01566
   Welch G., 2006, TECH REP
   Wich SA, 2008, ORYX, V42, P329, DOI 10.1017/S003060530800197X
   Wich SA, 2014, CURR BIOL, V24, P1659, DOI 10.1016/j.cub.2014.05.077
   Williamson E., 2011, REGIONAL ACTION PLAN, P49
   Williamson E. A., 2014, REVISED REGIONAL ACT, P49
   Woodford MH, 2002, ORYX, V36, P153, DOI 10.1017/S0030605302000224
   Zabih R., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P151
NR 74
TC 11
Z9 12
U1 1
U2 25
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0275-2565
EI 1098-2345
J9 AM J PRIMATOL
JI Am. J. Primatol.
PD MAR
PY 2017
VL 79
IS 3
AR e22627
DI 10.1002/ajp.22627
PG 12
WC Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Zoology
GA EL5HG
UT WOS:000394651600013
PM 28095593
OA Green Submitted, Green Accepted
DA 2022-02-03
ER

PT J
AU Ooi, YK
   Ibrahim, H
   Mahyuddin, MN
AF Ooi, Yoong Khang
   Ibrahim, Haidi
   Mahyuddin, Muhammad Nasiruddin
TI Enhanced Dense Space Attention Network for Super-Resolution Construction
   From Single Input Image
SO IEEE ACCESS
LA English
DT Article
DE Superresolution; Convolutional neural networks; Residual neural
   networks; Interpolation; Licenses; Image reconstruction; Feature
   extraction; Computational and artificial intelligence; image processing;
   image resolution; image quality; machine learning algorithms
ID CONVOLUTIONAL NEURAL-NETWORK; RECONSTRUCTION; INTERPOLATION; BIOMETRICS
AB In some applications, such as surveillance and biometrics, image enlargement is required to inspect small details on the image. One of the image enlargement approaches is by using convolutional neural network (CNN)-based super-resolution construction from a single image. The first CNN-based image super-resolution algorithm is the super-resolution CNN (SRCNN) developed in 2014. Since then, many researchers have proposed several versions of CNN-based algorithms for image super-resolution to improve the accuracy or reduce the model's running time. Currently, some algorithms still suffered from the vanishing-gradient problem and relied on a large number of layers. Thus, the motivation of this work is to reduce the vanishing-gradient problem that can improve the accuracy, and at the same time, reduce the running time of the model. In this paper, an enhanced dense space attention network (EDSAN) model is proposed to overcome the problems. The EDSAN model adopted a dense connection and residual network to utilize all the features to correlate the low-level feature and high-level feature as much as possible. Besides, implementing the convolution block attention module (CBAM) layer and multiscale block (MSB) helped reduce the number of layers required to achieve comparable results. The model is evaluated through peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) metrics. EDSAN achieved the most significant improvement, about 1.42% when compared to the CRN model using the Set5 dataset at a scale factor of 3. Compared to the ERN model, EDSAN performed the best, with a 1.22% improvement made when using the Set5 dataset at a scale factor of 4. In terms of overall performance, EDSAN performed very well in all datasets at a scale factor of 2 and 3. In conclusion, EDSAN successfully solves the problems above, and it can be used in different applications such as biometric identification applications and real-time video applications.
C1 [Ooi, Yoong Khang; Ibrahim, Haidi; Mahyuddin, Muhammad Nasiruddin] Univ Sains Malaysia, Sch Elect & Elect Engn, Engn Campus, Nibong Tebal 14300, Pulau Pinang, Malaysia.
C3 Universiti Sains Malaysia
RP Ibrahim, H (corresponding author), Univ Sains Malaysia, Sch Elect & Elect Engn, Engn Campus, Nibong Tebal 14300, Pulau Pinang, Malaysia.
EM haidi_ibrahim@ieee.org
RI Mahyuddin, Muhammad Nasiruddin/D-1778-2015; Ibrahim, Haidi/B-3131-2011
OI Mahyuddin, Muhammad Nasiruddin/0000-0002-9955-3152; Ibrahim,
   Haidi/0000-0002-6401-1791
FU Universiti Sains MalaysiaUniversiti Sains Malaysia [1001/PELECT/8014052]
FX This work was supported by the Universiti Sains Malaysia, under Research
   University Grant 1001/PELECT/8014052.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Alonso-Fernandez F, 2019, IEEE ACCESS, V7, P6519, DOI 10.1109/ACCESS.2018.2889395
   Anwar S, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3390462
   Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Barron JT, 2019, PROC CVPR IEEE, P4326, DOI 10.1109/CVPR.2019.00446
   Basodi Sunitha, 2020, Big Data Mining and Analytics, V3, P196, DOI 10.26599/BDMA.2020.9020004
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chang K, 2018, IEEE SIGNAL PROC LET, V25, P596, DOI 10.1109/LSP.2018.2815003
   Chu JH, 2018, IEEE SIGNAL PROC LET, V25, P946, DOI 10.1109/LSP.2018.2820057
   Dai S., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383028
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Du XBA, 2021, IEEE COMPUT SOC CONF, P888, DOI 10.1109/CVPRW53098.2021.00099
   Duanmu CJ, 2020, IEEE ACCESS, V8, P140599, DOI 10.1109/ACCESS.2020.3013401
   Gao XD, 2019, IEEE ACCESS, V7, P15767, DOI 10.1109/ACCESS.2018.2889760
   Ha BH, 2019, SUSTAIN CITIES SOC, V47, DOI 10.1016/j.scs.2019.101496
   Hou JR, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10051856
   Hsu CC, 2019, IEEE INT CONF COMP V, P3643, DOI 10.1109/ICCVW.2019.00449
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Jingru Hou, 2019, 2019 IEEE 4th International Conference on Image, Vision and Computing (ICIVC), P341, DOI 10.1109/ICIVC47709.2019.8981305
   Khan MA, 2021, ELECTRON LETT, V57, P436, DOI 10.1049/ell2.12148
   Nguyen K, 2018, PATTERN RECOGN, V78, P23, DOI 10.1016/j.patcog.2018.01.002
   Kim J, 2016, PROC CVPR IEEE, P1646, DOI 10.1109/CVPR.2016.182
   Kim J, 2016, OCEANS 2016 MTS/IEEE MONTEREY, DOI 10.1109/OCEANS.2016.7761209
   Lan RS, 2021, IEEE T CYBERNETICS, V51, P115, DOI 10.1109/TCYB.2019.2952710
   Li K, 2020, IET IMAGE PROCESS, V14, P2273, DOI 10.1049/iet-ipr.2019.1438
   Li XJ, 2022, J MOD POWER SYST CLE, V10, P131, DOI [10.35833/MPCE.2020.000183, 10.1007/s11554-019-00925-3]
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J, 2013, P 3 INT C MULT TECHN, P463, DOI 10.2991/icmt-13.2013.57
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu J, 2020, IEEE ACCESS, V8, P201055, DOI 10.1109/ACCESS.2020.3036155
   Ma Yi, 2008, IEEE C COMP VIS PATT, P1, DOI 10.1109/CVPR.2008.4587647
   Maier A, 2019, Z MED PHYS, V29, P86, DOI 10.1016/j.zemedi.2018.12.003
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Nasrollahi K, 2014, MACH VISION APPL, V25, P1423, DOI 10.1007/s00138-014-0623-4
   Ooi YK, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10070867
   RASTI P, 2016, P INT C ART MOT DEF, V1, P175, DOI DOI 10.1007/978-3-319-41778-3
   Shamsolmoali P, 2019, SIGNAL PROCESS-IMAGE, V79, P13, DOI 10.1016/j.image.2019.08.008
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409106
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Singh, 2013, INT J COMPUT APPL, V65, P1
   Snoeyink, 2020, P COMP VIS PATT REC, P1
   Sun N, 2019, IEEE ACCESS, V7, P186470, DOI 10.1109/ACCESS.2019.2960828
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tong CS, 2007, MULTIDIM SYST SIGN P, V18, P153, DOI 10.1007/s11045-007-0023-2
   Tong K, 2020, IMAGE VISION COMPUT, V97, DOI 10.1016/j.imavis.2020.103910
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang MJ, 2021, J PARALLEL DISTR COM, V152, P57, DOI 10.1016/j.jpdc.2021.02.016
   Wang W, 2020, SENS IMAGING, V21, DOI 10.1007/s11220-020-00285-4
   Wang W, 2019, INT J COMPUT INT SYS, V12, P1592, DOI 10.2991/ijcis.d.191209.001
   Wang ZJ, 2021, IEEE T CYBERNETICS, V51, P1175, DOI [10.1109/TCYB.2020.2977956, 10.1109/TCYB.2020.2977677]
   Xu J, 2018, IEEE IMAGE PROC, P71, DOI 10.1109/ICIP.2018.8451696
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yu L., 2018, P IEEE INT C EL SYST, P1
   Yue LW, 2016, SIGNAL PROCESS, V128, P389, DOI 10.1016/j.sigpro.2016.05.002
   Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8
   Zou Y, 2021, OPT LASER ENG, V146, DOI 10.1016/j.optlaseng.2021.106717
NR 58
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2021
VL 9
BP 126837
EP 126855
DI 10.1109/ACCESS.2021.3111983
PG 19
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Telecommunications
GA UR3QL
UT WOS:000696666400001
OA gold
DA 2022-02-03
ER

PT J
AU Bae, K
   Park, KR
   Kim, J
AF Bae, Kwanghyuk
   Park, Kang Ryoung
   Kim, Jaihie
TI Tracking head positions in three-dimensional space by using a single
   omnidirectional camera
SO OPTICAL ENGINEERING
LA English
DT Article
DE catadioptric omnidirectional camera; three-dimensional head position;
   circular constraint; tracking three-dimensional position
ID SYSTEM
AB We propose a novel method using a single catadioptric omnidirectional camera to track 3-D head positions in nonintrusive biometric systems. Existing methods have not been able to deal with cases in which the ground plane is nonorthogonal to the optical axis of the camera. To overcome this problem, our proposed method presents the following three improvements: (i) By using 1-D tracking of the feet and head positions combined with a circular constraint in images instead of conventional 2-D tracking methods, the accuracy and computational efficiency of the tracking process is significantly improved, (ii) based on the detected 2-D head position in images with the precalibration information of the camera, the 3-D head positions can be obtained, and (iii) the 3-D head positions can be obtained without having the optical axis of the camera orthogonal to the ground plane. This makes the proposed method feasible in practical environments that use omnidirectional camera systems. Experimental results showed that the proposed method was able to track 3-D head positions at real-time speed. (C) 2008 Society of Photo-Optical Instrumentation Engineers.
C1 [Bae, Kwanghyuk; Kim, Jaihie] Yonsei Univ, Sch Elect & Elect Engn, Biomed Engn Res Ctr, Seoul 120749, South Korea.
   [Park, Kang Ryoung] Dongguk Univ, Dept Elect Engn, Biomed Engn Res Ctr, Seoul 100715, South Korea.
C3 Yonsei University; Dongguk University
RP Bae, K (corresponding author), Yonsei Univ, Sch Elect & Elect Engn, Biomed Engn Res Ctr, 134 Shinchon Dong, Seoul 120749, South Korea.
EM jhkim@yonsei.ac.kr
CR BAE K, 2004, P 19 INT TECH C CIRC
   Bae K, 2006, LECT NOTES COMPUT SC, V4105, P167
   Chen XL, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P150, DOI 10.1109/ICCV.2003.1238330
   Cui YT, 1998, 1998 IEEE WORKSHOP ON VISUAL SURVEILLANCE, PROCEEDINGS, P2
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   Dornaika F, 2002, LECT NOTES COMPUT SC, V2353, P606
   Geyer C, 2002, IEEE T PATTERN ANAL, V24, P687, DOI 10.1109/34.1000241
   Gonzalez Rafael C, 2002, DIGITAL IMAGE PROCES
   Greiffenhagen M, 2001, P IEEE, V89, P1498, DOI 10.1109/5.959343
   Guo G., 2005, TR2005044 MITS EL RE
   Hall B, 2001, 2001 IEEE INTELLIGENT TRANSPORTATION SYSTEMS - PROCEEDINGS, P657, DOI 10.1109/ITSC.2001.948738
   Jankovic N. D., 2005, 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, P3093
   Jankovic ND, 2005, IEEE INT CONF ROBOT, P1234
   Kang SB, 2000, PROC CVPR IEEE, P201, DOI 10.1109/CVPR.2000.855820
   NAYAR SK, 1998, P DARPA IM UND WORKS, P93
   Negin M, 2000, COMPUTER, V33, P70, DOI 10.1109/2.820042
   Scotti G, 2005, IEE P-VIS IMAGE SIGN, V152, P250, DOI 10.1049/ip-vis:20041302
   SCOTTI G, 2005, P IEEE INT C IM PROC
   Thirthala S, 2005, PROC CVPR IEEE, P1198
   Trivedi M, 2000, IEEE SYS MAN CYBERN, P804, DOI 10.1109/ICSMC.2000.885948
   VASSEUR P, 2004, P 15 ANN BR MACH VIS
   VONSEELEN UM, 1999, P IEEE WORKSH AUT ID, P169
   Welch G., 1995, INTRO KALMAN FILTER
   Wilhelm T, 2004, ROBOT AUTON SYST, V48, P31, DOI 10.1016/j.robot.2004.05.004
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   ZHOU X, 2003, P ACM INT WORKSH VID
   [No title captured]
NR 27
TC 1
Z9 1
U1 0
U2 0
PU SPIE-SOC PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98225 USA
SN 0091-3286
EI 1560-2303
J9 OPT ENG
JI Opt. Eng.
PD APR
PY 2008
VL 47
IS 4
AR 047205
DI 10.1117/1.2909667
PG 13
WC Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Optics
GA 306IT
UT WOS:000256242200045
DA 2022-02-03
ER

PT J
AU Elngar, AA
   Kayed, M
AF Elngar, Ahmed A.
   Kayed, Mohammed
TI Vehicle Security Systems using Face Recognition based on Internet of
   Things
SO OPEN COMPUTER SCIENCE
LA English
DT Article
DE Internet of things; face recognition; haar-cascade; principle component
   analysis; vehicle security systems
AB Nowadays, the automobile sector is one of the hottest applications, where vehicles can be intelligent by using IoT technology. But unfortunately, these vehicles suffer from many crimes. Hence it has become a big challenge for the IoT to avoid such these crimes from professional thieves. This paper presents a proposal for the development of a vehicle guard and alarm system using biometric authentication based on IoT technology. Whereas, for vehicle security issues; the proposed system VSS - IoT gives only full access for authorized vehicle's driver based on the interface of a Raspberry Pi 3 Model B+ development board, Pi camera, PIR sensor, and smart-phone. Therefore, if the proposed system detects an unauthorized person inside the vehicle, then the system will notify and send his image to vehicle's owner and/or to a police workstation through the Internet, as well as, its location in case the vehicle is stolen or damaged. The proposed system is tested on two datasets that are ORL dataset and our dataset. The experimental results of the VSS - IoT showed that the accuracy is 98.2% on ORL dataset, whereas 99.6% when applied on our dataset. Besides, the VSS - IoT enhances the sensitivity to 97.7% which is important for real-time. As well as the result demonstrated that the proposed system took shorter time 0.152 sec under different illumination conditions, when the value of the threshold is 3 * 10(3) and 3.50 * 10(3). Therefore, the VSS - IoT is very robust and reliable for face recognition when deployed on the low-power processor.
C1 [Elngar, Ahmed A.; Kayed, Mohammed] Beni Suef Univ, Fac Comp & Artificial Intelligence, Bani Suwayf 62511, Egypt.
C3 Egyptian Knowledge Bank (EKB); Beni Suef University
RP Elngar, AA (corresponding author), Beni Suef Univ, Fac Comp & Artificial Intelligence, Bani Suwayf 62511, Egypt.
EM elngar_7@yahoo.co.uk; mskayed@gmail.com
CR Ahmed A. E., 2018, INT J NETWORK SECURI, V20, P489, DOI [10.6633/IJNS.201805.20(3).11, DOI 10.6633/IJNS.201805.20(3).11]
   ArunSasi, 2013, IJRET INT J RES ENG, V2
   Attar T, 2018, 2018 IEEE 4TH INTERNATIONAL SYMPOSIUM ON SMART ELECTRONIC SYSTEMS (ISES 2018), P195, DOI 10.1109/iSES.2018.00050
   Bavya R, 2014, 2014 INTERNATIONAL CONFERENCE ON INFORMATION COMMUNICATION AND EMBEDDED SYSTEMS (ICICES)
   Da'san M, 2015, 2015 INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGY RESEARCH (ICTRC), P40, DOI 10.1109/ICTRC.2015.7156416
   Ebrahimpour R., 2011, INT J HYBRID INFORM, V4
   Fernandez MCD, 2014, IEEE REGION 10 SYMP, P672, DOI 10.1109/TENCONSpring.2014.6863118
   Jian X., 2009, P 2009 IEEE INT C NE
   Liu ZG, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON VEHICULAR ELECTRONICS AND SAFETY (ICVES), P48, DOI 10.1109/ICVES.2013.6619601
   Mahesh R. P., 2018, P 2 INT C INV COMM C
   Narayan T. D., 2016, IOSR J COMPUTER ENG, V18
   Novosel R., 2017, INT EL COMP SCI C ER
   PUSHPALATHA K, 2011, INT J ARTIFICIAL INT, V29
   Ravi S, 2013, 5 INT C ADV REC TECH, P6
   Sarvesh V. A., 2016, INT J ADV RES ELECT, V5
   Shaik M. A., 2013, INT J ENG TRENDS TEC, V4
   Shruthi K, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING (ICIP), P755, DOI 10.1109/INFOP.2015.7489483
   Tabassum J. Kh. M.R., 2016, SPVRYANS INT J ENG S, V3, P1
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Varsha G., 2014, INT J ADV RES COMPUT, V3
   Viola Paul, 2001, C COMP VIS PATT REC
   Vivek K. S., 2016, 1 IEEE INT C POW EL
   Zhixiong L., 2005, IEEE INT C VEH EL SA, DOI [10.1109/ICVES.2005.1563666, DOI 10.1109/ICVES.2005.1563666]
NR 23
TC 3
Z9 3
U1 0
U2 0
PU DE GRUYTER POLAND SP Z O O
PI WARSAW
PA BOGUMILA ZUGA 32A STR, 01-811 WARSAW, MAZOVIA, POLAND
SN 2299-1093
J9 OPEN COMPUT SCI
JI Open Comput. Sci.
PD MAR 20
PY 2020
VL 10
IS 1
BP 17
EP 29
DI 10.1515/comp-2020-0003
PG 13
WC Computer Science, Theory & Methods
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA MV1JZ
UT WOS:000556119900001
OA gold
DA 2022-02-03
ER

PT C
AU Roy, ND
   Biswas, A
AF Roy, Nilanjana Dutta
   Biswas, Arindam
GP IEEE
TI Detection of Bifurcation Angles in a Retinal Fundus Image
SO 2015 EIGHTH INTERNATIONAL CONFERENCE ON ADVANCES IN PATTERN RECOGNITION
   (ICAPR)
LA English
DT Proceedings Paper
CT 8th International Conference on Advances in Pattern Recognition (ICAPR)
CY JAN 04-07, 2015
CL Kolkata, INDIA
DE biometric authentication; e-passport; bifurcation angle
AB Security issues related to fake passports etc., used in various situations like air travel etc., protecting the identity of a person is an essential task. To prevent someone's identity from being misused, they need to be protected by one of the most reliable sources of the user authentication process. This paper proposes an approach for achieving security using a novel technique for personal authentication to protect them from intruders. Here, the biometric feature used for authentication is the retinal vessel tree. The structure of the retinal vessels is unique for each individual, it is very difficult to tamper with and it remains same throughout the life time of a human being. Using these natural characteristics of human retina, this paper shows a way of extraction and calculation of one of the most crucial features of retina, i.e., bifurcation angle to prepare an e-passport which would be able to identify each person in this world uniquely. In this way the chances of theft of the passport will be reduced at the lowest level of the system. The performance of the proposed approach is evaluated using the experimental observation. Among many retinal features, the distinct bifurcation points have been generated and the angles are calculated of the same bifurcation point. The simplicity and efficiency of the proposed method makes it readily to be applied alone or incorporated with other existing security methods.
C1 [Roy, Nilanjana Dutta] Inst Engn & Management, Dept Comp Sci & Engn, Kolkata, India.
   [Biswas, Arindam] Indian Inst Engn Sci & Technol, Dept Informat Technol, Sibpur, Howrah, India.
C3 Institute of Engineering & Management (IEM), Kolkata; Indian Institute
   of Engineering Science Technology Shibpur (IIEST)
RP Roy, ND (corresponding author), Inst Engn & Management, Dept Comp Sci & Engn, Kolkata, India.
EM nilanjanaduttaroy@gmail.com; barindam@gmail.com
RI Dutta Roy, Nilanjana/AAC-7347-2022; Biswas, Arindam/X-9730-2019
OI Biswas, Arindam/0000-0002-2141-0215
CR Arul P., J THEORETICAL APPL I, P107
   BAE K, 2003, P 4 INT C AUD VID BA, V2688, P1059
   Balasubramanian R., 2011, INT J COMPUTER APPL, V19
   Chong S. C., 2006, ICB
   Chung Y, 2007, PROCEEDINGS OF THE FRONTIERS IN THE CONVERGENCE OF BIOSCIENCE AND INFORMATION TECHNOLOGIES, P709, DOI 10.1109/FBIT.2007.151
   Dutta S, 2008, LECT NOTES COMPUT SC, V5259, P38, DOI 10.1007/978-3-540-88458-3_4
   Kasaei S, 1997, TENCON IEEE REGION, P303, DOI 10.1109/TENCON.1997.647317
   Nilanjana Dutta Roy, 2014, SPRINGERS LNCS PUBLI
   Pratt W. K., 1991, DIGITAL IMAGE PROCES
   Rahman Mahfuzur, 2001, IEEE INT C MULT EXP, P52
   Saha S., 2013, INT J LATEST RES SCI, V2, P105
   Sonka M., 1999, IMAGE PROCESSING ANA
   Tripathi K., 2011, INT J COMPUTER APPL, V14
   Wolak Ronald G., 1998, NETWORK SECURITY BIO
   Zabidi SA, 2004, LECT NOTES COMPUT SC, V3214, P312
NR 15
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-4799-7458-0
PY 2015
BP 242
EP +
PG 6
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BF2PW
UT WOS:000380489000061
DA 2022-02-03
ER

PT C
AU Luo, BN
   Shen, J
   Cheng, SY
   Wang, YP
   Pantic, M
AF Luo, Bingnan
   Shen, Jie
   Cheng, Shiyang
   Wang, Yupang
   Pantic, Maja
GP IEEE Comp Soc
TI Shape Constrained Network for Eye Segmentation in the Wild
SO 2020 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV)
SE IEEE Winter Conference on Applications of Computer Vision
LA English
DT Proceedings Paper
CT IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
CY MAR 01-05, 2020
CL Snowmass, CO
AB Semantic segmentation of eyes has long been a vital pre-processing step in many biometric applications. Majority of the works focus only on high resolution eye images, while little has been done to segment the eyes from low quality images in the wild. However, this is a particularly interesting and meaningful topic, as eyes play a crucial role in conveying the emotional state and mental well-being of a person. In this work, we take two steps toward solving this problem: (1) We collect and annotate a challenging eye segmentation dataset containing 8882 eye patches from 4461 facial images of different resolutions, illumination conditions and head poses; (2) We develop a novel eye segmentation method, Shape Constrained Network (SCN), that incorporates shape prior into the segmentation network training procedure. Specifically, we learn the shape prior from our dataset using VAE-GAN, and leverage the pre-trained encoder and discriminator to regularise the training of SegNet. To improve the accuracy and quality of predicted masks, we replace the loss of SegNet with three new losses: Intersection-over-Union (IoU) loss, shape discriminator loss and shape embedding loss. Extensive experiments shows that our method outperforms state-of-the-art segmentation and landmark detection methods in terms of mean IoU (mIoU) accuracy and the quality of segmentation masks. The dataset is available at https://ibug.doc.ic.ac.uk/resources/ibug-eye-segmentation-dataset/
C1 [Luo, Bingnan; Shen, Jie; Wang, Yupang; Pantic, Maja] Imperial Coll London, Dept Comp, London, England.
   [Shen, Jie; Cheng, Shiyang; Pantic, Maja] Samsung AI Ctr, Cambridge, England.
C3 Imperial College London
RP Shen, J (corresponding author), Imperial Coll London, Dept Comp, London, England.; Shen, J (corresponding author), Samsung AI Ctr, Cambridge, England.
EM bingnan.luo16@imperial.ac.uk; jie.shen07@imperial.ac.uk;
   shiyang.c@samsung.com; yujiang.wang14@imperial.ac.uk;
   m.pantic@imperial.ac.uk
OI Luo, Bingnan/0000-0002-9367-4359
CR Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240
   Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bulat A., 2017, ICCV
   Cai H, 2018, BRIT MACH VIS C
   Caruana R, 2001, ADV NEUR IN, V13, P402
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676
   Daugman J, 2009, ESSENTIAL GUIDE TO IMAGE PROCESSING, 2ND EDITION, P715, DOI 10.1016/B978-0-12-374457-9.00025-1
   Deng JK, 2019, INT J COMPUT VISION, V127, P599, DOI 10.1007/s11263-018-1134-y
   Eslami SMA, 2014, INT J COMPUT VISION, V107, P155, DOI 10.1007/s11263-013-0669-1
   Everingham M, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P441
   Hood BM, 1998, PSYCHOL SCI, V9, P131, DOI 10.1111/1467-9280.00024
   IOFFE S, 2015, ARXIV 1502 03167, V1502, DOI DOI 10.1007/S13398-014-0173-7.2
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Kingma D., 2015, ADAM METHOD STOCHAST
   Kingma D. P., 2013, ARXIV13126114
   Larsen A. B. L., 2015, ARXIV151209300
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   LeCun Y, 2007, P IEEE C COMP VIS PA, DOI [10.1109/CVPR.2007.383157, DOI 10.1109/CVPR.2007.383157]
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma B, 2015, IEEE I CONF COMP VIS, P4400, DOI 10.1109/ICCV.2015.500
   Minear M, 2004, BEHAV RES METH INS C, V36, P630, DOI 10.3758/BF03206543
   Oktay O, 2018, IEEE T MED IMAGING, V37, P384, DOI 10.1109/TMI.2017.2743464
   Peer P, 2005, CVL FACE DATABASE
   Polatsek Patrik, 2013, EYE BLINK DETECTION, P18
   Rahman Md Atiqur, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P234, DOI 10.1007/978-3-319-50835-1_22
   Ravishankar H., 2017, Medical Image Computing and Computer Assisted Intervention - MICCAI 2017. 20th International Conference. Proceedings: LNCS 10433, P203, DOI 10.1007/978-3-319-66182-7_24
   Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3
   Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002
   Salakhutdinov Ruslan, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/CVPR.2015.7298594
   Smith Brian A, 2013, P 26 ANN ACM S US IN, P271, DOI DOI 10.1145/2501988.2501994
   Timm F, 2011, VISAPP 2011: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, P125
   Triggs B., 2005, PROC CVPR IEEE, V1, P886, DOI DOI 10.1109/CVPR.2005.177
   Wang YJ, 2019, INT J COMPUT VISION, V127, P625, DOI 10.1007/s11263-018-1130-2
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yang M., 2018, IEEE C COMP VIS PATT
   Yu ZL, 2018, PROCEEDINGS OF 2018 IEEE 4TH INFORMATION TECHNOLOGY AND MECHATRONICS ENGINEERING CONFERENCE (ITOEC 2018), P1288, DOI 10.1109/ITOEC.2018.8740417
NR 41
TC 2
Z9 2
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
SN 2472-6737
BN 978-1-7281-6553-0
J9 IEEE WINT CONF APPL
PY 2020
BP 1941
EP 1949
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BQ1ZB
UT WOS:000578444802002
OA Green Submitted
DA 2022-02-03
ER

PT J
AU Chang, CI
AF Chang, Chein-I
TI Multiparameter Receiver Operating Characteristic Analysis for Signal
   Detection and Classification
SO IEEE SENSORS JOURNAL
LA English
DT Article
DE Detection rate (DR); false alarm (FA) rate; Neyman-Pearson (NP)
   detection; receiver operating characteristic (ROC); signal detection;
   signal classification; 3-D ROC
ID MIXED PIXEL CLASSIFICATION; TARGET DETECTION; IMAGE CLASSIFICATION;
   RECOGNITION; ROC; PROBABILITY; ALGORITHMS; SYSTEM
AB Receiver operating characteristic (ROC) analysis is a widely used evaluation tool in signal processing and communications, and medical diagnosis for performance analysis. It utilizes 2-D curves plotted by detection rate (P-D) against false alarm rate (P-F) to assess effectiveness of a detector, sensor/device for detection. However, P-D and P-F are actually dependent parameters resulting from a more crucial but implicit parameter hidden in the ROC curves, threshold tau, which is determined by the cost of implementing a detector or sensor/device, except only the case that when the Bayes theory is used for detection, tau is completely determined by the Bayes cost. This paper extends the traditional ROC analysis for single-signal detection to detection and classification of multiple signals. It also explores relationships among the three parameters, P-D, P-F, and tau, and further develops a new concept of multiparameter ROC analysis, which uses 3-D ROC curves plotted by three parameters, P-D, P-F, and tau, to evaluate effectiveness of detection performance based on interrelationship among P-D, P-F, and tau, rather then only P-D and P-F used by 2-D ROC analysis. As a result of a 3-D ROC curve, three 2-D ROC curves can be also derived: the conventional 2-D ROC curve plotted by P-D versus P-F and two new 2-D ROC curves plotted based on P-D versus and P-F versus tau. In order to demonstrate the utility of 3-D ROC analysis, four applications are considered: hyperspectral target detection, medical diagnosis, chemical/biological agent detection, and biometric recognition.
C1 [Chang, Chein-I] Univ Maryland, Dept Comp Sci & Elect Engn, Remote Sensing Signal & Image Proc Lab, Baltimore, MD 21250 USA.
   [Chang, Chein-I] Natl Chung Hsing Univ, Dept Elect Engn, Taichung 402, Taiwan.
C3 University System of Maryland; University of Maryland Baltimore;
   National Chung Hsing University
RP Chang, CI (corresponding author), Univ Maryland, Dept Comp Sci & Elect Engn, Remote Sensing Signal & Image Proc Lab, Baltimore, MD 21250 USA.
EM cchang@umbc.edu
FU National Science Council, TaiwanMinistry of Science and Technology,
   Taiwan [NSC 98-2811-E-005-024, NSC 98-2221-E-005-096]
FX Manuscript received September 28, 2008; revised December 08, 2008;
   accepted December 29, 2008. Current version published February 24, 2010.
   The work of C.-I Chang was supported by the National Science Council,
   Taiwan, under NSC 98-2811-E-005-024 and NSC 98-2221-E-005-096. The
   associate editor coordinating the review of this paper and approving it
   for publication was Dr. Patti Gillespi.
CR Alsing SG, 1999, P SOC PHOTO-OPT INS, V3718, P449, DOI 10.1117/12.359981
   Baumann JM, 2005, P SOC PHOTO-OPT INS, V5807, P380, DOI 10.1117/12.606011
   Blasch E, 2001, P SOC PHOTO-OPT INS, V4380, P397, DOI 10.1117/12.436966
   Blasch E, 1999, P SOC PHOTO-OPT INS, V3721, P740, DOI 10.1117/12.357689
   Blasch E, 2000, PROC SPIE, V4055, P442, DOI 10.1117/12.380598
   BLASCH E, 2003, P SPIE 2003, V5095, P288
   Blasch E., 2008, HDB MULTISENSOR DATA
   Chang C.-I, 2006, HAND HELD DEVICE DET, DOI [10.1117/2.1200612.0507, DOI 10.1117/2.1200612.0507]
   Chang C.-I., 2003, HYPERSPECTRAL IMAGIN
   Chang CI, 2002, IEEE T GEOSCI REMOTE, V40, P1065, DOI 10.1109/TGRS.2002.1010894
   Chang CI, 2001, INT GEOSCI REMOTE SE, P2355, DOI 10.1109/IGARSS.2001.978000
   Chang CI, 1998, IEEE T GEOSCI REMOTE, V36, P898, DOI 10.1109/36.673681
   Chang CI, 2000, IEEE T GEOSCI REMOTE, V38, P1144, DOI 10.1109/36.843007
   Chang CI, 2000, IEEE T GEOSCI REMOTE, V38, P1044, DOI 10.1109/36.841984
   CHEN CC, 2005, MED IMAGING SYSTEMS, V4, P297
   Du Q, 2004, IEEE T GEOSCI REMOTE, V42, P892, DOI 10.1109/TGRS.2003.821887
   Du Y., 2011, RETHINKING EFFECTIVE, DOI [10.1117/2.1200711.0815, DOI 10.1117/2.1200711.0815]
   Du YZ, 2008, OPT LASER ENG, V46, P477, DOI 10.1016/j.optlaseng.2008.01.005
   Du YZ, 2003, J ELECTRON IMAGING, V12, P410, DOI 10.1117/1.1584050
   Harsanyi J.C., 1993, DETECTION CLASSIFICA
   HARSANYI JC, 1994, IEEE T GEOSCI REMOTE, V32, P779, DOI 10.1109/36.298007
   Heinz DC, 2001, IEEE T GEOSCI REMOTE, V39, P529, DOI 10.1109/36.911111
   LIU W, 2005, OPTICSEAST CHEM BIOL
   METZ CE, 1986, INVEST RADIOL, V21, P720, DOI 10.1097/00004424-198609000-00009
   Parker DR, 2005, ELECTRON LETT, V41, P279, DOI 10.1049/el:20047523
   Parker DR, 2005, OPT ENG, V44, DOI 10.1117/1.2042307
   PLANO S, 2003, P SPIE 2003, V5095, P315
   Poor H., 1985, INTRO SIGNAL DETECTI
   SUEN CY, 1980, P IEEE, V68, P469, DOI 10.1109/PROC.1980.11675
   Swets JA, 1982, EVALUATION DIAGNOSTI
   Thai B, 2002, IEEE T GEOSCI REMOTE, V40, P599, DOI 10.1109/TGRS.2002.1000320
   Wang CM, 2003, IEEE T MED IMAGING, V22, P50, DOI 10.1109/TMI.2002.806858
   Wang S, 2005, P ANN INT IEEE EMBS, P7545
NR 33
TC 86
Z9 89
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1530-437X
EI 1558-1748
J9 IEEE SENS J
JI IEEE Sens. J.
PD MAR
PY 2010
VL 10
IS 3
BP 423
EP 442
DI 10.1109/JSEN.2009.2038120
PG 20
WC Engineering, Electrical & Electronic; Instruments & Instrumentation;
   Physics, Applied
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering; Instruments & Instrumentation; Physics
GA 561RN
UT WOS:000274996500008
DA 2022-02-03
ER

PT J
AU Khan, S
   Akram, A
   Usman, N
AF Khan, Sikandar
   Akram, Adeel
   Usman, Nighat
TI Real Time Automatic Attendance System for Face Recognition Using Face
   API and OpenCV
SO WIRELESS PERSONAL COMMUNICATIONS
LA English
DT Article
DE Convolution neural network (CNN); RCNN; Darknet; Face database; YOLO V3
   (You only look once); SQlite 3
AB Traditionally, the attendance of students has been a major concern for the colleges and the faculty has to spend a lot of time and is a tedious job to mark attendance manually. Current biometric attendance system is not automatic that's why wastes a lot of time, difficult to maintain and requires a queue for scanning fingerprints to mark their attendance. In Modern era everyone has Smartphone and connected via internet every time. In this paper attendance monitoring will be done through smart phone available with almost all faculty members. Some of popular object detection algorithms are back propagation neural network, region based convolution network (RCNN), faster RCNN, single shot detector. Our unified structure is based on YOLO V3 (You only look once) algorithm for face detection and Microsoft Azure using face API for face recognition (face database). The unique part is camera installed in classroom will take picture twice one at the start and one at the end to ensure students has attended complete class. YOLO V3 will first count the students in an image followed by identifying faces as known and unknown generating spreadsheets separately and an email is send at the end of month to students, parents and faculty. The designed system performs efficient in real time implementation for counting and detection. Our entire system has proven to gather high accuracy in face detection and performance.
C1 [Khan, Sikandar; Akram, Adeel] UET, Dept Telecom Engn, Taxila, Pakistan.
   [Usman, Nighat] BULC, Dept Comp Sci, Lahore, Pakistan.
RP Khan, S (corresponding author), UET, Dept Telecom Engn, Taxila, Pakistan.
EM sikandarkhan@cuiatd.edu.pk; adeel.akram@uettaxila.edu.pk;
   nighatusman03@gmail.com
CR Albuhairi T., 2019, 1 INT C SUST TECHN C, P125
   Arsenovic M., 2017, IEEE 15 INT S INT SY
   Das S., 2017, CNN ARCHITECTURES LE
   Fawaz A, 2019, IEEE ITN STEM ED C I
   Geethapriya S., 2019, INT J ENG ADV TECHNO, V8, P1
   Helmi R., 2019, IEEE INT C AUT CONTR
   Kim C, 2018, J BIG DATA-GER, V5, DOI 10.1186/s40537-018-0131-x
   Li J, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9183750
   Maj M, 2018, APPSILON DATA SCI
   McCluskey CP, 2004, CRIME DELINQUENCY, V50, P214, DOI 10.1177/0011128703258942
   Miji D., 2019, 18 INT S INFOTEH JAH
   Rosebrock A., 2018, YOLO OBJECT DETECTIO
   Shoewu O., 2012, PACIFIC J SCI TECHNO, V13, P300
   Varadharajan E, 2016, PROCEEDINGS OF 2016 ONLINE INTERNATIONAL CONFERENCE ON GREEN ENGINEERING AND TECHNOLOGIES (IC-GET)
   Xing Y, 2019, IEEE 4 INT C CLOUD C
   Yadav V., 2019, 2019 INT C MACH LEAR
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
NR 17
TC 4
Z9 4
U1 10
U2 67
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 0929-6212
EI 1572-834X
J9 WIRELESS PERS COMMUN
JI Wirel. Pers. Commun.
PD JUL
PY 2020
VL 113
IS 1
BP 469
EP 480
DI 10.1007/s11277-020-07224-2
EA MAR 2020
PG 12
WC Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Telecommunications
GA ME0CG
UT WOS:000522693600002
DA 2022-02-03
ER

PT J
AU Al Alkeem, E
   Yeun, CY
   Yun, J
   Yoo, PD
   Chae, M
   Rahman, A
   Asyhari, AT
AF Al Alkeem, Ebrahim
   Yeun, Chan Yeob
   Yun, Jaewoong
   Yoo, Paul D.
   Chae, Myungsu
   Rahman, Arafatur
   Asyhari, A. Taufiq
TI Robust Deep Identification using ECG and Multimodal Biometrics for
   Industrial Internet of Things
SO AD HOC NETWORKS
LA English
DT Article
DE Personal identification; multimodal biometrics; deep learning; gender
   classification; electrocardiogram; fingerprint; face recognition;
   feature-level fusion
ID RECOGNITION; FUSION
AB The use of electrocardiogram (ECG) data for personal identification in Industrial Internet of Things can achieve near-perfect accuracy in an ideal condition. However, real-life ECG data are often exposed to various types of noises and interferences. A reliable and enhanced identification method could be achieved by employing additional features from other biometric sources. This work, thus, proposes a novel robust and reliable identification technique grounded on multimodal biometrics, which utilizes deep learning to combine fingerprint, ECG and facial image data, particularly useful for identification and gender classification purposes. The multimodal approach allows the model to deal with a range of input domains removing the requirement of independent training on each modality, and inter-domain correlation can improve the model generalization capability on these tasks. In multitask learning, losses from one task help to regularize others, thus, leading to better overall performances. The proposed approach merges the embedding of multimodality by using feature-level and score level fusions. To the best of our understanding, the key concepts presented herein is a pioneering work combining multimodality, multitasking and different fusion methods. The proposed model achieves a better generalization on the benchmark dataset used while the feature-level fusion outperforms other fusion methods. The proposed model is validated on noisy and incomplete data with missing modalities and the analyses on the experimental results are provided.
C1 [Al Alkeem, Ebrahim; Yeun, Chan Yeob] Khalifa Univ, Ctr Cyber Phys Syst, EECS Dept, Abu Dhabi, U Arab Emirates.
   [Al Alkeem, Ebrahim] Nawah Energy Co, Secur & Safety, Abu Dhabi, U Arab Emirates.
   [Yun, Jaewoong; Chae, Myungsu] NOTA Inc, Res Inst, Daejeon, South Korea.
   [Yoo, Paul D.] Univ London, Birkbeck Coll, CSIS, Mulet St, London WC1E 7HX, England.
   [Rahman, Arafatur] Univ Malaysia Pahang, Fac Comp, Pahang, Malaysia.
   [Asyhari, A. Taufiq] Birmingham City Univ, CDT, Birmingham B4 7XG, W Midlands, England.
C3 Khalifa University of Science & Technology; University of London;
   Birkbeck University London; Universiti Malaysia Pahang; Birmingham City
   University
RP Yeun, CY (corresponding author), Khalifa Univ, Ctr Cyber Phys Syst, EECS Dept, Abu Dhabi, U Arab Emirates.
EM chan.yeun@ku.ac.ae
FU Center for Cyber-Physical Systems, Khalifa University
   [8474000137-RC1C2PS-T3]
FX This work is supported in part by the Center for Cyber-Physical Systems,
   Khalifa University, under Grant Number 8474000137-RC1C2PS-T3. The
   authors declare no conflict of interest.
CR Abhishek A. M, 2015, INT J COMPUTER APPL, V111, P33
   Al Alkeem E, 2019, IEEE ACCESS, V7, P123069, DOI 10.1109/ACCESS.2019.2937357
   AlAlkeem E., 2020, BMC BIOINFORMATICS, V21, P1
   Aslam J. A., 2001, SIGIR Forum, P276
   Aslam J. A., 2000, SIGIR Forum, V34, P379
   Biel L, 2001, IEEE T INSTRUM MEAS, V50, P808, DOI 10.1109/19.930458
   Cappelli R., 2007, BIOMETRIC TECHNOLOGY, V15, P7, DOI DOI 10.1016/S0969-4765(07)70140-6
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chergui O., 2016, 2016 INT C INF TECHN, P1
   Chetty, 2011, ADV BIOMETRIC TECHNO
   Coutinho David Pereira, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3858, DOI 10.1109/ICPR.2010.940
   Daugman J., 2000, BIOMETRIC DECISION L
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Duda R. O., 2012, PATTERN CLASSIFICATI
   Dummett M., 1997, PRINCIPLES ELECTORAL
   EL-SAYED A., 2015, IJACSA INT J ADV COM, V6
   Ergin S., 2014, 2014 9 IB C INF SYST, P1
   Fan JF, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101545
   Fang SC, 2009, PATTERN RECOGN, V42, P1824, DOI 10.1016/j.patcog.2008.11.020
   Gahi Y., 2008, 2008 NEW TECHNOLOGIE, P1
   Gavrilova M.L., 2013, INFORM SCI REFERENCE
   Goldberger AL, 2000, CIRCULATION, V101, pE215, DOI 10.1161/01.CIR.101.23.e215
   Hinton G., 2012, NEURAL NETWORKS MACH, V264
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   HO TK, 1994, IEEE T PATTERN ANAL, V16, P66, DOI 10.1109/34.273716
   Hond D, 1997, BMVC
   Huo G, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.6.063020
   Isin A, 2017, PROCEDIA COMPUT SCI, V120, P268, DOI 10.1016/j.procs.2017.11.238
   Israel SA, 2005, PATTERN RECOGN, V38, P133, DOI 10.1016/j.patcog.2004.05.014
   Israel SA, 2004, 32ND APPLIED IMAGERY PATTERN RECOGNITION WORKSHOP, PROCEEDINGS, P226
   Jain AK, 1997, P IEEE, V85, P1365, DOI 10.1109/5.628674
   Jinlong Ji, 2018, 2018 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI), P124, DOI 10.1109/BHI.2018.8333385
   김정균, 2017, [Journal of the Institute of Electronics and Information Engineers, 전자공학회논문지], V54, P100
   Kim KS, 2005, P ANN INT IEEE EMBS, P1114
   Kim SK, 2019, IEEE ACCESS, V7, P94858, DOI 10.1109/ACCESS.2019.2927079
   King DB, 2015, ACS SYM SER, V1214, P1
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Labati RD, 2019, PATTERN RECOGN LETT, V126, P78, DOI 10.1016/j.patrec.2018.03.028
   Lam L, 1997, IEEE T SYST MAN CY A, V27, P553, DOI 10.1109/3468.618255
   Lee J, 2012, J APPL MATH
   Lin YQ, 2011, PROC CVPR IEEE, P1689, DOI 10.1109/CVPR.2011.5995477
   Lugovaya T.S., 2005, THESIS
   Melnik O, 2004, IEEE T PATTERN ANAL, V26, P973, DOI 10.1109/TPAMI.2004.48
   Ming Li, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1326, DOI 10.1109/ICPR.2010.330
   Odinaka I, 2012, IEEE T INF FOREN SEC, V7, P1812, DOI 10.1109/TIFS.2012.2215324
   Plataniotis K. N., 2006, P BIOM S SPEC SESS R, P1
   Rattani A, 2007, 2007 FIRST IEEE INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS, P85
   Rautiainen M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P933
   Ross A, 2005, PROC SPIE, V5779, P196, DOI 10.1117/12.606093
   Ross A, 2003, PATTERN RECOGN LETT, V24, P2115, DOI 10.1016/S0167-8655(03)00079-5
   Ross A. A, 2006, HDB MULTIBIOMETRICS, V6
   Ruder S, 2017, ARXIV170605098
   Salloum R, 2017, INT CONF ACOUST SPEE, P2062, DOI 10.1109/ICASSP.2017.7952519
   Schnabel A., 1995, BIOMED TECH, V1, pS317, DOI DOI 10.1515/BMTE.1995.40.S1.317
   Snelick R, 2005, IEEE T PATTERN ANAL, V27, P450, DOI 10.1109/TPAMI.2005.57
   Sun L, 2012, IEEE T BIO-MED ENG, V59, P3348, DOI 10.1109/TBME.2012.2213597
   Szegedy C, 2017, THIRTY-FIRST AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4278
   Velayudhan A., 2016, IOSR J ELECT COMMUNI, V3, P641
   Vishi K., 2018, ARXIV PREPRINT ARXIV
   WANG Y, 2008, EURASIP J ADV SIG PR, V2008, P1
   Yaacoubi C., 2020, P 2 INT C DIG TOOLS, P1
   Zhou T, 2020, MED IMAGE ANAL, V60, DOI 10.1016/j.media.2019.101630
   Zhou T, 2019, IEEE T MED IMAGING, V38, P2411, DOI 10.1109/TMI.2019.2913158
   Zokaee S., 2012, INT J ELECT COMPUTER, V2, P261
   강경우, 2012, [Computer and Information, 전자공학회논문지 - CI], V49, P1
NR 66
TC 0
Z9 0
U1 7
U2 7
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1570-8705
EI 1570-8713
J9 AD HOC NETW
JI Ad Hoc Netw.
PD OCT 1
PY 2021
VL 121
AR 102581
DI 10.1016/j.adhoc.2021.102581
PG 13
WC Computer Science, Information Systems; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UO7JX
UT WOS:000694870500003
OA Green Accepted
DA 2022-02-03
ER

PT J
AU Pala, P
   Seidenari, L
   Berretti, S
   Del Bimbo, A
AF Pala, Pietro
   Seidenari, Lorenzo
   Berretti, Stefano
   Del Bimbo, Alberto
TI Enhanced skeleton and face 3D data for person re-identification from
   depth cameras
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Person re-identification; Depth camera; Body skeleton; 3D face; Boosting
AB Person re-identification is typically performed using 2D still images or videos, where photometric appearance is the main visual cue used to discover the presence of a target subject when switching from different camera views across time. This invalidates any application where a person may change dress across subsequent acquisitions as can be the case of patients monitoring at home. Differently from RGB data, 3D information as acquired by depth cameras can open the way to person re-identification based on biometric cues such as distinguishing traits of the body or face. However, the accuracy of skeleton and face geometry extracted from depth data is not always adequate to enable person recognition, since both these features are affected by the pose of the subject and the distance from the camera. In this paper, we propose a method to derive a robust skeleton representation from a depth sequence and to complement it with a highly discriminative face feature. This is obtained by selecting skeleton and face samples based on their quality and using the temporal redundancy across the sequence to derive and refine cumulated models for both of them. Extracting skeleton and face features from such cumulated models and combining them for the recognition allow us to improve rank-1 re-identification accuracy compared to individual cues. A comparative evaluation on three benchmark datasets also shows results at the state-of-the-art. (C) 2019 Elsevier Ltd. All rights reserved.
C1 [Pala, Pietro; Seidenari, Lorenzo; Berretti, Stefano; Del Bimbo, Alberto] Media Integrat & Commun Ctr, Viale Morgagni 65, I-50134 Florence, Italy.
RP Pala, P (corresponding author), Media Integrat & Commun Ctr, Viale Morgagni 65, I-50134 Florence, Italy.
EM pietro.pala@unifi.it
RI Berretti, Stefano/U-9004-2019; Seidenari, Lorenzo/AAA-1848-2020
OI Berretti, Stefano/0000-0003-1219-4386; Seidenari,
   Lorenzo/0000-0003-4816-0268
CR Appel R., 2013, JMLR WORKSHOP C P, DOI DOI 10.5555/3042817.3043003
   Baltieri D, 2015, INT J COMPUT VISION, V111, P345, DOI 10.1007/s11263-014-0747-z
   Barbosa IB, 2012, LECT NOTES COMPUT SC, V7583, P433, DOI 10.1007/978-3-642-33863-2_43
   Bedagkar-Gala A, 2014, IMAGE VISION COMPUT, V32, P270, DOI 10.1016/j.imavis.2014.02.001
   Berretti S, 2013, IEEE T INF FOREN SEC, V8, P374, DOI 10.1109/TIFS.2012.2235833
   Berretti Stefano, 2018, ACM T MULTIM COMPUT, V14, P1, DOI DOI 10.1145/3182179
   Bondi E, 2016, IEEE T INF FOREN SEC, V11, P2843, DOI 10.1109/TIFS.2016.2601059
   Boureau Y. L., 2010, P 27 INT C MACH LEAR, P111
   Crete F, 2007, PROC SPIE, V6492, DOI 10.1117/12.702790
   D'Orazio T, 2012, IEEE IMAGE PROC, P1601, DOI 10.1109/ICIP.2012.6467181
   Devanne M, 2016, INT C PATT RECOG, P895, DOI 10.1109/ICPR.2016.7899749
   Ferrari C, 2017, IEEE T MULTIMEDIA, V19, P2666, DOI 10.1109/TMM.2017.2707341
   Gheissari N., 2006, P IEEE COMP SOC C CO, V2, P1528, DOI DOI 10.1109/CVPR.2006.223
   Gong SG, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6296-4_1
   Haque A, 2016, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2016.138
   Karianakis Nikolaos, 2017, CORR
   Lin Chunli, 2010, 2010 2nd International Conference on Networking and Digital Society (ICNDS 2010), P589, DOI 10.1109/ICNDS.2010.5479416
   Lisanti G, 2015, IEEE T PATTERN ANAL, V37, P1629, DOI 10.1109/TPAMI.2014.2369055
   Liu H, 2017, CAAI T INTELL TECHNO, V2, P48, DOI 10.1016/j.trit.2017.04.001
   Mogelmose A, 2013, IEEE COMPUT SOC CONF, P301, DOI 10.1109/CVPRW.2013.52
   Munaro M, ONE SHOT PERSON REID, P161
   Munaro M, 2014, IEEE INT CONF ROBOT, P4512, DOI 10.1109/ICRA.2014.6907518
   Pala F, 2016, IEEE T CIRC SYST VID, V26, P788, DOI 10.1109/TCSVT.2015.2424056
   Pala P, 2018, P EUR WORKSH 3D OBJ, P1, DOI [10.2312/3dor.20181058, DOI 10.2312/3DOR.20181058]
   Rafi Umer, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P67, DOI 10.1109/CVPRW.2015.7301338
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Satta Riccardo, 2013, Proceedings of the 8th International Conference on Computer Vision Theory and Applications. VISAPP 2013, P407
   Shotton J, 2013, IEEE T PATTERN ANAL, V35, P2821, DOI 10.1109/TPAMI.2012.241
   Sivapalan S., 2011, P INT JOINT C BIOM I, P1, DOI [DOI 10.1109/IJCB.2011.6117504, 10.1155/2011/375897]
   Velardo C., 2012, WIAMIS, P1, DOI [10.1109/WIAMIS.2012.6226747., DOI 10.1109/WIAMIS.2012.6226747]
   Wang XG, 2013, PATTERN RECOGN LETT, V34, P3, DOI 10.1016/j.patrec.2012.07.005
   Wu AC, 2017, IEEE T IMAGE PROCESS, V26, P2588, DOI 10.1109/TIP.2017.2675201
   Zheng Liang, 2016, PERSON RE IDENTIFICA
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
NR 34
TC 3
Z9 4
U1 0
U2 1
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD APR
PY 2019
VL 79
BP 69
EP 80
DI 10.1016/j.cag.2019.01.003
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HT3SX
UT WOS:000464484600009
DA 2022-02-03
ER

PT J
AU Le, THN
   Luu, K
   Zhu, CC
   Savvides, M
AF Le, T. Hoang Ngan
   Luu, Khoa
   Zhu, Chenchen
   Savvides, Marios
TI Semi self-training beard/moustache detection and segmentation
   simultaneously
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Beard/moustache; Detection and segmentation; Super pixel; Random Ferns;
   Support Vector Machine; Histogram of Gabor (HoG); Histogram of Oriented
   Gradient of Gabor (HOGG)
AB This paper presents a robust, fully automatic and semi self-training system to detect and segment facial beard/moustache simultaneously in challenging facial images. Based on the observation that some certain facial areas, e.g. cheeks, do not typically contain any facial hair whereas the others, e.g. brows, often contain facial hair, a self-trained model is first built using a testing image itself. To overcome the limitation of that facial hairs in brows regions and beard/moustache regions are different in length, density, color, etc., a pre-trained model is also constructed using training data. The pre-trained model is only pursued when the self-trained model produces low confident classification results. In the proposed system, we employ the superpixel together a combination of two classifiers, i.e. Random Ferns (rFerns) and Support Vector Machines (SVM) to obtain good classification performance as well as improve time efficiency. A feature vector, consisting of Histogram of Gabor (HoG) and Histogram of Oriented Gradient of Gabor (HOGG) at different directions and frequencies, is generated from both the bounding box of the superpixel and the super pixel foreground. The segmentation result is then refined by our proposed aggregately searching strategy in order to deal with inaccurate landmarking points. Experimental results have demonstrated the robustness and effectiveness of the proposed system. It is evaluated in images drawn from three entire databases i.e. the Multiple Biometric Grand Challenge (MBGC) still face database, the NIST color Facial Recognition Technology FERET database and a large subset from Pinellas County database. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Le, T. Hoang Ngan] Carnegie Mellon Univ, CyLab Biometr Ctr, Pittsburgh, PA 15213 USA.
   Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.
C3 Carnegie Mellon University; Carnegie Mellon University
RP Le, THN (corresponding author), Carnegie Mellon Univ, CyLab Biometr Ctr, Pittsburgh, PA 15213 USA.
RI Luu, Khoa/AAQ-8540-2021
OI Luu, Khoa/0000-0003-2104-0901
CR Achanta R., 2010, SLIC SUPERPIXELS EPF
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Le THN, 2015, INT CONF BIOMETR, P507, DOI 10.1109/ICB.2015.7139066
   Le THN, 2013, IEEE T IMAGE PROCESS, V22, P3097, DOI 10.1109/TIP.2013.2259835
   Le THN, 2012, IEEE IMAGE PROC, P165, DOI 10.1109/ICIP.2012.6466821
   Nguyen MH, 2008, COMPUT GRAPH FORUM, V27, P627, DOI 10.1111/j.1467-8659.2008.01160.x
   Ozuysal M, 2010, IEEE T PATTERN ANAL, V32, P448, DOI 10.1109/TPAMI.2009.23
   Ozuysal V. L. M., 2007, FAST KEYPOINT RECOGN, P1
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Pierrard Jean-Sebastien, 2008, THESIS
   Seshadri K, 2009, 2009 IEEE 3RD INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS, P319
NR 12
TC 10
Z9 10
U1 0
U2 4
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2017
VL 58
BP 214
EP 223
DI 10.1016/j.imavis.2016.07.009
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA EN2MN
UT WOS:000395844700020
DA 2022-02-03
ER

PT J
AU Chen, HN
   Chen, YW
   Tian, X
   Jiang, RX
AF Chen, Haonan
   Chen, Yaowu
   Tian, Xiang
   Jiang, Rongxin
TI A Cascade Face Spoofing Detector Based on Face Anti-Spoofing R-CNN and
   Improved Retinex LBP
SO IEEE ACCESS
LA English
DT Article
DE Face; Feature extraction; Detectors; Lighting; Image quality; Face
   detection; Image color analysis; Face spoofing detection; faster R-CNN;
   crystal loss; Retinex; attention fusion; guided filter
ID LIVENESS; IMAGE
AB In consideration of secure and convenient, face gains increasing attention in variety of fields during the past decades. Since human face is most accessible from our daily life and preserves the richest information, face based biometric systems are widely used in person authentication applications. However, face recognition systems are always challenged by face spoofing attacks. Although, researchers have proposed many face spoofing detection methods, which have achieved great performances, we aim to develop a method to counter face spoofing, which combines the face detection stage and face spoofing detection stage together. In this paper, we design face anti-spoofing region-based convolutional neural network (FARCNN), based on improved Faster region-based convolutional neural network (R-CNN) framework. Motivated by face detection, we regard the face spoofing detection as a three-way classification to distinguish real face, fake face and background. We extend the typical Faster R-CNN scheme by optimizing several important strategies, including roi-pooling feature fusion and adding Crystal Loss function to the original multi-task loss function. In addition, an improved Retinex based LBP is presented to handle the different illumination conditions in face spoofing detection. Finally, these two detectors are further cascaded and achieve promising performances on the benchmark databases: CASIA-FASD, REPLAY-ATTACK and OULU-NPU. Besides, for the purpose of verifying the generalization capacity of the proposed cascade detector, we perform experiments on cross-databases and the results testify the effectiveness of our proposed method.
C1 [Chen, Haonan; Chen, Yaowu; Jiang, Rongxin] Zhejiang Univ, Inst Adv Digital Technol & Instrument, Hangzhou 310027, Zhejiang, Peoples R China.
   [Chen, Yaowu] Zhejiang Univ, Zhejiang Prov Key Lab Network Multimedia Technol, Hangzhou 310027, Zhejiang, Peoples R China.
   [Tian, Xiang] Zhejiang Univ, Minist Educ China, Embedded Syst Engn Res Ctr, Hangzhou 310027, Zhejiang, Peoples R China.
   [Jiang, Rongxin] Zhejiang Univ, State Key Lab Ind Control Technol, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University; Zhejiang University; Ministry of Education, China;
   Zhejiang University; Zhejiang University
RP Chen, YW (corresponding author), Zhejiang Univ, Inst Adv Digital Technol & Instrument, Hangzhou 310027, Zhejiang, Peoples R China.; Chen, YW (corresponding author), Zhejiang Univ, Zhejiang Prov Key Lab Network Multimedia Technol, Hangzhou 310027, Zhejiang, Peoples R China.
EM cyw@mail.bme.zju.edu.cn
OI Chen, Haonan/0000-0001-8885-5051
FU Fundamental Research Funds for the Central UniversitiesFundamental
   Research Funds for the Central Universities
FX This work was supported by the Fundamental Research Funds for the
   Central Universities.
CR Anjos A., 2011, BIOM IJCB 2011 INT J, P1, DOI DOI 10.1109/IJCB.2011.6117503
   Atoum Y, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P319, DOI 10.1109/BTAS.2017.8272713
   Benlamoudi A, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.4.043007
   Bharadwaj S, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-34
   Bharadwaj S, 2013, IEEE COMPUT SOC CONF, P105, DOI 10.1109/CVPRW.2013.23
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Boulkenafet Z, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P688, DOI 10.1109/BTAS.2017.8272758
   Boulkenafet Z, 2018, IMAGE VISION COMPUT, V77, P1, DOI 10.1016/j.imavis.2018.04.007
   Boulkenafet Z, 2016, IEEE T INF FOREN SEC, V11, P1818, DOI 10.1109/TIFS.2016.2555286
   Boulkenafet Z, 2015, IEEE IMAGE PROC, P2636, DOI 10.1109/ICIP.2015.7351280
   Boulkenafet Z, 2017, IEEE INT CONF AUTOMA, P612, DOI 10.1109/FG.2017.77
   Chakka M. M., 2011, BIOM IJCB, P1
   Chen D, 2016, LECT NOTES COMPUT SC, V9909, P122, DOI 10.1007/978-3-319-46454-1_8
   Chingovska I., 2012, P INT C BIOM SPEC IN, DOI DOI 10.1109/VTCFALL.2012.6399116
   de Freitas Pereira Tiago, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P121, DOI 10.1007/978-3-642-37410-4_11
   Feng LT, 2016, J VIS COMMUN IMAGE R, V38, P451, DOI 10.1016/j.jvcir.2016.03.019
   Galbally J, 2014, INT C PATT RECOG, P1173, DOI 10.1109/ICPR.2014.211
   Girdhar R., 2017, ADV NEURAL INFORM PR, P34
   GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gragnaniello D, 2015, IEEE T INF FOREN SEC, V10, P849, DOI 10.1109/TIFS.2015.2404294
   Gupta A, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P611, DOI 10.1145/3242969.3264985
   Hadid A., 2011, BIOM IJCB 2011 INT J, P1, DOI [DOI 10.1109/IJCB.2011.6117510, 10.1109/IJCB.2011.6117510]
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang HZ, 2017, IEEE INT CONF AUTOMA, P650, DOI [10.1109/FG.2017.82, 10.1109/MWSYM.2017.8058653]
   Kollreider K, 2008, PROC CVPR IEEE, P1200
   Komulainen J, 2013, P INT C BIOM, P1
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Li HL, 2018, IEEE T INF FOREN SEC, V13, P1794, DOI 10.1109/TIFS.2018.2801312
   Li JW, 2004, P SOC PHOTO-OPT INS, V5404, P296, DOI 10.1117/12.541955
   Li L, 2019, IEEE T INF FOREN SEC, V14, P2246, DOI 10.1109/TIFS.2019.2895212
   Li L, 2017, IEEE IMAGE PROC, P101, DOI 10.1109/ICIP.2017.8296251
   Li LF, 2016, CRYSTALS, V6, DOI 10.3390/cryst6040045
   Li WC, 2017, INT CONF ACOUST SPEE, P3156, DOI 10.1109/ICASSP.2017.7952738
   Li YZ, 2016, LECT NOTES COMPUT SC, V9907, P420, DOI 10.1007/978-3-319-46487-9_26
   Liu Y., 2018, ARXIV180311097
   Liu YJ, 2018, PROC CVPR IEEE, P389, DOI 10.1109/CVPR.2018.00048
   Menotti D, 2015, IEEE T INF FOREN SEC, V10, P864, DOI 10.1109/TIFS.2015.2398817
   Pan G, 2007, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2007.4409068
   Patel K, 2016, LECT NOTES COMPUT SC, V9967, P611, DOI 10.1007/978-3-319-46654-5_67
   Pereira TD, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-2
   Pinto A, 2015, IEEE T IMAGE PROCESS, V24, P4726, DOI 10.1109/TIP.2015.2466088
   Ranjan Rajeev, 2018, ARXIV180401159
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tan XY, 2010, LECT NOTES COMPUT SC, V6316, P504, DOI 10.1007/978-3-642-15567-3_37
   TANG XO, 2010, P 11 EUR C COMP VIS, p1G14
   Tirunagari S, 2015, IEEE T INF FOREN SEC, V10, P762, DOI 10.1109/TIFS.2015.2406533
   Wang Z., 2018, ARXIV181105118
   Wen D, 2015, IEEE T INF FOREN SEC, V10, P746, DOI 10.1109/TIFS.2015.2400395
   Yang J., 2014, ARXIV14085601
   Yang JS, 2013, IEEE GLOB COMM CONF, P1, DOI 10.1109/GLOCOM.2013.6831038
   Zhiwei Zhang, 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P26, DOI 10.1109/ICB.2012.6199754
   Zhou Z, 2017, PROC CVPR IEEE, P6776, DOI 10.1109/CVPR.2017.717
   Zhu CC, 2017, ADV COMPUT VIS PATT, P57, DOI 10.1007/978-3-319-61657-5_3
NR 55
TC 1
Z9 2
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2019
VL 7
BP 170116
EP 170133
DI 10.1109/ACCESS.2019.2955383
PG 18
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Telecommunications
GA NB3ZQ
UT WOS:000560454900072
OA gold
DA 2022-02-03
ER

PT C
AU Malik, J
   Sainarayanan, G
   Dahiya, R
AF Malik, Jyoti
   Sainarayanan, G.
   Dahiya, Ratna
BE Atkinson, GA
   Deepika, CL
   Kandaswamy, A
TI Min Max Threshold Range (MMTR) Based Approach in Palmprint
   Authentication by Sobel Code Method
SO PROCEEDINGS OF THE INTERNATIONAL CONFERENCE AND EXHIBITION ON BIOMETRICS
   TECHNOLOGY
SE Procedia Computer Science
LA English
DT Proceedings Paper
CT International Conference and Exhibition on Biometrics Technology
CY SEP 02-04, 2010
CL Coimbatore, INDIA
DE Palmprint identification; Region of Interest (ROI); Sobel Code; Hamming
   Distance
AB Palmprint recognition is an effective biometric authentication method to automatically identify a person's identity. Palmprint is rich in features like geometry features, line features, datum points, delta features and minutiae features. Several edge detection methods are available to extract line feature from the palmprint. In this paper, the hand image is pre-processed to get the desired Region of Interest (ROI) / palmprint. Multiscale Sobel Code operators of different orientations (0 degrees, 45 degrees, 90 degrees, and 135 degrees) are applied to the palmprint to extract Sobel-Palmprint features in different directions. The Sobel-Palmprint features extracted are stored in Sobel-Palmprint feature vector and matched using Hamming Distance similarity measurement method. In addition, a Min Max Threshold Range (MMTR) method is proposed that helps in increasing overall system accuracy by matching a person with multiple threshold values. In this technique, firstly the person is authenticated at global level using Reference threshold. Secondly, the person is authenticated at local level using range of Minimum and Maximum thresholds defined for a person. Generally, personal authentication is done using reference threshold but there are chances of false acceptance. So, by using the Minimum and Maximum Thresholds range of false accepted persons at personal level, a person is identified to be false accepted or genuinely accepted. MMTR is an effective technique to increase the accuracy of the palmprint authentication system by reducing the False Acceptance Rate (FAR). Experimental results indicate that the proposed method improves the False Acceptance Rate drastically. (C) 2010 Published by Elsevier Ltd
C1 [Malik, Jyoti; Dahiya, Ratna] Natl Inst Technol, Dept Elect Engn, Kurukshetra, Haryana, India.
   [Sainarayanan, G.] ICT Acad Tamil Nadu, Madras, Tamil Nadu, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Kurukshetra
RP Malik, J (corresponding author), Natl Inst Technol, Dept Elect Engn, Kurukshetra, Haryana, India.
EM jyoti_reck@yahoo.com; sainarayanan@ictact.in; ratna_dahiya@yahoo.com
CR [Anonymous], POLYU PALMPR DAT
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Duta N, 2002, PATTERN RECOGN LETT, V23, P477, DOI 10.1016/S0167-8655(01)00179-9
   Eddins Steven L., DIGITAL IMAGE PROCES
   Han CC, 2003, PATTERN RECOGN, V36, P371
   Jain AK, 2004, IEEE T CIRC SYST VID, V14, P4, DOI 10.1109/TCSVT.2003.818349
   Kumar A, 2003, LECT NOTES COMPUT SC, V2688, P668
   Leung MKH, 2007, IEEE PERVAS COMPUT, V6, P40, DOI 10.1109/MPRV.2007.78
   Martin Alvin, 2000, P COMPUTER SOC
   Pankanti Sharath, 2000, P COMPUTER SOC
   Poon C, 2004, INT C PATT RECOG, P533, DOI 10.1109/ICPR.2004.1333827
   Shu W., 2001, INT J IMAGE GRAPHICS, V1, P135
   Srinivasan Nirupama, 2006, IEEE INT WORKSH MEAS
   Wong Edward, 2006, P 3 INT C ART INT EN
   Wong K.Y.E., 2007, MAL JAP INT S ADV TE
   Wong K.Y.E., 2008, P 1 SEM ENG INF TECH, P208
   Wong KYE, 2008, 2008 10TH INTERNATIONAL CONFERENCE ON CONTROL AUTOMATION ROBOTICS & VISION: ICARV 2008, VOLS 1-4, P1338, DOI 10.1109/ICARCV.2008.4795716
   Wu XQ, 2006, IEEE T SYST MAN CY A, V36, P978, DOI 10.1109/TSMCA.2006.871797
   Wu XQ, 2004, PATTERN RECOGN, V37, P1987, DOI 10.1016/j.patcog.2004.02.015
   Zhang D, 2003, IEEE T PATTERN ANAL, V25, P1041, DOI 10.1109/TPAMI.2003.1227981
NR 20
TC 3
Z9 3
U1 1
U2 2
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA SARA BURGERHARTSTRAAT 25, PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 1877-0509
J9 PROCEDIA COMPUT SCI
PY 2010
VL 2
BP 149
EP 158
DI 10.1016/j.procs.2010.11.019
PG 10
WC Computer Science, Artificial Intelligence; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology; Radiology,
   Nuclear Medicine & Medical Imaging
GA BTJ63
UT WOS:000287102400019
OA gold
DA 2022-02-03
ER

PT J
AU Hollingsworth, KP
   Bowyer, KW
   Flynn, PJ
AF Hollingsworth, Karen P.
   Bowyer, Kevin W.
   Flynn, Patrick J.
TI The Best Bits in an Iris Code
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article; Proceedings Paper
CT 1st IEEE International Conference on Biometrics - Theory, Applications
   and Systems (BTAS 07)
CY SEP 27-29, 2007
CL Crystal City, VA
DE Iris biometrics; iris code; texture filter; false reject rate
ID RECOGNITION
AB Iris biometric systems apply filters to iris images to extract information about iris texture. Daugman's approach maps the filter output to a binary iris code. The fractional Hamming distance between two iris codes is computed and decisions about the identity of a person are based on the computed distance. The fractional Hamming distance weights all bits in an iris code equally. However, not all of the bits in an iris code are equally useful. Our research is the first to present experiments documenting that some bits are more consistent than others. Different regions of the iris are compared to evaluate their relative consistency and, contrary to some previous research, we find that the middle bands of the iris are more consistent than the inner bands. The inconsistent-bit phenomenon is evident across genders and different filter types. Possible causes of inconsistencies, such as segmentation, alignment issues, and different filters, are investigated. The inconsistencies are largely due to the coarse quantization of the phase response. Masking iris code bits corresponding to complex filter responses near the axes of the complex plane improves the separation between the match and nonmatch Hamming distance distributions.
C1 [Hollingsworth, Karen P.; Bowyer, Kevin W.; Flynn, Patrick J.] Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
C3 University of Notre Dame
RP Hollingsworth, KP (corresponding author), Univ Notre Dame, Dept Comp Sci & Engn, 384 Fitzpatrick Hall, Notre Dame, IN 46556 USA.
EM kholling@nd.edu; kwb@cse.nd.edu; flynn@nd.edu
RI Flynn, Patrick J/J-3388-2013
OI Flynn, Patrick J/0000-0002-5446-114X; Bowyer, Kevin/0000-0002-7562-4390
CR Bolle RM, 2004, INT C PATT RECOG, P927, DOI 10.1109/ICPR.2004.1334411
   Bowyer KW, 2008, COMPUT VIS IMAGE UND, V110, P281, DOI 10.1016/j.cviu.2007.08.005
   Broussard R. P., 2008, P SPIE, V6944
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676
   DU Y, 2005, P IEEE INT C AC SPEE, V2, P961
   Glenstrup A., 1995, THESIS U COPENHAGEN
   HOLLINGSWORTH K, 2007, P IEEE INT C BIOM TH
   Krichen E., 2008, OSIRIS OPEN SOURCE I
   Liu XM, 2005, FOURTH IEEE WORKSHOP ON AUTOMATIC IDENTIFICATION ADVANCED TECHNOLOGIES, PROCEEDINGS, P118
   Ma L, 2004, PATTERN RECOGN, V37, P1287, DOI 10.1016/j.patcog.2004.02.001
   Miyazawa K, 2005, IEEE IMAGE PROC, P2001
   *NAT I STAND TECH, 2008, IR CHALL EV 2005 WOR
   Phillips P, 2007, 7408 NISTIR
   THORNTON J, 2007, P IEEE INT C BIOM TH
   Tisse C.-L., 2002, P VISION INTERFACE, P294
   WILDES R, 2005, BIOMETRIC SYSTEMS, P63
NR 17
TC 142
Z9 147
U1 1
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD JUN
PY 2009
VL 31
IS 6
BP 964
EP 973
DI 10.1109/TPAMI.2008.185
PG 10
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA 431YF
UT WOS:000265100000001
PM 19372603
DA 2022-02-03
ER

PT C
AU Kim, AY
   Lee, SH
AF Kim, Ae-Young
   Lee, Sang-Ho
GP IEEE
TI Authentication protocol using fuzzy eigenface vault based on MoC
SO 9TH INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION TECHNOLOGY:
   TOWARD NETWORK INNOVATION BEYOND EVOLUTION, VOLS 1-3
LA English
DT Proceedings Paper
CT 9th International Conference on Advanced Communication Technology (ICACT
   2007)
CY FEB 12-14, 2007
CL Phoenix Pk, SOUTH KOREA
DE fuzzy vault; eigenface; authentication; smart card
ID FINGERPRINTS
AB In this paper, the eigenface-based fuzzy vault scheme, called a fuzzy eigenface vault, is designed and examined, and authentication protocol using the fuzzy eigenface vault is designed based on a smartcard and analyzed security of the proposed authentication protocol. Especially, face recognition for user authentication has several advantages such as natural, nonintrusive, convenience, and easy to use. However, it is difficult to apply biometrics on cryptographic security because of different images in each acquisition phase. Furthermore face data can be vulnerable to modification attack and illegally let out. From that, the need of secure biometrics authentication methods has been increased and we propose a new authentication protocol using fuzzy eigenface vault and smart card.
   The proposed protocol has three advantages compared to previous works. First, to get security, accuracy and convenience without reluctance at same time, we use eigenface in a fuzzy vault scheme which is suitable to combine biometric authentication and cryptography. Second, to enhance security, secret data which is for construction of the fuzzy eigenface vault is saved on a smart card. Third, to reduce a trans mission-based attack, a transmission of message is occurred just one time in a login phase.
C1 [Kim, Ae-Young; Lee, Sang-Ho] Ewha Womans Univ, Dept Comp Sci & Engn, Seoul, South Korea.
C3 Ewha Womans University
RP Kim, AY (corresponding author), Ewha Womans Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM kay@ewhain.net; shlee@ewha.ac.kr
CR Adams J., 2000, BTT              AUG, P8
   CHANG Y, 2004, IEEE C MULT EXP 2004
   Chirillo J., 2003, IMPLEMENTING BIOMETR
   Clancy T Charles, 2003, P ACM SIGMM 2003 MUL, P45
   GOH A, 2003, P IFIP
   Jain AK, 1997, P IEEE, V85, P1365, DOI 10.1109/5.628674
   Juels A, 1999, 6TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P28
   Juels A, 2002, ISIT: 2002 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P408, DOI 10.1109/ISIT.2002.1023680
   KIM A, 2007, P APIS
   KIM AY, 2005, KOR COMPL C P KISS, P232
   Li SZ., 2004, HDB FACE RECOGNITION
   MALTONI D, 2003, HDG FINGERPRINT RECO
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   ULUDAG U, 2005, LNCS, P310
   Uludag U., 2004, P WORKSH BIOM CHALL, P13
NR 15
TC 4
Z9 4
U1 0
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-89-5519-131-8
PY 2007
BP 1771
EP +
DI 10.1109/ICACT.2007.358714
PG 2
WC Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Telecommunications
GA BFX85
UT WOS:000245348102062
DA 2022-02-03
ER

PT J
AU Batchuluun, G
   Naqvi, RA
   Kim, W
   Park, KR
AF Batchuluun, Ganbayar
   Naqvi, Rizwan Ali
   Kim, Wan
   Park, Kang Ryoung
TI Body-movement-based human identification using convolutional neural
   network
SO EXPERT SYSTEMS WITH APPLICATIONS
LA English
DT Article
DE Low-illumination corridor; The front and back view of body; Human
   identification; Convolutional neural network
ID GAIT RECOGNITION; IMAGE; TEMPLATE; DISTANCE
AB Biometric technology based on human gait identifies humans at a far distance even if the individual's face is covered, hidden, or not visible to cameras in dark environments. Previous studies based on human gait were conducted considering both bright and dark environments for human identification in surveillance systems. The studies conducted in low-illumination environments (dark environments) are based on side view images (horizontal walking) of subjects. However, there are cases in which people only show the front and back views of their bodies while they are walking in low-illumination corridors. In these views, it is difficult to identify humans by using conventional features such as cycle, cadence, stride length of walking, and distance between points (ankle, knee, and hip). Additionally, the cases of problems such as people carrying cellphones and/or small personal items (a purse, bag, clothes, etc.) have critical effects on the accuracy of human identification. To overcome these problems, we propose a new human identification technique, which is based on the front and back view images of a human, captured by using a thermal camera sensor. Our technique uses movements of the human body for identification, particularly movement of the head, shoulders, and legs. We have used a convolutional neural network for feature extraction and classification in this study. Five datasets were compiled by collecting data of 80 people including men and women in both bright and dark environments. The experimental results with our collected data and open database showed a higher performance by using our method compared to those of previous studies. (C) 2018 Elsevier Ltd. All rights reserved.
C1 [Batchuluun, Ganbayar; Naqvi, Rizwan Ali; Kim, Wan; Park, Kang Ryoung] Dongguk Univ, Div Elect & Elect Engn, 30 Pildong Ro 1 Gil, Seoul 100715, South Korea.
C3 Dongguk University
RP Park, KR (corresponding author), Dongguk Univ, Div Elect & Elect Engn, 30 Pildong Ro 1 Gil, Seoul 100715, South Korea.
EM parkgr@dongguk.edu
RI Batchuluun, Ganbayar/AAT-6377-2020; Naqvi, Rizwan Ali/AAW-9242-2020
OI Batchuluun, Ganbayar/0000-0003-1456-5697; 
FU MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information
   Technology Research Center) support program [IITP-2017-2013-0-00684];
   NRF - Korean government, MSIT [NRF-2016M3A9E1915855]; National Research
   Foundation of Korea (NRF) - Korea government (Ministry of Science and
   ICT) [NRF-2017R1C1B5074062]
FX This research was supported by the MSIT (Ministry of Science and ICT),
   Korea, under the ITRC (Information Technology Research Center) support
   program (IITP-2017-2013-0-00684) supervised by the HIP (Institute for
   Information & communications Technology Promotion), by the Bio & Medical
   Technology Development Program of the NRF funded by the Korean
   government, MSIT (NRF-2016M3A9E1915855), and by the National Research
   Foundation of Korea (NRF) grant funded by the Korea government (Ministry
   of Science and ICT) (NRF-2017R1C1B5074062).
CR Ali H., 2011, SERSC ORG, V4, P141
   Bashir K., 2009, BMVC, P1
   Bouchrika I, 2011, J FORENSIC SCI, V56, P882, DOI 10.1111/j.1556-4029.2011.01793.x
   Chen JJ, 2014, SHOCK VIB, V2014, DOI [10.1155/2014/416530, 10.1155/2014/524351]
   Collins R. T., 2002, P 4 AS PAC C COOP ED, P1
   Cunado D, 2003, COMPUT VIS IMAGE UND, V90, P1, DOI 10.1010/SI077-3142(03)00008-0
   Dadashi F., 2009, 2 INT C IM SIGN PROC, P1
   Daoliang Tan, 2007, CVPR '07. IEEE Conference on Computer Vision and Pattern Recognition, P1
   DeCann B., 2010, SPIE C BIOM TECHN HU, P1
   Deshmukh P. R., 2016, INT J ADV RES COMPUT, V6, P495
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Ge WF, 2017, PROC CVPR IEEE, P10, DOI 10.1109/CVPR.2017.9
   Geng X, 2008, IEEE WORK APP COMP, P94
   Glorot X., 2011, DEEP SPARSE RECTIFIE, P315
   Goffredo M, 2008, 2008 IEEE SECOND INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS (BTAS), P154
   Guan Y., 2013, P 6 IAPR INT C BIOM, P1, DOI DOI 10.1109/ICB.2013.6612965
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   Heaton J., 2015, ARTIFICIAL INTELLIGE, V3
   Hong H. G., 2017, SENSORS, V17, P1
   Hu Ng, 2011, International Journal of New Computer Architectures and their Applications, V1, P358
   Huelke DF, 1998, P ANN C ASS, P93
   Hwang Doosung, 2008, 1 WORKSH IM PROC THE, P1
   Iwama H., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P113, DOI 10.1109/BTAS.2012.6374565
   Jamaludin A., 2017, P 3 WORKSH DEEP LEAR, P1
   Kale A, 2004, IEEE T IMAGE PROCESS, V13, P1163, DOI 10.1109/TIP.2004.832865
   Kale A., 2006, THESIS
   Kim D., 2009, INT J SIGNAL PROCESS, V2, P1, DOI DOI 10.1021/BC800472A
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kusakunniran W, 2012, IEEE T SYST MAN CY B, V42, P1654, DOI 10.1109/TSMCB.2012.2197823
   Kusakunniran W, 2011, IEEE IMAGE PROC, P545, DOI 10.1109/ICIP.2011.6116403
   Kusakunniran W, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P49, DOI 10.1109/AVSS.2009.44
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JH, 2015, SENSORS-BASEL, V15, P10580, DOI 10.3390/s150510580
   Lee L, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P155, DOI 10.1109/AFGR.2002.1004148
   Lv ZW, 2015, SENSORS-BASEL, V15, P932, DOI 10.3390/s150100932
   Ming D, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON VIRTUAL ENVIRONMENTS, HUMAN-COMPUTER INTERFACES AND MEASUREMENT SYSTEMS, P319, DOI 10.1109/VECIMS.2009.5068916
   Ng H, 2013, INT ARAB J INF TECHN, V10, P519
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Parkhi OM, 2015, P BR MACH VIS, P1, DOI DOI 10.5244/C.29.41
   QIN HW, 2016, PROC CVPR IEEE, P3456, DOI DOI 10.1109/CVPR.2016.376
   Simonyan K., 2015, P ICLR
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tan D., 2007, P IEEE INT C IM PROC
   Tan DL, 2007, LECT NOTES COMPUT SC, V4642, P222
   Tan DL, 2006, INT C PATT RECOG, P1000
   Wang C, 2010, LECT NOTES COMPUT SC, V6311, P257, DOI 10.1007/978-3-642-15549-9_19
   Wang L, 2003, IEEE T PATTERN ANAL, V25, P1505, DOI 10.1109/TPAMI.2003.1251144
   Whittle M.W., 2007, GAIT ANAL INTRO, V4th ed.
   Wu ZF, 2017, IEEE T PATTERN ANAL, V39, P209, DOI 10.1109/TPAMI.2016.2545669
   전지혜, 2009, [Signal Processing, 전자공학회논문지 - SP], V46, P97
   Zhang E, 2010, SIGNAL PROCESS, V90, P2295, DOI 10.1016/j.sigpro.2010.01.024
   Zhao GY, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P529
   Zhao X. L., 2010, P IEEE INT C COMP AP
   Zhou XL, 2006, INT C PATT RECOG, P529
NR 55
TC 18
Z9 19
U1 0
U2 72
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4174
EI 1873-6793
J9 EXPERT SYST APPL
JI Expert Syst. Appl.
PD JUL 1
PY 2018
VL 101
BP 56
EP 77
DI 10.1016/j.eswa.2018.02.016
PG 22
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Operations Research & Management Science
GA GA7ER
UT WOS:000428498300005
DA 2022-02-03
ER

PT J
AU Hasan, M
   Riaz, MH
   Rahman, MA
AF Hasan, Mahamudul
   Riaz, Md Hasnat
   Rahman, Md Auhidur
TI Authentication Techniques in Cloud and Mobile Cloud Computing
SO INTERNATIONAL JOURNAL OF COMPUTER SCIENCE AND NETWORK SECURITY
LA English
DT Article
DE Could Computing; Mobile Cloud computing; Authentication;
   Confidentiality; Integrity
ID SECURITY; PRIVACY
AB A major challenge in cloud and mobile cloud computing is to ensure security and privacy of user's personal information (e.g., financial data, health record, location information) from malicious attacks. It is important for a cloud service provider (CSP) to establish trust and gain confidence by providing proper security and privacy to the clients. Authentication is important for establishing accountability and authorization of the users while allocating cloud resources. Researchers have proposed several techniques, such as token-based, image and biometric based, to make the authentication process more efficient, secure and user friendly. In this paper, we discuss different authentication techniques proposed for both cloud and mobile cloud computing environments. We categorize the algorithms based on its input, i.e. the credentials required for validating users. However, we emphasize that the classification is not precise, as it is difficult to classify the authentication algorithms relying on more than one user credentials (multi-factor authentication). To understand the complexity and delay of an authentication process, we focus on the number of entities involved in an authentication process and the number of handshakes taking place between them. We also compare the authentication algorithms on the basis of design principles and popular security attacks.
C1 [Hasan, Mahamudul; Riaz, Md Hasnat] Noakhali Sci & Technol Univ, Dept Comp Sci & Telecommun Engn, Sonapur, Bangladesh.
   [Rahman, Md Auhidur] Noakhali Sci & Technol Univ, Inst Informat Technol, Sonapur, Bangladesh.
C3 Noakhali Science & Technology University (NSTU); Noakhali Science &
   Technology University (NSTU)
RP Hasan, M (corresponding author), Noakhali Sci & Technol Univ, Dept Comp Sci & Telecommun Engn, Sonapur, Bangladesh.
CR Abdo J. B., 2013, INT C COMP APPL TECH, P1
   Abdo JB, 2014, IEEE MEDITERR ELECT, P151, DOI 10.1109/MELCON.2014.6820523
   Ahmed A., 2016, P 10 INT C INT SYST, P1
   Alizadeh M, 2016, J NETW COMPUT APPL, V61, P59, DOI 10.1016/j.jnca.2015.10.005
   Alizadeh M, 2014, P INT CONF INTELL, P615, DOI 10.1109/ISMS.2014.111
   Alizadeh M, 2013, INT WIREL COMMUN, P660, DOI 10.1109/IWCMC.2013.6583636
   Babaeizadeh M., 2015, ENG TECHNOLOGY, V9, P655
   Banyal Rohitash Kumar, 2013, 2013 Fifth International Conference on Computational Intelligence, Modelling and Simulation (CIMSim), P105, DOI 10.1109/CIMSim.2013.25
   Bong J, 2016, 2016 INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN), P445, DOI 10.1109/ICOIN.2016.7427156
   Cavoukian A, 2008, INT FED INFO PROC, V261, P57
   Chiang JK, 2013, 2013 INTERNATIONAL SYMPOSIUM ON BIOMETRICS AND SECURITY TECHNOLOGIES (ISBAST), P116, DOI 10.1109/ISBAST.2013.22
   Choudhury A. J., 2011, Proceedings of the 2011 IEEE Asia-Pacific Services Computing Conference (APSCC), P110, DOI 10.1109/APSCC.2011.14
   Chow R, 2010, PROCEEDINGS OF THE 2010 ACM WORKSHOP CLOUD COMPUTING SECURITY WORKSHOP (CCSW'10:), P1, DOI 10.1145/1866835.1866837
   Dinesha H., 2012, COMP COMM APPL ICCCA, P1, DOI DOI 10.1109/ICCCA.2012.6179130
   Dinh H. T., 2011, WIRELESS COMMUNICATI
   Fathi R, 2015, 2015 IEEE 8TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING, P516, DOI 10.1109/CLOUD.2015.75
   Giuffrida C, 2014, LECT NOTES COMPUT SC, V8550, P92
   Grzonkowski S., 2011, Proceedings of the 1st IEEE First International Conference on Consumer Electronics - Berlin (IEEE ICCE-Berlin 2011), P83, DOI 10.1109/ICCE-Berlin.2011.6031855
   Gurav Shraddha M., 2014, 2014 International Conference on Electronic Systems, Signal Processing and Computing Technologies (ICESC), P479, DOI 10.1109/ICESC.2014.90
   Hajivali Mostafa, 2013, 2013 International Conference on ICT Convergence (ICTC), P807, DOI 10.1109/ICTC.2013.6675484
   He D., 2016, IEEE SYSTEMS J, VPP, P1
   Hongbin Liang, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications Workshops, P191, DOI 10.1109/INFCOMW.2011.5928806
   Jakobsson M., 2009, P 4 USENIX C HOT TOP
   Jana D, 2013, INT C ADV COMP SCI I, P113, DOI 10.1109/ICACSIS.2013.6761561
   Jivanadham L. B., 2013, INF EL VIS ICIEV 201, P1
   Khan RH, 2011, INF ASS SEC IAS 2011, P372, DOI [10.1109/ISIAS.2011.6122782, DOI 10.1109/ISIAS.2011.6122782]
   Larcom JA, 2013, IEEE GLOBE WORK, P470, DOI 10.1109/GLOCOMW.2013.6825032
   Lishan Kang, 2010, Proceedings 2010 Second International Conference on Multimedia Information Networking and Security (MINES 2010), P851, DOI 10.1109/MINES.2010.180
   Lomotey R., 2013, 2013 IEEE 9 WORLD C, P448
   Mahesh B. B., 2014, CLOUD COMP EM MARK C, P1
   Mansour A, 2016, 2016 INTERNATIONAL CONFERENCE ON WIRELESS NETWORKS AND MOBILE COMMUNICATIONS (WINCOM), pP278
   Ming-Huang Guo, 2012, 2012 15th International Symposium on Wireless Personal Multimedia Communications (WPMC 2012), P177
   Minqi Zhou, 2010, Proceedings 2010 Sixth International Conference on Semantics Knowledge and Grid (SKG 2010), P105, DOI 10.1109/SKG.2010.19
   Moghaddam FF, 2014, 2014 IEEE 5TH CONTROL AND SYSTEM GRADUATE RESEARCH COLLOQUIUM (ICSGRC), P208, DOI 10.1109/ICSGRC.2014.6908723
   Nonaka  H., 2004, P INT C COMP INT IST, V1, P19
   Omri F, 2013, 2013 13TH INTERNATIONAL CONFERENCE ON ITS TELECOMMUNICATIONS (ITST), P325, DOI 10.1109/ITST.2013.6685567
   Padma P, 2016, 2016 INTERNATIONAL CONFERENCE ON INVENTIVE COMPUTATION TECHNOLOGIES (ICICT), VOL 1, P275
   Panneerselvam J., 2012, 2012 Third International Conference on Emerging Intelligent Data and Web Technologies (EIDWT 2012), P316, DOI 10.1109/EIDWT.2012.20
   Rahimi MR, 2014, MOBILE NETW APPL, V19, P133, DOI 10.1007/s11036-013-0477-4
   Rassan IAL, 2014, 2014 INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE AND COMPUTATIONAL INTELLIGENCE (CSCI), VOL 1, P157, DOI 10.1109/CSCI.2014.33
   Reshmi G, 2015, INT CONF INTERNET, P58, DOI 10.1109/ICITST.2015.7412056
   Rong CM, 2013, COMPUT ELECTR ENG, V39, P47, DOI 10.1016/j.compeleceng.2012.04.015
   Roth J, 2013, INT CONF BIOMETR
   Ruj S, 2014, IEEE T PARALL DISTR, V25, P384, DOI 10.1109/TPDS.2013.38
   Saevanee H, 2008, INT C COMP ELEC ENG, P82, DOI 10.1109/ICCEE.2008.157
   Sampalli S., 2013, 32 INT PERF COMP COM, P1
   Sang-Ho Shin, 2012, Proceedings of the 2012 IEEE 1st International Conference on Cloud Networking (CLOUDNET 2012), P176, DOI 10.1109/CloudNet.2012.6483680
   Sarvabhatla M., 2015, COMM SYST NETW COMSN, P1
   SHI E, 2011, IMPL AUTH THROUGH, V6531, P99
   Stefan D., 2010, INT C COLL COMP NETW, P1
   Subashini S, 2011, J NETW COMPUT APPL, V34, P1, DOI 10.1016/j.jnca.2010.07.006
   Tien-Ho Chen, 2011, Proceedings of the 2011 Fifth IEEE/FTRA International Conference on Multimedia and Ubiquitous Engineering (MUE 2011), P155, DOI 10.1109/MUE.2011.69
   Tsai CJ, 2014, 2014 9TH INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND NETWORKING IN CHINA (CHINACOM), P353, DOI 10.1109/CHINACOM.2014.7054316
   Tsai JL, 2015, IEEE SYST J, V9, P805, DOI 10.1109/JSYST.2014.2322973
   Umanandhini D., 2012, COMP COMM NETW TECHN, P1
   Xiao ZF, 2013, IEEE COMMUN SURV TUT, V15, P843, DOI 10.1109/SURV.2012.060912.00182
   Xie W, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON RFID (RFID), P168, DOI 10.1109/RFID.2013.6548151
   Yassin AA, 2015, 2015 INTERNATIONAL SYMPOSIUM ON ARTIFICIAL INTELLIGENCE AND SIGNAL PROCESSING (AISP), P1, DOI 10.1109/AISP.2015.7123517
   Yassin AA, 2012, SECOND INTERNATIONAL CONFERENCE ON CLOUD AND GREEN COMPUTING / SECOND INTERNATIONAL CONFERENCE ON SOCIAL COMPUTING AND ITS APPLICATIONS (CGC/SCA 2012), P282, DOI 10.1109/CGC.2012.91
   Yu-Jia Chen, 2011, Proceedings of the 2011 International Conference on Parallel Processing Workshops (ICPPW 2011), P184, DOI 10.1109/ICPPW.2011.6
   Zhao K, 2013, PR CHINAGRID, P28, DOI 10.1109/ChinaGrid.2013.10
   Zwattendorfer B, 2012, INT CONF CLOUD COMPU, P397, DOI 10.1109/CCIS.2012.6664435
NR 62
TC 1
Z9 1
U1 0
U2 2
PU INT JOURNAL COMPUTER SCIENCE & NETWORK SECURITY-IJCSNS
PI SEOUL
PA DAE-SANG OFFICE 301, SANGDO 5 DONG 509-1, SEOUL, 00000, SOUTH KOREA
SN 1738-7906
J9 INT J COMPUT SCI NET
JI Int. J. Comput. Sci. Netw. Secur.
PD NOV 30
PY 2017
VL 17
IS 11
BP 28
EP 39
PG 12
WC Computer Science, Information Systems
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA FQ4XE
UT WOS:000418361100004
DA 2022-02-03
ER

PT J
AU Chen, YY
   Cho, CW
   Lin, SH
   Lai, HY
   Lo, YC
   Chen, SY
   Chang, YJ
   Huang, WT
   Chen, CH
   Jaw, FS
   Tsang, S
   Tsai, ST
AF Chen, You-Yin
   Cho, Chien-Wen
   Lin, Sheng-Huang
   Lai, Hsin-Yi
   Lo, Yu-Chun
   Chen, Shin-Yuan
   Chang, Yuan-Jen
   Huang, Wen-Tzeng
   Chen, Chin-Hsing
   Jaw, Fu-Shan
   Tsang, Siny
   Tsai, Sheng-Tsung
TI A vision-based regression model to evaluate Parkinsonian gait from
   monocular image sequences
SO EXPERT SYSTEMS WITH APPLICATIONS
LA English
DT Article
DE Human motion analysis; Parkinsonian gait; Linear discriminant analysis
   (LDA); Classification; Regression
ID DISEASE RATING-SCALE; MOTOR EXAMINATION; SYSTEM; RECOGNITION; DIAGNOSIS
AB Parkinson's Disease (PD) is a common neurodegenerative disorder with progressive loss of dopaminergic and other sub-cortical neurons. Among various approaches, gait analysis is commonly used to help identify the biometric features of PD. There have been some studies to date on both the classification of PD and estimation of gait parameters. However, it is also important to construct a regression system that can evaluate the degree of abnormality in PD patients. In this paper, we intended to develop a PD gait regression model that is capable of predicting the severity of motor dysfunction from given gait image sequences. We used a model-free strategy and thus avoided the critical demands of segmentation and parameter estimation. Furthermore, we used linear discriminant analysis (LDA) to increase the feature efficiency by maximizing and minimizing the between- and within-group variations. Regression was also achieved by assessing the spatial and temporal information through classification and finally by using these two new indices for linear regression. According to the experiments, the outcomes significantly correlated with the sum of sub-scores from the Unified Parkinson's Disease Rating Scale (UPDRS): motor examination section with r = 0.92 and 0.85 for training and testing, respectively, with p < 0.0001. Compared with conventional methods, our system provided a better evaluation of PD abnormality. (C) 2011 Elsevier Ltd. All rights reserved.
C1 [Chen, You-Yin; Cho, Chien-Wen; Lai, Hsin-Yi] Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu 300, Taiwan.
   [Lin, Sheng-Huang] Tzu Chi Univ, Tzu Chi Gen Hosp, Dept Neurol, Hualien 970, Taiwan.
   [Lin, Sheng-Huang; Lo, Yu-Chun; Jaw, Fu-Shan] Natl Taiwan Univ, Inst Biomed Engn, Coll Med, Taipei 100, Taiwan.
   [Chen, Shin-Yuan; Tsai, Sheng-Tsung] Tzu Chi Univ, Tzu Chi Gen Hosp, Dept Neurosurg, Hualien 970, Taiwan.
   [Chang, Yuan-Jen; Chen, Chin-Hsing] Cent Taiwan Univ Sci & Technol, Dept Management Informat Syst, Taichung 406, Taiwan.
   [Huang, Wen-Tzeng] Minghsin Univ Sci & Technol, Dept Comp Sci & Informat Engn, Hsinchu 304, Taiwan.
   [Tsang, Siny] Univ Virginia, Dept Psychol, Charlottesville, VA 22904 USA.
C3 National Yang Ming Chiao Tung University; Buddhist Tzu Chi General
   Hospital; Hualien Tzu Chi Hospital; Tzu Chi University; National Taiwan
   University; Buddhist Tzu Chi General Hospital; Hualien Tzu Chi Hospital;
   Tzu Chi University; Central Taiwan University Science & Technology;
   University of Virginia
RP Chen, YY (corresponding author), Natl Chiao Tung Univ, Dept Elect Engn, 1001 Ta Hsueh Rd, Hsinchu 300, Taiwan.
EM irradiance@so-net.net.tw
OI Tsai, Sheng-Tzung/0000-0001-6611-9440
CR Chang R, 2000, DISABIL REHABIL, V22, P97, DOI 10.1080/096382800297169
   Chien SL, 2006, PARKINSONISM RELAT D, V12, P438, DOI 10.1016/j.parkreldis.2006.04.004
   Cho CW, 2009, EXPERT SYST APPL, V36, P7033, DOI 10.1016/j.eswa.2008.08.076
   Christopher G., 2008, MOVEMENT DISORD, V23, P2129
   CUNADO D, 1999, P 2 INT C AUD VID BA, P43
   Dogantekin E, 2009, EXPERT SYST APPL, V36, P11282, DOI 10.1016/j.eswa.2009.03.021
   Gafurov D, 2006, J COMPUT, V1, P51, DOI 10.4304/jcp.1.7.51-59
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Huang PS, 1999, ARTIF INTELL ENG, V13, P359, DOI 10.1016/S0954-1810(99)00008-4
   Jankovic J, 2008, J NEUROL NEUROSUR PS, V79, P368, DOI 10.1136/jnnp.2007.131045
   Jolliffe I.T., 2002, PRINCIPAL COMPONENT, V2nd
   Kohle M, 1997, COMP MED SY, P138, DOI 10.1109/CBMS.1997.596423
   LAKANY HM, 1999, P IEE C MOT AN TRACK
   Lin CL, 2009, EXPERT SYST APPL, V36, P12049, DOI 10.1016/j.eswa.2009.03.012
   Lin SH, 2008, J NEUROSURG, V109, P238, DOI 10.3171/JNS/2008/109/8/0238
   Lu H. H. S., 2008, HDB COMPUTATIONAL ST, V3, P813
   Melnick Marsha E, 2002, Rehab Manag, V15, P46
   National Parkinson Fundation, 2009, HOEHN YAHR STAG PARK
   Paulson HL, 2004, MOVEMENT DISORDERS N, P233
   RICHARDS M, 1994, MOVEMENT DISORD, V9, P89, DOI 10.1002/mds.870090114
   Salarian A, 2004, IEEE T BIO-MED ENG, V51, P1434, DOI 10.1109/TBME.2004.827933
   Stebbins GT, 1998, MOVEMENT DISORD, V13, P633, DOI 10.1002/mds.870130404
   YAM CY, 2002, P 5 AS C COMP VIS, P1
   Yang B.-S., 2008, J ADV ENG, V3, P291
   Zhang R, 2007, IMAGE VISION COMPUT, V25, P321, DOI 10.1016/j.imavis.2005.10.007
NR 25
TC 11
Z9 11
U1 0
U2 14
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4174
EI 1873-6793
J9 EXPERT SYST APPL
JI Expert Syst. Appl.
PD JAN
PY 2012
VL 39
IS 1
BP 520
EP 526
DI 10.1016/j.eswa.2011.07.042
PG 7
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Operations Research & Management Science
GA 837PE
UT WOS:000296214900055
DA 2022-02-03
ER

PT J
AU Chatterjee, A
   Bhatia, V
   Prakash, S
AF Chatterjee, Amit
   Bhatia, Vimal
   Prakash, Shashi
TI Anti-spoof touchless 3D fingerprint recognition system using single shot
   fringe projection and biospeckle analysis
SO OPTICS AND LASERS IN ENGINEERING
LA English
DT Article
ID OPTICAL COHERENCE TOMOGRAPHY; SPECKLE ANALYSIS; PROFILOMETRY
AB Fingerprint is a unique, un-alterable and easily collected biometric of a human being. Although it is a 3D biological characteristic, traditional methods are designed to provide only a 2D image. This touch based mapping of 3D shape to 2D image losses information and leads to nonlinear distortions. Moreover, as only topographic details are captured, conventional systems are potentially vulnerable to spoofing materials (e.g. artificial fingers, dead fingers, false prints, etc.). In this work, we demonstrate an anti-spoof touchless 3D fingerprint detection system using a combination of single shot fringe projection and biospeckle analysis. For fingerprint detection using fringe projection, light from a low power LED source illuminates a finger through a sinusoidal grating. The fringe pattern modulated because of features on the fingertip is captured using a CCD camera. Fourier transform method based frequency filtering is used for the reconstruction of 3D fingerprint from the captured fringe pattern. In the next step, for spoof detection using biospeckle analysis a visuo-numeric algorithm based on modified structural function and non-normalized histogram is proposed. High activity biospeckle patterns are generated because of interaction of collimated laser light with internal fluid flow of the real finger sample. This activity reduces abruptly in case of layered fake prints, and is almost absent in dead or fake fingers. Furthermore, the proposed setup is fast, low-cost, involves non-mechanical scanning and is highly stable.
C1 [Chatterjee, Amit; Bhatia, Vimal] Indian Inst Technol, Discipline Elect Engn, Signals & Software Grp, Indore 453446, Madhya Pradesh, India.
   [Prakash, Shashi] Devi Ahilya Univ, Inst Engn & Technol, Dept Elect & Instrumentat Engn, Photon Lab, Khandwa Rd, Indore 452001, Madhya Pradesh, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Indore; Devi Ahilya University
RP Prakash, S (corresponding author), Devi Ahilya Univ, Inst Engn & Technol, Dept Elect & Instrumentat Engn, Photon Lab, Khandwa Rd, Indore 452001, Madhya Pradesh, India.
EM sprakash_davv@rediffmail.com
RI Bhatia, Vimal/AAV-8504-2020; Chatterjee, Amit/AAO-1288-2020
FU Department of Electronics and Information Technology (DeitY), India;
   Science and Engineering Research Board (SERB), Department of Science and
   Technology [EMR/2016/003115]
FX This work is supported by Department of Electronics and Information
   Technology (DeitY), India, and Science and Engineering Research Board
   (SERB), Department of Science and Technology with file number
   EMR/2016/003115. The authors are grateful to the anonymous reviewers for
   several constructive suggestions that have helped increase the quality
   of the work presented.
CR Auksorius E, 2015, BIOMED OPT EXPRESS, V6, P4465, DOI 10.1364/BOE.6.004465
   Bossen A, 2010, IEEE PHOTONIC TECH L, V22, P507, DOI 10.1109/LPT.2010.2041347
   Cheng YZ, 2006, APPL OPTICS, V45, P9238, DOI 10.1364/AO.45.009238
   Dhanotia J, 2016, APPL OPTICS, V55, P5316, DOI 10.1364/AO.55.005316
   Dolinak D, 2006, FORENSIC PATHOLOGY P
   Godinho RP, 2012, OPT LASER ENG, V50, P366, DOI 10.1016/j.optlaseng.2011.10.023
   Gorthi SS, 2010, OPT LASER ENG, V48, P133, DOI 10.1016/j.optlaseng.2009.09.001
   HALLIDAY AM, 1956, J PHYSIOL-LONDON, V134, P600, DOI 10.1113/jphysiol.1956.sp005668
   Hawthorne M. R., 2009, FINGERPRINTS ANAL UN, V1st
   Hong L, 1998, IEEE T PATTERN ANAL, V20, P777, DOI 10.1109/34.709565
   Huang SJ, 2014, OPT LASER ENG, V52, P123, DOI 10.1016/j.optlaseng.2013.07.001
   Jayanthy AK, 2011, OPT LASER ENG, V49, P553, DOI 10.1016/j.optlaseng.2010.12.003
   Jie Y, 2006, PATTERN RECOGN, V39, P143, DOI 10.1016/j.patcog.2005.08.005
   Kemao Q., 2015, OPT LASER ENG, V66, P67
   Labati RD, 2015, IEEE T SYS MAN CYB S, V46, P01
   Liu GJ, 2013, APPL OPTICS, V52, P5473, DOI 10.1364/AO.52.005473
   Menning G, 1998, MOLD MAKING HDB PLAS
   Moreira J, 2014, OPT LASER ENG, V61, P8, DOI 10.1016/j.optlaseng.2014.04.005
   Pajuelo M, 2003, OPT LASER ENG, V40, P13, DOI 10.1016/S0143-8166(02)00063-5
   Parziale G, 2006, INT C BIOM, P224
   Rajsekhar G, 2012, OPT LASERS ENGG, V50, piii
   Rollins AM, 2002, J BIOMED OPT, V7, P123, DOI 10.1117/1.1428291
   Schafer E A, 1886, J Physiol, V7, P111
   Stoykova E, 2015, OPT EXPRESS, V23, P25128, DOI 10.1364/OE.23.025128
   Su XY, 2010, OPT LASER ENG, V48, P191, DOI 10.1016/j.optlaseng.2009.03.012
   Su XY, 2001, OPT LASER ENG, V35, P263, DOI 10.1016/S0143-8166(01)00023-9
   Tripathi MM, 2014, BIOMED OPT EXPRESS, V5, P817, DOI 10.1364/BOE.5.000817
   Wang Y, 2010, MODELLING SIMULATION, P1
   Zdunek A, 2014, OPT LASER ENG, V52, P276, DOI 10.1016/j.optlaseng.2013.06.017
NR 29
TC 26
Z9 26
U1 0
U2 33
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0143-8166
EI 1873-0302
J9 OPT LASER ENG
JI Opt. Lasers Eng.
PD AUG
PY 2017
VL 95
BP 1
EP 7
DI 10.1016/j.optlaseng.2017.03.007
PG 7
WC Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Optics
GA EV3TO
UT WOS:000401682200001
DA 2022-02-03
ER

PT J
AU Roy, A
   Sural, S
   Mukherjee, J
AF Roy, Aditi
   Sural, Shamik
   Mukherjee, Jayanta
TI A hierarchical method combining gait and phase of motion with
   spatiotemporal model for person re-identification
SO PATTERN RECOGNITION LETTERS
LA English
DT Article
DE Re-identification; Gait; Pose Energy Image; Phase of motion; Dynamic
   programing; Spatiotemporal model
ID ENERGY IMAGE; RECOGNITION; TRACKING; IDENTIFICATION; SURVEILLANCE;
   CAMERAS; HUMANS; FUSION; VIEWS
AB Re-identification refers to the problem of establishing correspondence among various observations of the same subject viewed at different time instances in different camera positions. We propose a hierarchical approach for re-identifying a subject by combining gait with phase of motion and a spatiotemporal model. The fundamental nature of the gait biometric of being amenable to capturing from a distance even at low resolution without active co-operation of subjects, has motivated us to use it for re-identification. We use two features related to a subject's motion dynamics, one is his exit/entry phase of motion and the other is his gait signature. An additional third feature is obtained from the spatiotemporal model of the camera network which is learnt during the training phase in the form of a multivariate probability density of space-time variables (entry/exit location, exit velocity, and inter-camera travel time) using kernel density estimation. Once all these three features have been computed, correspondences are established by dynamic programing based maximum likelihood (ML) estimation. The performance of our method has been evaluated on a real data set featuring a two-camera and a three-camera network in a hallway monitoring situation. The proposed approach shows promising results on both the data sets. (c) 2012 Elsevier B.V. All rights reserved.
C1 [Roy, Aditi; Sural, Shamik] Indian Inst Technol Kharagpur, Sch Informat Technol, Kharagpur, W Bengal, India.
   [Mukherjee, Jayanta] Indian Inst Technol Kharagpur, Dept CSE, Kharagpur, W Bengal, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Kharagpur; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Kharagpur
RP Roy, A (corresponding author), Indian Inst Technol Kharagpur, Sch Informat Technol, Kharagpur, W Bengal, India.
EM aditi.roy@sit.iitkgp.ernet.in; shamik@cse.iitkgp.ernet.in;
   jay@cse.iitkgp.ernet.in
FU Ministry of Communications and Information Technology, Govt. of India
   [1(4)/2009-METMD]; Council of Scientific and Industrial Research, Govt.
   of IndiaCouncil of Scientific & Industrial Research (CSIR) - India; 
   [22(0554)/11/EMR-11]
FX This work is funded by Ministry of Communications and Information
   Technology, Govt. of India under project Grant No. 1(4)/2009-ME&TMD and
   also by project Grant No. 22(0554)/11/EMR-11 sponsored by the Council of
   Scientific and Industrial Research, Govt. of India.
CR Boulgouris NV, 2005, IEEE SIGNAL PROC MAG, V22, P78, DOI 10.1109/MSP.2005.1550191
   Chen CH, 2009, PATTERN RECOGN LETT, V30, P977, DOI 10.1016/j.patrec.2009.04.012
   Chen CH, 2009, IEEE T SYST MAN CY C, V39, P114, DOI 10.1109/TSMCC.2008.2001716
   Collins RT, 2001, P IEEE, V89, P1456, DOI 10.1109/5.959341
   Cunado D, 2003, COMPUT VIS IMAGE UND, V90, P1, DOI 10.1010/SI077-3142(03)00008-0
   De Marsico M., 2010, P IEEE INT C IM PROC, P26
   Doretto G., 2011, J AMB INTEL HUM COMP, V2, P1
   Duda R., 2000, PATTERN CLASSIFICATI
   Grgic M, 2011, MULTIMED TOOLS APPL, V51, P863, DOI 10.1007/s11042-009-0417-2
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   Huang XX, 2009, INT CONF ACOUST SPEE, P1469, DOI 10.1109/ICASSP.2009.4959872
   Javed O, 2008, COMPUT VIS IMAGE UND, V109, P146, DOI 10.1016/j.cviu.2007.01.003
   Kale A, 2004, IEEE T IMAGE PROCESS, V13, P1163, DOI 10.1109/TIP.2004.832865
   Kettnaker V, 1999, CVPR, V2, P2544, DOI [10.1109/CVPR.1999.784638, DOI 10.1109/CVPR.1999.784638]
   Khan SM, 2006, LECT NOTES COMPUT SC, V3954, P133
   Lu JW, 2007, PATTERN RECOGN LETT, V28, P2401, DOI 10.1016/j.patrec.2007.08.004
   Makris D, 2004, PROC CVPR IEEE, P205
   Nixon MS, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P139, DOI 10.1109/AFGR.2004.1301521
   Park U, 2006, INT C PATT RECOG, P1204
   Pham T., 2007, P 1 AC IEEE INT C SM, P25
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rahimi A, 2004, PROC CVPR IEEE, P187
   Ran Y, 2010, IEEE T SYST MAN CY B, V40, P1009, DOI 10.1109/TSMCB.2010.2044173
   Rasmussen C, 1998, PROC CVPR IEEE, P16, DOI 10.1109/CVPR.1998.698582
   Roy A., SIGNAL PROC IN PRESS
   Sarkar S, 2005, IEEE T PATTERN ANAL, V27, P162, DOI 10.1109/TPAMI.2005.39
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Stauffer C., 2005, P IEEE WORKSH MOT VI, V2, P96
   Veeraraghavan A, 2009, IEEE T IMAGE PROCESS, V18, P1326, DOI 10.1109/TIP.2009.2017143
   Wechsler Harry, 2010, Intelligent Information Management, V2, P499, DOI 10.4236/iim.2010.29060
   Yang XC, 2008, SIGNAL PROCESS, V88, P2350, DOI 10.1016/j.sigpro.2008.03.006
   Zhang E, 2010, SIGNAL PROCESS, V90, P2295, DOI 10.1016/j.sigpro.2010.01.024
   Zhang R, 2007, IMAGE VISION COMPUT, V25, P321, DOI 10.1016/j.imavis.2005.10.007
NR 33
TC 23
Z9 25
U1 0
U2 14
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD OCT 15
PY 2012
VL 33
IS 14
SI SI
BP 1891
EP 1901
DI 10.1016/j.patrec.2012.02.003
PG 11
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 010HI
UT WOS:000309085100009
DA 2022-02-03
ER

PT C
AU Mohammad, AS
   Al-Ani, JA
AF Mohammad, Ahmad Saeed
   Al-Ani, Jabir Alshehabi
GP IEEE
TI Convolutional Neural Network for Ethnicity Classification using Ocular
   Region in Mobile Environment<bold> </bold>
SO 2018 10TH COMPUTER SCIENCE AND ELECTRONIC ENGINEERING CONFERENCE (CEEC)
SE Computer Science and Electronic Engineering Conference
LA English
DT Proceedings Paper
CT 10th Computer Science and Electronic Engineering Conference (CEEC)
CY SEP 19-21, 2018
CL Univ Essex, Colchester, ENGLAND
HO Univ Essex
ID IRIS RECOGNITION
AB Smart Phones are the most widely spread devices over the world. Many people rely on their phones to store sensitive data like financial, personal, or even private photos for family. Thus, the security in mobile environment became one of the essential needs nowadays. In this paper, we presented a new approach to support security on mobile environment using ethnicity biometric. Accordingly, we present six different models that the squeezed models competes to work in mobile environment. These models were designed as a convolutional Neural Network (CNN) to classify five different ethnicities after choosing the desired Region Of Interest (ROI) as extended ocular region from the facial images of the standard dataset FERET. The highest performance CNN model (Model-02) had a classification accuracy of 98.35% with 25, 941 parameters while the squeezed CNN models (Model-05) shows a classification accuracy of 97.35% with just a 8, 117 parameters. The reported results indicated a gain of 68.7% of parameters reduction with 0.52% of loss in accuracy. This will enable the proposed model (Model-05) to work smoothly and efficiently on mobile environment.<bold> </bold>
C1 [Mohammad, Ahmad Saeed] Univ Missouri, Sch Comp & Engn, Kansas City, MO 64110 USA.
   [Mohammad, Ahmad Saeed] Mustansiriyah Univ, Coll Engn, Baghdad, Iraq.
   [Al-Ani, Jabir Alshehabi] Univ Essex, Sch Comp Sci & Elect Engn, Colchester CO4 3SQ, Essex, England.
C3 University of Missouri System; University of Missouri Kansas City;
   Mustansiriya University; University of Essex
RP Mohammad, AS (corresponding author), Univ Missouri, Sch Comp & Engn, Kansas City, MO 64110 USA.; Mohammad, AS (corresponding author), Mustansiriyah Univ, Coll Engn, Baghdad, Iraq.
EM asm2x9@mail.umkc.edu; jajals@essex.ac.uk
RI Al-Ani, Jabir Alshehabi/AAW-1809-2021
OI Al-Ani, Jabir Alshehabi/0000-0002-0553-2538; Mohammad, Ahmad
   Saeed/0000-0001-6141-2605
CR Aginako N, 2017, PATTERN RECOGN LETT, V91, P52, DOI 10.1016/j.patrec.2017.01.021
   Ciresan D. C., 2011, FLEXIBLE HIGH PERFOR, P1237, DOI 10.5591/978-1-57735-516-8/IJCAI11-210
   Cook Steve, 2017, Biometric Technology Today, V2017, P9, DOI 10.1016/S0969-4765(17)30056-5
   De Luca A, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1411, DOI 10.1145/2702123.2702141
   Feng Q, 2018, FUTURE GENER COMP SY, V84, P239, DOI 10.1016/j.future.2017.07.040
   Galdi C, 2017, PATTERN RECOGN LETT, V91, P44, DOI 10.1016/j.patrec.2017.01.023
   Huang D, 2014, IMAGE VISION COMPUT, V32, P1181, DOI 10.1016/j.imavis.2014.06.009
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Mohamed Ahmed, 2017, 2017 IEEE Power & Energy Society General Meeting, DOI 10.1109/PESGM.2017.8274563
   Mohammad Ahmad Saeed, 2017, 2017 9th Computer Science and Electronic Engineering (CEEC). Proceedings, P219, DOI 10.1109/CEEC.2017.8101628
   Mohammad A. S., 2018, J COMPUTING SCI COLL, V33, P136
   Pereira T. D. F., 2015, PERIOCULAR BIOMETRIC
   Raja KB, 2017, PATTERN RECOGN LETT, V91, P27, DOI 10.1016/j.patrec.2016.12.025
   Raja KB, 2015, PATTERN RECOGN LETT, V57, P33, DOI 10.1016/j.patrec.2014.09.006
   Samangouei P., 2015, P IEEE 7 INT C BIOM, P1
   2015, 2015 INT C BIOM ICB, P535
NR 16
TC 1
Z9 1
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2472-1530
BN 978-1-5386-7275-4
J9 COMPUT SCI ELECTR
PY 2018
BP 293
EP 298
PG 6
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BM7CB
UT WOS:000467722000055
DA 2022-02-03
ER

PT J
AU Arashloo, SR
AF Arashloo, Shervin Rahimzadeh
TI Matrix-Regularized One-Class Multiple Kernel Learning for Unseen Face
   Presentation Attack Detection
SO IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY
LA English
DT Article
DE Unseen face presentation attack detection; one-class Fisher null
   projection; multiple kernel learning; matrix regularisation; zero-shot
   learning
ID ONE-CLASS CLASSIFICATION; SPOOFING DETECTION; RECOGNITION
AB The functionality of face biometric systems is severely challenged by presentation attacks (PA's), and especially those attacks that have not been available during the training phase of a PA detection (PAD) subsystem. Among other alternatives, the one-class classification (OCC) paradigm is an applicable strategy that has been observed to provide good generalisation against unseen attacks. Following an OCC approach for the unseen face PAD from RGB images, this work advocates a matrix-regularised multiple kernel learning algorithm to make use of several sources of information each constituting a different view of the face PAD problem. In particular, drawing on the one-class null Fisher classification principle, we characterise different deep CNN representations as kernels and propose a multiple kernel learning (MKL) algorithm subject to an (r, p)-norm (1 <= r, p) matrix regularisation constraint. The propose MKL algorithm is formulated as a saddle point Lagrangian optimisation task for which we present an effective optimisation algorithm with guaranteed convergence. An evaluation of the proposed one-class MKL algorithm on both general object images in an OCC setting as well as on different face PAD datasets in an unseen zero-shot attack detection setting illustrates the merits of the proposed method compared to other one-class multiple kernel and deep end-to-end CNN-based methods.
C1 [Arashloo, Shervin Rahimzadeh] Bilkent Univ, Dept Comp Engn, Fac Engn, TR-06800 Ankara, Turkey.
C3 Ihsan Dogramaci Bilkent University
RP Arashloo, SR (corresponding author), Bilkent Univ, Dept Comp Engn, Fac Engn, TR-06800 Ankara, Turkey.
EM s.rahimzadeh@cs.bilkent.edu.tr
CR Alpaydin E, 2014, ADAPT COMPUT MACH LE, P1
   [Anonymous], 2017, 3010732017 ISOIEC
   Arashloo SR, 2021, IEEE T CIRC SYST VID, V31, P4084, DOI 10.1109/TCSVT.2020.3046505
   Arashloo SR, 2021, IEEE T NEUR NET LEAR, V32, P999, DOI 10.1109/TNNLS.2020.2979823
   Arashloo SR, 2017, IEEE ACCESS, V5, P13868, DOI 10.1109/ACCESS.2017.2729161
   Arashloo SR, 2014, IEEE T INF FOREN SEC, V9, P2100, DOI 10.1109/TIFS.2014.2359587
   Bai J., P INT C LEARN REPR, P2021
   Benner P, 2013, CONCURR COMP-PRACT E, V25, P1170, DOI 10.1002/cpe.2933
   Bhattacharjee S., 2019, RECENT ADV FACE PRES, P207
   Bodesheim P, 2013, PROC CVPR IEEE, P3374, DOI 10.1109/CVPR.2013.433
   Boulkenafet Z, 2018, IMAGE VISION COMPUT, V77, P1, DOI 10.1016/j.imavis.2018.04.007
   Boulkenafet Z, 2015, IEEE IMAGE PROC, P2636, DOI 10.1109/ICIP.2015.7351280
   Boulkenafet Z, 2017, IEEE INT CONF AUTOMA, P612, DOI 10.1109/FG.2017.77
   Boyd S, 2014, CONVEX OPTIMIZATION
   Bucak SS, 2014, IEEE T PATTERN ANAL, V36, P1354, DOI 10.1109/TPAMI.2013.212
   Chalapathy R, 2018, ANOMALY DETECTION US
   Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882
   Chen HN, 2020, IEEE T INF FOREN SEC, V15, P578, DOI 10.1109/TIFS.2019.2922241
   Chen LF, 2000, PATTERN RECOGN, V33, P1713, DOI 10.1016/S0031-3203(99)00139-9
   Chingovska I., 2012, P INT C BIOM SPEC IN, DOI DOI 10.1109/VTCFALL.2012.6399116
   Cortes C., 2009, ADV NEURAL INF PROCE, P396
   Costa-Pazo A., 2016, P INT C BIOM SPEC IN, DOI DOI 10.1109/BIOSIG.2016.7736936
   deSouza G. B., 2018, ARXIV180607492
   Du D., 1995, NONCONVEX OPTIMIZATI
   Edmunds T., 2017, THESIS U GRENOBLE AL
   Edmunds T, 2018, J VIS COMMUN IMAGE R, V50, P314, DOI 10.1016/j.jvcir.2017.12.004
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fatemifar S, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107696
   Fatemifar S, 2019, INT CONF BIOMETR
   Fatemifar S, 2019, INT CONF ACOUST SPEE, P8464, DOI 10.1109/ICASSP.2019.8682253
   Feng H., 2020, ABS200503922 CORR
   Feng LT, 2016, J VIS COMMUN IMAGE R, V38, P451, DOI 10.1016/j.jvcir.2016.03.019
   Ferreira PM, 2019, LECT NOTE INFORM, VP-296
   Gahyun Kim, 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P67, DOI 10.1109/ICB.2012.6199760
   Galbally J, 2014, INT C PATT RECOG, P1173, DOI 10.1109/ICPR.2014.211
   George A, 2020, IEEE T INF FOREN SEC, V15, P42, DOI 10.1109/TIFS.2019.2916652
   Gonen M, 2013, PATTERN RECOGN, V46, P795, DOI 10.1016/j.patcog.2012.09.002
   Gonen M, 2011, J MACH LEARN RES, V12, P2211
   Griffin G., 2007, 7694 CALT PAS
   Gu YF, 2016, IEEE T GEOSCI REMOTE, V54, P3235, DOI 10.1109/TGRS.2015.2514161
   Hadsell Raia, 2006, IEEE C COMP VIS PATT
   Hamdan B, 2018, EGYPT INFORM J, V19, P75, DOI 10.1016/j.eij.2017.10.001
   Han YN, 2018, IEEE T NEUR NET LEAR, V29, P4997, DOI 10.1109/TNNLS.2017.2785329
   Han YN, 2018, IEEE T NEUR NET LEAR, V29, P486, DOI 10.1109/TNNLS.2016.2635151
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heusch G., 2019, REMOTE BLOOD PULSE A, P267
   Hinrichs Chris, 2012, Adv Neural Inf Process Syst, V2012, P1430
   Nguyen HP, 2019, IEEE ACCESS, V7, P175429, DOI 10.1109/ACCESS.2019.2957273
   HOFFMAN JD, 2001, NUMERICAL METHODS EN
   Hoffmann H, 2007, PATTERN RECOGN, V40, P863, DOI 10.1016/j.patcog.2006.07.009
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kemmler M, 2013, PATTERN RECOGN, V46, P3507, DOI 10.1016/j.patcog.2013.06.005
   Khan SS, 2014, KNOWL ENG REV, V29, P345, DOI 10.1017/S026988891300043X
   Killioglu M, 2017, 2017 IEEE 15TH INTERNATIONAL SYMPOSIUM ON APPLIED MACHINE INTELLIGENCE AND INFORMATICS (SAMI), P87, DOI 10.1109/SAMI.2017.7880281
   Kloft M, 2011, J MACH LEARN RES, V12, P953
   Kollreider K, 2009, IMAGE VISION COMPUT, V27, P233, DOI 10.1016/j.imavis.2007.05.004
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Lanckriet GRG, 2004, BIOINFORMATICS, V20, P2626, DOI 10.1093/bioinformatics/bth294
   Li L, 2018, J VIS COMMUN IMAGE R, V54, P182, DOI 10.1016/j.jvcir.2018.05.009
   Lin YY, 2011, IEEE T PATTERN ANAL, V33, P1147, DOI 10.1109/TPAMI.2010.183
   Liu JC, 2017, PROC CVPR IEEE, P4123, DOI 10.1109/CVPR.2017.439
   Liu YJ, 2019, PROC CVPR IEEE, P4675, DOI 10.1109/CVPR.2019.00481
   Loosli G., 2016, USING SVDD SIMPLEMKL
   Ma Ke, 2021, IEEE Trans Pattern Anal Mach Intell, VPP, DOI 10.1109/TPAMI.2021.3087514
   Mao Q, 2015, IEEE T NEUR NET LEAR, V26, P1134, DOI 10.1109/TNNLS.2014.2334137
   Marcel S., 2019, HDB BIOMETRIC ANTISP
   Nikisins O, 2018, INT CONF BIOMETR, P75, DOI 10.1109/ICB2018.2018.00022
   Oza P, 2019, IEEE SIGNAL PROC LET, V26, P277, DOI 10.1109/LSP.2018.2889273
   Peng F, 2018, MULTIMED TOOLS APPL, V77, P8883, DOI 10.1007/s11042-017-4780-0
   Perera P., 2018, ABS180105365 CORR
   Perera P, 2019, IEEE T IMAGE PROCESS, V28, P5450, DOI 10.1109/TIP.2019.2917862
   Perez-Cabo D, 2019, IEEE COMPUT SOC CONF, P1591, DOI 10.1109/CVPRW.2019.00201
   Pinto A, 2015, IEEE T IMAGE PROCESS, V24, P4726, DOI 10.1109/TIP.2015.2466088
   Pinto A, 2015, IEEE T INF FOREN SEC, V10, P1025, DOI 10.1109/TIFS.2015.2395139
   Qin YX, 2020, NEUROCOMPUTING, V417, P384, DOI 10.1016/j.neucom.2020.08.068
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Ramachandra R, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3038924
   Rattani A, 2014, 2014 IEEE/IAPR INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2014)
   Rattani A, 2015, IEEE T INF FOREN SEC, V10, P2447, DOI 10.1109/TIFS.2015.2464772
   Sahidullah M., 2019, INTRO VOICE PRESENTA, P321
   Saleh B, 2013, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2013.107
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/CVPR.2015.7298594
   Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531
   Stewart G. W., 2001, MATRIX ALGORITHMS BA, V1
   Sun W., IEEE T INF FORENSICS, V15, P3181
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   VONNEUMANN J, 1937, ERGEBNISSE MATH K, P00083
   Wang T, 2013, INT CONF BIOMETR
   Wen D, 2015, IEEE T INF FOREN SEC, V10, P746, DOI 10.1109/TIFS.2015.2400395
   Xia H, 2014, IEEE T PATTERN ANAL, V36, P536, DOI 10.1109/TPAMI.2013.149
   Xiao CP, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON POWER AND RENEWABLE ENERGY (ICPRE), P1
   Xiong F, 2018, INT CONF BIOMETR THE
   Yan F, 2012, J MACH LEARN RES, V13, P607
   Yang J., 2014, ARXIV14085601
   Yang JW, 2015, IEEE T INF FOREN SEC, V10, P797, DOI 10.1109/TIFS.2015.2403306
   Yaojie Liu A. J., 2019, ABS190402860 CORR
   Yu DG, 2015, 2015 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P903, DOI 10.1109/GlobalSIP.2015.7418328
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 100
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1556-6013
EI 1556-6021
J9 IEEE T INF FOREN SEC
JI IEEE Trans. Inf. Forensic Secur.
PY 2021
VL 16
BP 4635
EP 4647
DI 10.1109/TIFS.2021.3111766
PG 13
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA WA0BV
UT WOS:000702562700002
DA 2022-02-03
ER

PT J
AU Attia, A
   Akhtar, Z
   Chalabi, NE
   Maza, S
   Chahir, Y
AF Attia, Abdelouahab
   Akhtar, Zahid
   Chalabi, Nour Elhouda
   Maza, Sofiane
   Chahir, Youssef
TI Deep rule-based classifier for finger knuckle pattern recognition system
SO EVOLVING SYSTEMS
LA English
DT Article
DE Deep rule based classifier; BSIF; Gabor filter bank; Finger knuckle
   pattern
AB In this paper, we proposed a novel finger knuckle pattern (FKP) based personal authentication system using multilayer deep rule based (DRB) classifier. The presented approach is completely data-driven and fully automatic. However, the DRB classifier is generic and can be used in variety of classification or prediction problems. In particular, from the input finger knuckle, two kinds of features (i.e., Binarized Statistical Image Features and Gabor Filer bank) are extracted, which are then fed to fuzzy rules based DRB classifier to determine whether the user is genuine or impostor. Experimental results in the form of accuracy, error equal rate (EER) and receiver operating characteristic (ROC) curves demonstrate that presented DRB classifier is a powerful tool in FKP based biometric identification system. Experiments are reported using publicly available FKP PolyU database provided by University of Hong Kong. Experiments using this database show that the presented framework, in this study, can attain performance better than previously proposed methods. Moreover, score level fusion of all FKP modalities with BSIF + DRB yielded an equal error rate of 0.19% and an accuracy of 99.65%.
C1 [Attia, Abdelouahab; Maza, Sofiane] Mohamed El Bachir El Ibrahimi Univ Bordj Bou Arre, LMSE Lab, Bordj Bou Arreridj 34000, Algeria.
   [Akhtar, Zahid] SUNY Albany, Polytech Inst, Albany, NY USA.
   [Attia, Abdelouahab; Chalabi, Nour Elhouda] Mohamed El Bachir El Ibrahimi Univ Bordj Bou Arre, Dept Comp Sci, Bordj Bou Arreridj 34000, Algeria.
   [Chahir, Youssef] Univ Caen, Image Team GREYC, CNRS, UMR, Caen, France.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Albany; Centre National de la Recherche Scientifique (CNRS);
   Normandie Universite; Universite de Caen Normandie
RP Attia, A (corresponding author), Mohamed El Bachir El Ibrahimi Univ Bordj Bou Arre, LMSE Lab, Bordj Bou Arreridj 34000, Algeria.
EM attia.abdelouahab@gmail.com
RI Maza, Sofiane/ABF-1700-2021
OI ATTIA, Abdelouahab/0000-0003-1558-7273
CR Adeoye OS., 2010, INT J COMPUT APPL, V9, P1
   Akhtar Z, 2011, LECT NOTES COMPUT SC, V6978, P159, DOI 10.1007/978-3-642-24085-0_17
   Akhter Z., 2011, Proceedings of the 2011 International Symposium on Electronic System Design (ISED 2011), P1, DOI 10.1109/ISED.2011.24
   Angelov P, 2016, CHALLENGES DEEP LEAR, P489
   Angelov P, 2020, NEURAL NETWORKS, V130, P185, DOI 10.1016/j.neunet.2020.07.010
   Angelov P, 2012, INT J GEN SYST, V41, P163, DOI 10.1080/03081079.2011.634807
   Angelov PP, 2018, INFORM SCIENCES, V463, P196, DOI 10.1016/j.ins.2018.06.048
   Angelov PP, 2017, AUTONOMOUS LEARNING
   Aoyama S, 2014, INFORM SCIENCES, V268, P53, DOI 10.1016/j.ins.2013.08.025
   Attia A, 2018, EVOL SYST, P1
   Bao RJ, 2018, IEEE T FUZZY SYST, V26, P1324, DOI 10.1109/TFUZZ.2017.2719619
   Cappelli R, 2010, IEEE T PATTERN ANAL, V32, P2128, DOI 10.1109/TPAMI.2010.52
   Chaa M, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.1.013018
   Chalabi N. E., 2020, ICTACT J IMAGE VIDEO, V10, P2153
   Chlaoua R, 2019, EVOL SYST-GER, V10, P261, DOI 10.1007/s12530-018-9227-y
   El-Tarhouni W, 2014, INT C MICROELECTRON, P184, DOI 10.1109/ICM.2014.7071837
   Ferrer MA, 2005, CAR C SECUR, P74, DOI 10.1109/CCST.2005.1594835
   Gu XW, 2018, APPL SOFT COMPUT, V68, P53, DOI 10.1016/j.asoc.2018.03.032
   Gu XW, 2018, IEEE GEOSCI REMOTE S, V15, P345, DOI 10.1109/LGRS.2017.2787421
   Hammouche R., 2020, ICTACT J IMAGE VIDEO, V10, P2125
   Heidari H, 2020, TURK J ELECTR ENG CO, V28, P238, DOI 10.3906/elk-1906-12
   Jaswal G, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON IDENTITY, SECURITY AND BEHAVIOR ANALYSIS (ISBA)
   Jaswal G, 2017, MULTIMED TOOLS APPL, V76, P18955, DOI 10.1007/s11042-017-4475-6
   Jaswal G, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2938727
   Kannala J, 2012, INT C PATT RECOG, P1363
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Malarvizhi N, 2019, MULTIMED TOOLS APPL, P1
   Muthukumar A, 2019, PATTERN RECOGN LETT, V125, P150, DOI 10.1016/j.patrec.2019.04.007
   PolyU, 2010, HONG KONG POL U POLY
   Qian JJ, 2016, NEUROCOMPUTING, V213, P162, DOI 10.1016/j.neucom.2015.11.135
   Sargano AB, 2017, IEEE IJCNN, P463, DOI 10.1109/IJCNN.2017.7965890
   Shariatmadar ZS, 2013, J CIRCUIT SYST COMP, V22, DOI 10.1142/S0218126613500503
   Shen LL, 2006, PATTERN ANAL APPL, V9, P273, DOI 10.1007/s10044-006-0033-y
   Singh S, 2019, FKP IRIS BASED MULTI
   Soltanpour S, 2017, PATTERN RECOGN, V72, P391, DOI 10.1016/j.patcog.2017.08.003
   Thapar D, 2019, IEEE IJCNN
   Wang R., 2014, ARXIV14095188
   Woodard DL, 2005, COMPUT VIS IMAGE UND, V100, P357, DOI 10.1016/j.cviu.2005.06.003
   Zeinali B, 2014, IRAN CONF ELECTR ENG, P500, DOI 10.1109/IranianCEE.2014.6999594
   Zhai YK, 2018, LECT NOTES COMPUT SC, V10996, P11, DOI 10.1007/978-3-319-97909-0_2
   Zhang D., 2018, ADV BIOMETRICS, P85, DOI [10.1007/978-3-319-61545-5_5, DOI 10.1007/978-3-319-61545-5_5]
   Zhang L, 2012, PATTERN RECOGN, V45, P2522, DOI 10.1016/j.patcog.2012.01.017
   Zhang L, 2011, PATTERN RECOGN, V44, P1990, DOI 10.1016/j.patcog.2010.06.007
NR 43
TC 4
Z9 4
U1 0
U2 1
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 1868-6478
EI 1868-6486
J9 EVOL SYST-GER
JI Evol. Syst.
PD DEC
PY 2021
VL 12
IS 4
BP 1015
EP 1029
DI 10.1007/s12530-020-09359-w
EA OCT 2020
PG 15
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WP2PQ
UT WOS:000582763300001
OA Green Submitted
DA 2022-02-03
ER

PT J
AU Chrisinger, BW
   King, AC
AF Chrisinger, Benjamin W.
   King, Abby C.
TI Stress experiences in neighborhood and social environments (SENSE): a
   pilot study to integrate the quantified self with citizen science to
   improve the built environment and health
SO INTERNATIONAL JOURNAL OF HEALTH GEOGRAPHICS
LA English
DT Article
DE Citizen science; Quantified self; Chronic stress; Allostatic load;
   Electrodermal activity; Built environment; Sensors; Leaflet
ID ALLOSTATIC LOAD; NOISE; IMPACT
AB Background: Identifying elements of one's environment-observable and unobservable-that contribute to chronic stress including the perception of comfort and discomfort associated with different settings, presents many methodological and analytical challenges. However, it also presents an opportunity to engage the public in collecting and analyzing their own geospatial and biometric data to increase community member understanding of their local environments and activate potential environmental improvements. In this first-generation project, we developed a methodology to integrate geospatial technology with biometric sensing within a previously developed, evidence-based "citizen science" protocol, called "Our Voice." Participants used a smartphone/tablet-based application, called the Discovery Tool (DT), to collect photos and audio narratives about elements of the built environment that contributed to or detracted from their well-being. A wrist-worn sensor (Empatica E4) was used to collect time-stamped data, including 3-axis accelerometry, skin temperature, blood volume pressure, heart rate, heartbeat inter-beat interval, and electrodermal activity (EDA). Open-source R packages were employed to automatically organize, clean, geocode, and visualize the biometric data.
   Results: In total, 14 adults (8 women, 6 men) were successfully recruited to participate in the investigation. Participants recorded 174 images and 124 audio files with the DT. Among captured images with a participant-determined positive or negative rating (n = 131), over half were positive (58.8%, n = 77). Within-participant positive/negative rating ratios were similar, with most participants rating 53.0% of their images as positive (SD 21.4%). Significant spatial clusters of positive and negative photos were identified using the Getis-Ord Gi* local statistic, and significant associations between participant EDA and distance to DT photos, and street and land use characteristics were also observed with linear mixed models. Interactive data maps allowed participants to (1) reflect on data collected during the neighborhood walk, (2) see how EDA levels changed over the course of the walk in relation to objective neighborhood features (using basemap and DT app photos), and (3) compare their data to other participants along the same route.
   Conclusions: Participants identified a variety of social and environmental features that contributed to or detracted from their well-being. This initial investigation sets the stage for further research combining qualitative and quantitative data capture and interpretation to identify objective and perceived elements of the built environment influence our embodied experience in different settings. It provides a systematic process for simultaneously collecting multiple kinds of data, and lays a foundation for future statistical and spatial analyses in addition to more in-depth interpretation of how these responses vary within and between individuals.
C1 [Chrisinger, Benjamin W.; King, Abby C.] Stanford Univ, Sch Med, Dept Med, Stanford Prevent Res Ctr, 1070 Arastradero Rd,Suite 100, Palo Alto, CA 94304 USA.
   [King, Abby C.] Stanford Univ, Sch Med, Dept Hlth Res & Policy, Palo Alto, CA 94304 USA.
C3 Stanford University; Stanford University
RP Chrisinger, BW (corresponding author), Stanford Univ, Sch Med, Dept Med, Stanford Prevent Res Ctr, 1070 Arastradero Rd,Suite 100, Palo Alto, CA 94304 USA.
EM bchris@stanford.edu
RI King, Abby/AAD-5257-2021
OI Chrisinger, Benjamin/0000-0002-1480-6481; King, Abby/0000-0002-7949-8811
FU Stanford Clinical and Translational Science Award (CTSA)United States
   Department of Health & Human ServicesNational Institutes of Health (NIH)
   - USANIH National Center for Advancing Translational Sciences (NCATS)
   [UL1 TR001085]; NIH/NHLBIUnited States Department of Health & Human
   ServicesNational Institutes of Health (NIH) - USANIH National Heart Lung
   & Blood Institute (NHLBI) [T32 HL007034]; NATIONAL CENTER FOR ADVANCING
   TRANSLATIONAL SCIENCESUnited States Department of Health & Human
   ServicesNational Institutes of Health (NIH) - USANIH National Center for
   Advancing Translational Sciences (NCATS) [UL1TR001085] Funding Source:
   NIH RePORTER; NATIONAL HEART, LUNG, AND BLOOD INSTITUTEUnited States
   Department of Health & Human ServicesNational Institutes of Health (NIH)
   - USANIH National Heart Lung & Blood Institute (NHLBI) [T32HL007034]
   Funding Source: NIH RePORTER
FX This work was supported by the Stanford Clinical and Translational
   Science Award (CTSA) to Spectrum (UL1 TR001085). The CTSA program is led
   by the National Center for Advancing Translational Sciences (NCATS) at
   the National Institutes of Health (NIH). The content is solely the
   responsibility of the authors and does not necessarily represent the
   official views of the NIH. Dr. Chrisinger was also supported by an
   NIH/NHLBI institutional postdoctoral training grant (T32 HL007034).
CR Althoff T, 2017, NATURE, V547, P336, DOI 10.1038/nature23018
   [Anonymous], HOT SPOT AN GET ORD
   [Anonymous], LAND US DATASF CIT C
   [Anonymous], GAL TAB E LIT 7 0 8
   Aspinall P, 2013, BRIT J SPORT MED, P1, DOI [10.1136/bjsports-2012-091877, DOI 10.1136/BJSPORTS-2012-091877]
   Barrett MA, 2013, BIG DATA-US, V1, P168, DOI 10.1089/big.2013.0027
   Bates D, J STAT SOFTW
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Belon AP, 1982, SOC SCI MED, V2016, P18
   Buman MP, 2015, PUBLIC HEALTH NUTR, V18, P994, DOI 10.1017/S136898001400127X
   Buman Matthew P, 2013, Am J Prev Med, V44, pe41, DOI 10.1016/j.amepre.2012.11.028
   Carlson JA, 2015, MED SCI SPORT EXER, V47, P662, DOI 10.1249/MSS.0000000000000446
   Cheng J., 2017, LEAFLET CREATE INTER
   Chrisinger BW, 2018, FRONT PUBLIC HEALTH, V6, DOI 10.3389/fpubh.2018.00089
   Cohen S, 1982, ADV ENVIRON PSYCHOL, P295
   Cushing CC, 2014, J PEDIATR PSYCHOL, V39, P138, DOI 10.1093/jpepsy/jst083
   Dowd JB, 2009, INT J EPIDEMIOL, V38, P1297, DOI 10.1093/ije/dyp277
   Drewnowski A, 2016, BMC PUBLIC HEALTH, V16, DOI 10.1186/s12889-016-3798-y
   Ellis K, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P431, DOI 10.1145/2638728.2641673
   ESRI, 2017, ARCMAP 10 5 1
   Evans GW, 1995, PSYCHOL SCI, V6, P333, DOI 10.1111/j.1467-9280.1995.tb00522.x
   Feinerer I., 2017, TM TEXT MINING PACKA
   GETIS A, 1992, GEOGR ANAL, V24, P189, DOI 10.1111/j.1538-4632.1992.tb00261.x
   Hammer MS, 2014, ENVIRON HEALTH PERSP, V122, P115, DOI 10.1289/ehp.1307272
   Healey JA, 2005, IEEE T INTELL TRANSP, V6, P156, DOI 10.1109/TITS.2005.848368
   Honold J, 2012, J ENVIRON PSYCHOL, V32, P305, DOI 10.1016/j.jenvp.2012.05.002
   Juarez PD, 2014, INT J ENV RES PUB HE, V11, P12866, DOI 10.3390/ijerph111212866
   Juster RP, 2010, NEUROSCI BIOBEHAV R, V35, P2, DOI 10.1016/j.neubiorev.2009.10.002
   Keene DE, 2011, J URBAN HEALTH, V88, P417, DOI 10.1007/s11524-011-9582-5
   King Abby C, 2016, Transl J Am Coll Sports Med, V1, P30
   KONO S, 1988, J SOUND VIB, V127, P573, DOI 10.1016/0022-460X(88)90385-9
   Lang D., 2018, [wordcloud2: create word cloud by 'htmlwidget', R package version 0.2.1]
   Lovallo William R., 2015, STRESS HLTH BIOL PSY
   McCormack GR, 2011, INT J BEHAV NUTR PHY, V8, DOI 10.1186/1479-5868-8-125
   MCEWEN BS, 1993, ARCH INTERN MED, V153, P2093, DOI 10.1001/archinte.153.18.2093
   ORD JK, 1995, GEOGR ANAL, V27, P286, DOI 10.1111/j.1538-4632.1995.tb00912.x
   Paneto GG., 2017, J BUILD CONSTR PLAN, V05, P45, DOI [10.4236/jbcpr.2017.52004, DOI 10.4236/jbcpr.2017.52004, DOI 10.4236/JBCPR.2017.52004]
   Paunovic K, 2009, SCI TOTAL ENVIRON, V407, P3707, DOI 10.1016/j.scitotenv.2009.02.033
   Resch B, 2013, PROGR LOCATION BASED, P391, DOI DOI 10.1007/978-3-642-34203-5_22
   Roe JJ, 2013, ENVIRON SCI-TOKYO, V1, P93, DOI DOI 10.12988/ES.2013.3109
   Ruginski I, 2017, VISUALIZING INTERACT
   Sheats JL, 2014, ANN BEHAV MED, V47
   Shull PB, 2014, GAIT POSTURE, V40, P11, DOI 10.1016/j.gaitpost.2014.03.189
   Stansfeld S., 2011, REV ENV HLTH, V15, P43
   Swan M, 2012, J SENS ACTUAR NETW, V1, P217, DOI 10.3390/jsan1030217
   Szeto I, 2017, OUR VOICE DISCOVERY
   Taylor S., 2015, EMBC
   Theall KP, 2012, AM J EPIDEMIOL, V176, pS164, DOI 10.1093/aje/kws185
   Wand M, 1995, R PORT KERNSMOOTH FU
   Wang C. C., 2000, HLTH PROMOTION PRACT, V1, P81, DOI [10.1177/152483990000100113, DOI 10.1177/152483990000100113]
   Whooley M., 2014, P 28 INT BCS HUM COM, P151, DOI DOI 10.14236/EWIC/HCI2014.16
   Winter SJ, 2016, J IMMIGR MINOR HEALT, V18, P1126, DOI 10.1007/s10903-015-0241-x
   Zeile P., 2016, GI FORUM, V1, P204, DOI DOI 10.1553/GISCIENCE2016_01_S204
   Zeile P, 2015, LECT NOTES GEOINF CA, P209, DOI 10.1007/978-3-319-18368-8_11
NR 54
TC 26
Z9 26
U1 4
U2 28
PU BMC
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON N1 9XW, ENGLAND
SN 1476-072X
J9 INT J HEALTH GEOGR
JI Int. J. Health Geogr.
PD JUN 5
PY 2018
VL 17
AR 17
DI 10.1186/s12942-018-0140-1
PG 13
WC Public, Environmental & Occupational Health
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Public, Environmental & Occupational Health
GA GI4NP
UT WOS:000434348700001
PM 29871687
OA Green Published, gold
DA 2022-02-03
ER

PT J
AU Al Maadeed, S
   Jiang, XD
   Rida, I
   Bouridane, A
AF Al Maadeed, Somaya
   Jiang, Xudong
   Rida, Imad
   Bouridane, Ahmed
TI Palmprint identification using sparse and dense hybrid representation
SO MULTIMEDIA TOOLS AND APPLICATIONS
LA English
DT Article
DE Biometric; Palmprint; Sparse representation for classification
ID ROBUST FACE RECOGNITION; EXTRACTION; SELECTION; FEATURES; SYSTEM;
   SCHEME; DCT
AB Among various palmprint identification methods proposed in the literature, Sparse Representation for Classification (SRC) is very attractive, offering high accuracy. Although SRC has good discriminative ability, its performance strongly depends on the quality of the training data. In fact, palmprint images do not only contain identity information but they also have other information such as illumination and distortions due the acquisition conditions. In this case, SRC may not be able to classify the identity of palmprint well in the original space since samples from the same class show large variations. To overcome this problem, we propose in this work to exploit sparse-and-dense hybrid representation (SDR) for palmprint identification. Indeed, this type of representations that are based on the dictionary learning from the training data has shown its great advantage to overcome the limitations of SRC. Extensive experiments are conducted on two publicly available palmprint datasets: multispectral and PolyU. The obtained results clearly show the ability of the proposed method to outperform both the state-of-the-art holistic approaches and the coding palmprint identification methods.
C1 [Al Maadeed, Somaya; Rida, Imad] Qatar Univ, Dept Comp Sci & Engn, Doha, Qatar.
   [Jiang, Xudong] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Bouridane, Ahmed] Northumbria Univ, Dept Comp Sci & Digital Technol, Newcastle Upon Tyne, Tyne & Wear, England.
C3 Qatar University; Nanyang Technological University & National Institute
   of Education (NIE) Singapore; Nanyang Technological University;
   Northumbria University
RP Al Maadeed, S (corresponding author), Qatar Univ, Dept Comp Sci & Engn, Doha, Qatar.
EM S.alali@qu.edu.qa; exdjiang@ntu.edu.sg; rida.imad@gmail.com;
   ahmed.bouridane@northumbria.ac.uk
RI Rida, Imad/AAA-5044-2022; Jiang, Xudong/B-1555-2008
OI Rida, Imad/0000-0003-2789-5070; Jiang, Xudong/0000-0002-9104-2315;
   Al-maadeed, Somaya/0000-0002-0241-2899
FU Qatar National Research Fund through National Priority Research Program
   (NPRP) [6-249-1-053]
FX This publication was made possible using a grant from the Qatar National
   Research Fund through National Priority Research Program (NPRP) No.
   6-249-1-053. The contents of this publication are solely the
   responsibility of the authors and do not necessarily represent the
   official views of the Qatar National Research Fund or Qatar University.
CR BADRINATH GS, 2008, 1 WORKSH IM PROC THE, P1, DOI DOI 10.1109/IPTA.2008.4743763
   Bertsekas DP., 2014, CONSTRAINED OPTIMIZA
   Charfi N, 2017, MULTIMED TOOLS APPL, V76, P20457, DOI 10.1007/s11042-016-3987-9
   Connie T, 2005, IMAGE VISION COMPUT, V23, P501, DOI 10.1016/j.imavis.2005.01.002
   Cui JR, 2015, MULTIMED TOOLS APPL, V74, P10989, DOI 10.1007/s11042-014-1887-4
   De Marsico M, 2013, IEEE T SYST MAN CY-S, V43, P149, DOI 10.1109/TSMCA.2012.2192427
   Fei LK, 2017, INT J IMAGE GRAPH, V17, DOI 10.1142/S0219467817500206
   Fei LK, 2016, NEUROCOMPUTING, V218, P264, DOI 10.1016/j.neucom.2016.08.048
   Fei LK, 2016, PATTERN RECOGN LETT, V69, P35, DOI 10.1016/j.patrec.2015.10.003
   Fei LK, 2016, PATTERN RECOGN, V49, P89, DOI 10.1016/j.patcog.2015.08.001
   Guo XM, 2017, MACH VISION APPL, V28, P283, DOI 10.1007/s00138-017-0821-y
   Guo ZH, 2009, PATTERN RECOGN LETT, V30, P1219, DOI 10.1016/j.patrec.2009.05.010
   Hammami M, 2014, MULTIMED TOOLS APPL, V68, P1023, DOI 10.1007/s11042-012-1109-x
   Han CC, 2003, PATTERN RECOGN, V36, P371
   Hengjian Li, 2012, Proceedings of the 2012 International Conference on Measurement, Information and Control (MIC), P563, DOI 10.1109/MIC.2012.6273448
   Hennings-Yeomans PH, 2007, IEEE T INF FOREN SEC, V2, P613, DOI 10.1109/TIFS.2007.902039
   Hong DF, 2016, NEUROCOMPUTING, V174, P999, DOI 10.1016/j.neucom.2015.10.031
   Hong DF, 2015, NEUROCOMPUTING, V151, P511, DOI 10.1016/j.neucom.2014.09.013
   Hu DW, 2007, PATTERN RECOGN, V40, P339, DOI 10.1016/j.patcog.2006.06.022
   Huang DS, 2008, PATTERN RECOGN, V41, P1316, DOI 10.1016/j.patcog.2007.08.016
   Jia W, 2008, PATTERN RECOGN, V41, P1504, DOI 10.1016/j.patcog.2007.10.011
   Jia W, 2017, IEEE T IMAGE PROCESS, V26, P4483, DOI 10.1109/TIP.2017.2705424
   Jiang XD, 2015, IEEE T PATTERN ANAL, V37, P1067, DOI 10.1109/TPAMI.2014.2359453
   Jing XY, 2004, IEEE T SYST MAN CY B, V34, P2405, DOI 10.1109/TSMCB.2004.837586
   Kong A, 2006, PATTERN RECOGN, V39, P478, DOI 10.1016/j.patcog.2005.08.014
   Kong AWK, 2004, INT C PATT RECOG, P520, DOI 10.1109/ICPR.2004.1334184
   Laadjel M, 2015, NEUROCOMPUTING, V152, P179, DOI 10.1016/j.neucom.2014.11.005
   LAI J, 2016, TIP, V25, P3261, DOI DOI 10.1109/TIP.2016.2545249
   Leng L, 2017, MULTIMED TOOLS APPL, V76, P333, DOI 10.1007/s11042-015-3058-7
   Li G, 2017, PATTERN RECOGN, V61, P29, DOI 10.1016/j.patcog.2016.06.025
   Lu GM, 2003, PATTERN RECOGN LETT, V24, P1463, DOI 10.1016/S0167-8655(02)00386-0
   Luo YT, 2016, PATTERN RECOGN, V50, P26, DOI 10.1016/j.patcog.2015.08.025
   Meraoumia A, 2015, MULTIMED TOOLS APPL, V74, P955, DOI 10.1007/s11042-013-1706-3
   Mokni R, 2017, MULTIMED TOOLS APPL, V76, P23981, DOI 10.1007/s11042-016-4088-5
   Mokni R, 2016, J INF ASSUR SECUR, V11, P77
   Mu MR, 2011, NEUROCOMPUTING, V74, P3351, DOI 10.1016/j.neucom.2011.05.026
   Naseem I, 2010, IEEE T PATTERN ANAL, V32, P2106, DOI 10.1109/TPAMI.2010.128
   Raghavendra R, 2015, EURASIP J INF SECUR, DOI 10.1186/s13635-015-0022-z
   Raghavendra R, 2014, PATTERN RECOGN, V47, P2205, DOI 10.1016/j.patcog.2013.12.011
   Rida I, 2018, IEEE ACCESS, V6, P3241, DOI 10.1109/ACCESS.2017.2787666
   Rida I, 2016, IEEE SIGNAL PROC LET, V23, P154, DOI 10.1109/LSP.2015.2507200
   Rida I, 2016, SIGNAL IMAGE VIDEO P, V10, P463, DOI 10.1007/s11760-015-0766-4
   Rigamonti R, 2011, PROC CVPR IEEE, P1545, DOI 10.1109/CVPR.2011.5995313
   Sang HF, 2009, LECT NOTES COMPUT SC, V5552, P831, DOI 10.1007/978-3-642-01510-6_93
   Shi QF, 2011, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2011.5995556
   Srinivas BG, 2009, COMM COM INF SC, V40, P250, DOI 10.1007/978-3-642-03547-0_24
   SUN Z, 1934, TIP, V23, P3922, DOI DOI 10.1109/TIP.2014.2332396
   Sun ZN, 2005, PROC CVPR IEEE, P279
   Tabejamaat M, 2017, MULTIMED TOOLS APPL, P1
   Tabejamaat M, 2017, MULTIMED TOOLS APPL, V76, P9387, DOI 10.1007/s11042-016-3544-6
   Tamrakar D, 2016, J VIS COMMUN IMAGE R, V40, P432, DOI 10.1016/j.jvcir.2016.07.008
   Tamrakar D, 2016, MULTIMED TOOLS APPL, V75, P5777, DOI 10.1007/s11042-015-2541-5
   Tamrakar D, 2015, PROCEDIA COMPUT SCI, V54, P491, DOI 10.1016/j.procs.2015.06.056
   Wang M, 2006, 8 INT C SIGN PROC, V4
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu XQ, 2003, PATTERN RECOGN LETT, V24, P2829, DOI 10.1016/S0167-8655(03)00141-7
   Xu Y, 2013, NEUROCOMPUTING, V103, P164, DOI 10.1016/j.neucom.2012.08.038
   Zhang D, 2003, IEEE T PATTERN ANAL, V25, P1041, DOI 10.1109/TPAMI.2003.1227981
   Zhang D, 2010, IEEE T INSTRUM MEAS, V59, P480, DOI 10.1109/TIM.2009.2028772
   Zhang L, 2015, IEEE T PATTERN ANAL, V37, P1730, DOI 10.1109/TPAMI.2014.2372764
   Zhang L, 2012, IEEE SIGNAL PROC LET, V19, P663, DOI 10.1109/LSP.2012.2211589
   Zheng Q, 2016, IEEE T INF FOREN SEC, V11, P633, DOI 10.1109/TIFS.2015.2503265
   Zuo W., 2008, 19 INT C PATT REC IC, P1, DOI 10.1109/ICPR.2008.4761868
NR 63
TC 17
Z9 17
U1 0
U2 9
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1380-7501
EI 1573-7721
J9 MULTIMED TOOLS APPL
JI Multimed. Tools Appl.
PD MAR
PY 2019
VL 78
IS 5
BP 5665
EP 5679
DI 10.1007/s11042-018-5655-8
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA HT7RW
UT WOS:000464763100032
DA 2022-02-03
ER

PT C
AU Vishal, D
   Afaque, HS
   Vishnu, DS
   Ramesh, TK
AF Vishal, Dasari
   Afaque, H. Saliq
   Vishnu, D. Saketh
   Ramesh, T. K.
GP IEEE
TI A Mobile Integrated Classroom
SO 2017 5TH IEEE INTERNATIONAL CONFERENCE ON MOOCS, INNOVATION AND
   TECHNOLOGY IN EDUCATION (MITE)
LA English
DT Proceedings Paper
CT 5th IEEE International Conference on MOOCs, Innovation and Technology in
   Education (MITE)
CY OCT 27-28, 2017
CL Bangalore, INDIA
DE Mobile platform; Speech to text conversion; Instant Chatting; finger
   print scanners; collaborative learning; attendance using mobile devices;
   smart note
AB Education is the first step to explore the world through knowledge. In an endeavor to make the learning process more engaging through mobile platforms which perhaps attract the students and help them to gain knowledge with interest. Efforts have been done to make the process of online tests and information delivery to the students, yet there is a gap in knowledge transfer between teachers and students. Here we have proposed a platform that utilizes the combination of biometric scanners, speech to text conversion techniques and image processing techniques to make the process of learning effective, interesting, and collaborative. We have added a tool that uses today's modern mobile technology to take attendance and reduce the wastage of valuable time in the class. Also we developed a tool, called Smart notes that have ability to reduce the burden of taking notes on students and worry about the knowledge gap between sections of the same campus. To reach out to every student we have found that the Instant Chatting tool introduced will be helpful to clear all the doubts. This tool can be used to for group discussions and to perform group activities. Hence the platform introduced integrated with three tools provides an excellent quality of education to fulfill every students need in comparison to the existing methods.
C1 [Vishal, Dasari; Afaque, H. Saliq; Vishnu, D. Saketh; Ramesh, T. K.] Amrita Univ, Dept Elect & Commun Engn, Bengaluru, India.
C3 Amrita Vishwa Vidyapeetham; Amrita Vishwa Vidyapeetham Bengaluru
RP Vishal, D (corresponding author), Amrita Univ, Dept Elect & Commun Engn, Bengaluru, India.
EM vishaldasari1996@gmail.com; saliq889.h@gmail.com;
   sakethvishnu.d@gmail.com; tk_ramesh@blr.amrita.edu
OI Dasari, Vishal/0000-0002-5996-2189
CR Ally M., 2014, RUSC-UNIV KNOWL SOC, V11, P142, DOI [10.7238/rusc.v11i1.2033, DOI 10.7238/rusc.v11i1.2033]
   Alves G. R., 2007, REMOTE EXPT INTEGRAT, P102
   Duncan D. K., 2012, ASTRONOMY ED REV, V11, P1, DOI DOI 10.3847/AER2012011
   Gonzalez Alvaro, 2012, PATT REC ICPR 2012 2
   Kumar Manoj, 2011, INT JOURNEL MANAGING, V3
   Mahesh G, 2016, SMART PHONE INTEGRAT
   Naismith L., 2004, LIT REV MOBILE TECHN
   Sajid M, 2014, CONCEPTUAL MODEL AUT
   Xu D., MULTIAGENT BASED E L
   [No title captured]
NR 10
TC 0
Z9 0
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-5386-3189-8
PY 2017
BP 125
EP 130
DI 10.1109/MITE.2017.00028
PG 6
WC Computer Science, Interdisciplinary Applications; Education &
   Educational Research
WE Conference Proceedings Citation Index - Science (CPCI-S); Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)
SC Computer Science; Education & Educational Research
GA BM0AA
UT WOS:000458537800024
DA 2022-02-03
ER

PT J
AU Henrot, P
   Iochum, S
   Batch, T
   Coffinet, L
   Blum, A
   Roland, J
AF Henrot, P
   Iochum, S
   Batch, T
   Coffinet, L
   Blum, A
   Roland, J
TI Current multiplanar imaging of the stapes
SO AMERICAN JOURNAL OF NEURORADIOLOGY
LA English
DT Article
ID TEMPORAL BONE; CT
AB PURPOSE: CT analysis of the stapes is difficult in the axial plane (AP), because of its oblique orientation. Oblique axial reformations could provide a more precise analysis of the stapes in normal and pathologic conditions.
   MATERILS AND METHODS: CT of the temporal bone was performed in 31 patients. Only the normal side was examined in the AP and oblique axial plane (OAP), in the plane of the stapes superstructure. Conspicuousness of each stapes component was evaluated in both planes by 2 independent readers. Reproducibility between the 2 readers (R1 and R2) and comparison of conspicuousness between the AP and the OAP in the analysis of the stapes crura were evaluated. The normal position of the stapes arch in relationship to the footplate was determined in the OAP by using biometric landmarks.
   RESULTS: Conspicuousness of the stapes crura was increased by using OAP. The conspicuousness of the anterior crus was enhanced in 38% with the OAP according to R1 (P <.05) and 32% according to R2 (P <.05). The conspicuousness of the posterior crus was enhanced in 35% with the OAP according to R1 (P <.05), but not significantly enhanced in 22% with the OAP according to R2 (P =.095). Analysis of conspicuousness of the stapes crura was reproducible according to the kappa test.
   A perpendicular line to the footplate intersecting its midportion crosses the stapes head and the long process of the incus in the OAP in normal patients.
   CONCLUSION. OAP could enhance the CT analysis of the stapes and provide useful biometric landmarks in pathologic conditions.
C1 Ctr Alexis Vautrin, Dept Radiol, F-54511 Vandoeuvre Les Nancy, France.
   Univ Nancy 1, CHU Nancy, Dept Radiol Guilloz, Nancy, France.
   Univ Nancy 1, CHU Nancy, Dept Head & Neck Surg, Nancy, France.
C3 UNICANCER; Institut de cancerologie de Lorraine (ICL); CHU de Nancy;
   Universite de Lorraine; CHU de Nancy; Universite de Lorraine
RP Henrot, P (corresponding author), Ctr Alexis Vautrin, Dept Radiol, Ave Bourgogne, F-54511 Vandoeuvre Les Nancy, France.
RI Blum, Alain/ABH-1784-2021
OI Blum, Alain/0000-0002-9980-7052
CR Caldemeyer KS, 1999, AM J ROENTGENOL, V172, P1675, DOI 10.2214/ajr.172.6.10350314
   Lemmerling MM, 1997, RADIOLOGY, V203, P251, DOI 10.1148/radiology.203.1.9122403
   Manolidis S, 2003, ORL J OTO-RHINO-LARY, V65, P71, DOI 10.1159/000070769
   Meriot P, 1997, RADIOGRAPHICS, V17, P1445, DOI 10.1148/radiographics.17.6.9397457
   SWARTZ JD, 1985, ANN OTO RHINOL LARYN, V94, P263
   SWARTZ JD, 1998, IMAGING TEMPORAL BON, P332
   Venema HW, 1999, RADIOLOGY, V213, P375, DOI 10.1148/radiology.213.2.r99nv11375
NR 7
TC 15
Z9 16
U1 0
U2 0
PU AMER SOC NEURORADIOLOGY
PI OAK BROOK
PA 2210 MIDWEST RD, OAK BROOK, IL 60521 USA
SN 0195-6108
J9 AM J NEURORADIOL
JI Am. J. Neuroradiol.
PD SEP
PY 2005
VL 26
IS 8
BP 2128
EP 2133
PG 6
WC Clinical Neurology; Neuroimaging; Radiology, Nuclear Medicine & Medical
   Imaging
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Neurosciences & Neurology; Radiology, Nuclear Medicine & Medical Imaging
GA 964RB
UT WOS:000231897100043
PM 16155170
DA 2022-02-03
ER

PT J
AU Rose, RA
   Annadhason, A
AF Rose, R. Amala
   Annadhason, A.
TI GHT based automatic kidney image segmentation using modified AAM and
   GBDT
SO HEALTH AND TECHNOLOGY
LA English
DT Article
DE Kidney classification; CAD; Biometric; Segmentation; AAM; GBDT; Kidney
   renal cortex; Renal column; Renal medulla and renal pelvis
ID ULTRASOUND
AB These days age development to be more prominent in biometric angle. Particularly CAD machine is essentially renowned for ordering and division. In this theory Kidney issue and the division systems are examined. The kidney inconvenience recognizable proof and the finding in logical region are well focused on kidney's external layer. Accordingly, the inward inconvenience isn't yet considered in each case. This is mulled over and another time is produced in this examination for division of Kidney using GBDT (Gradient Boosting Decision Tree) thought. The exploration managed with a novel proficient componentThis research was handled with novel efficient mechanism named as GBDT. A systematic technique termed GBDT was utilized to enhance the predictive model. In the process of renal cortex phase localization, a technique which integrates Generalized Hough Transform (GHT) with Active Appearance Models (AAM) was enforced for kidney localization to appraise the renal cortex thickness. The AAM method always matches a new data to the appearance model by minimizing the intensity of root mean square (RMS) between the new data as well as appearance model instance. And finally, from the result of the localization phase, the proposed method GBDT was employed to segregate the kidney into various components. Then an accumulator matrix indicating the possible position of an object was constructed pursuant to the R-table, where the training set data is normally used in this form of table. The results were evaluated to reveal the higher achievement of the proposed system.
C1 [Rose, R. Amala] St Johns Coll Arts & Sci, Dept Comp Sci, Kanyakumari, India.
   [Rose, R. Amala] MS Univ, Tirunelveli, India.
   [Annadhason, A.] Govt Arts & Sci Coll, Dept Comp Sci, Ramanathapuram, India.
RP Rose, RA (corresponding author), St Johns Coll Arts & Sci, Dept Comp Sci, Kanyakumari, India.; Rose, RA (corresponding author), MS Univ, Tirunelveli, India.
EM amalarose10@gmail.com; annadhasa@gmail.com
CR Anupriya K, 2018, 2018 INT C SOFT COMP, P1, DOI [10.1109/ICSNS.2018.8573687, DOI 10.1109/ICSNS.2018.8573687]
   BalaAnand M, 2020, INT J PARALLEL PROG, V48, P329, DOI 10.1007/s10766-018-0598-2
   BalaAnand M., 2015, INT J TECHNOLOGY ENG, V7, P157
   Balaji M, 2018, 2018 INT C SOFT COMP, P1, DOI [10.1109/NTMS.2018.8328728, DOI 10.1109/ICSNS.2018.8573616]
   Balakrishnan K, 2005, IEEE WCNC, P2137
   Beland MD, 2010, AM J ROENTGENOL, V195, pW146, DOI 10.2214/AJR.09.4104
   Blankholm AD, 2015, ACAD RADIOL, V22, P1368, DOI 10.1016/j.acra.2015.06.015
   Chen XJ, 2012, ACAD RADIOL, V19, P562, DOI 10.1016/j.acra.2012.01.005
   Gloger O, 2018, PATTERN RECOGN, V84, P288, DOI 10.1016/j.patcog.2018.07.018
   Gloger O, 2015, IEEE T BIO-MED ENG, V62, P2338, DOI 10.1109/TBME.2015.2425935
   He JC, 2012, KIDNEY INT, V81, P22, DOI 10.1038/ki.2011.314
   Jin C, 2016, IEEE T MED IMAGING, V35, P1395, DOI 10.1109/TMI.2015.2512606
   Maram B, 2019, SERV ORIENTED COMPUT, V13, P3, DOI 10.1007/s11761-018-0249-x
   Nagappan VK, 2018, ASIA PACIFIC J RES, VI
   Roth HR, 2015, LECT NOTES COMPUT SC, V9349, P556, DOI 10.1007/978-3-319-24553-9_68
   Rusinek H, 2016, MAGN RESON MATER PHY, V29, P197, DOI 10.1007/s10334-015-0504-5
   Shi F, 2015, IEEE T MED IMAGING, V34, P441, DOI 10.1109/TMI.2014.2359980
   van Gastel MDA, 2018, ABDOM RADIOL, V43, P1215, DOI 10.1007/s00261-017-1285-2
   Wouters OJ, 2015, NAT REV NEPHROL, V11, P491, DOI 10.1038/nrneph.2015.85
   Xie J, 2005, IEEE T MED IMAGING, V24, P45, DOI 10.1109/TMI.2004.837792
   Xie L, 2018, AM J PHYSL RENAL PHY
   Yang X, 2017, THIRTY-FIRST AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1633
   Yaqub M, 2014, IEEE T MED IMAGING, V33, P258, DOI 10.1109/TMI.2013.2284025
   Yaqub M., 2010, P MED IM UND AN WARW, P261
NR 24
TC 1
Z9 1
U1 1
U2 1
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 2190-7188
EI 2190-7196
J9 HEALTH TECHNOL-GER
JI Health Technol.
PD JAN
PY 2020
VL 10
IS 1
SI SI
BP 353
EP 362
DI 10.1007/s12553-019-00297-5
PG 10
WC Medical Informatics
WE Emerging Sources Citation Index (ESCI)
SC Medical Informatics
GA KU8CC
UT WOS:000519938500033
DA 2022-02-03
ER

PT C
AU Rehman, YAU
   Po, LM
   Liu, MY
AF Rehman, Yasar Abbas Ur
   Po, Lai Man
   Liu, Mengyang
GP IEEE
TI Deep Learning for Face Anti-Spoofing: An End-to-End Approach
SO 2017 SIGNAL PROCESSING: ALGORITHMS, ARCHITECTURES, ARRANGEMENTS, AND
   APPLICATIONS (SPA 2017)
SE Signal Processing Algorithms Architectures Arrangements and Applications
LA English
DT Proceedings Paper
CT Conference on Signal Processing: Algorithms, Architectures,
   Arrangements, and Applications (SPA)
CY SEP 20-22, 2017
CL Poznan, POLAND
AB The importance of face anti-spoofing algorithms in biometric authentication systems is becoming indispensable. Recently, the success of Convolution Neural Networks (CNN) in key application areas of computer vision has encouraged its use in face biometrics for face anti-spoofing and verification applications. However, small training data has restricted the use of deep CNN architectures for face anti-spoofing applications. In this paper, we develop an end-to-end CNN architecture for face anti-spoofing application. i.e. a deep CNN architecture which directly map the raw input face images to the corresponding output classes. Additionally, an efficient training strategy has been proposed to enable the use of deeper CNN structures for face anti-spoofing applications and to enable the growth of training data in autonomous way. For training a CNN architecture, we propose a 5ORS-30SeC-1E (50 Random Samples-30 Sub-epochs Count-1Epoch) training strategy. The training data is randomly sampled during each forward-pass through the CNN architecture and 30 such passes counts for 1 complete epoch. An 11-layer VGG network with 2 derived VGG-11 networks have been trained for face anti-spoofing on CASIA-FASD dataset. Experimental results show significant improvement on various face-spoofing scenarios. A 3% improvement over state of the art approaches has been reported for Overall Test (OT) while achieving a lowest EER of 5%.
C1 [Rehman, Yasar Abbas Ur; Po, Lai Man; Liu, Mengyang] City Univ Hong Kong, Dept Elect Engn, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Rehman, YAU (corresponding author), City Univ Hong Kong, Dept Elect Engn, Kowloon, Hong Kong, Peoples R China.
EM yaurehman2-c@my.cityu.edu.hk; eelmpo@cityu.edu.hk;
   mengyaliu7-c@my.cityu.edu.hk
OI rehman, yasar/0000-0002-2945-7181; Liu, Mengyang/0000-0003-3527-6907
FU Internal SRG Funding from City University of Hong Kong [7004430]
FX The work described in this paper was substantially supported by Internal
   SRG Funding from City University of Hong Kong under the Project No.
   7004430.
CR Alotaibi A, 2017, SIGNAL IMAGE VIDEO P, V11, P713, DOI 10.1007/s11760-016-1014-2
   Alotaibi A, 2016, PROCEEDINGS OF 2016 INTERNATIONAL CONFERENCE ON OPTOELECTRONICS AND IMAGE PROCESSING (ICOIP 2016), P1, DOI [10.1109/OPTIP.2016.7528488, 10.1109/OCEANS.2016.7761105]
   Boulkenafet Z, 2017, SIGNAL PROC SEC TEC, P299, DOI 10.1007/978-3-319-47301-7_13
   Feng L., 2016, J ELECTRON IMAGING, V25
   Feng LT, 2016, J VIS COMMUN IMAGE R, V38, P451, DOI 10.1016/j.jvcir.2016.03.019
   Galbally J, 2014, IEEE ACCESS, V2, P1530, DOI 10.1109/ACCESS.2014.2381273
   GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gragnaniello D, 2016, 2016 12TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P193, DOI 10.1109/SITIS.2016.38
   Hadid A., 2011, BIOM IJCB 2011 INT J, P1, DOI [DOI 10.1109/IJCB.2011.6117510, 10.1109/IJCB.2011.6117510]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Krizhevsky A., 2012, INT C NEUR INF PROC, V25, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
   Li LN, 2016, CELL BIOSCI, V6, DOI 10.1186/s13578-016-0090-x
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Maatta J, 2012, IET BIOMETRICS, V1, P3, DOI 10.1049/iet-bmt.2011.0009
   Menotti D, 2015, IEEE T INF FOREN SEC, V10, P864, DOI 10.1109/TIFS.2015.2398817
   Ramachandra R, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3038924
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/CVPR.2015.7298594
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Waris M.-A., 2013, SIGN PROC C EUSIPCO, P1
   Yang J., 2014, ARXIV14085601
   Zhang Z, 2012, J NANOMATER, V2012, DOI 10.1155/2012/238605
NR 25
TC 9
Z9 10
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2326-0262
BN 978-8-3620-6530-1
J9 SIG P ALGO ARCH ARR
PY 2017
BP 195
EP 200
PG 6
WC Computer Science, Hardware & Architecture; Engineering, Electrical &
   Electronic; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Telecommunications
GA BJ5FU
UT WOS:000425864300037
DA 2022-02-03
ER

PT C
AU Hildebrandt, M
   Kiltz, S
   Dittmann, J
   Vielhauer, C
AF Hildebrandt, Mario
   Kiltz, Stefan
   Dittmann, Jana
   Vielhauer, Claus
BE Alattar, AM
   Memon, ND
   Heitzenrater, CD
TI An enhanced feature set for pattern recognition based contrast
   enhancement of contact-less captured latent fingerprints in digitized
   crime scene forensics
SO MEDIA WATERMARKING, SECURITY, AND FORENSICS 2014
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT Conference on Media Watermarking, Security, and Forensics
CY FEB 03-05, 2014
CL San Francisco, CA
DE Digitized forensics; latent fingerprints; crime scene forensics; pattern
   recognition
AB In crime scene forensics latent fingerprints are found on various substrates. Nowadays primarily physical or chemical preprocessing techniques are applied for enhancing the visibility of the fingerprint trace. In order to avoid altering the trace it has been shown that contact-less sensors offer a non-destructive acquisition approach. Here, the exploitation of fingerprint or substrate properties and the utilization of signal processing techniques are an essential requirement to enhance the fingerprint visibility. However, especially the optimal sensory is often substrate-dependent.
   An enhanced generic pattern recognition based contrast enhancement approach for scans of a chromatic white light sensor is introduced in Hildebrandt et al.(1) using statistical, structural and Benford's law(2) features for blocks of 50 micron. This approach achieves very good results for latent fingerprints on cooperative, non-textured, smooth substrates. However, on textured and structured substrates the error rates are very high and the approach thus unsuitable for forensic use cases.
   We propose the extension of the feature set with semantic features derived from known Gabor filter based exemplar fingerprint enhancement techniques by suggesting an Epsilon-neighborhood of each block in order to achieve an improved accuracy (called fingerprint ridge orientation semantics). Furthermore, we use rotation invariant Hu moments as an extension of the structural features and two additional preprocessing methods (separate X-and Y Sobel operators). This results in a 408-dimensional feature space.
   In our experiments we investigate and report the recognition accuracy for eight substrates, each with ten latent fingerprints: white furniture surface, veneered plywood, brushed stainless steel, aluminum foil, "Golden-Oak" veneer, non-metallic matte car body finish, metallic car body finish and blued metal. In comparison to Hildebrandt et al.,(1) our evaluation shows a significant reduction of the error rates by 15.8 percent points on brushed stainless steel using the same classifier. This also allows for a successful biometric matching of 3 of the 8 latent fingerprint samples with the corresponding exemplar fingerprint on this particular substrate. For contrast enhancement analysis of classification results we suggest to use known Visual Quality Indexes (VQI)(3) as a contrast enhancement quality indicator and discuss our first preliminary results using the exemplary chosen VQI Edge Similarity Score (ESS),(4) showing a tendency that higher image differences between a substrate containing a fingerprint and a substrate with a blank surface correlate with a higher recognition accuracy between a latent fingerprint and an exemplar fingerprint. Those first preliminary results support further research into VQIs as contrast enhancement quality indicator for a given feature space.
C1 [Hildebrandt, Mario; Kiltz, Stefan; Dittmann, Jana; Vielhauer, Claus] Univ Magdeburg, Dept Comp Sci, Res Grp Multimedia & Secur, D-39106 Magdeburg, Germany.
C3 Otto von Guericke University
RP Hildebrandt, M (corresponding author), Univ Magdeburg, Dept Comp Sci, Res Grp Multimedia & Secur, POB 4120, D-39106 Magdeburg, Germany.
CR ASME, 2009, SURF TEXT SURF ROUGH
   Benford F., 1938, P AM PHILOS SOC, V78, P551, DOI [10.1016/S0378-4371(00)00633-6, DOI 10.2307/984802]
   Bleay SM, 2012, FINGERPRINT SOURCE B
   Crane NJ, 2007, J FORENSIC SCI, V52, P48, DOI 10.1111/j.1556-4029.2006.00330.x
   Cross Match Technologies, 2014, CROSS MATCH LITE XE
   Dixon L, 2001, CHANGES STANDARDS AD
   Fries Research & Technology GmbH, 2013, CHROM WHIT LIGHT SEN
   Hall M, 2009, SIGKDD EXPLOR, V11, P10, DOI [10.1145/1656274.1656278, DOI 10.1145/1656274.1656278]
   Hildebrandt M., 2011, P 17 INT C DIG SIGN, P1, DOI DOI 10.1109/ICDSP.2011.6004969
   Hildebrandt M., 2013, MTS IEEE OCEANS 2013, P1
   Hofbauer H., 2011, 2011 3rd European Workshop on Visual Information Processing, P162, DOI 10.1109/EuVIP.2011.6045514
   Hofbauer H., 2014, VIS QUAL IND IMPL
   Hong L, 1998, IEEE T PATTERN ANAL, V20, P777, DOI 10.1109/34.709565
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/TIT.1962.1057692
   Makrushin Andrey, 2012, P SPIE, V8436
   Mao YN, 2006, IEEE T IMAGE PROCESS, V15, P2061, DOI 10.1109/TIP.2006.873426
   OpenCV dev team, 2014, IM MOM OPENCV 2 4 8
   Qadir G., 2010, P SPIE, V7723
   Sirchie, 2009, REFL ULTR IM SYST RU
   The National Institute of Standards and Technology, 2014, NIST BIOM IM SOFTW
NR 20
TC 1
Z9 1
U1 0
U2 4
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 978-0-8194-9945-5
J9 PROC SPIE
PY 2014
VL 9028
AR 902808
DI 10.1117/12.2039074
PG 15
WC Mathematical & Computational Biology; Optics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Mathematical & Computational Biology; Optics
GA BA4EE
UT WOS:000335494300008
DA 2022-02-03
ER

PT J
AU Koptyra, K
   Ogiela, MR
AF Koptyra, Katarzyna
   Ogiela, Marek R.
TI Multiply information coding and hiding using fuzzy vault
SO SOFT COMPUTING
LA English
DT Article
DE Image steganography; Multiply information hiding; Fuzzy vault schemes
ID GENERATION
AB In this paper we show some new ideas of hiding several secrets with the use of fuzzy vault construction and, in addition, a few systems based on presented concept. The main assumption is that all pieces of information are put into single vault and each of the secrets is linked with individual key. The keys are required to be disjunctive, and each of them should be in form of unordered set. In recovering process, revealed information is dependent on used key. Presented method is error-tolerant and thus is suitable for working together with biometrics. The interesting characteristic of multi-secret fuzzy vault is that the adversary is not able to distinguish it from the single-secret version. The article describes also a number of information systems built on the basis of presented scheme, including cooperation with false steganography (where every biometric trait encodes different steganographic key), password management and multi-user systems.
C1 [Koptyra, Katarzyna; Ogiela, Marek R.] AGH Univ Sci & Technol, 30 Mickiewicza Ave, PL-30059 Krakow, Poland.
C3 AGH University of Science & Technology
RP Ogiela, MR (corresponding author), AGH Univ Sci & Technol, 30 Mickiewicza Ave, PL-30059 Krakow, Poland.
EM kkoptyra@agh.edu.pl; mogiela@agh.edu.pl
RI Ogiela, Marek R/A-7735-2013
OI Ogiela, Marek R/0000-0002-8298-8627
FU AGH University of Science and Technology [11.11.120.329]
FX This work has been supported by the AGH University of Science and
   Technology research Grant No 11.11.120.329.
CR Aboalsamh H.A, 2011, INT J CIRCUITS SYSTE, V5, P9
   Castiglione A., 2011, 2011 International Conference on Broadband, Wireless Computing, Communication and Applications, P363, DOI 10.1109/BWCCA.2011.60
   Castiglione A, 2012, COMPUT MATH APPL, V63, P437, DOI 10.1016/j.camwa.2011.07.068
   Cherry D, 2014, BASICS DIGITAL PRIVA, P19
   Clancy T Charles, 2003, P ACM SIGMM 2003 MUL, P45
   Freire-Santos M, 2006, PROC SPIE, V6202, DOI 10.1117/12.665875
   Hachaj T, 2012, J ELECTRON IMAGING, V21, DOI 10.1117/1.JEI.21.4.043017
   Harrington JL, 2005, MORGAN KAUFMANN SERI, P205
   Hong SY, 2017, IEEE ACCESS, V5, P2779, DOI 10.1109/ACCESS.2017.2664804
   Jang-Jaccard J, 2014, J COMPUT SYST SCI, V80, P973, DOI 10.1016/j.jcss.2014.02.005
   Juels A, 2006, DESIGN CODE CRYPTOGR, V38, P237, DOI 10.1007/s10623-005-6343-z
   Khalil-Hani M, 2013, FUTURE GENER COMP SY, V29, P800, DOI 10.1016/j.future.2012.02.002
   Koptyra K, 2015, 2015 10TH INTERNATIONAL CONFERENCE ON BROADBAND AND WIRELESS COMPUTING, COMMUNICATION AND APPLICATIONS (BWCCA 2015), P183, DOI 10.1109/BWCCA.2015.87
   Lee S, 2011, KSII T INTERNET INF, V5, P1783, DOI 10.3837/tiis.2011.10.006
   Lee YJ, 2007, BIOMETRIC KEY BINDIN, P800
   Nguyen MT, 2014, LECT NOTES COMPUT SC, V8860, P204, DOI 10.1007/978-3-319-12778-1_16
   Moon D, 2007, FINGERPRINT TEMPLATE, P1141
   Moon D, 2009, IEICE ELECTRON EXPR, V6, P993, DOI 10.1587/elex.6.993
   Nandakumar K, 2007, IEEE T INF FOREN SEC, V2, P744, DOI 10.1109/TIFS.2007.908165
   Neuner S, 2016, DIGIT INVEST, V18, pS76, DOI 10.1016/j.diin.2016.04.010
   Ntalianis K, 2016, IEEE T EMERG TOP COM, V4, P156, DOI 10.1109/TETC.2015.2400135
   Ogiela L, 2014, J NETW COMPUT APPL, V38, P34, DOI 10.1016/j.jnca.2013.05.005
   Ogiela MR, 2015, SOFT COMPUT, V19, P3331, DOI 10.1007/s00500-015-1728-z
   Ogiela MR, 2009, STUD COMPUT INTELL, V226, P13, DOI 10.1007/978-3-642-02937-0_2
   Reichl D, 2003, KEEPASSTHE OFFICIAL
   Uludag U, 2005, LECT NOTES COMPUT SC, V3546, P310
   Uludag U, 2004, P IEEE, V92, P948, DOI 10.1109/JPROC.2004.827372
   Wojtowicz W, 2015, SECUR COMMUN NETW, V8, P1672, DOI 10.1002/sec.1114
   Yang SL, 2005, INT CONF ACOUST SPEE, P609
NR 29
TC 9
Z9 9
U1 1
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 1432-7643
EI 1433-7479
J9 SOFT COMPUT
JI Soft Comput.
PD JUN
PY 2019
VL 23
IS 12
BP 4357
EP 4366
DI 10.1007/s00500-018-3089-x
PG 10
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HW8OD
UT WOS:000466948100032
DA 2022-02-03
ER

PT C
AU Yoon, HJ
   Carmichael, TR
   Tourassi, G
AF Yoon, Hong-Jun
   Carmichael, Tandy R.
   Tourassi, Georgia
BE MelloThoms, CR
   Kupinski, MA
TI Temporal Stability of Visual Search-Driven Biometrics
SO MEDICAL IMAGING 2015: IMAGE PERCEPTION, OBSERVER PERFORMANCE, AND
   TECHNOLOGY ASSESSMENT
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT Conference on Medical Imaging - Image Perception, Observer Performance,
   and Technology Assessment
CY FEB 25-26, 2015
CL Orlando, FL
DE eye tracking; perceptual organization; user modeling
ID EYE-MOVEMENTS
AB Previously, we have shown the potential of using an individual's visual search pattern as a possible biometric. That study focused on viewing images displaying dot-patterns with different spatial relationships to determine which pattern can be more effective in establishing the identity of an individual. In this follow-up study we investigated the temporal stability of this biometric. We performed an experiment with 16 individuals asked to search for a predetermined feature of a random-dot pattern as we tracked their eye movements. Each participant completed four testing sessions consisting of two dot patterns repeated twice. One dot pattern displayed concentric circles shifted to the left or right side of the screen overlaid with visual noise, and participants were asked which side the circles were centered on. The second dot-pattern displayed a number of circles (between 0 and 4) scattered on the screen overlaid with visual noise, and participants were asked how many circles they could identify. Each session contained 5 untracked tutorial questions and 50 tracked test questions (200 total tracked questions per participant). To create each participant's "fingerprint", we constructed a Hidden Markov Model (HMM) from the gaze data representing the underlying visual search and cognitive process. The accuracy of the derived HMM models was evaluated using cross-validation for various time-dependent train-test conditions. Subject identification accuracy ranged from 17.6% to 41.8% for all conditions, which is significantly higher than random guessing (1/16 = 6.25%). The results suggest that visual search pattern is a promising, temporally stable personalized fingerprint of perceptual organization.
C1 [Yoon, Hong-Jun; Tourassi, Georgia] Oak Ridge Natl Lab, Biomed Sci & Engn Ctr, Hlth Data Sci Inst, Oak Ridge, TN 37831 USA.
   [Carmichael, Tandy R.] Tennessee Technol Univ, Dept Elect & Comp Engn, Cookeville, TN 38505 USA.
C3 United States Department of Energy (DOE); Oak Ridge National Laboratory;
   Tennessee Technological University
RP Yoon, HJ (corresponding author), Oak Ridge Natl Lab, Biomed Sci & Engn Ctr, Hlth Data Sci Inst, One Bethel Valley Rd, Oak Ridge, TN 37831 USA.
OI Tourassi, Georgia/0000-0002-9418-9638
CR [Anonymous], S2 EYE TRACK
   Bednarik R, 2005, LECT NOTES COMPUT SC, V3540, P780
   Deravi F, 2011, BIOSIGNALS 2011, P335
   Duchowski A.T., 2007, EYE TRACKING METHODO, V2nd
   Galdi Chiara, 2013, Pattern Recognition. 5th Mexican Conference, MCPR 2013. Proceedings: LNCS 7914, P136, DOI 10.1007/978-3-642-38989-4_14
   Goudelis G, 2008, J MULTIMODAL USER IN, V2, P217, DOI 10.1007/s12193-009-0020-x
   Holland CD, 2013, INT CONF BIOMETR
   Kasprowski P, 2004, LECT NOTES COMPUT SC, V3087, P248
   KURYLO DD, 1994, PSYCHOL AGING, V9, P562, DOI 10.1037/0882-7974.9.4.562
   Nappi M., 2012, P 2012 IEEE WORKSH B, P1
   Ortega-Garcia J, 2004, IEEE SIGNAL PROC MAG, V21, P50, DOI 10.1109/MSP.2004.1276113
   Rigas I., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P217, DOI 10.1109/BTAS.2012.6374580
   Rigas I, 2012, PATTERN RECOGN LETT, V33, P786, DOI 10.1016/j.patrec.2012.01.003
   Taramasco O., RHMM HIDDEN MARKOV M
   Wertheimer M., 1923, SOURCE BOOK GESTALT, DOI [10.1037/11496-005, DOI 10.1037/11496-0051938, DOI 10.1037/11496-005]
   Yoon HJ, 2014, PROC SPIE, V9037, DOI 10.1117/12.2044303
NR 16
TC 1
Z9 1
U1 0
U2 1
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 978-1-62841-506-3
J9 PROC SPIE
PY 2015
VL 9416
AR 94160U
DI 10.1117/12.2082801
PG 7
WC Optics; Radiology, Nuclear Medicine & Medical Imaging
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Optics; Radiology, Nuclear Medicine & Medical Imaging
GA BC6OS
UT WOS:000354266600026
OA Green Submitted
DA 2022-02-03
ER

PT C
AU Joshi, JC
   Nangia, SA
   Tiwari, K
   Gupta, KK
AF Joshi, J. C.
   Nangia, S. A.
   Tiwari, Kamlesh
   Gupta, K. K.
GP IEEE
TI Finger Knuckleprint Based Personal Authentication using Siamese Network
SO 2019 6TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED
   NETWORKS (SPIN)
LA English
DT Proceedings Paper
CT 6th International Conference on Signal Processing and Integrated
   Networks (SPIN)
CY MAR 07-08, 2019
CL Noida, INDIA
ID PATTERNS
AB Online security is a major concern today and incidents of forged identity cards and hacked passwords are common throughout the world. Therefore, there is a need for robust personal authentication mechanisms using biometrics for various access control systems. Popular biometric traits such as fingerprint have problems in rural areas, due to wearing down of fingerprint pattern from hard manual labor This is also a problem for people who work with calcium oxide, because it is known to dissolve the upper layers of the skin due to its basicity. This paper proposes a finger-knuckle-print (FKP) based human authentication system that is immune to the above problems because the finger dorsal region is not exposed to labor surfaces. The paper uses pre-processed knuckle ROI images to train a Siamese convolutional neural network model. The proposed algorithm has been validated using open-source PolyU finger-knuckle-print database from 165 individuals, and has achieved 99.24% CRR, 0.78% EER that is better than the state-of-the-art.
C1 [Joshi, J. C.; Nangia, S. A.; Tiwari, Kamlesh; Gupta, K. K.] Birla Inst Technol & Sci, Pilani, Rajasthan, India.
C3 Birla Institute of Technology & Science Pilani (BITS Pilani)
RP Gupta, KK (corresponding author), Birla Inst Technol & Sci, Pilani, Rajasthan, India.
EM guptakarunesh@gmail.com
RI Gupta, Karunesh/K-3538-2016
OI Gupta, Karunesh/0000-0002-0003-4601
CR Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Hadsell R., P 2006 IEEE COMP VIS, V2, P1735
   Jaswal G, 2017, MULTIMED TOOLS APPL, V76, P18955, DOI 10.1007/s11042-017-4475-6
   Jaswal Gaurav, 2017, IEEE INT C ID SEC BE, P1
   Kingma D., 2015, ADAM METHOD STOCHAST
   Kumar A, 2015, PATTERN RECOGN LETT, V68, P361, DOI 10.1016/j.patrec.2015.08.013
   Kumar A, 2014, IEEE T INF FOREN SEC, V9, P1288, DOI 10.1109/TIFS.2014.2328869
   Kumar A, 2009, IEEE T INF FOREN SEC, V4, P98, DOI 10.1109/TIFS.2008.2011089
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nigam A, 2016, NEUROCOMPUTING, V188, P190, DOI 10.1016/j.neucom.2015.04.126
   Perumal E, 2015, INT ARAB J INFORM TE, V12
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Tieniu Tan, 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P35
   Zhang L, 2011, PATTERN RECOGN, V44, P1990, DOI 10.1016/j.patcog.2010.06.007
   Zhang L, 2009, 2009 16TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOLS 1-6, P1981, DOI 10.1109/ICIP.2009.5413734
   Zhang L, 2009, LECT NOTES COMPUT SC, V5702, P141
NR 17
TC 4
Z9 4
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-7281-1380-7
PY 2019
BP 282
EP 286
PG 5
WC Engineering, Electrical & Electronic; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Telecommunications
GA BM9DX
UT WOS:000470844100058
DA 2022-02-03
ER

PT J
AU Sarris, I
   Ioannou, C
   Ohuma, EO
   Altman, DG
   Hoch, L
   Cosgrove, C
   Fathima, S
   Salomon, LJ
   Papageorghiou, AT
AF Sarris, I.
   Ioannou, C.
   Ohuma, E. O.
   Altman, D. G.
   Hoch, L.
   Cosgrove, C.
   Fathima, S.
   Salomon, L. J.
   Papageorghiou, A. T.
CA Int Fetal Newborn Growth Consortiu
TI Standardisation and quality control of ultrasound measurements taken in
   the INTERGROWTH-21(st) Project
SO BJOG-AN INTERNATIONAL JOURNAL OF OBSTETRICS AND GYNAECOLOGY
LA English
DT Article
DE Fetal biometry; INTERGROWTH-21(st); quality control; standardisation;
   ultrasound measurements
AB Meticulous standardisation and ongoing monitoring of adherence to measurement protocols during data collection are essential to ensure consistency and to minimise systematic error in multicentre studies. Strict ultrasound fetal biometric measurement protocols are used in the INTERGROWTH-21st Project so that data of the highest quality from different centres can be compared and potentially pooled. A central Ultrasound Quality Unit (USQU) has been set up to oversee this process. After initial training and standardisation, the USQU monitors the performance of all ultrasonographers involved in the project by continuously assessing the quality of the images and the consistency of the measurements produced. Ultrasonographers are identified when they exceed preset maximum allowable differences. Corrective action is then taken in the form of retraining or simply advice regarding changes in practice. This paper describes the procedures used, which can form a model for research settings involving ultrasound measurements.
C1 [Sarris, I.; Ioannou, C.; Ohuma, E. O.; Hoch, L.; Cosgrove, C.; Fathima, S.; Papageorghiou, A. T.] Univ Oxford, Nuffield Dept Obstet & Gynaecol, Oxford OX3 9DU, England.
   [Sarris, I.; Ioannou, C.; Ohuma, E. O.; Hoch, L.; Cosgrove, C.; Fathima, S.; Papageorghiou, A. T.] Univ Oxford, Oxford Maternal & Perinatal Hlth Inst, Green Templeton Coll, Oxford OX3 9DU, England.
   [Ohuma, E. O.; Altman, D. G.] Univ Oxford, Ctr Stat Med, Oxford OX3 9DU, England.
   [Fathima, S.] Univ Oxford, Dept Engn Sci, Inst Biomed Engn, Oxford OX3 9DU, England.
   [Salomon, L. J.] Univ Paris 05, Maternite Hop Necker Enfants Malad, AP HP, Paris, France.
C3 University of Oxford; University of Oxford; University of Oxford;
   University of Oxford; Assistance Publique Hopitaux Paris (APHP); Hopital
   Universitaire Necker-Enfants Malades - APHP; Universite de Paris
RP Papageorghiou, AT (corresponding author), Univ Oxford, John Radcliffe Hosp, Nuffield Dept Obstet & Gynaecol, Womens Ctr, Level 3, Oxford OX3 9DU, England.
EM aris.papageorghiou@obs-gyn.ox.ac.uk
RI Luna, Manuel Sanchez/H-2165-2014; Matijasevich, Alicia/C-5576-2009
OI Luna, Manuel Sanchez/0000-0001-9543-7392; Matijasevich,
   Alicia/0000-0003-0060-1589; Ioannou, Christos/0000-0002-3147-6613
FU Bill and Melinda Gates FoundationBill & Melinda Gates Foundation
   [49038]; Oxford Partnership Comprehensive Biomedical Research Centre;
   Department of Health NIHR Biomedical Research Centres funding
   schemeNational Institute for Health Research (NIHR)
FX This project was supported by the INTERGROWTH-21st Grant ID# 49038 from
   the Bill and Melinda Gates Foundation, for which we are very grateful.
   AT Papageorghiou and C Ioannou are supported by the Oxford Partnership
   Comprehensive Biomedical Research Centre with funding from the
   Department of Health NIHR Biomedical Research Centres funding scheme.
CR Papageorghiou AT, 2013, BJOG, DOI 10.1111/1471-0528.12313
   Salomon LJ, 2006, ULTRASOUND OBST GYN, V27, P34, DOI 10.1002/uog.2665
   Sarris I, 2012, ULTRASOUND OBST GYN, V39, P266, DOI 10.1002/uog.10082
   Sarris I, 2011, ULTRASOUND OBST GYN, V38, P681, DOI 10.1002/uog.8997
   Villar J, 2013, BJOG
NR 5
TC 52
Z9 54
U1 0
U2 8
PU WILEY-BLACKWELL
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1470-0328
J9 BJOG-INT J OBSTET GY
JI BJOG
PD SEP
PY 2013
VL 120
SU 2
SI SI
BP 33
EP 37
DI 10.1111/1471-0528.12315
PG 5
WC Obstetrics & Gynecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Obstetrics & Gynecology
GA 213HZ
UT WOS:000324048000005
PM 23841486
DA 2022-02-03
ER

PT J
AU Fuglsby, C
   Saunders, C
   Ommen, DM
   Buscaglia, J
   Caligiuri, MP
AF Fuglsby, Cami
   Saunders, Christopher
   Ommen, Danica M.
   Buscaglia, JoAnn
   Caligiuri, Michael P.
TI Elucidating the relationships between two automated handwriting feature
   quantification systems for multiple pairwise comparisons
SO JOURNAL OF FORENSIC SCIENCES
LA English
DT Article; Early Access
DE automated handwriting system; black box system; handwriting; questioned
   documents; statistical modeling; validity; white box system
AB Recent advances in complex automated handwriting identification systems have led to a lack of understandability of these systems' computational processes and features by the forensic handwriting examiners that they are designed to support. To mitigate this issue, this research studied the relationship between two systems: FLASH ID(R), an automated handwriting/black box system that uses measurements extracted from a static image of handwriting, and MovAlyzeR(R), a system that captures kinematic features from pen strokes. For this study, 33 writers each wrote 60 phrases from the London Letter using cursive writing and handprinting, which led to thousands of sample pairs for analysis. The dissimilarities between pairs of samples were calculated using two score functions (one for each system). The observed results indicate that dissimilarity scores based on kinematic spatial-geometric pen stroke features (e.g., amplitude and slant) have a statistically significant relationship with dissimilarity scores obtained using static, graph-based features used by the FLASH ID(R) system. Similar relationships were observed for temporal features (e.g., duration and velocity) but not pen pressure, and for both handprinting and cursive samples. These results strongly imply that both the current implementation of FLASH ID(R) and MovAlyzeR(R) rely on similar features sets when measuring differences in pairs of handwritten samples. These results suggest that studies of biometric discrimination using MovAlyzeR(R), specifically those based on the spatial-geometric feature set, support the validity of biometric matching algorithms based on FLASH ID(R) output.
C1 [Fuglsby, Cami; Saunders, Christopher] South Dakota State Univ, Dept Math & Stat, Brookings, SD 57007 USA.
   [Ommen, Danica M.] Iowa State Univ, Dept Stat, Ames, IA USA.
   [Buscaglia, JoAnn] FBI Lab, Res & Support Unit, Quantico, VA USA.
   [Caligiuri, Michael P.] Univ Calif San Diego, Dept Psychiat, La Jolla, CA 92093 USA.
C3 South Dakota State University; Iowa State University; University of
   California System; University of California San Diego
RP Caligiuri, MP (corresponding author), Univ Calif San Diego, Dept Psychiat, La Jolla, CA 92093 USA.
EM mcaligiuri@health.ucsd.edu
OI Ommen, Danica/0000-0001-9955-3817
FU National Institute of Justice [2017-DN-BX-0148]; National Science
   FoundationNational Science Foundation (NSF) [DGE-1828492]
FX This research was supported by the National Institute of Justice,
   2017-DN-BX-0148. Cami Fuglsby received additional support from the
   National Science Foundation, DGE-1828492.
CR Angel M., 2017, J AM SOC QUEST DOC E J AM SOC QUESTIONED, V20, P3
   Borsboom D, 2004, PSYCHOL REV, V111, P1061, DOI 10.1037/0033-295x.111.4.1061
   Caligiuri M., 2018, J AM SOC QUEST DOC E, V21, P3
   Caligiuri MP, 2012, FORENSIC SCI INT, V223, P228, DOI 10.1016/j.forsciint.2012.09.008
   CRONBACH LJ, 1955, PSYCHOL BULL, V52, P281, DOI 10.1037/h0040957
   Del Barrio E, 1999, ANN STAT, V27, P1230
   del Barrio E, 2000, TEST-SPAIN, V9, P1
   Franke K, 2003, FORENSIC SCI INT, V136, P84
   Fuglsby C, 2020, J FORENSIC SCI, V65, P2080, DOI 10.1111/1556-4029.14547
   Gantz DT., 2010, P 62 ANN SCI M AM AC, P431
   Hicklin RA., 2017, P 69 ANN SCI M AM AC, P480
   Messick S, 1995, ED MEAS, V14, P5, DOI [10.1111/j.17453992.1995.tb00881.x, 10.1111/j.1745-3992.1995.tb00881.x]
   Miller JJ, 2017, J FORENSIC SCI, V62, P722, DOI 10.1111/1556-4029.13345
   Mohammed LA, 2011, J FORENSIC SCI, V56, pS136, DOI 10.1111/j.1556-4029.2010.01584.x
   Ommen DM, 2021, FORENSIC SCI INT, V318, DOI 10.1016/j.forsciint.2020.110644
   Osborn AS., 1929, QUESTIONED DOCUMENTS, V2, P34
   PLAMONDON R, 1989, PATTERN RECOGN, V22, P107, DOI 10.1016/0031-3203(89)90059-9
   President's Council of Advisors on Science and Technology (PCAST), 2016, REP PRES FOR SCI CRI
   Requin J., 1980, TUTORIALS MOTOR BEHA, P525
   Rosen SL, 2015, SYSTEMS ENG, V18, P87, DOI 10.1002/sys.21290
   Saunders CP, 2011, ANN APPL STAT, V5, P381, DOI 10.1214/10-AOAS379
   Srihari SN., 2007, J FORENSIC DOC EXAM, V18, P1, DOI [10.31974/jfde28-15-26, DOI 10.31974/JFDE28-15-26]
   Strauss ME, 2009, ANNU REV CLIN PSYCHO, V5, P1, DOI 10.1146/annurev.clinpsy.032408.153639
   Ulery BT, 2011, P NATL ACAD SCI USA, V108, P7733, DOI 10.1073/pnas.1018707108
   Walch M., 2009, P 61 ANN SCI M AM AC, P381
   Walch M., 2008, P 60 ANN SCI M AM AC, P388
NR 26
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0022-1198
EI 1556-4029
J9 J FORENSIC SCI
JI J. Forensic Sci.
DI 10.1111/1556-4029.14914
EA OCT 2021
PG 9
WC Medicine, Legal
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Legal Medicine
GA WE7OW
UT WOS:000705811300001
PM 34634133
DA 2022-02-03
ER

PT C
AU Li, HJ
   Qiu, J
   Dong, JW
   Feng, G
AF Li, Hengjian
   Qiu, Jian
   Dong, Jiwen
   Feng, Guang
BE Falco, CM
   Jiang, X
TI Biometrics Encryption Combining Palmprint with Two-layer Error
   Correction Codes
SO NINTH INTERNATIONAL CONFERENCE ON DIGITAL IMAGE PROCESSING (ICDIP 2017)
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT 9th International Conference on Digital Image Processing (ICDIP)
CY MAY 19-22, 2017
CL Hong Kong, PEOPLES R CHINA
DE Biometrics; Palmprint features; Keys; Convolutional code; Cyclic code
AB To bridge the gap between the fuzziness of biometrics and the exactitude of cryptography, based on combining palmprint with two-layer error correction codes, a novel biometrics encryption method is proposed. Firstly, the randomly generated original keys are encoded by convolutional and cyclic two-layer coding. The first layer uses a convolution code to correct burst errors. The second layer uses cyclic code to correct random errors. Then, the palmprint features are extracted from the palmprint images. Next, they are fused together by XORing operation. The information is stored in a smart card. Finally, the original keys extraction process is the information in the smart card XOR the user's palmprint features and then decoded with convolutional and cyclic two-layer code. The experimental results and security analysis show that it can recover the original keys completely. The proposed method is more secure than a single password factor, and has higher accuracy than a single biometric factor.
C1 [Li, Hengjian; Qiu, Jian; Dong, Jiwen; Feng, Guang] Univ Jinan, Shandong Prov Key Lab Network Based Intelligent C, Jinan, Shandong, Peoples R China.
C3 University of Jinan
RP Dong, JW (corresponding author), Univ Jinan, Shandong Prov Key Lab Network Based Intelligent C, Jinan, Shandong, Peoples R China.
EM s.ise_dongjw@ujn.edu.cn
CR [高莹 Gao Ying], 2016, [密码学报, Journal of Cryptologic Research], V3, P157
   Hao F, 2006, IEEE T COMPUT, V55, P1081, DOI 10.1109/TC.2006.138
   Jules A., 2002, P IEEE INT S INF THE, V408
   Li M. Y., 2013, SCI TECHNOLOGY ENG, V13, P5371
   Lu G. M., 2007, J NANJING U POSTS TE, V27, P82
   Pei Z., 2010, FIRE CONTROL RADAR T, V39, P57
   Poly U, 2006, PALMPR DAT
   Ratha NK, 2007, IEEE T PATTERN ANAL, V29, P561, DOI 10.1109/TPAMI.2007.1004
   Soutar C, 1998, ICSA GUIDE CRYPTOGRA, P55
   Vinay B. K., 2014, INT J SCI RES, V3, P736
   Wang X. T., 2008, RADAR ANTAGONISM, V4, P32
   Zhang D, 2003, IEEE T PATTERN ANAL, V25, P1041, DOI 10.1109/TPAMI.2003.1227981
   [张宁 Zhang Ning], 2015, [密码学报, Journal of Cryptologic Research], V2, P159
NR 13
TC 1
Z9 1
U1 0
U2 3
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 978-1-5106-1305-8; 978-1-5106-1304-1
J9 PROC SPIE
PY 2017
VL 10420
AR UNSP 104201L
DI 10.1117/12.2281672
PN 1
PG 5
WC Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Imaging Science & Photographic Technology
GA BI4ZH
UT WOS:000412113300056
DA 2022-02-03
ER

PT C
AU Dhanalaxmi, B
   Tadisetty, S
AF Dhanalaxmi, Banavath
   Tadisetty, Srinivasulu
GP IEEE
TI Multimedia Cryptography- A Review
SO 2017 IEEE INTERNATIONAL CONFERENCE ON POWER, CONTROL, SIGNALS AND
   INSTRUMENTATION ENGINEERING (ICPCSI)
LA English
DT Proceedings Paper
CT IEEE International Conference on Power, Control, Signals and
   Instrumentation Engineering (IEEE ICPCSI)
CY SEP 21-22, 2017
CL Saveetha Engn Coll, Thandalam, INDIA
HO Saveetha Engn Coll
DE biometrics; multimodal; security; cipher; encryption
AB In Digital Communication and Multimedia application, security becomes an important role of transmission of data and images. The encryption is one of the ways to ensure security for multimedia data. The multimedia data comprises of text, image, audio and video. In recent years, the latest encryption technology has improved the security to multimedia data from unauthorized operations. Multimedia encryption techniques convert original data to another data that is hard to understand and is called cipher text. The process of encoding plain text messages into cipher text messages is called encryption and the reverse process of trans-forming cipher text back to plain text is called decryption. It has several applications like multimedia communication systems, medical imaging, Tele-medicine and military communication. At present, there is several conventional data encryption algorithms are available. The major three conventional methods are widely used. They are AES, RSA, and IDEA are being used for encryption of text and binary data. The basic methods have fixed length of key. However, the conventional algorithms are inefficient to use them directly in multimedia data and color image encryption because of high correlation among pixels. Moreover, the multimedia data are often high redundancy of large volumes and require real-time analysis for better results.
   The bio-cryptography is one of the latest emerging research. The bio cryptography will address all the limitations of conventional cryptography. It will provide the enhanced security to data and images while transmitting or in storage. This will be efficiently used for improving the multimedia data encryption and decryption. The basic cryptography techniques are secret key, public key and hash functions. However, the cost and complexity is high in bio cryptography. There-fore, the bio cryptography will be used to specialized applications. The interface between biometric and cryptography depends on type of application and level of security. The selection of physical and behavioral characteristics will also depend on the required design and applications. Hence, this paper will focus on to review the latest developments to report the challenges and achievements including fundamental appraisal.
C1 [Dhanalaxmi, Banavath; Tadisetty, Srinivasulu] Kakatiya Univ, KU Coll Engn & Technol, ECE Dept, Warangal, Andhra Pradesh, India.
C3 Kakatiya University
RP Dhanalaxmi, B (corresponding author), Kakatiya Univ, KU Coll Engn & Technol, ECE Dept, Warangal, Andhra Pradesh, India.
EM Dhanalaxmi_d@yahoo.co.in; drstadisetty@gmail.com
CR [Anonymous], CRYPTOGRAPHY NETWORK
   Banavath D., 2017, INT J ELECT ENG IJEE, V9, P160
   Bloisi D., 2003, 4 INT WORKSH INF HID, P289
   HosseinKamali Seyed, 2010, 2010 INT C EL INF EN
   Huang K., 2009, IEEE T
   Ito M., 2007, IAPR C MACH VIS APPL
   Jolfaei A., 2010, J TECHNICAL APPL INF
   [No title captured]
NR 8
TC 4
Z9 4
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-5386-0814-2
PY 2017
BP 764
EP 766
PG 3
WC Automation & Control Systems; Engineering, Electrical & Electronic;
   Instruments & Instrumentation
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Engineering; Instruments & Instrumentation
GA BM5AZ
UT WOS:000464679900142
DA 2022-02-03
ER

PT C
AU Yang, XF
   Sun, DM
AF Yang, Xiaofeng
   Sun, Dongmei
BE Baozong, Y
   Qiuqi, R
   Yao, Z
   Gaoyun AN
TI Feature-Level Fusion of Palmprint and Palm Vein Base on Canonical
   Correlation Analysis
SO PROCEEDINGS OF 2016 IEEE 13TH INTERNATIONAL CONFERENCE ON SIGNAL
   PROCESSING (ICSP 2016)
SE International Conference on Signal Processing
LA English
DT Proceedings Paper
CT 13th IEEE International Conference on Signal Processing (ICSP)
CY NOV 06-10, 2016
CL Chengdu, PEOPLES R CHINA
DE Canonical Correlation Analysis (CCA); Feature fusion; LBP; Palmprint;
   palm vein
AB Canonical Correlation Analysis (CCA) is a standard tool in statistical analysis that measures the linear relationship between two data sets. In this paper, a multi-biometric approach which combines palm print feature and palm vein feature based on Canonical Correlation Analysis is proposed. First, using a series of pre-processing and extracted the ROI (Region of interest) which has enhanced of the palmprint and palm vein images. Then, using local binary pattern (LBP) to extract the palm print and palm vein feature. Next, these two features are fused by CCA to form a combined feature which is applied to denote the identity of a person. This method makes it possible to fuse these features mentioned above together and decrease the dimension of the fusion feature. The results of experiments conducted on a database of 100 hands show that the CCA-based feature level fusion method has good performance.
C1 [Yang, Xiaofeng; Sun, Dongmei] Beijing Jiaotong Univ, Inst Informat Sci, Beijing, Peoples R China.
C3 Beijing Jiaotong University
RP Yang, XF (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing, Peoples R China.
EM 13164252675@163.com; dmsun@bjtu.edu.cn
CR Abdi H., 2010, ANAL METHODS, V2, P433
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.2307/2333955
   Jain AK, 2006, IEEE T INF FOREN SEC, V1, P125, DOI 10.1109/TIFS.2006.873653
   Kong AWK, 2004, LECT NOTES COMPUT SC, V3072, P761
   Luo F., 2005, J CHONGQING TECHNOL, P249
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   Qian- Ying L. I., 2010, CAAI T INTELLIGENT S
   Ramya M., 2012, 2012 International Conference on Advances in Engineering, Science and Management (ICAESM), P191
   Sinha G R, 2016, DEV ANAL MULTIMODAL
   [No title captured]
NR 11
TC 6
Z9 6
U1 0
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2164-5221
BN 978-1-5090-1345-6
J9 INT CONF SIGN PROCES
PY 2016
BP 1353
EP 1356
PG 4
WC Engineering, Electrical & Electronic; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Telecommunications
GA BI1JB
UT WOS:000406056300263
DA 2022-02-03
ER

PT C
AU Ambadiyil, S
   Prakash, D
   Sheeja, MK
   Pillai, VPM
AF Ambadiyil, Sajan
   Prakash, Divya
   Sheeja, M. K.
   Pillai, V. P. Mahadevan
TI Secure Storage and Analysis of Fingerprints for Criminal Investigation
   using Holographic Techniques
SO MATERIALS TODAY-PROCEEDINGS
LA English
DT Proceedings Paper
CT International Symposium on Photonics Applications and Nanomaterials
   (ISPAN)
CY OCT 28-30, 2015
CL Trivandrum, INDIA
DE Holography; Fingerprints; Data storage; Holographic magnification
AB In this age of highly advanced digital image processing, it is very difficult to store the biometric details like a fingerprint of a highly wanted criminal in a secure manner. Any digital storage methods can be forged, or the stored data can be easily destroyed or manipulated. Hence, a secure storage and easily recovered method need to be applied in such fields. Holographic data storage is one of the most secure storage methods available can be used to resolve this problem. In this proposed work, holographic data storage along with holographic magnification is used for the secure storage and analysis of the fingerprints of highly wanted criminals. In holographic magnification, a different laser with more dispersion than the one used for construction is used for reconstruction. This will result in the expansion of the stored data which makes it easier for the analysis. (C) 2017 Elsevier Ltd. All rights reserved.
C1 [Ambadiyil, Sajan] Ctr Dev Imaging Technol, Thiruvanthapuram 695027, Kerala, India.
   [Prakash, Divya; Sheeja, M. K.] SCT Coll Engn, Thiruvanthapuram 695018, Kerala, India.
   [Pillai, V. P. Mahadevan] Univ Kerala, Dept Optoelect, Kariyavattom 69558, Thiruvanthapura, India.
C3 University of Kerala
RP Ambadiyil, S (corresponding author), Ctr Dev Imaging Technol, Thiruvanthapuram 695027, Kerala, India.
EM ambadycdit@gmail.com
CR Coufal H. J, 2000, HOLOGRAPHIC DATA STO
   Davida G.I., 1999, P INT WORKSH COD CRY, P129
   Draper SC, 2007, INT CONF ACOUST SPEE, P129
   GIVENS MP, 1972, AM J PHYS, V40, P1311, DOI 10.1119/1.1986820
   Hesselink L, 2004, P IEEE, V92, P1231, DOI 10.1109/JPROC.2004.831212
   Jeong Tung H., FUNDAMENTALS PHOTONI
   Juels A, 2006, DESIGN CODE CRYPTOGR, V38, P237, DOI 10.1007/s10623-005-6343-z
   Nakamura A, 2015, FORENSIC SCI INT, V254, P100, DOI 10.1016/j.forsciint.2015.06.031
   Yzuel M. J., 2007, P ED TRAIN OPT PHOT
NR 9
TC 4
Z9 4
U1 0
U2 1
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 2214-7853
J9 MATER TODAY-PROC
JI Mater. Today-Proc.
PY 2017
VL 4
IS 2
BP 4389
EP 4395
DI 10.1016/j.matpr.2017.04.010
PN C
PG 7
WC Materials Science, Multidisciplinary
WE Conference Proceedings Citation Index - Science (CPCI-S); Conference Proceedings Citation Index - Social Science &amp; Humanities (CPCI-SSH)
SC Materials Science
GA FO0HQ
UT WOS:000416424700010
DA 2022-02-03
ER

PT C
AU Shahriar, H
   Haddad, H
   Islam, M
AF Shahriar, Hossain
   Haddad, Hisham
   Islam, Mahbubul
BE Reisman, S
   Ahamed, SI
   Demartini, C
   Conte, T
   Liu, L
   Claycomb, W
   Nakamura, M
   Tovar, E
   Cimato, S
   Lung, CH
   Takakura, H
   Yang, JJ
   Akiyama, T
   Zhang, Z
   Hasan, K
TI An Iris-Based Authentication Framework to Prevent Presentation Attacks
SO 2017 IEEE 41ST ANNUAL COMPUTER SOFTWARE AND APPLICATIONS CONFERENCE
   (COMPSAC), VOL 2
SE Proceedings International Computer Software and Applications Conference
LA English
DT Proceedings Paper
CT 41st IEEE Annual Computer Software and Applications Conference (COMPSAC)
CY JUL 04-08, 2017
CL Torino, ITALY
DE Iris liveness detection; Presentation attack; Haar-Cascade classifier;
   LBP classifier
AB Attacks on authentication services are major security concerns. Password-based authentication systems can be compromised using known techniques, such as brute force and dictionary-based attacks. Biometric-based authentication systems are becoming the preferred choice to replace password-based authentication systems. Among several variations of biometrics (e.g., face, eye, fingerprint), iris-based authentication is commonly used in various applications. In iris-based authentication systems, iris images from legitimate users are captured and certain features are extracted to be used for matching during the authentication process. Literature works suggest that iris-based authentication systems can be subject to presentation attacks where an attacker obtains printed copy of the victim's eye image and displays it in front of an authentication system to gain unauthorized access. Such attacks can be performed by displaying static eye images on mobile devices or ipads (known as screen attacks). Since human iris features so not changed, once the iris image is compromised, it is hard to avoid this type of attack. To address this challenge, this paper proposes a framework for iris code generation by considering the changes of the area between the pupil and the sclera due to light density level. The proposed approach relies on capturing iris images using near infrared light. We train Haar-Cascade and LBP classifiers to capture the area between the pupil and the cornea. The image of iris is then stored in the database. This approach also generates a QR code from the iris. The code acts as a password and the user is required to provide it during authentication. A prototype is built using OpenCV platform tool. The prototype has been tested using samples obtained from publicly available iris database. The initial results show that the proposed approach has lower false positive and false negative rates.
C1 [Shahriar, Hossain; Haddad, Hisham; Islam, Mahbubul] Kent State Univ, Dept Comp Sci, Dept Informat Technol, Kent, OH 44242 USA.
C3 Kent State University
RP Shahriar, H (corresponding author), Kent State Univ, Dept Comp Sci, Dept Informat Technol, Kent, OH 44242 USA.
EM hshahria@kennesaw.edu; hhaddad@kennesaw.edu;
   mislam9@students.kennesaw.edu
CR Boatwright Michelle, 2007, P 4 ANN C INF SEC CU
   Czajka A, 2015, IEEE T INF FOREN SEC, V10, P726, DOI 10.1109/TIFS.2015.2398815
   Czajka A, 2013, 2013 18TH INTERNATIONAL CONFERENCE ON METHODS AND MODELS IN AUTOMATION AND ROBOTICS (MMAR), P28
   Daugman J., IRIS RECOGNITION AIR
   Huang XY, 2013, IEEE WORK APP COMP, P252, DOI 10.1109/WACV.2013.6475026
   Kanematsu Masashi, 2007, SICE '07. 46th SICE Annual Conference, P361
   Karunya R., 2015, 2015 International Conference on Advanced Computing and Communication Systems (ICACCS). Proceedings, P1, DOI 10.1109/ICACCS.2015.7324134
   Li CC, 2015, VISUAL COMPUT, V31, P1419, DOI 10.1007/s00371-014-1023-5
   Mhatre RM, 2015, INT CONF COMPUT INTE, P1068, DOI 10.1109/CICN.2015.210
   Pacut A, 2006, CAR C SECUR, P122, DOI 10.1109/CCST.2006.313440
   Puhan N. B., 2011, 2011 IEEE 15th International Symposium on Consumer Electronics, P71, DOI 10.1109/ISCE.2011.5973786
   Raghavendra R, 2015, IEEE T IMAGE PROCESS, V24, P1060, DOI 10.1109/TIP.2015.2395951
   Ratha NK, 2001, IBM SYST J, V40, P614, DOI 10.1147/sj.403.0614
   Roberts J., 2016, EYE SCANNING ROLLS O
   Sheela SV, 2010, INT J COMPUT APPL, V3, P19
   Thavalengal S, 2016, IEEE T CONSUM ELECTR, V62, P95, DOI 10.1109/TCE.2016.7514667
   Uhl Andres, 2012, P 5 IARP INT C BIOM
   Zhao Y., 2010 2 WRI GLOB C IN, DOI [10.1109/gcis.2010.55, DOI 10.1109/GCIS.2010.55]
NR 18
TC 3
Z9 3
U1 0
U2 4
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 0730-3157
BN 978-1-5386-0367-3
J9 P INT COMP SOFTW APP
PY 2017
BP 504
EP 509
DI 10.1109/COMPSAC.2017.60
PG 6
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Software Engineering; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BJ4DA
UT WOS:000424861900096
DA 2022-02-03
ER

PT C
AU Bohm, C
   Kunath, P
   Pryakhin, A
   Schubert, M
AF Boehm, Christian
   Kunath, Peter
   Pryakhin, Alexey
   Schubert, Matthias
BE Papadias, D
   Zhang, D
   Kollios, G
TI Querying objects modeled by arbitrary probability distributions
SO ADVANCES IN SPATIAL AND TEMPORAL DATABASES, PROCEEDINGS
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 10th International Symposium on Advances in Spatial and Temporal
   Databases
CY JUL 16-18, 2007
CL Boston, MD
ID MIXTURE MODEL
AB In many modern applications such as biometric identification systems, sensor networks, medical imaging, geology, and multimedia databases, the data objects are not described exactly. Therefore, recent solutions propose to model data objects by probability density functions(pdf). Since a pdf describing an uncertain object is often not explicitly known, approximation techniques like Gaussian mixture models(GMM) need to be employed. In this paper, we introduce a method for efficiently indexing and querying GMMs allowing fast object retrieval for arbitrary shaped pdf. We consider probability ranking queries which are very important for probabilistic similarity search. Our method stores the components and weighting functions of each GMM in an index structure. During query processing the mixture models are dynamically reconstructed whenever necessary. In an extensive experimental evaluation, we demonstrate that GMMs yield a compact and descriptive representation of video clips. Additionally, we show that our new query algorithm outperforms competitive approaches when answering the given probabilistic queries on a database of GMMs comprising about 100.000 single Gaussians.
C1 [Boehm, Christian; Kunath, Peter; Pryakhin, Alexey; Schubert, Matthias] Univ Munich, Inst Comp Sci, Marchioninistr 15, Munich, Germany.
C3 University of Munich
RP Bohm, C (corresponding author), Univ Munich, Inst Comp Sci, Marchioninistr 15, Munich, Germany.
EM boehm@dbs.ifi.lmu.de; kunath@dbs.ifi.lmu.de; pryakhin@dbs.ifi.lmu.de;
   schubert@dbs.ifi.lmu.de
CR Berchtold S, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P28
   Bohm C., 2006, ICDE, P9
   Bohm C., 2006, P 18 INT C SCI STAT, P169
   Chang HS, 1999, IEEE T CIRC SYST VID, V9, P1269, DOI 10.1109/76.809161
   CHEN SC, 2002, SEMANTIC MODELS MULT
   CHENG R, 2007, EVALUATION PROBABILI, V32, P104
   Cheng R., 2004, P 30 INT C VER LARG, P876
   Cheng R., 2003, P ACM SIGMOD INT C M, P551, DOI DOI 10.1145/872757.872823
   Cheung SCS, 2002, IEEE IMAGE PROC, P621
   Dai XY, 2005, LECT NOTES COMPUT SC, V3633, P400
   DEB S, 2005, VIDEO DATA MANAGEMEN
   DESHPANDE A, 2004, P 30 INT C VER LARG
   DESHPANDE A, 2005, P CIDR
   Eiter T, 1997, ACTA INFORM, V34, P109, DOI 10.1007/s002360050075
   Faradjian A, 2002, PROC INT CONF DATA, P201, DOI 10.1109/ICDE.2002.994710
   Gavin DG, 2005, GLOBAL ECOL BIOGEOGR, V14, P491, DOI 10.1111/j.1466-822x.2005.00171.x
   Greenspan H, 2002, LECT NOTES COMPUT SC, V2353, P461
   Guttman A, 1984, RTREES DYNAMIC INDEX, P47, DOI DOI 10.1145/971697.602266
   Han Jiawei, 2006, DATA MINING CONCEPTS
   Lim CP, 2005, NEURAL COMPUT APPL, V14, P345, DOI 10.1007/s00521-005-0471-2
   LINDSAY BG, 1995, MIXTURE MODELS THEOR
   LJOSA V, 2007, P 23 INT C DAT ENG I
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   SRINIVASAN U, 2005, MANAGING MULTIMEDIA
   Tao Y., 2005, P VLDB, P922
   Titterington D. M., 1985, STAT ANAL FINITE MIX
   Witten I., 2005, DATA MINING PRACTICA, Vsecond
   Yang MH, 1998, PROC SPIE, V3656, P458, DOI 10.1117/12.333865
   Yoo SH, 2003, APPL ECON LETT, V10, P181, DOI 10.1080/1350485022000044101
   ZAJDEL W, 2003, P 15 DUTCH BELG ART, P371
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
NR 31
TC 5
Z9 5
U1 0
U2 0
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 0302-9743
EI 1611-3349
BN 978-3-540-73539-7
J9 LECT NOTES COMPUT SC
PY 2007
VL 4605
BP 294
EP +
PG 3
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BGL36
UT WOS:000248149700017
DA 2022-02-03
ER

PT J
AU Sakr, AS
   Plawiak, P
   Tadeusiewicz, R
   Hammad, M
AF Sakr, Ahmed S.
   Plawiak, Pawel
   Tadeusiewicz, Ryszard
   Hammad, Mohamed
TI Cancelable ECG biometric based on combination of deep transfer learning
   with DNA and amino acid approaches for human authentication
SO INFORMATION SCIENCES
LA English
DT Article
DE Authentication; Amino Acid; Biometrics; Cancelable; Deep Transfer
   Learning; DNA; ECG; Template Protection; SVM
ID CONVOLUTION NEURAL-NETWORK; FUSION; MODEL; GENERATION
AB Recently, electrocardiogram (ECG) signals have received a high level of attention as a physiological signal in the field of biometrics. It has presented great possibilities for its strength against counterfeit. However, the ECG feature templates are irreplaceable, and a compromised template implies a permanent loss of identity. Therefore, several studies have been introduced biometric template protection techniques such as cancelable techniques to protect the original template in case it is stolen or lost. In this research, a cancelable ECG approach is proposed to protect the ECG feature template for human authentication. In our system, we first employed some image processing techniques for preprocessing the input ECG signals. Then, a deep transfer learning approach is employed to extract the deep ECG features. Later, the proposed cancelable approach based on DNA and amino acid is applied to protect the deep feature templates. Lastly, a Support Vector Machine (SVM) is employed for authentication. Extensive experiments on two commonly used datasets coupled with comprehensive theoretical analysis demonstrate the highest accuracy of the proposed system and the strong resilience of the system to various security and privacy attacks. Results show that the proposed cancelable method meets all requirements of cancelable biometrics such as irreversibility, revocability, and unlinkability. (c) 2021 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
C1 [Sakr, Ahmed S.] Menoufia Univ, Fac Comp & Informat, Dept Informat Syst, Al Minufiyah, Egypt.
   [Plawiak, Pawel] Cracow Univ Technol, Fac Comp Sci & Telecommun, Dept Comp Sci, Warszawska 24, PL-31155 Krakow, Poland.
   [Plawiak, Pawel] Polish Acad Sci, Inst Theoret & Appl Informat, Baltycka 5, PL-44100 Gliwice, Poland.
   [Tadeusiewicz, Ryszard] AGH Univ Sci & Technol, Dept Biocybernet & Biomed Engn, Krakow, Poland.
   [Hammad, Mohamed] Menoufia Univ, Fac Comp & Informat, Dept Informat Technol, Menoufia, Egypt.
C3 Egyptian Knowledge Bank (EKB); Menofia University; Cracow University of
   Technology; Polish Academy of Sciences; Institute of Theoretical &
   Applied Informatics of the Polish Academy of Sciences; AGH University of
   Science & Technology; Egyptian Knowledge Bank (EKB); Menofia University
RP Plawiak, P (corresponding author), Cracow Univ Technol, Fac Comp Sci & Telecommun, Dept Comp Sci, Warszawska 24, PL-31155 Krakow, Poland.; Plawiak, P (corresponding author), Polish Acad Sci, Inst Theoret & Appl Informat, Baltycka 5, PL-44100 Gliwice, Poland.
RI Plawiak, Pawel/K-8151-2013
OI Plawiak, Pawel/0000-0002-4317-2801
CR Basu S, 2019, J SYST ARCHITECT, V94, P24, DOI 10.1016/j.sysarc.2019.02.005
   Bousseljot R., 1995, BIOMED ENG-BIOMED TE, V40, P317, DOI DOI 10.1515/BMTE.1995.40.S1.317
   Chen PT, 2017, P ANN INT IEEE EMBS, P3497, DOI 10.1109/EMBC.2017.8037610
   Dey N, 2013, IEEE INT ADV COMPUT, P732
   Ding C, 1997, ELECTRON LETT, V33, P677, DOI 10.1049/el:19970440
   Figuera C, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0159654
   Goldberger AL, 2000, CIRCULATION, V101, pE215, DOI 10.1161/01.CIR.101.23.e215
   Gupta D, 2017, ANAL VIDHYA
   Hammad M., 2017, P 2017 INT C BIOM EN, P39
   Hammad M, 2021, MULTIMEDIA SYST, DOI 10.1007/s00530-020-00728-8
   Hammad M, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3033072
   Hammad M, 2021, EXPERT SYST, V38, DOI 10.1111/exsy.12547
   Hammad M, 2019, FUTURE GENER COMP SY, V101, P180, DOI 10.1016/j.future.2019.06.008
   Hammad M, 2019, IEEE ACCESS, V7, P26527, DOI 10.1109/ACCESS.2018.2886573
   Hammad M, 2019, COMPUT SECUR, V81, P107, DOI 10.1016/j.cose.2018.11.003
   Hammad M, 2019, MULTIMED TOOLS APPL, V78, P1857, DOI 10.1007/s11042-018-6300-2
   Hammad M, 2018, MEASUREMENT, V125, P634, DOI 10.1016/j.measurement.2018.05.033
   Hu, 2010, J ENG COMPUTER INNOV, V2, P12
   Ihsanto E, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10093304
   Israel SA, 2005, PATTERN RECOGN, V38, P133, DOI 10.1016/j.patcog.2004.05.014
   Kalsi S, 2018, J MED SYST, V42, DOI 10.1007/s10916-017-0851-z
   Karimian Nima, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P257, DOI 10.1109/TBIOM.2020.2992274
   Kaur H, 2019, PATTERN RECOGN LETT, V126, P31, DOI 10.1016/j.patrec.2018.02.016
   Kim H, 2019, IEEE ACCESS, V7, P9232, DOI 10.1109/ACCESS.2019.2891817
   Kim H, 2017, P ANN INT IEEE EMBS, P454, DOI 10.1109/EMBC.2017.8036860
   Ksiazek W, 2020, BIOCYBERN BIOMED ENG, V40, P1512, DOI 10.1016/j.bbe.2020.08.007
   Labati RD, 2019, PATTERN RECOGN LETT, V126, P78, DOI 10.1016/j.patrec.2018.03.028
   Li R, 2020, PATTERN RECOGN LETT, V129, P70, DOI 10.1016/j.patrec.2019.11.005
   Li YZ, 2020, NEUROCOMPUTING, V391, P83, DOI 10.1016/j.neucom.2020.01.019
   Lugovaya T.S., 2005, THESIS
   Lynn HM, 2019, IEEE ACCESS, V7, P145395, DOI 10.1109/ACCESS.2019.2939947
   Manisha, 2020, ARTIF INTELL REV, V53, P3403, DOI 10.1007/s10462-019-09767-8
   Nandakumar K, 2015, IEEE SIGNAL PROC MAG, V32, P88, DOI 10.1109/MSP.2015.2427849
   Oak Rajvardhan, 2018, Intelligent Computing and Information and Communication. Proceedings of 2nd International Conference, ICICC 2017. Advances in Intelligent Systems and Computing (673), P173, DOI 10.1007/978-981-10-7245-1_18
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P54, DOI 10.1109/MSP.2015.2434151
   Patro KK, 2020, J SUPERCOMPUT, V76, P858, DOI 10.1007/s11227-019-03022-1
   Press W.H., 1990, COMPUTERS PHYS AM I, V4, P669, DOI [10.1063/1.4822961, DOI 10.1063/1.4822961]
   Ran HH, 2021, NEUROCOMPUTING, V441, P52, DOI 10.1016/j.neucom.2021.01.122
   Reddy GV, 2020, COGN SYST RES, V62, P23, DOI 10.1016/j.cogsys.2020.03.002
   Reddy MI, 2020, BIOSYSTEMS, V197, DOI 10.1016/j.biosystems.2020.104207
   Rui Z, 2019, IEEE ACCESS, V7, P5994, DOI 10.1109/ACCESS.2018.2889996
   Sabry M., 2010, INT J COMPUT SCI INF, V8, P129
   Sakr A.S, CMC COMPUT MAT CONT, V70, P123
   Sedik A, 2021, IEEE ACCESS, V9, P94780, DOI 10.1109/ACCESS.2021.3088341
   Sedik A, 2021, NEURAL COMPUT APPL, DOI 10.1007/s00521-020-05410-8
   Sitaula C, 2021, APPL INTELL, V51, P2850, DOI 10.1007/s10489-020-02055-x
   Srivastva R, 2021, INFORM SCIENCES, V558, P208, DOI 10.1016/j.ins.2021.01.001
   Tammina S., 2019, INT J SCI RES PUBL, V9, P143, DOI [10.29322/IJSRP.9.10.2019.p9420, DOI 10.29322/IJSRP.9.10.2019.P9420]
   Unar JA, 2014, PATTERN RECOGN, V47, P2673, DOI 10.1016/j.patcog.2014.01.016
   Wu ZF, 2019, PATTERN RECOGN, V90, P119, DOI 10.1016/j.patcog.2019.01.006
NR 50
TC 0
Z9 0
U1 9
U2 9
PU ELSEVIER SCIENCE INC
PI NEW YORK
PA STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA
SN 0020-0255
EI 1872-6291
J9 INFORM SCIENCES
JI Inf. Sci.
PD MAR
PY 2022
VL 585
BP 127
EP 143
DI 10.1016/j.ins.2021.11.066
PG 17
WC Computer Science, Information Systems
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XK9JB
UT WOS:000727771200008
OA hybrid
DA 2022-02-03
ER

PT J
AU Sumathy, G
   Renjit, JA
AF Sumathy, G.
   Renjit, J. Arokia
TI Distance-Based Method used to Localize the Eyeball Effectively for
   Cerebral Palsy Rehabilitation
SO JOURNAL OF MEDICAL SYSTEMS
LA English
DT Article
DE Canny edge detection algorithm; Circular Hough Transform; Eye; Iris;
   Cerebral PalsyKids
AB Iris plays a vital role in human life for object identification. Many models and techniques were proposed and suggested for detecting the Iris, but the accuracy was not achieved up to the level and its frequently used for biometric application. The Proposed Work divided into two steps, at first, we detect the entire eye region outer layer by using mathematics first order derivatives by applying combinations of canny edge detection and circular hough transform. The next, we detect the inner portion of eye region that is Iris region is detected by combination of sobel edge detector and circular hough transform, As the results thereby reducing the error rate, marking the edges closest to the actual edges for maximizing the localization, indicating edges and also detect the inner and outer layer of the eye portions accurately. Finally this process is applied for cerebral palsy Children to detect the misalignment of eye and obtain the deviation position and results are compared with normal children eyes. In this context, image processing techniques are being recommended as a performance evaluation tool in cerebral palsy kids.
C1 [Sumathy, G.] Sathyabama Inst Sci & Technol, Dept CSE, Chennai, Tamil Nadu, India.
   [Renjit, J. Arokia] Jeppiaar Engn Coll, Dept CSE, Chennai, Tamil Nadu, India.
C3 Sathyabama Institute of Science & Technology
RP Sumathy, G (corresponding author), Sathyabama Inst Sci & Technol, Dept CSE, Chennai, Tamil Nadu, India.
EM sumathyjayaram@gmail.com; arokiarenjith@gmail.com
RI jayaram, sumathy/AAG-1938-2021; J, AROKIA RENJIT/M-5761-2018
OI jayaram, sumathy/0000-0001-6759-5967; J, AROKIA
   RENJIT/0000-0003-3895-2907
FU DST-SERB (Department of Science and Technology-Science and Engineering
   Research Board) [EMR/2017/000073]
FX This Project is fully supported by DST-SERB (Department of Science and
   Technology-Science and Engineering Research Board), Grant Number
   (EMR/2017/000073).
CR Ahmed Z., 2016, ACM DIGITAL LIB, P1
   Chang WD, 2017, J NEUROENG REHABIL, V14, DOI 10.1186/s12984-017-0303-5
   Chouhan B, 2011, INT J COMPUTER SCI C, V2, P239, DOI [10.1007/978-3-642-23223-7_30, DOI 10.1007/978-3-642-23223-7_30]
   Garg U, 2016, SPRINGER THESES-RECO, P1, DOI 10.1007/978-3-319-22207-3_1
   George A, 2016, IET COMPUT VIS, V10, P660, DOI 10.1049/iet-cvi.2015.0316
   Halder N, 2016, IOSR J VLSI SIGNAL P, V6, P41
   Hoshino K, 2014, IEEE ENG MED BIO, P6339, DOI 10.1109/EMBC.2014.6945078
   Illavarason P, 2018, J MED IMAG HEALTH IN, V8, P1804, DOI 10.1166/jmihi.2018.2515
   Illavarason P., 2018, INT J BIOMEDICAL ENG
   Illavarason P, 2019, LECT NOTES ELECT ENG
   Ionescu C, 2015, E-HEALTH BIOENG CONF
   Jan F, 2017, COMPUT ELECTR ENG, V62, P166, DOI 10.1016/j.compeleceng.2016.11.031
   Jan F, 2014, COMPUT ELECTR ENG, V40, P215, DOI 10.1016/j.compeleceng.2014.05.004
   Kabade A. L., 2016, INT J ADV RES ELECT, V5, P1292
   Kumar V, 2016, J ELECTR COMPUT ENG, V2016, DOI 10.1155/2016/4709876
   Lakhmani S, 2014, J COMPUTER SCI INFOR, P1
   Nithya A., 2017, J ARTIF INTELL, V10, P76
   Okokpujie K, 2018, LECT NOTES ELECTR EN, V450, P203, DOI 10.1007/978-981-10-6454-8_26
   P Illavarason, 2018, Procedia Computer Science, V132, P128, DOI 10.1016/j.procs.2018.05.174
   Ramlee RA, 2017, IOP CONF SER-MAT SCI, V210, DOI 10.1088/1757-899X/210/1/012031
   Saad Iman A., 2014, INT J EMERGING TREND, V3, P61
   Vijayakumar K, 2019, CLUSTER COMPUT, V22, P10789, DOI 10.1007/s10586-017-1176-x
   Vijayakumar K, 2017, J AMB INTEL HUM COMP, DOI [10.1007/s12652-017-0503-7, DOI 10.1007/S12652-017-0503-7]
   Vijayakumar K., 2017, SPECIAL ISSUE BIOMED, P1
   Zhao X, 2017, ITM WEB CONF, V12, DOI 10.1051/itmconf/20171205006
NR 25
TC 0
Z9 0
U1 3
U2 6
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0148-5598
EI 1573-689X
J9 J MED SYST
JI J. Med. Syst.
PD AUG
PY 2019
VL 43
IS 8
AR 262
DI 10.1007/s10916-019-1405-3
PG 12
WC Health Care Sciences & Services; Medical Informatics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Health Care Sciences & Services; Medical Informatics
GA IH4DM
UT WOS:000474441900004
PM 31267270
DA 2022-02-03
ER

PT J
AU Atienza-Vanacloig, V
   Andreu-Garcia, G
   Lopez-Garcia, F
   Valiente-Gonzalez, JM
   Puig-Pons, V
AF Atienza-Vanacloig, Vicente
   Andreu-Garcia, Gabriela
   Lopez-Garcia, Fernando
   Valiente-Gonzalez, Jose M.
   Puig-Pons, Vicente
TI Vision-based discrimination of tuna individuals in grow-out cages
   through a fish bending model
SO COMPUTERS AND ELECTRONICS IN AGRICULTURE
LA English
DT Article
DE Shape modelling; Fish detection; Underwater video processing; Computer
   vision; Image segmentation; Automatic biomass estimation
AB This paper proposes a robust deformable adaptive 2D model, based on computer vision methods, that automatically fits the body (ventral silhouette) of Bluefin tuna while swimming. Our model (without human intervention) adjusts to fish shape and size, obtaining fish orientation, bending to fit their flexion motion and has proved robust enough to overcome possible segmentation inaccuracies. Once the model has been successfully fitted to the fish it can ensure that the detected object is a tuna and not parts of fish or other objects. Automatic requirements of the fishing industry like biometric measurement, specimen counting or catch biomass estimation could then be addressed using a stereoscopic system and meaningful information extracted from our model. We also introduce a fitting procedure based on a fitting parameter - Fitting Error Index (FEI) - which permits us to know the quality of the results. In the experiments our model has achieved very high success rates (up to 90%) discriminating individuals in highly complex images acquired for us in real conditions in the Mediterranean Sea. Conclusions and future improvements to the proposed model are also discussed. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Atienza-Vanacloig, Vicente; Andreu-Garcia, Gabriela; Lopez-Garcia, Fernando; Valiente-Gonzalez, Jose M.] Univ Politecn Valencia, Inst Control Syst & Ind Comp AI2, Camino Vera S-N, E-46022 Valencia, Spain.
   [Puig-Pons, Vicente] Univ Politecn Valencia, Inst Invest Gestio Integrada Zones Costaneres IGI, Camino Vera S-N, E-46022 Valencia, Spain.
C3 Universitat Politecnica de Valencia; Universitat Politecnica de Valencia
RP Atienza-Vanacloig, V (corresponding author), Univ Politecn Valencia, Inst Control Syst & Ind Comp AI2, Camino Vera S-N, E-46022 Valencia, Spain.
EM vatienza@disca.upv.es; gandreu@disca.upv.es; flopez@disca.upv.es;
   jvalient@disca.upv.es
RI Atienza-Vanacloig, Vicente/AAA-8985-2019; Lopez-Garcia,
   Fernando/G-9543-2013; Gonzalez, Jose Miguel Valiente/Y-9493-2019;
   Andreu-Garcia, Gabriela/I-1794-2015
OI Atienza-Vanacloig, Vicente/0000-0001-9017-8707; Lopez-Garcia,
   Fernando/0000-0002-2079-1589; Gonzalez, Jose Miguel
   Valiente/0000-0003-4055-8976; Andreu-Garcia,
   Gabriela/0000-0001-5448-6171
FU EU Commission (BIACOP project) [2013/410/EU]; ACUSTUNA (MINECO/FEDER,
   UE) [CTM2015-70446-R]
FX This work was partially supported by the EU Commission [2013/410/EU]
   (BIACOP project). We acknowledge funding of ACUSTUNA project ref.
   CTM2015-70446-R (MINECO/FEDER, UE).
CR Costa C, 2006, AQUACULT ENG, V35, P218, DOI 10.1016/j.aquaeng.2006.02.003
   DEWAR H, 1994, J EXP BIOL, V192, P45
   Dios JRMD, 2003, ROBOTICA, V21, P233, DOI 10.1017/S0263574702004733
   Espinosa V., 2011, P 34 SCAND S PHYS AC
   FLETCHER R, 1963, COMPUT J, V6, P163, DOI 10.1093/comjnl/6.2.163
   FLETCHER R, 1980, PRACTICAL METHODS OP, V1
   Harvey E, 2003, FISH RES, V63, P315, DOI 10.1016/S0165-7836(03)00080-8
   Hawkins J. D., 2003, J EXP BIOL, V206, P2749
   Lee DJ, 2004, PROC SPIE, V5606, P37, DOI 10.1117/12.571789
   Lines JA, 2001, COMPUT ELECTRON AGR, V31, P151, DOI 10.1016/S0168-1699(00)00181-2
   PETROU M, 1999, IMAGE PROCESSING FUN
   Piccardi M, 2004, IEEE SYS MAN CYBERN, P3099, DOI 10.1109/ICSMC.2004.1400815
   Shortis M., 2007, IS T SPIE ELECT IMAG
   Shortis M. R., 2013, P SPIE, V8791
   Spampinato C., 2010, ARTEMIS 10
   Tillett R, 2000, COMPUT VIS IMAGE UND, V79, P123, DOI 10.1006/cviu.2000.0847
   Zion B, 2007, COMPUT ELECTRON AGR, V56, P34, DOI 10.1016/j.compag.2006.12.007
   Zion B, 2012, COMPUT ELECTRON AGR, V88, P125, DOI 10.1016/j.compag.2012.07.010
NR 18
TC 17
Z9 17
U1 6
U2 61
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0168-1699
EI 1872-7107
J9 COMPUT ELECTRON AGR
JI Comput. Electron. Agric.
PD NOV 15
PY 2016
VL 130
BP 142
EP 150
DI 10.1016/j.compag.2016.10.009
PG 9
WC Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture; Computer Science
GA EF7NC
UT WOS:000390515300013
OA Green Published
DA 2022-02-03
ER

PT J
AU Li, L
   Xia, ZQ
   Hadid, A
   Jiang, XY
   Zhang, HX
   Feng, XY
AF Li, Lei
   Xia, Zhaoqiang
   Hadid, Abdenour
   Jiang, Xiaoyue
   Zhang, Haixi
   Feng, Xiaoyi
TI Replayed Video Attack Detection Based on Motion Blur Analysis
SO IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY
LA English
DT Article
DE Replayed video attack; motion blur analysis; 1D CNN; local similar
   pattern
ID FACE SPOOFING DETECTION; IMAGE QUALITY; LIVENESS DETECTION; RECOGNITION
AB Face presentation attacks are the main threats to face recognition systems, and many presentation attack detection (PAD) methods have been proposed in recent years. Although these methods have achieved significant performance in some specific intrusion modes, difficulties still exist in addressing replayed video attacks. That is because the replayed fake faces contain a variety of aliveness signals, such as eye blinking and facial expression changes. Replayed video attacks occur when attackers try to invade biometric systems by presenting face videos in front of the cameras, and these videos are often launched by a liquid-crystal display (LCD) screen. Due to the smearing effects and movements of LCD, videos captured from the real and replayed fake faces present different motion blurs, which are reflected mainly in blur intensity variation and blur width. Based on these descriptions, a motion blur analysis-based method is proposed to deal with the replayed video attack problem. We first present a 1D convolutional neural network (CNN) for motion blur intensity variation description in the time domain, which consists of a serial of 1D convolutional and pooling filters. Then, a local similar pattern (LSP) feature is introduced to extract blur width. Finally, features extracted from ID CNN and LSP are fused to detect the replayed video attacks. Extensive experiments on two standard face PAD databases, i.e., relay-attack and OULU-NPU, indicate that our proposed method based on the motion blur analysis significantly outperforms the state-of-the-art methods and shows excellent generalization capability.
C1 [Li, Lei; Xia, Zhaoqiang; Hadid, Abdenour; Jiang, Xiaoyue; Zhang, Haixi; Feng, Xiaoyi] Northwestern Polytech Univ, Xian 710129, Shaanxi, Peoples R China.
   [Hadid, Abdenour] Univ Oulu, Oulu 90014, Finland.
C3 Northwestern Polytechnical University; University of Oulu
RP Xia, ZQ; Feng, XY (corresponding author), Northwestern Polytech Univ, Xian 710129, Shaanxi, Peoples R China.
EM lilei_upu@mail.nwpu.edu.cn; zxia@nwpu.edu.cn; hadid@ee.oulu.fi;
   xjiang@nwpu.edu.cn; dennisbang@live.cn; fengxiao@nwpu.edu.cn
RI Xia, Zhaoqiang/AAC-4021-2019
OI Xia, Zhaoqiang/0000-0003-0630-3339; Li, Lei/0000-0003-4498-6126; Hadid,
   Abdenour/0000-0001-9092-735X
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [61702419]; Natural Science Basic Research
   Plan of Shaanxi Province of China [2018JQ6090]; Fundamental Research
   Funds for the Central UniversitiesFundamental Research Funds for the
   Central Universities [3102015BJ(II) ZS016]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61702419, in part by the Natural Science
   Basic Research Plan of Shaanxi Province of China under Grant 2018JQ6090,
   and in part by the Fundamental Research Funds for the Central
   Universities under Grant 3102015BJ(II) ZS016.
CR Aggarwal V, 2016, P IEEE I C SERV COMP, P1, DOI 10.1109/SCC.2016.134
   Akhtar Z., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P283, DOI 10.1109/BTAS.2012.6374590
   Alotaibi A, 2016, PROCEEDINGS OF 2016 INTERNATIONAL CONFERENCE ON OPTOELECTRONICS AND IMAGE PROCESSING (ICOIP 2016), P1, DOI [10.1109/OPTIP.2016.7528488, 10.1109/OCEANS.2016.7761105]
   Anjos A., 2011, BIOM IJCB 2011 INT J, P1, DOI DOI 10.1109/IJCB.2011.6117503
   Bao W, 2009, PROCEEDINGS OF 2009 INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND SIGNAL PROCESSING, P233
   Bharadwaj S, 2013, IEEE COMPUT SOC CONF, P105, DOI 10.1109/CVPRW.2013.23
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Boulkenafet Z, 2016, INT CONF BIOMETR
   Boulkenafet Z, 2016, IEEE T INF FOREN SEC, V11, P1818, DOI 10.1109/TIFS.2016.2555286
   Boulkenafet Z, 2015, IEEE IMAGE PROC, P2636, DOI 10.1109/ICIP.2015.7351280
   Boulkenafet Z, 2017, IEEE INT CONF AUTOMA, P612, DOI 10.1109/FG.2017.77
   Chaudhry R, 2009, PROC CVPR IEEE, P1932, DOI 10.1109/CVPRW.2009.5206821
   Chingovska I., 2012, P INT C BIOM SPEC IN, DOI DOI 10.1109/VTCFALL.2012.6399116
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   de Freitas Pereira Tiago, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P121, DOI 10.1007/978-3-642-37410-4_11
   Erdogmus N., 2013, SPOOFING 2D FACE REC, P1
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Feng LT, 2016, J VIS COMMUN IMAGE R, V38, P451, DOI 10.1016/j.jvcir.2016.03.019
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Glorot X., 2011, DEEP SPARSE RECTIFIE, P315
   Hadid A., 2011, BIOM IJCB 2011 INT J, P1, DOI [DOI 10.1109/IJCB.2011.6117510, 10.1109/IJCB.2011.6117510]
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   ISO I.O. f.S., 2016, 3010712016 ISOIEC
   Ji Z, 2016, IEEE IMAGE PROC, P1474, DOI 10.1109/ICIP.2016.7532603
   Kim S, 2014, SENSORS-BASEL, V14, P22471, DOI 10.3390/s141222471
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee SH, 2001, J KOREAN PHYS SOC, V39, pS42
   Li H., 2016, P INT C IM PROC THEO, P1
   Li HL, 2018, IEEE T INF FOREN SEC, V13, P2639, DOI 10.1109/TIFS.2018.2825949
   Li L., 2018, P IEEE INT C IM PROC, P101, DOI [10.1109/ICIP.2017.8296251., DOI 10.1109/ICIP.2017.8296251]
   Li L, 2018, J VIS COMMUN IMAGE R, V54, P182, DOI 10.1016/j.jvcir.2018.05.009
   Li L, 2018, IET BIOMETRICS, V7, P3, DOI 10.1049/iet-bmt.2017.0089
   Li X., 2017, P 2016 23 INT C PATT, P4244, DOI DOI 10.1109/ICPR.2016.7900300
   Li Y., 2014, P 9 ACM S INF COMP C, V14, P413
   Lin HY, 2004, IEEE IMAGE PROC, P3407
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Omar  L., 2015, P 7 UK COMP VIS STUD
   Pan G, 2007, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2007.4409068
   Pan G, 2011, TELECOMMUN SYST, V47, P215, DOI 10.1007/s11235-010-9313-3
   Parkhi OM, 2015, P BR MACH VIS, P1, DOI DOI 10.5244/C.29.41
   Patel K, 2016, LECT NOTES COMPUT SC, V9967, P611, DOI 10.1007/978-3-319-46654-5_67
   Patel K, 2015, INT CONF BIOMETR, P98, DOI 10.1109/ICB.2015.7139082
   Pavlidis I, 2000, IEEE WORKSHOP ON COMPUTER VISION BEYOND THE VISIBLE SPECTRUM: METHODS AND APPLICATIONS, PROCEEDINGS, P15, DOI 10.1109/CVBVS.2000.855246
   Pinto A, 2015, IEEE T IMAGE PROCESS, V24, P4726, DOI 10.1109/TIP.2015.2466088
   Pourreza HR, 2011, IEEE IMAGE PROC, P837, DOI 10.1109/ICIP.2011.6116687
   Phan QT, 2016, IEEE IMAGE PROC, P404, DOI 10.1109/ICIP.2016.7532388
   Rumelhart DE, 1995, BACKPROPAGATION THEO
   Sajjadi M. S. M., 2016, ENHANCENET SINGLE IM, P1
   Sepas-Moghaddam A, 2017, IEEE IMAGE PROC, P3815, DOI 10.1109/ICIP.2017.8296996
   Simonyan K., 2015, P ICLR
   Sluyterman AAS, 2006, J SOC INF DISPLAY, V14, P681, DOI 10.1889/1.2336093
   Smith DF, 2015, IEEE T INF FOREN SEC, V10, P736, DOI 10.1109/TIFS.2015.2398819
   Smith S. W., 1997, SCI ENG GUIDE DIGITA
   Sorel M, 2008, IEEE T IMAGE PROCESS, V17, P105, DOI 10.1109/TIP.2007.912928
   Tan XY, 2010, LECT NOTES COMPUT SC, V6316, P504, DOI 10.1007/978-3-642-15567-3_37
   Tirunagari S, 2015, IEEE T INF FOREN SEC, V10, P762, DOI 10.1109/TIFS.2015.2406533
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wadhwa N, 2014, IEEE INT CONF COMPUT
   Wen D, 2015, IEEE T INF FOREN SEC, V10, P746, DOI 10.1109/TIFS.2015.2400395
   Wu HY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185561
   Xu ZQ, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P141, DOI 10.1109/ACPR.2015.7486482
   Yang J., 2014, LEARN CONVOLUTIONAL
   Yang L., 2009, P 2 INT C IM SIGN PR, P1, DOI DOI 10.1109/CCPR.2009.5344092
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882
   Zhang SS, 2011, 2011 12TH INTERNATIONAL CONFERENCE ON ELECTRONIC PACKAGING TECHNOLOGY AND HIGH DENSITY PACKAGING (ICEPT-HDP), P436
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao PZ, 2016, 2016 IEEE INTERNATIONAL INSTRUMENTATION AND MEASUREMENT TECHNOLOGY CONFERENCE PROCEEDINGS, P1
   Zhiwei Zhang, 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P26, DOI 10.1109/ICB.2012.6199754
NR 69
TC 14
Z9 16
U1 4
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1556-6013
EI 1556-6021
J9 IEEE T INF FOREN SEC
JI IEEE Trans. Inf. Forensic Secur.
PD SEP
PY 2019
VL 14
IS 9
BP 2246
EP 2261
DI 10.1109/TIFS.2019.2895212
PG 16
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA IA8VP
UT WOS:000469836900001
OA Green Accepted
DA 2022-02-03
ER

PT C
AU Ramirez, ES
   Acosta-Guadarrama, JC
   Munoz, JMM
   Guerrero, JD
   Gonzalez-Fraga, JA
AF Santiago Ramirez, Everardo
   Acosta-Guadarrama, J. C.
   Mejia Munoz, Jose Manuel
   Dominguez Guerrero, Josue
   Gonzalez-Fraga, J. A.
BE CarrascoOchoa, JA
   MartinezTrinidad, JF
   OlveraLopez, JA
   Salas, J
TI Facial Re-identification on Non-overlapping Cameras and in Uncontrolled
   Environments
SO PATTERN RECOGNITION, MCPR 2019
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 11th Mexican Conference on Pattern Recognition (MCPR)
CY JUN 26-29, 2019
CL Queretaro, MEXICO
DE Face re-identification and recognition; Biometrics; Correlation filters
ID FACE RECOGNITION; PERSON REIDENTIFICATION
AB Face re-identification is an essential task in automatic video surveillance where the identity of the person is known previously. It aims to verify if other cameras have observed a specific face detected by a camera. However, this is a challenging task because of the reduced resolution, and changes in lighting and background available in surveillance video sequences. Furthermore, the face to get re-identified suffers changes in appearance due to expression, pose, and scale. Algorithms need robust descriptors to perform re-identification under these challenging conditions. Among various types of approaches available, correlation filters have properties that can be exploited to achieve a successful re-identification. Our proposal makes use of this approach to exploit both the shape and content of more representative facial images captured by a camera in a field of view. The resulting correlation filters can characterize the face of a person in a field of view; they are good at discriminating faces of different people, tolerant to variable illumination and slight variations in the rotation (in/out of plane) and scale. Further, they allow identifying a person from the first time that has appeared in the camera network. Matching the correlation filters generated in the field of views allows establishing a correspondence between the faces of the same person viewed by different cameras. These results show that facial re-identification under real-world surveillance conditions and biometric context can be successfully performed using correlation filters adequately designed.
C1 [Santiago Ramirez, Everardo; Acosta-Guadarrama, J. C.; Mejia Munoz, Jose Manuel; Dominguez Guerrero, Josue] Univ Autonoma Ciudad Juarez, Inst Ingn & Tecnol, Ciudad Juarez, Chihuahua, Mexico.
   [Gonzalez-Fraga, J. A.] Univ Autonoma Baja California, Fac Ciencias, Ensenada, Baja California, Mexico.
C3 Universidad Autonoma de Ciudad Juarez; Universidad Autonoma de Baja
   California
RP Ramirez, ES (corresponding author), Univ Autonoma Ciudad Juarez, Inst Ingn & Tecnol, Ciudad Juarez, Chihuahua, Mexico.
EM everardo.santiago@uacj.mx; juan.acosta@uacj.mx; jose.mejia@uacj.mx;
   josue.dominguez@uacj.mx; angel_fraga@uabc.edu.mx
RI GONZALEZ-FRAGA, JOSE ANGEL/B-3487-2008
OI GONZALEZ-FRAGA, JOSE ANGEL/0000-0003-2144-8835; Dominguez,
   Josue/0000-0002-6252-9781
CR An L, 2013, IEEE J EM SEL TOP C, V3, P155, DOI 10.1109/JETCAS.2013.2256752
   Apicella A, 2017, LECT NOTES COMPUT SC, V10484, P637, DOI 10.1007/978-3-319-68560-1_57
   Bauml Martin, 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P441, DOI 10.1109/AVSS.2010.42
   Bauml M, 2014, P 11 INT C ADV VID S
   Bedagkar-Gala A, 2014, IMAGE VISION COMPUT, V32, P270, DOI 10.1016/j.imavis.2014.02.001
   CASASENT D, 1986, APPL OPTICS, V25, P2343, DOI 10.1364/AO.25.002343
   Chen YQ, 2018, IMAGE VISION COMPUT, V79, P25, DOI 10.1016/j.imavis.2018.09.001
   Cui Z, 2014, NEUROCOMPUTING, V135, P306, DOI 10.1016/j.neucom.2013.12.004
   Gong SG, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6296-4_1
   Li Wei, 2017, CORR
   Liu Z, 2015, LECT NOTES COMPUT SC, V9008, P35, DOI 10.1007/978-3-319-16628-5_3
   Ma BP, 2014, IMAGE VISION COMPUT, V32, P379, DOI 10.1016/j.imavis.2014.04.002
   Ren YT, 2018, PATTERN RECOGN, V75, P99, DOI 10.1016/j.patcog.2017.04.012
   Ruchay A, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/8284123
   Santiago-Ramirez E., 2012, INT M EL ENG RES ENI
   Santiago-Ramirez E, 2016, SIGNAL PROCESS-IMAGE, V43, P54, DOI 10.1016/j.image.2016.02.002
   Soleymani R, 2018, EXPERT SYST APPL, V101, P271, DOI 10.1016/j.eswa.2018.01.023
   Vijayakumar Bhaga Vatula Kumar, 2005, CORRELATION PATTERN
   Wang GJ, 2015, NEUROCOMPUTING, V151, P1500, DOI 10.1016/j.neucom.2014.10.032
   Wang JY, 2018, PATTERN RECOGN, V74, P241, DOI 10.1016/j.patcog.2017.09.024
   Wang J, 2018, PATTERN RECOGN, V74, P38, DOI 10.1016/j.patcog.2017.09.014
   Wang Q, 2017, ADV OPT PHOTONICS, V9, P1, DOI 10.1364/AOP.9.000001
   Wang YJ, 2019, PATTERN RECOGN LETT, V128, P559, DOI 10.1016/j.patrec.2018.04.009
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Watson G, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.5.051215
   Wong Y., 2011, IEEE BIOM WORKSH COM, P81
   Yang B, 2013, NEUROCOMPUTING, V120, P365, DOI 10.1016/j.neucom.2012.10.032
   Zheng L., 2016, IEEE T PATTERN ANAL, V14, P1
NR 28
TC 0
Z9 0
U1 0
U2 3
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-030-21077-9; 978-3-030-21076-2
J9 LECT NOTES COMPUT SC
PY 2019
VL 11524
BP 170
EP 182
DI 10.1007/978-3-030-21077-9_16
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BO0XT
UT WOS:000493816400016
DA 2022-02-03
ER

PT J
AU Knausgard, KM
   Wiklund, A
   Sordalen, TK
   Halvorsen, KT
   Kleiven, AR
   Jiao, L
   Goodwin, M
AF Knausgard, Kristian Muri
   Wiklund, Arne
   Sordalen, Tonje Knutsen
   Halvorsen, Kim Tallaksen
   Kleiven, Alf Ring
   Jiao, Lei
   Goodwin, Morten
TI Temperate fish detection and classification: a deep learning based
   approach
SO APPLIED INTELLIGENCE
LA English
DT Article; Early Access
DE Biometric fish classification; Temperate species; Deep learning; Object
   detection; CNN; Underwater video
AB A wide range of applications in marine ecology extensively uses underwater cameras. Still, to efficiently process the vast amount of data generated, we need to develop tools that can automatically detect and recognize species captured on film. Classifying fish species from videos and images in natural environments can be challenging because of noise and variation in illumination and the surrounding habitat. In this paper, we propose a two-step deep learning approach for the detection and classification of temperate fishes without pre-filtering. The first step is to detect each single fish in an image, independent of species and sex. For this purpose, we employ the You Only Look Once (YOLO) object detection technique. In the second step, we adopt a Convolutional Neural Network (CNN) with the Squeeze-and-Excitation (SE) architecture for classifying each fish in the image without pre-filtering. We apply transfer learning to overcome the limited training samples of temperate fishes and to improve the accuracy of the classification. This is done by training the object detection model with ImageNet and the fish classifier via a public dataset (Fish4Knowledge), whereupon both the object detection and classifier are updated with temperate fishes of interest. The weights obtained from pre-training are applied to post-training as a priori. Our solution achieves the state-of-the-art accuracy of 99.27% using the pre-training model. The accuracies using the post-training model are also high; 83.68% and 87.74% with and without image augmentation, respectively. This strongly indicates that the solution is viable with a more extensive dataset.
C1 [Knausgard, Kristian Muri] Univ Agder UiA, Dept Engn Sci, N-4879 Grimstad, Norway.
   [Wiklund, Arne; Jiao, Lei; Goodwin, Morten] UiA, Ctr Artificial Intelligence Res, N-4879 Grimstad, Norway.
   [Sordalen, Tonje Knutsen; Kleiven, Alf Ring] Inst Marine Res IMR, Flodevigen Res Stn, N-4817 His, Norway.
   [Sordalen, Tonje Knutsen] UiA, Ctr Coastal Res CCR, Dept Nat Sci, N-4630 Kristiansand, Norway.
   [Halvorsen, Kim Tallaksen] Inst Marine Res IMR, Ecosyst Acoust Grp, Flodevigen Res Stn, N-4817 His, Norway.
C3 University of Agder; University of Agder; Institute of Marine Research -
   Norway; University of Agder; Institute of Marine Research - Norway
RP Knausgard, KM (corresponding author), Univ Agder UiA, Dept Engn Sci, N-4879 Grimstad, Norway.
EM kristianmk@ieee.org
FU University of Agder
FX Open Access funding provided by University of Agder
CR Andersen PA, 2018, LECT NOTES ARTIF INT, V11311, P143, DOI 10.1007/978-3-030-04191-5_11
   Bochkovskiy A., 2020, ARXIV
   Chen G, 2017, PROC INT C TOOLS ART, P24, DOI 10.1109/ICTAI.2017.00016
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fordham S, 2016, SQUALUS ACANTHIAS IU
   Francour P., 1999, NAT SICIL, V23, P155
   Halvorsen KT, 2017, ICES J MAR SCI, V74, P660, DOI 10.1093/icesjms/fsw221
   Halvorsen KT, 2016, ICES J MAR SCI, V73, P2586, DOI 10.1093/icesjms/fsw135
   Hu J, 2017, ARXIVABS170901507
   Huang PX, 2013, FISH RECOGNITION GRO
   Jin L., 2017, OCEANS 2017 ABERDEEN, P1, DOI [DOI 10.1109/OCEANSE.2017.8084645, 10.1109/OCEANSE.2017.8084645.]
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A, 2012, P1097
   Krizhevsky A., 2012, INT C NEUR INF PROC, V25, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
   Li X., 2015, P OCEANS 2015, P1, DOI [10.23919/OCEANS.2015.7404464, DOI 10.23919/OCEANS.2015.7404464]
   Liu SS, 2018, OCEANS 2018 MTS/IEEE CHARLESTON
   Lopez-Vazquez V, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20030726
   Olsvik E, 2019, LECT NOTES ARTIF INT, V11606, P89, DOI 10.1007/978-3-030-22999-3_9
   Pedersen M, 2019, CVPR WORKSH
   Pelletier D, 2011, FISH RES, V107, P84, DOI 10.1016/j.fishres.2010.10.011
   Perry D, 2018, FRONT MAR SCI, V4, DOI 10.3389/fmars.2017.00440
   Qin HW, 2016, NEUROCOMPUTING, V187, P49, DOI 10.1016/j.neucom.2015.10.122
   Rathi D., 2017, 2017 9 INT C ADV PAT, P1, DOI DOI 10.1109/ICAPR.2017.8593044
   Redmon J., 2018, ARXIV180402767
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Taheri-Garavand A, 2020, J FOOD ENG, V278, DOI 10.1016/j.jfoodeng.2020.109930
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Wenwei Xu, 2018, 2018 5th International Conference on Computational Science and Computational Intelligence (CSCI), P313, DOI 10.1109/CSCI46756.2018.00067
   White DJ, 2006, FISH RES, V80, P203, DOI 10.1016/j.fishres.2006.04.009
   Yosinski J, 2014, ADV NEUR IN, V27
NR 32
TC 2
Z9 2
U1 17
U2 33
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0924-669X
EI 1573-7497
J9 APPL INTELL
JI Appl. Intell.
DI 10.1007/s10489-020-02154-9
EA MAR 2021
PG 14
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RA3OW
UT WOS:000631328100001
OA Green Published, hybrid, Green Submitted
DA 2022-02-03
ER

PT J
AU Ricciardi, C
   Jonsson, H
   Jacob, D
   Improta, G
   Recenti, M
   Gislason, MK
   Cesarelli, G
   Esposito, L
   Minutolo, V
   Bifulco, P
   Gargiulo, P
AF Ricciardi, Carlo
   Jonsson, Halldor, Jr.
   Jacob, Deborah
   Improta, Giovanni
   Recenti, Marco
   Gislason, Magnus Kjartan
   Cesarelli, Giuseppe
   Esposito, Luca
   Minutolo, Vincenzo
   Bifulco, Paolo
   Gargiulo, Paolo
TI Improving Prosthetic Selection and Predicting BMD from Biometric
   Measurements in Patients Receiving Total Hip Arthroplasty
SO DIAGNOSTICS
LA English
DT Article
DE database analyses; electromyography; machine learning; clinical decision
   making; total hip arthroplasty
ID BONE; FRACTURE
AB There are two surgical approaches to performing total hip arthroplasty (THA): a cemented or uncemented type of prosthesis. The choice is usually based on the experience of the orthopaedic surgeon and on parameters such as the age and gender of the patient. Using machine learning (ML) techniques on quantitative biomechanical and bone quality data extracted from computed tomography, electromyography and gait analysis, the aim of this paper was, firstly, to help clinicians use patient-specific biomarkers from diagnostic exams in the prosthetic decision-making process. The second aim was to evaluate patient long-term outcomes by predicting the bone mineral density (BMD) of the proximal and distal parts of the femur using advanced image processing analysis techniques and ML. The ML analyses were performed on diagnostic patient data extracted from a national database of 51 THA patients using the Knime analytics platform. The classification analysis achieved 93% accuracy in choosing the type of prosthesis; the regression analysis on the BMD data showed a coefficient of determination of about 0.6. The start and stop of the electromyographic signals were identified as the best predictors. This study shows a patient-specific approach could be helpful in the decision-making process and provide clinicians with information regarding the follow up of patients.
C1 [Ricciardi, Carlo] Univ Hosp Naples Federico II, Dept Adv Biomed Sci, I-80131 Naples, Italy.
   [Ricciardi, Carlo; Jacob, Deborah; Recenti, Marco; Gislason, Magnus Kjartan; Gargiulo, Paolo] Reykjavik Univ, Inst Biomed & Neural Engn, IS-102 Reykjavik, Iceland.
   [Jonsson, Halldor, Jr.] Univ Iceland, Fac Med, IS-102 Reykjavik, Iceland.
   [Jonsson, Halldor, Jr.] Landspitali Hosp, Orthopaed Clin, IS-102 Reykjavik, Iceland.
   [Improta, Giovanni] Univ Hosp Naples Federico II, Dept Publ Hlth, I-80125 Naples, Italy.
   [Cesarelli, Giuseppe] Univ Naples Federico II, Dept Chem Mat & Prod Engn, I-80125 Naples, Italy.
   [Cesarelli, Giuseppe] Ist Italiano Tecnol, I-80125 Naples, Italy.
   [Esposito, Luca; Minutolo, Vincenzo] Univ Campania Luigi Vanvitelli, Dept Engn, I-81100 Aversa, CE, Italy.
   [Bifulco, Paolo] Univ Hosp Naples Federico II, Dept Elect Engn & Informat Technol, I-80125 Naples, Italy.
   [Gargiulo, Paolo] Landspitali Hosp, Dept Sci, IS-102 Reykjavik, Iceland.
C3 University of Naples Federico II; Reykjavik University; University of
   Iceland; Landspitali National University Hospital; University of Naples
   Federico II; University of Naples Federico II; Istituto Italiano di
   Tecnologia - IIT; Universita della Campania Vanvitelli; University of
   Naples Federico II; Landspitali National University Hospital
RP Ricciardi, C (corresponding author), Univ Hosp Naples Federico II, Dept Adv Biomed Sci, I-80131 Naples, Italy.; Ricciardi, C (corresponding author), Reykjavik Univ, Inst Biomed & Neural Engn, IS-102 Reykjavik, Iceland.
EM carloricciardi.93@gmail.com; halldor@landspitali.is; dcrjacob@gmail.com;
   ing.improta@gmail.com; marco18@ru.is; magnuskg@ru.is;
   giuseppe.cesarelli@unina.it; luca.esposito@unicampania.it;
   vincenzo.minutolo@unicampania.it; pabifulc@unina.it;
   paologar@landspitali.is
RI Giovanni, Improta/N-9126-2015; Minutolo, Vincenzo/C-3860-2009;
   Cesarelli, Giuseppe/ABI-8486-2020; Improta, Giovanni/AAW-6538-2021
OI Giovanni, Improta/0000-0002-9485-1687; Minutolo,
   Vincenzo/0000-0002-7787-4844; Cesarelli, Giuseppe/0000-0001-8303-5900;
   Gargiulo, Paolo/0000-0002-5049-4817; Ricciardi,
   Carlo/0000-0001-7290-6432; Bifulco, Paolo/0000-0002-9585-971X
FU University of Reykjavik; Icelandic National Hospital [A-2014-072];
   Rannis (Rannis Icelandic Research Fund (Rannsoknasjodur)) [152368-051];
   A&C M-C Foundation of Translational Myology, Padova, Italy
FX This research was supported jointly by the University of Reykjavik and
   the Icelandic National Hospital (Landspitali Scientific Fund; PI: Paolo
   Gargiulo; Title: Bone modeling in patients undergoing THA; Project
   Number: A-2014-072) with additional funding support from Rannis (Rannis
   Icelandic Research Fund (Rannsoknasjodur); PI: Paolo Gargiulo; Title:
   Clinical evaluation score for Total Hip Arthroplasty planning and
   postoperative assessment; Project Number: 152368-051). The authors wish
   to thank the A&C M-C Foundation of Translational Myology, Padova, Italy
   for sponsorship the publication.
CR Behrouzi A, 2018, EUR J TRANSL MYOL, V28, P288, DOI 10.4081/ejtm.2018.7372
   Bishop NE, 1996, J BONE JOINT SURG BR, V78B, P349, DOI 10.1302/0301-620X.78B3.0780349
   Burgess LC, 2019, CURR PHYS MED REHAB, V7, P275, DOI 10.1007/s40141-019-00225-8
   Cabitza F, 2018, FRONT BIOENG BIOTECH, V6, DOI 10.3389/fbioe.2018.00075
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Christen P, 2015, BIOMECH MODEL MECHAN, V14, P427, DOI 10.1007/s10237-014-0602-8
   Cvecka J, 2015, EUR J TRANSL MYOL, V25, P249, DOI 10.4081/ejtm.2015.5280
   D'Addio G, 2019, 15 MED C MED BIOL EN, DOI 10.1007/978-3-030-31635-8_110
   Esposito L, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10186409
   Esposito L, 2018, COMPUT METHOD BIOMEC, V21, P663, DOI 10.1080/10255842.2018.1508570
   Esposito L, 2016, COMPUT METHOD BIOMEC, V19, P257, DOI 10.1080/10255842.2015.1014347
   Fraldi M, 2017, MECH TIME-DEPEND MAT, V21, P45, DOI 10.1007/s11043-016-9317-9
   Fraldi M, 2010, BIOMECH MODEL MECHAN, V9, P389, DOI 10.1007/s10237-009-0183-0
   Gargiulo P, 2019, ENCYCLOPEDIA OF BIOMEDICAL ENGINEERING, VOL 2, P119, DOI 10.1016/B978-0-12-801238-3.99920-3
   Gargiulo P, 2018, P I MECH ENG H, V232, DOI 10.1177/0954411918797971
   Gargiulo P, 2013, ARTIF ORGANS, V37, P567, DOI 10.1111/aor.12033
   Gislason MK, 2020, CLIN BIOMECH, V78, DOI 10.1016/j.clinbiomech.2020.105092
   Haeberle HS, 2019, J ARTHROPLASTY, V34, P2201, DOI 10.1016/j.arth.2019.05.055
   Hailer NP, 2010, ACTA ORTHOP, V81, P34, DOI 10.3109/17453671003685400
   Harris AHS, 2019, CLIN ORTHOP RELAT R, V477, P452, DOI 10.1097/CORR.0000000000000601
   HUISKES R, 1992, CLIN ORTHOP RELAT R, P124
   Improta G, 2019, TQM J, V31, P526, DOI [10.1108/ TQM-10- 2018-0142, DOI 10.1108/TQM-10-2018-0142]
   Improta G, 2020, IFMBE PROC, V76, P793, DOI 10.1007/978-3-030-31635-8_95
   Izzo GM, 2012, EUR J TRANSL MYOL, V22, P69, DOI 10.4081/ejtm.2012.1795
   Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X
   LAWRENCE JM, 1994, J BONE JOINT SURG AM, V76A, P965, DOI 10.2106/00004623-199407000-00002
   Magnusson B, 2015, EUR J TRANSL MYOL, V25, P101, DOI 10.4081/ejtm.2015.4913
   Minutolo V, 2020, MECCANICA, V55, P1603, DOI 10.1007/s11012-020-01189-z
   MORELAND JR, 1995, CLIN ORTHOP RELAT R, P141
   Navarro SM, 2018, J ARTHROPLASTY, V33, P3617, DOI 10.1016/j.arth.2018.08.028
   Norouzi A, 2018, EUR J TRANSL MYOL, V28, P280, DOI 10.4081/ejtm.2018.7355
   Petursson P, 2015, COMPUT MATH METHOD M, V2015, DOI 10.1155/2015/162481
   Piwek L, 2016, PLOS MED, V13, DOI 10.1371/journal.pmed.1001953
   Radl R, 2000, J BONE JOINT SURG BR, V82B, P1151, DOI 10.1302/0301-620X.82B8.11030
   Ramkumar P., 2019, ORTHOPAEDIC P BONE J, P101
   Ramkumar PN, 2019, J ARTHROPLASTY, V34, P632, DOI 10.1016/j.arth.2018.12.030
   Recenti M., 2020, P 2020 IEEE INT S ME, P1
   Recenti M, 2020, EUR J TRANSL MYOL, V30, P121, DOI 10.4081/ejtm.2019.8892
   Ricciardi C., 2020, METHODS PROGRAMS BIO, DOI 10.1016/j.cmpb.2020.105712
   RICCIARDI C, 2020, SCI REP UK, V0010
   Ricciardi C., 2020, TQM J, V32, P461, DOI [10.1108/TQM-06-2019-0159, DOI 10.1108/TQM-06-2019-0159, 10.1108/TQM- 06-2019-0159]
   Ricciardi C, 2020, COMPUT METH PROG BIO, V189, DOI 10.1016/j.cmpb.2020.105343
   Robbins SM, 2020, J ARTHROPLASTY, V35, P1891, DOI 10.1016/j.arth.2020.02.037
   Romeo V, 2020, ANTICANCER RES, V40, P271, DOI 10.21873/anticanres.13949
   Sarabon N, 2020, EUR J TRANSL MYOL, V30, P125, DOI 10.4081/ejtm.2019.8898
   Skytta ET, 2011, ACTA ORTHOP, V82, P1, DOI 10.3109/17453674.2010.548029
   Stanzione A, 2020, J DIGIT IMAGING, V33, P879, DOI 10.1007/s10278-020-00336-y
   Taaffe DR, 2001, J BONE MINER RES, V16, P1343, DOI 10.1359/jbmr.2001.16.7.1343
   Taylor MJ, 2019, EUR J TRANSL MYOL, V29, P283, DOI 10.4081/ejtm.2019.8285
   Tougui I, 2020, HEALTH TECHNOL-GER, V10, P1137, DOI 10.1007/s12553-020-00438-1
   Tyson Y, 2019, ACTA ORTHOP, V90, P421, DOI 10.1080/17453674.2019.1624336
   Venesmaa PK, 2001, J BONE MINER RES, V16, P1056, DOI 10.1359/jbmr.2001.16.6.1056
   Wolff J., 1893, DMW DTSCH MEDIZINISC, V19, P1222, DOI [10.1055/s-0028-1144106, DOI 10.1055/S-0028-1144106]
   Yamada H, 2009, J ORTHOP SCI, V14, P228, DOI 10.1007/s00776-008-1317-4
NR 54
TC 6
Z9 6
U1 1
U2 3
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2075-4418
J9 DIAGNOSTICS
JI Diagnostics
PD OCT
PY 2020
VL 10
IS 10
AR 815
DI 10.3390/diagnostics10100815
PG 13
WC Medicine, General & Internal
WE Science Citation Index Expanded (SCI-EXPANDED)
SC General & Internal Medicine
GA OP7UR
UT WOS:000588292700001
PM 33066350
OA Green Published, gold
DA 2022-02-03
ER

PT J
AU Toor, AS
   Wechsler, H
   Nappi, M
AF Toor, Andeep S.
   Wechsler, Harry
   Nappi, Michele
TI Question action relevance and editing for visual question answering
SO MULTIMEDIA TOOLS AND APPLICATIONS
LA English
DT Article
DE Computer vision; Visual question answering; Deep learning; Action
   recognition; Image understanding; Question relevance
AB Visual Question Answering (VQA) expands on the Turing Test, as it involves the ability to answer questions about visual content. Current efforts in VQA, however, still do not fully consider whether a question about visual content is relevant and if it is not, how to edit it best to make it answerable. Question relevance has only been considered so far at the level of a whole question using binary classification and without the capability to edit a question to make it grounded and intelligible. The only exception to this is our prior research effort into question part relevance that allows for relevance and editing based on object nouns. This paper extends previous work on object relevance to determine the relevance for a question action and leverage this capability to edit an irrelevant question to make it relevant. Practical applications of such a capability include answering biometric-related queries across a set of images, including people and their action (behavioral biometrics). The feasibility of our approach is shown using Context-Collaborative VQA (C2VQA) Action/Relevance/Edit (ARE). Our results show that our proposed approach outperforms all other models for the novel tasks of question action relevance (QAR) and question action editing (QAE) by a significant margin. The ultimate goal for future research is to address full-fledged W5 + type of inquires (What, Where, When, Why, Who, and How) that are grounded to and reference video using both nouns and verbs in a collaborative context-aware fashion.
C1 [Toor, Andeep S.; Wechsler, Harry] George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
   [Nappi, Michele] Univ Salerno, Dipartimento Informat, Fisciano, Italy.
C3 George Mason University; University of Salerno
RP Toor, AS (corresponding author), George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
EM andeep.toor@gmail.com; wechsler@gmu.edu; mnappi@unisa.it
RI Nappi, Michele/X-3089-2019
CR Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   de Vries H., 2016, ARXIV161108481
   Geman D, 2015, P NATL ACAD SCI USA, V112, P3618, DOI 10.1073/pnas.1422953112
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Z., 2015, ARXIV150801991
   Krishna R., 2016, ARXIV160207332
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mallya A, 2016, LECT NOTES COMPUT SC, V9905, P414, DOI 10.1007/978-3-319-46448-0_25
   Mikolov T., 2013, NEURIPS, V26, P3111
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI 10.3115/v1/D14-1162
   Ray A., 2016, ARXIV160606622
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Ronchi Matteo Ruggero, 2015, P BRIT MACH VIS C BM
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Toor A. S., 2017, P 15 INT WORKSH CONT, P4
   Toor AS, 2018, PATTERN RECOGN LETT, V113, P29, DOI 10.1016/j.patrec.2017.02.012
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu DF, 2017, PROC CVPR IEEE, P4187, DOI 10.1109/CVPR.2017.446
NR 22
TC 6
Z9 6
U1 7
U2 13
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1380-7501
EI 1573-7721
J9 MULTIMED TOOLS APPL
JI Multimed. Tools Appl.
PD FEB
PY 2019
VL 78
IS 3
BP 2921
EP 2935
DI 10.1007/s11042-018-6097-z
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA HK7MC
UT WOS:000458171600014
DA 2022-02-03
ER

PT J
AU Wang, J
   Wang, GQ
   Zhou, M
AF Wang, Jun
   Wang, Guoqing
   Zhou, Mei
TI Bimodal Vein Data Mining via Cross-Selected-Domain Knowledge Transfer
SO IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY
LA English
DT Article
DE Hand vein information; gender classification; personal identification;
   transfer learning; coarse-to-fine; task-driven; LDM; supervised feature
   selection
ID SOFT BIOMETRIC TRAITS; GENDER CLASSIFICATION; FACE-RECOGNITION;
   PREDICTION; PATTERNS; FEATURES
AB Recent success in large-scale image recognition challenge (i.e., ImageNet) fully demonstrates the capability of deep neural network (DNN) in learning complex and semantic representation, and this also motivates the generation of transfer learning model, which fine-tunes state-of-the-art DNN models with other small-scale databases for better performance. Driven by such an idea, a task-specific DNN model fine-tuned from VGG-face is constructed for both gender and identity recognition with hand vein information. Unlike the traditional transfer learning models, which fine-tune directly from source to target, we leverage the coarse-to-fine scheme to train the task-specific models in a step-aware way, such that the inherent correlation between the neighboring databases could serve as initialization base to relieve the problem of over-fitting, which is inevitable with the small-scaled hand vein database, and also speed up the convergence. Besides, the task-driven network training idea, which involves joint optimization of linear regression classifier and network parameters, is also adopted during training of each model to obtain more discriminative representation for specified tasks. Instead of adopting the trained linear regression classifier for gender and identity classification, the large margin distribution machine (LDM) is introduced to ensure the discriminative and generalization performance of the model simultaneously, and it should be noted that before feeding the gender feature vector into the LDM, a supervised feature selection step is incorporated to improve the classification performance by discarding the redundant feature and highlighting the important ones for gender classification. Rigorous experiments using the lab-made database are conducted to demonstrate the effectiveness and feasibility of the proposed model. What is more, additional experiment with a subset of the PolyU database illustrates its generalization ability and robustness.
C1 [Wang, Jun; Wang, Guoqing] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221000, Peoples R China.
   [Zhou, Mei] East China Normal Univ, Shanghai Key Lab Multidimens Informat Proc, Shanghai 200241, Peoples R China.
C3 China University of Mining & Technology; East China Normal University
RP Wang, GQ (corresponding author), China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221000, Peoples R China.
EM WJ999LX@163.com; wangguoqingcumt@hotmail.com; mzhou@ce.ecnu.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [61379143]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61379143.
CR Abdulla W., 2013, P 8 CHIN C BIOM REC, P314
   Alexandre LA, 2010, PATTERN RECOGN LETT, V31, P1422, DOI 10.1016/j.patrec.2010.02.010
   Amayeh Gholamreza, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563122
   Antipov G, 2016, PATTERN RECOGN LETT, V70, P59, DOI 10.1016/j.patrec.2015.11.011
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Arun K. S., 2011, 2011 IEEE Recent Advances in Intelligent Computational Systems (RAICS 2011), P163, DOI 10.1109/RAICS.2011.6069294
   Baluja S, 2007, INT J COMPUT VISION, V71, P111, DOI 10.1007/s11263-006-8910-9
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   BRUNELLI R, 1993, IEEE T PATTERN ANAL, V15, P1042, DOI 10.1109/34.254061
   Buchala S, 2005, INT J SYST SCI, V36, P931, DOI 10.1080/00207720500381573
   Cao L, 2008, P 16 ACM INT C MULT, P725, DOI DOI 10.1145/1459359.1459470
   Chen H-F, 2009, P 2 INT C INT SCI IN, P1207
   Danisman T, 2014, INT C PATT RECOG, P3144, DOI 10.1109/ICPR.2014.542
   Dantcheva A, 2016, IEEE T INF FOREN SEC, V11, P441, DOI 10.1109/TIFS.2015.2480381
   Demirkus M., 2012, CVPR WORKSH, P130
   Demirkus M., 2010, 2010 IEEE COMP SOC C, P55
   Ding ZM, 2017, IEEE T IMAGE PROCESS, V26, P660, DOI 10.1109/TIP.2016.2631887
   Eidinger E, 2014, IEEE T INF FOREN SEC, V9, P2170, DOI 10.1109/TIFS.2014.2359646
   Ergun H, 2016, 2016 24TH SIGNAL PROCESSING AND COMMUNICATION APPLICATION CONFERENCE (SIU), P2173, DOI 10.1109/SIU.2016.7496204
   Fellous JM, 1997, VISION RES, V37, P1961, DOI 10.1016/S0042-6989(97)00010-2
   Gaiwak A, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON PERSONAL WIRELESS COMMUNICATIONS, P441, DOI 10.1109/ICPWC.2005.1431384
   Golomb B., 1991, ADV NEURAL INFORMATI, V3, P572
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang D, 2015, IEEE T CYBERNETICS, V45, P1823, DOI 10.1109/TCYB.2014.2360894
   Huang Gary B., 2007, 0749 U MASS
   Jain A, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P159, DOI 10.1109/AFGR.2004.1301524
   Jain AK, 2004, PROC SPIE, V5404, P561, DOI 10.1117/12.542890
   Jia W, 2014, IEEE T SYST MAN CY-S, V44, P385, DOI 10.1109/TSMC.2013.2258010
   Kang BJ, 2011, OPT ENG, V50, DOI 10.1117/1.3530023
   Krizhevsky A., 2012, INT C NEUR INF PROC, V25, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
   Kumar A, 2009, IEEE T IMAGE PROCESS, V18, P2127, DOI 10.1109/TIP.2009.2023153
   Ladoux PO, 2009, LECT NOTES COMPUT SC, V5558, P1290, DOI 10.1007/978-3-642-01793-3_130
   Lee EC, 2009, INT J IMAG SYST TECH, V19, P179, DOI 10.1002/ima.20193
   Levi Gil, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P34, DOI 10.1109/CVPRW.2015.7301352
   Li XO, 2010, PROC CVPR IEEE, P2590, DOI 10.1109/CVPR.2010.5539969
   Lian HC, 2006, LECT NOTES COMPUT SC, V3972, P202
   Lin CL, 2004, IEEE T CIRC SYST VID, V14, P199, DOI 10.1109/TCSVT.2003.821975
   Lin Y, 2014, IEEE T INF FOREN SEC, V9, P147, DOI 10.1109/TIFS.2013.2291314
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu L, 2009, INT CONF ACOUST SPEE, P1065, DOI 10.1109/ICASSP.2009.4959771
   MacGregor P., 1991, ADV IMAGING, V6, P52
   Maiorana E, 2016, IEEE T INF FOREN SEC, V11, P163, DOI 10.1109/TIFS.2015.2481870
   Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156
   Malki S., 2007, P SOC PHOTO-OPT INS, V6590
   Miura N, 2004, MACH VISION APPL, V15, P194, DOI 10.1007/s00138-004-0149-2
   Moghaddam B, 2002, IEEE T PATTERN ANAL, V24, P707, DOI 10.1109/34.1000244
   Morel JM, 2009, SIAM J IMAGING SCI, V2, P438, DOI 10.1137/080732730
   Nalla PR, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2616281
   Nian F., 2015, ACM INT C INT MULT C, P77
   Niinuma K, 2010, IEEE T INF FOREN SEC, V5, P771, DOI 10.1109/TIFS.2010.2075927
   NOCEDAL J, 1980, MATH COMPUT, V35, P773, DOI 10.2307/2006193
   Ozbulak G, 2016, 2016 INT C BIOM SPEC, P1, DOI [10.1109/BIOSIG.2016.7736925, DOI 10.1109/BIOSIG.2016.7736925]
   Parkhi OM, 2015, P BR MACH VIS, P1, DOI DOI 10.5244/C.29.41
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Ren CX, 2016, IEEE T CYBERNETICS, V46, P2656, DOI 10.1109/TCYB.2015.2484356
   Rosdi BA, 2011, SENSORS-BASEL, V11, P11357, DOI 10.3390/s111211357
   Shan CF, 2008, NEUROCOMPUTING, V71, P1931, DOI 10.1016/j.neucom.2007.09.023
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/CVPR.2015.7298594
   Song W, 2011, PATTERN RECOGN LETT, V32, P1541, DOI 10.1016/j.patrec.2011.04.021
   Suarez Pascual J Enrique, 2010, Proceedings of the 2010 Sixth International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIHMSP 2010), P318, DOI 10.1109/IIHMSP.2010.85
   Sun ZH, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P165, DOI 10.1109/ACV.2002.1182176
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang J., 2014, INT J SIGNAL PROCESS, V6, P323
   Wang JF, 2008, INTERNET J MED UPDAT, V3, P22
   Wang Jun, 2014, International Journal of Bioautomation, V18, P337
   Wang YD, 2010, LECT NOTES COMPUT SC, V6215, P490, DOI 10.1007/978-3-642-14922-1_61
   Wiskott L., 1995, INT WORKSH AUT FAC G, P92
   Wunderlich RE, 2001, MED SCI SPORT EXER, V33, P605
   Xu J, 2013, IET COMPUT VIS, V7, P48, DOI 10.1049/iet-cvi.2011.0193
   Xu W., 2011, OPTIMAL ONE PASS LAR
   Ylioinas J, 2011, LECT NOTES COMPUT SC, V6688, P676, DOI 10.1007/978-3-642-21227-7_63
   Yoo JH, 2005, LECT NOTES COMPUT SC, V3708, P138
   Zhang BC, 2010, PATTERN RECOGN LETT, V31, P2337, DOI 10.1016/j.patrec.2010.07.006
   Zhang D, 2003, IEEE T PATTERN ANAL, V25, P1041, DOI 10.1109/TPAMI.2003.1227981
   Zhang D, 2009, LECT NOTES COMPUT SC, V5558, P1010, DOI 10.1007/978-3-642-01793-3_102
   Zhang H, 2012, J NANOMATER, V2012, DOI [10.1155/2012/217412, 10.1186/2049-1891-3-26]
   Zhang Kaipeng, 2016, P IEEE C COMP VIS PA, P34
   Zhao S, 2008, PROCEEDINGS OF 2008 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P1172, DOI 10.1109/ICMLC.2008.4620581
   Zhou Y, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0116446
   Zhou YB, 2011, IEEE T INF FOREN SEC, V6, P1259, DOI 10.1109/TIFS.2011.2158423
   Zhou YH, 2016, IEEE T KNOWL DATA EN, V28, P1749, DOI 10.1109/TKDE.2016.2535283
   Zhou Z, 2012, IEEE T SYST MAN CY A, V42, P571, DOI 10.1109/TSMCA.2011.2170416
NR 83
TC 17
Z9 18
U1 12
U2 56
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1556-6013
EI 1556-6021
J9 IEEE T INF FOREN SEC
JI IEEE Trans. Inf. Forensic Secur.
PD MAR
PY 2018
VL 13
IS 3
BP 733
EP 744
DI 10.1109/TIFS.2017.2766039
PG 12
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA FR0AJ
UT WOS:000418723000015
DA 2022-02-03
ER

PT J
AU Aderinola, TB
   Connie, T
   Ong, TS
   Yau, WC
   Teoh, ABJ
AF Aderinola, Timilehin B.
   Connie, Tee
   Ong, Thian Song
   Yau, Wei-Chuen
   Teoh, Andrew Beng Jin
TI Learning Age From Gait: A Survey
SO IEEE ACCESS
LA English
DT Article
DE Estimation; Feature extraction; Sensors; Legged locomotion; Cameras;
   Task analysis; Sensor phenomena and characterization; Age estimation;
   age group classification; gait; gait age; gait feature extraction
ID PERFORMANCE EVALUATION; RECOGNITION; CLASSIFICATION; PATTERNS; DATABASE;
   GENDER; FACE; FUSION; SYSTEM
AB Age is an important human attribute that needs to be determined for various purposes, including security, health, human identification, and law enforcement. Hence, there is an increasing research interest in automatic age estimation using biometric traits such as face and gait. In recent years, gait analysis has received growing attention due to the pervasive nature of video surveillance. Gait signals that measure the manner of walking can be obtained using vision and sensor-based techniques. Individual gait patterns obtainable from videos, images, or sensors are shown unconsciously and are not easily obscured. Additionally, gait signals can be obtained unobtrusively with cameras placed at a long distance because gait does not require high-resolution images. However, the extraction of age-associated gait features is a challenging task due to various gait covariates. These covariates include clothing and view changes for vision-based gait; walking slope and footwear for sensor-based gait. This paper provides a survey of scientific literature on age estimation using gait features. We focus on the approaches to extracting age-associated gait features, namely, vision-based and sensor-based approaches, how they may be affected by the different covariates, and domain-specific applications. To make this work useful for as wide of an audience as possible, we also include discussions on key topics such as existing datasets, evaluation strategies, and open challenges that should be addressed in the future.
C1 [Aderinola, Timilehin B.; Connie, Tee; Ong, Thian Song] Multimedia Univ, Fac Informat Sci & Technol, Malacca 75450, Malaysia.
   [Yau, Wei-Chuen] Xiamen Univ Malaysia, Sch Elect & Comp Engn, Sepang 43900, Malaysia.
   [Teoh, Andrew Beng Jin] Yonsei Univ, Sch Elect & Elect Engn, Coll Engn, Seoul 03722, South Korea.
C3 Multimedia University; Xiamen University Malaysia Campus; Yonsei
   University
RP Yau, WC (corresponding author), Xiamen Univ Malaysia, Sch Elect & Comp Engn, Sepang 43900, Malaysia.
EM tee.connie@mmu.edu.my; wcyau@xmu.edu.my
OI Tee, Connie/0000-0002-0901-3831
FU Xiamen University Malaysia Research Fund [XMUMRF/2019-C4/IECE/0011];
   Multimedia University Graduate Research Assistant Scheme [MMUI/190045]
FX This work was supported by Xiamen University Malaysia Research Fund
   (Grant No. XMUMRF/2019-C4/IECE/0011) and Multimedia University Graduate
   Research Assistant Scheme under Grant MMUI/190045.
CR Abirami B, 2020, MATER TODAY-PROC, V33, P4646, DOI 10.1016/j.matpr.2020.08.298
   Aderinola T. B., P INT C INN INF COMM, V2021, P71, DOI [10.1007/978-981-16-0873-5_6, DOI 10.1007/978-981-16-0873-5_6]
   Ahad MAR, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20082424
   Alexander NB, 1996, J AM GERIATR SOC, V44, P434, DOI 10.1111/j.1532-5415.1996.tb06417.x
   Ashkenazy Y, 2002, PHYSICA A, V316, P662, DOI 10.1016/S0378-4371(02)01453-X
   Begg RK, 2005, IEEE T BIO-MED ENG, V52, P828, DOI 10.1109/TBME.2005.845241
   Berksan M, 2019, GENDER RECOGNITION A
   Bobick A. F., 2001, P IEEE COMP SOC C CO, V1, P1, DOI [10.1109/CVPR.2001.990506, DOI 10.1109/CVPR.2001.990506]
   Bogin B, 2010, INT J ENV RES PUB HE, V7, P1047, DOI 10.3390/ijerph7031047
   Burt C, 2020, BIOMETRICS COULD HEL
   Callisaya ML, 2008, J GERONTOL A-BIOL, V63, P165, DOI 10.1093/gerona/63.2.165
   Carletti V, 2020, IEEE T PATTERN ANAL, V42, P2113, DOI 10.1109/TPAMI.2019.2910522
   Chang KY, 2015, IEEE T IMAGE PROCESS, V24, P785, DOI 10.1109/TIP.2014.2387379
   Chao H., ARXIV210203247
   Chao H., 2018, ARXIV181106186
   Chen SX, 2018, IEEE T MULTIMEDIA, V20, P2209, DOI 10.1109/TMM.2017.2786869
   Chen SX, 2018, IEEE 1ST CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2018), P226, DOI 10.1109/MIPR.2018.00055
   Chuen BKY, 2015, ASIAPAC SIGN INFO PR, P800, DOI 10.1109/APSIPA.2015.7415382
   Connie T, 2015, 2015 3RD INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGY (ICOICT), P574, DOI 10.1109/ICoICT.2015.7231488
   Connor P, 2018, COMPUT VIS IMAGE UND, V167, P1, DOI 10.1016/j.cviu.2018.01.007
   Davis JW, 2001, LECT NOTES COMPUT SC, V2091, P295
   De Zhang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3834, DOI 10.1109/ICPR.2010.934
   Dou H., 2021, ARXIV210101394
   Fan C., 2020, P IEEE CVF C COMP VI, P14225
   Fang H.-S., 2016, ARXIV161200137
   Fasel B, 2015, J BIOMECH, V48, P3199, DOI 10.1016/j.jbiomech.2015.07.001
   Gafurov D, 2006, LECT NOTES COMPUT SC, V4277, P479
   Gafurov D, 2007, 2007 IEEE WORKSHOP ON AUTOMATIC IDENTIFICATION ADVANCED TECHNOLOGIES, PROCEEDINGS, P220, DOI 10.1109/AUTOID.2007.380623
   Gillani S. I., 2020, P INT C EM TRENDS SM, P1, DOI [10.1109/ICETST49965.2020.9080735, DOI 10.1109/ICETST49965.2020.9080735]
   Giordano C., 2019, INDEPENDENT
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   Hediyeh H, 2013, TRANSPORT RES REC, P31, DOI 10.3141/2393-04
   Hema M., 2019, P 3 INT C TRENDS EL, P1163, DOI [10.1109/ICOEI.2019.8862788, DOI 10.1109/ICOEI.2019.8862788]
   Hofmann M, 2014, J VIS COMMUN IMAGE R, V25, P195, DOI 10.1016/j.jvcir.2013.02.006
   Ince Omer F., 2014, International Journal of Computer and Communication Engineering, V3, P120, DOI 10.7763/IJCCE.2014.V3.304
   Islam T. U., P INT C INN COMP COM, V2021, P947, DOI [10.1007/978-981-15-5148-2_82, DOI 10.1007/978-981-15-5148-2_82]
   Iwama H, 2012, IEEE T INF FOREN SEC, V7, P1511, DOI 10.1109/TIFS.2012.2204253
   Kang D., 2018, AP NEWS 1106
   Khabir KM, 2019, 2019 JOINT 8TH INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV) AND 2019 3RD INTERNATIONAL CONFERENCE ON IMAGING, VISION & PATTERN RECOGNITION (ICIVPR) WITH INTERNATIONAL CONFERENCE ON ACTIVITY AND BEHAVIOR COMPUTING (ABC), P371, DOI 10.1109/ICIEV.2019.8858521
   Khamsemanan N, 2018, IEEE T INF FOREN SEC, V13, P119, DOI 10.1109/TIFS.2017.2738611
   Kim W, 2018, 2018 19TH IEEE/ACIS INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, ARTIFICIAL INTELLIGENCE, NETWORKING AND PARALLEL/DISTRIBUTED COMPUTING (SNPD), P70, DOI 10.1109/SNPD.2018.8441030
   Ko SU, 2012, ARCH GERONTOL GERIAT, V55, P474, DOI 10.1016/j.archger.2012.04.004
   Ko SU, 2011, J BIOMECH, V44, P1974, DOI 10.1016/j.jbiomech.2011.05.005
   Kobayashi Y, 2016, GAIT POSTURE, V46, P11, DOI 10.1016/j.gaitpost.2016.01.021
   Kovac J, 2019, MULTIMED TOOLS APPL, V78, P5621, DOI 10.1007/s11042-017-5469-0
   Kowalski E, 2019, GAIT POSTURE, V67, P133, DOI 10.1016/j.gaitpost.2018.10.008
   Kung SM, 2019, HUM MOVEMENT SCI, V66, P600, DOI 10.1016/j.humov.2019.06.012
   Lahmiri S, 2019, IEEE T INSTRUM MEAS, V68, P2545, DOI 10.1109/TIM.2018.2866316
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   Li X, 2018, MULTIMED TOOLS APPL, V77, P28333, DOI 10.1007/s11042-018-6049-7
   Li X, 2020, PSYCHOTHER RES, V30, P604, DOI 10.1080/10503307.2019.1649730
   Liao RJ, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107069
   Lu JW, 2014, IEEE T INF FOREN SEC, V9, P51, DOI 10.1109/TIFS.2013.2291969
   Lu JW, 2013, IEEE T HUM-MACH SYST, V43, P249, DOI 10.1109/TSMCC.2012.2192727
   Lu JW, 2010, IEEE T INF FOREN SEC, V5, P761, DOI 10.1109/TIFS.2010.2069560
   Ma C., 2020, ARXIV200508625
   Makihara Y, 2011, LECT NOTES COMPUT SC, V6493, P440
   Makihara Yasushi, 2011, P INT J C BIOM IJCB, DOI 10.1109/ijcb.2011.6117531
   Mansouri N, 2018, IET COMPUT VIS, V12, P69, DOI 10.1049/iet-cvi.2017.0055
   Marin-Jimenez MJ, 2017, IEEE IMAGE PROC, P106, DOI 10.1109/ICIP.2017.8296252
   Nabila M, 2018, IET BIOMETRICS, V7, P116, DOI 10.1049/iet-bmt.2016.0176
   Nixon M. S., 1999, P IEE C MOT AN TRACK, P3, DOI [10.1049/ic:19990573, DOI 10.1049/IC:19990573]
   Padme S. E., 2015, IJSR, V4, P1927, DOI [10.21275/v4i12.nov152411, DOI 10.21275/V4I12.NOV152411]
   Patua Rupali, 2021, Progress in Advanced Computing and Intelligent Engineering. Proceedings of ICACIE 2019. Advances in Intelligent Systems and Computing (AISC 1198), P62, DOI 10.1007/978-981-15-6584-7_7
   Phillips PJ, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P137, DOI 10.1109/AFGR.2002.1004145
   PITTENGER JB, 1990, PERCEPT PSYCHOPHYS, V48, P124, DOI 10.3758/BF03207078
   Prakash C, 2018, ARTIF INTELL REV, V49, P1, DOI 10.1007/s10462-016-9514-6
   Prince F, 1997, GAIT POSTURE, V5, P128, DOI 10.1016/S0966-6362(97)01118-1
   Punyani P., 2018, ADV ELECT COMMUNICAT, P325
   Punyani P, 2018, INT J IMAGE DATA FUS, V9, P222, DOI 10.1080/19479832.2018.1423644
   Qiu S, 2016, IEEE T INSTRUM MEAS, V65, P939, DOI 10.1109/TIM.2015.2504078
   Riaz Q, 2015, SENSORS-BASEL, V15, P31999, DOI 10.3390/s151229907
   Rijun Liao, 2017, Biometric Recognition. 12th Chinese Conference, CCBR 2017. Proceedings: LNCS 10568, P474, DOI 10.1007/978-3-319-69923-3_51
   Roether CL, 2009, J VISION, V9, DOI 10.1167/9.6.15
   Russel NS, 2021, IET IMAGE PROCESS, V15, P239, DOI 10.1049/ipr2.12024
   Saihui Hou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P382, DOI 10.1007/978-3-030-58545-7_22
   Sakata Atsuya, 2019, IPSJ Transactions on Computer Vision and Applications, V11, DOI 10.1186/s41074-019-0054-2
   Sakata A., 2020, P IEEE INT JOINT C B, P1, DOI [10.1109/IJCB48548.2020.9304914, DOI 10.1109/IJCB48548.2020.9304914]
   Sakata A, 2019, LECT NOTES COMPUT SC, V11367, P55, DOI 10.1007/978-3-030-21074-8_5
   Saleh M. M., 2017, P 2017 IEEE POW EN S P IEEE INT C COMP IN, P1, DOI [10.1109/ICCIC.2017.8523836, DOI 10.1109/ICCIC.2017.8523836]
   Sarkar S, 2005, IEEE T PATTERN ANAL, V27, P162, DOI 10.1109/TPAMI.2005.39
   Shen W., 2017, ARXIV171207195
   Sutherland D, 1997, GAIT POSTURE, V6, P163, DOI 10.1016/S0966-6362(97)00029-5
   Takemura Noriko, 2018, IPSJ Transactions on Computer Vision and Applications, V10, DOI 10.1186/s41074-018-0039-6
   Thakurta A. G., 2016, ADV APPL PHYSL, V1, P24, DOI [10.11648/j.aap.20160102.12, DOI 10.11648/J.AAP.20160102.12]
   Ngo TT, 2014, PATTERN RECOGN, V47, P228, DOI 10.1016/j.patcog.2013.06.028
   Vajdi A., 2019, ARXIV190503109
   Wang L, 2003, IEEE T PATTERN ANAL, V25, P1505, DOI 10.1109/TPAMI.2003.1251144
   Weizhi An, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P421, DOI 10.1109/TBIOM.2020.3008862
   Wu Y, 2014, IEEE ENG MED BIO, P6915, DOI 10.1109/EMBC.2014.6945218
   Xie JC, 2019, IEEE T INF FOREN SEC, V14, P2500, DOI 10.1109/TIFS.2019.2902823
   Xu C., 2021, P IEEE WINT C APPL C, P3460
   Xu C., 2021, IEEE T BIOMETRICS BE, DOI [10.1109/TBIOM.2021.3080300, DOI 10.1109/TBIOM.2021.3080300]
   Xu C., 2017, IPSJ T COMPUT VIS AP, V9, P1
   Xu C, 2019, MACH VISION APPL, V30, P629, DOI 10.1007/s00138-019-01015-x
   Yang C., 2015, P 4 INT C COMP SCI N, V1, P266, DOI [10.1109/ICCSNT.2015.7490749, DOI 10.1109/ICCSNT.2015.7490749]
   Yoo HW, 2017, T KOREAN SOC MEC ENG, V41, P1035, DOI 10.3795/KSME-A.2017.41.11.1035
   Zhang K., 2018, ARXIV180510445
   Zhang S., 2019, 2019 INT C BIOM ICB, P1, DOI [10.1109/icb45273.2019.8987240, DOI 10.1109/ICB45273.2019.8987240]
   Zhang YY, 2021, IEEE ACCESS, V9, P40550, DOI 10.1109/ACCESS.2021.3061684
   Zhang Z., 2019, ARXIV190404925
   Zhao HY, 2019, IEEE SENS J, V19, P8514, DOI 10.1109/JSEN.2018.2866802
   Zhou Yzh, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-61423-2
   Zhu HP, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2733-4
NR 104
TC 1
Z9 1
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2021
VL 9
BP 100352
EP 100368
DI 10.1109/ACCESS.2021.3095477
PG 17
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Engineering; Telecommunications
GA TL9TB
UT WOS:000675194300001
OA gold
DA 2022-02-03
ER

PT J
AU Mohamed, MM
   Nessiem, MA
   Batliner, A
   Bergler, C
   Hantke, S
   Schmitt, M
   Baird, A
   Mallol-Ragolta, A
   Karas, V
   Amiriparian, S
   Schuller, BW
AF Mohamed, Mostafa M.
   Nessiem, Mina A.
   Batliner, Anton
   Bergler, Christian
   Hantke, Simone
   Schmitt, Maximilian
   Baird, Alice
   Mallol-Ragolta, Adria
   Karas, Vincent
   Amiriparian, Shahin
   Schuller, Bjorn W.
TI Face mask recognition from audio: The MASC database and an overview on
   the mask challenge
SO PATTERN RECOGNITION
LA English
DT Article
DE COVID-19; Deep learning; Masks; Voice biometrics; Acoustic modelling
ID SPEECH; CLASSIFICATION; EMOTION
AB The sudden outbreak of COVID-19 has resulted in tough challenges for the field of biometrics due to its spread via physical contact, and the regulations of wearing face masks. Given these constraints, voice biometrics can offer a suitable contact-less biometric solution; they can benefit from models that clas-sify whether a speaker is wearing a mask or not. This article reviews the Mask Sub-Challenge (MSC) of the INTERSPEECH 2020 COMputational PARalinguistics challengE (ComParE), which focused on the fol-lowing classification task: Given an audio chunk of a speaker, classify whether the speaker is wearing a mask or not. First, we report the collection of the Mask Augsburg Speech Corpus (MASC) and the base-line approaches used to solve the problem, achieving a performance of 71 . 8% Unweighted Average Re-call (UAR). We then summarise the methodologies explored in the submitted and accepted papers that mainly used two common patterns: (i) phonetic-based audio features, or (ii) spectrogram representations of audio combined with Convolutional Neural Networks (CNNs) typically used in image processing. Most approaches enhance their models by adapting ensembles of different models and attempting to increase the size of the training data using various techniques. We review and discuss the results of the partici-pants of this sub-challenge, where the winner scored a UAR of 80 . 1% . Moreover, we present the results of fusing the approaches, leading to a UAR of 82 . 6% . Finally, we present a smartphone app that can be used as a proof of concept demonstration to detect in real-time whether users are wearing a face mask; we also benchmark the run-time of the best models. (c) 2021 Elsevier Ltd. All rights reserved.
C1 [Mohamed, Mostafa M.; Nessiem, Mina A.; Batliner, Anton; Schmitt, Maximilian; Baird, Alice; Mallol-Ragolta, Adria; Karas, Vincent; Amiriparian, Shahin; Schuller, Bjorn W.] Univ Augsburg, Chair Embedded Intelligence Hlth Care & Wellbeing, Augsburg, Germany.
   [Schuller, Bjorn W.] Imperial Coll London, GLAM Grp Language Audio & Mus, London, England.
   [Bergler, Christian] FAU Erlangen Nuremberg, Pattern Recognit Lab, Erlangen, Germany.
   [Mohamed, Mostafa M.; Nessiem, Mina A.] AI R&D Team SyncPilot GmbH, Augsburg, Germany.
   [Hantke, Simone; Amiriparian, Shahin; Schuller, Bjorn W.] AudEERING GmbH, Gilching, Germany.
C3 University of Augsburg; Imperial College London; University of Erlangen
   Nuremberg
RP Mohamed, MM (corresponding author), Univ Augsburg, Chair Embedded Intelligence Hlth Care & Wellbeing, Augsburg, Germany.; Mohamed, MM (corresponding author), AI R&D Team SyncPilot GmbH, Augsburg, Germany.
EM mostafa.mohamed@student.uni-augsburg.de
OI Mohamed, Mostafa M./0000-0002-3175-2817
CR Alafif T, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph18031117
   Amiriparian S., 2017, P DET CLASS AC SCEN, P17
   Amiriparian S., 2020, DEEP UNSUPERVISED RE
   Amiriparian S., 2019, THESIS TU MUNCHEN
   Amiriparian S, 2017, INTERSPEECH, P3512, DOI 10.21437/Interspeech.2017-434
   Amodei D, 2016, PR MACH LEARN RES, V48
   Batliner A., 2020, T AFFECTIVE COMPUTIN, P1
   Batliner A.M., 2014, COMPUTATIONAL PARALI
   Bishop C.M., 2006, PATTERN RECOGN
   Boles A., 2017, 12 SYST SYST ENG C S, P1
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Brown C., 2020, ACM SIGKDD INT C KNO, P3474, DOI DOI 10.1145/3394486.3412865
   Cen F, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107737
   Chan A., 2007, HIEROGLYPHS BUILDING
   Chu DK, 2020, LANCET, V395, P1973, DOI 10.1016/S0140-6736(20)31142-9
   Corey RM., 2021, HEAR J, V5, P36
   Deng J, 2013, INT CONF AFFECT, P511, DOI 10.1109/ACII.2013.90
   Deshpande G., 2020, OVERVIEW AUDIO SIGNA
   Nguyen DD, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-85130-8
   Eyben F, 2016, SPRINGER THESES-RECO, P1, DOI 10.1007/978-3-319-27299-3
   Eyben Florian, 2013, P 21 ACM INT C MULT, P835
   Freitag M, 2018, J MACH LEARN RES, V18
   Gomez-Barrero M., 2021, BIOMETRICS ERA COVID
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hu ZL, 2018, PATTERN RECOGN, V83, P134, DOI 10.1016/j.patcog.2018.05.014
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Illium S., 2020, P INTERSPEECH, P2052
   Isaac E., 2015, TEST HYPOTHESIS CONC
   Jain A. K., 2011, INTRO BIOMETRICS
   Kawase T, 2005, NEUROSCI LETT, V382, P254, DOI 10.1016/j.neulet.2005.03.050
   Klumpp P., 2020, P INTERSPEECH, P2057
   Koike T., 2020, P INTERSPEECH, P2047
   Kong QQ, 2020, IEEE-ACM T AUDIO SPE, V28, P2880, DOI 10.1109/TASLP.2020.3030497
   Li GQ, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107610
   Lim H, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3325
   Llamas C., 2008, YORK PAPERS LINGUIST, V2, P80
   Markitantov M., 2020, P INTERSPEECH, P2072
   McLaren M, 2016, INTERSPEECH, P818, DOI 10.21437/Interspeech.2016-1129
   Mendel LL, 2008, J AM ACAD AUDIOL, V19, P686, DOI 10.3766/jaaa.19.9.4
   Minaee S., 2019, BIOMETRICS RECOGNITI
   Mohan P., INNOVATIONS ELECT EL, V2021, P657
   Montacie C., 2020, P INTERSPEECH, P2062
   Nessiem Mina A., 2021, 2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS), P183, DOI 10.1109/CBMS52027.2021.00069
   Nzuva S., 2019, J INFORM ENG APPL, V9, P43
   Orman O.D., 2001, SPEAK ODYSS THE SPEA, P219
   Park D. S., 2019, P ANN C INT SPEECH C, P2613
   REYNOLDS DA, 1995, SPEECH COMMUN, V17, P91, DOI 10.1016/0167-6393(95)00009-D
   Ristea N.-C., 2020, P INTERSPEECH, P2102
   Saeidi R, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1012
   Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Sch?tze, 2009, INTRO INFORM RETRIEV
   Schmitt M, 2017, J MACH LEARN RES, V18
   Schuller B.W., 2020, P INT 2021, P2042, DOI [10.21437/Interspeech.2020-32, DOI 10.21437/INTERSPEECH.2020-32]
   Schuller B, 2013, INTERSPEECH, P148
   Schuller B, 2009, INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2009, VOLS 1-5, P336
   Schuller B, 2011, SPEECH COMMUN, V53, P1062, DOI 10.1016/j.specom.2011.01.011
   Shuja J, 2021, APPL INTELL, V51, P1296, DOI 10.1007/s10489-020-01862-6
   Sigona F., 2018, J INTERDISCIPLINARY, P5
   Simonyan K., 2015, PROC INT C LEARN REP
   Sundararajan K, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3190618
   Szep J., 2020, P INTERSPEECH, P2087
   van Doremalen N, 2020, NEW ENGL J MED, V382, P1564, DOI [10.1056/NEJMc2004973, 10.1101/2020.03.09.20033217]
   Wasserstein RL, 2016, AM STAT, V70, P129, DOI 10.1080/00031305.2016.1154108
   Wayman J., 2000, DEFINITION BIOMETRIC
   Wayman J, 2005, INTRO BIOMETRIC AUTH
   Weninger F, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00292
   Wittum K.J., 2013, J ACOUST SOC AM, DOI [10.1121/1.4800719, DOI 10.1121/1.4800719]
   Woo R.H., 2006, OD THE SPEAK LANG RE, P1
   Wu H., 2020, MASK DETECTION BREAT
   Xu X., 2021, P 36 ANN ACM S APPL, P580
   Yang Z., 2020, P INTERSPEECH, P2092
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK
NR 74
TC 1
Z9 1
U1 5
U2 5
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0031-3203
EI 1873-5142
J9 PATTERN RECOGN
JI Pattern Recognit.
PD FEB
PY 2022
VL 122
AR 108361
DI 10.1016/j.patcog.2021.108361
PG 11
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA XY4VU
UT WOS:000736972900001
PM 34629550
OA Green Published, Bronze
DA 2022-02-03
ER

PT C
AU Kroger, JL
   Raschke, P
   Bhuiyan, TR
AF Kroeger, Jacob Leon
   Raschke, Philip
   Bhuiyan, Towhidur Rahman
GP ACM
TI Privacy Implications of Accelerometer Data: A Review of Possible
   Inferences
SO PROCEEDINGS OF 2019 THE 3RD INTERNATIONAL CONFERENCE ON CRYPTOGRAPHY,
   SECURITY AND PRIVACY (ICCSP 2019) WITH WORKSHOP 2019 THE 4TH
   INTERNATIONAL CONFERENCE ON MULTIMEDIA AND IMAGE PROCESSING (ICMIP 2019)
LA English
DT Proceedings Paper
CT 3rd International Conference on Cryptography, Security and Privacy
   (ICCSP) / 4th International Conference on Multimedia and Image
   Processing (ICMIP)
CY JAN 19-21, 2019
CL Univ Malaya, Kuala Lumpur, MALAYSIA
HO Univ Malaya
DE Accelerometer; Sensor; Privacy; Side channel; Inference attack
ID PHYSICAL-ACTIVITY; ACTIVITY RECOGNITION; PERSONALITY; PATTERNS;
   CHILDREN; VALIDITY; SENSORS; GENDER; MOTION; WRIST
AB Accelerometers are sensors for measuring acceleration forces. They can be found embedded in many types of mobile devices, including tablet PCs, smartphones, and smartwatches. Some common uses of built-in accelerometers are automatic image stabilization, device orientation detection, and shake detection. In contrast to sensors like microphones and cameras, accelerometers are widely regarded as not privacy-intrusive. This sentiment is reflected in protection policies of current mobile operating systems, where third-party apps can access accelerometer data without requiring security permission. It has been shown in experiments, however, that seemingly innocuous sensors can be used as a side channel to infer highly sensitive information about people in their vicinity. Drawing from existing literature, we found that accelerometer data alone may be sufficient to obtain information about a device holder's location, activities, health condition, body features, gender, age, personality traits, and emotional state. Acceleration signals can even be used to uniquely identify a person based on biometric movement patterns and to reconstruct sequences of text entered into a device, including passwords. In the light of these possible inferences, we suggest that accelerometers should urgently be re-evaluated in terms of their privacy implications, along with corresponding adjustments to sensor protection mechanisms.
C1 [Kroeger, Jacob Leon; Bhuiyan, Towhidur Rahman] Tech Univ Berlin, Hardenbergstr 32, D-10623 Berlin, Germany.
   [Raschke, Philip] Tech Univ Berlin, Ernst Reuter Pl 7, D-10587 Berlin, Germany.
C3 Technical University of Berlin; Technical University of Berlin
RP Kroger, JL (corresponding author), Tech Univ Berlin, Hardenbergstr 32, D-10623 Berlin, Germany.
EM kroeger@tu-berlin.de; philip.raschke@tu-berlin.de;
   t.bhuiyan@campus.tu-berlin.de
RI Kroger, Jacob Leon/AAJ-1507-2021
OI Kroger, Jacob Leon/0000-0003-3559-8869
CR Ahmed MS, 2010, PROC INT C TOOLS ART, P296, DOI 10.1109/ICTAI.2010.115
   Alvarez Gonzalo G, 2004, Prog Cardiovasc Nurs, V19, P56, DOI 10.1111/j.0889-7204.2004.02422.x
   [Anonymous], 2018, WEARABLE DEVICES HAV
   Arnold Z, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON HEALTHCARE INFORMATICS (ICHI 2015), P417, DOI 10.1109/ICHI.2015.59
   Artese A, 2017, PSYCHOL AGING, V32, P131, DOI 10.1037/pag0000158
   Aviv AJ, 2012, 28TH ANNUAL COMPUTER SECURITY APPLICATIONS CONFERENCE (ACSAC 2012), P41
   Bai XL, 2017, EURASIP J INF SECUR, DOI 10.1186/s13635-017-0061-8
   Beltramelli T., 2015, ARXIV151205616
   Bhagat YA, 2014, 2014 IEEE Healthcare Innovation Conference (HIC), P56, DOI 10.1109/HIC.2014.7038873
   Bojinov H., 2014, ARXIV14081416
   Borghese MM, 2018, SLEEP HEALTH, V4, P110, DOI 10.1016/j.sleh.2017.09.006
   Cai L., 2011, HOTSEC, P9
   Chan CB, 2003, OBES RES, V11, P1563, DOI 10.1038/oby.2003.208
   Chernbumroong S., 2011, 2011 5 INT C SOFTW K, P1, DOI [10.1109/SKIMA.2011.6089975, DOI 10.1109/SKIMA.2011.6089975]
   Cho SH, 2004, CLIN BIOMECH, V19, P145, DOI 10.1016/j.clinbiomech.2003.10.003
   Crouter SE, 2003, MED SCI SPORT EXER, V35, P1455, DOI 10.1249/01.MSS.0000078932.61440.A2
   Dai JT, 2011, PANCREAS, V40, P233, DOI 10.1097/MPA.0b013e3181f7e09f
   Das Anupam, 2016, NETW DISTR SYST SEC
   Davarci E, 2017, EUR SIGNAL PR CONF, P2201, DOI 10.23919/EUSIPCO.2017.8081600
   Ferrari GLD, 2017, CIENC SAUDE COLETIVA, V22, P3689, DOI 10.1590/1413-812320172211.21962015
   Dey S., 2014, NETW DISTR SYST SEC
   Dusan S. V., 2016, System and method of detecting a user's voice activity using an accelerometer, Patent No. [US9438985B2, 9438985]
   Englebienne G., 2012, P 2 ACM INT WORKSH I, P23
   Foerster F, 1999, COMPUT HUM BEHAV, V15, P571, DOI 10.1016/S0747-5632(99)00037-0
   Freudiger J, 2012, LECT NOTES COMPUT SC, V7035, P31
   Garcia-Ceja E, 2016, IEEE J BIOMED HEALTH, V20, P1053, DOI 10.1109/JBHI.2015.2446195
   Gharani P., 2017, COMPUTING RES REPOSI
   Golle P, 2009, LECT NOTES COMPUT SC, V5538, P390, DOI 10.1007/978-3-642-01516-8_26
   Gruenenfelder-Steiger AE, 2017, GEROPSYCH, V30, P119, DOI 10.1024/1662-9647/a000172
   Gupta P, 2014, IEEE T BIO-MED ENG, V61, P1780, DOI 10.1109/TBME.2014.2307069
   Han Jun, 2012, COMM SYST NETW COMSN
   Hnat T.W., 2012, SENSYS 12, P309, DOI 10.1145/2426656.2426687
   Hua J., 2015, ARXIV150505958
   Jago R, 2005, AM J PREV MED, V28, P447, DOI 10.1016/j.amepre.2005.02.007
   Jain Ankita, 2016, 2016 International Conference on Computational Techniques in Information and Communication Technologies (ICCTICT). Proceedings, P597, DOI 10.1109/ICCTICT.2016.7514649
   Kanning M, 2010, J SPORT EXERCISE PSY, V32, P253, DOI 10.1123/jsep.32.2.253
   Khan AM, 2010, IEEE T INF TECHNOL B, V14, P1166, DOI 10.1109/TITB.2010.2051955
   Killian J., 2018, SMARTPHONE BASED INT
   Klasnja P, 2009, LECT NOTES COMPUT SC, V5538, P176, DOI 10.1007/978-3-642-01516-8_13
   Kwapisz J. R., 2010, P 4 IEEE INT C BIOM, P1, DOI [10.1109/BTAS.2010.5634532, DOI 10.1109/BTAS.2010.5634532]
   Lee J.-V., 2013, INT J SMART HOME, V7, P16
   Lei Z, 2017, J ACOUST SOC AM, V141, P3916, DOI [10.1121/1.4988844, DOI 10.1121/1.4988844]
   Li SQ, 2016, AER ADV ENG RES, V90, P1
   Liu CJ, 2015, ELEC COMP C, P1874, DOI 10.1109/ECTC.2015.7159855
   Liu JY, 2009, PERVASIVE MOB COMPUT, V5, P657, DOI 10.1016/j.pmcj.2009.07.007
   Mannini A, 2013, MED SCI SPORT EXER, V45, P2193, DOI 10.1249/MSS.0b013e31829736d6
   Mayora, 2013, SMART SENS MEAS INST, V2, P195, DOI [10.1007/978-3-642-32538-0_9, DOI 10.1007/978-3-642-32538-0_9]
   Menz HB, 2003, AGE AGEING, V32, P137, DOI 10.1093/ageing/32.2.137
   MERRIFIELD HH, 1971, ERGONOMICS, V14, P411, DOI 10.1080/00140137108931260
   Migueles J, 2017, SPORTS MED, V47
   Mohapatra P., 2017, Energy-efficient, accelerometer-based hotword detection to launch a voice-control system, Patent No. [US20170316779A1, 20170316779]
   Munguia Tapia E., 2008, USING MACHINE LEARNI
   Nickel C., 2012, 2012 Eighth International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP), P16, DOI 10.1109/IIH-MSP.2012.11
   Olsen AF, 2016, INT CONF SYST INFORM, P410, DOI 10.1109/ICSAI.2016.7810990
   Owusu Emmanuel, 2012, P 12 WORKSH MOB COMP, DOI 10.1145/2162081.2162095
   Pesonen AK, 2018, J CLIN SLEEP MED, V14, P585, DOI 10.5664/jcsm.7050
   Primo A, 2014, IEEE COMPUT SOC CONF, P98, DOI 10.1109/CVPRW.2014.20
   QI XY, 2015, MOBICOM, P155
   Richardson S., 2017, LEFT THEIR OWN DEVIC
   Saleheen N, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P999, DOI 10.1145/2750858.2806897
   Singh P., 2013, P 3 ACM S COMP DEV N
   Smidt G., 1972, AM J PHYS MED, V50, P285
   Song Y., 2014, COMPUTING RES REPOSI
   Srilekha R., 2015, INT J SCI TECHNOLOGY, V1, P96
   Tang Q, 2014, 8 INT C PERV COMP TE
   Taraldsen K, 2012, MATURITAS, V71, P13, DOI 10.1016/j.maturitas.2011.11.003
   Thomaz E, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P1029, DOI 10.1145/2750858.2807545
   Vaiana R., 2014, MOD APPL SCI, V8, P88, DOI 10.5539/mas.v8n1p88
   van Hees VT, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0142533
   Warburton DER, 2006, CAN MED ASSOC J, V174, P801, DOI 10.1503/cmaj.051351
   Weiss G. M., 2011, P 5 INT WORKSH KNOWL
   Weiss GM, 2016, PROCEEDINGS OF 3RD IEEE/ACM INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS, (DSAA 2016), P682, DOI 10.1109/DSAA.2016.89
   Williamson J. R, 2015, ESTIMATING LOAD CARR, P1
   Wilson KE, 2015, MED SCI SPORT EXER, V47, P1691, DOI 10.1249/MSS.0000000000000570
   Wilson P, 2014, GLOBALSOILMAP: BASIS OF THE GLOBAL SPATIAL SOIL INFORMATION SYSTEM, P473
   Xu Z., 2012, P 5 ACM C SEC PRIV W, DOI DOI 10.1145/2185448.2185465
   Xu Z, 2015, 2015 7TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY IN MEDICINE AND EDUCATION (ITME), P610, DOI 10.1109/ITME.2015.134
   Yanai HF, 2016, COMM COM INF SC, V617, P542, DOI 10.1007/978-3-319-40548-3_90
   Yang CC, 2010, SENSORS-BASEL, V10, P7772, DOI 10.3390/s100807772
   Zeitzer JM, 2018, J GERONTOL A-BIOL, V73, P682, DOI 10.1093/gerona/glw250
   Zhang L., 2015, P 13 ANN INT C MOB S, P301
   Zhang S, 2009, SENSORS-BASEL, V9, P1499, DOI 10.3390/s90301499
   Zhang Z, 2016, PEERJ, V4, DOI 10.7717/peerj.2258
NR 83
TC 6
Z9 6
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
BN 978-1-4503-6618-2
PY 2019
BP 81
EP 87
DI 10.1145/3309074.3309076
PG 7
WC Computer Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BN0TS
UT WOS:000473540000016
OA Bronze
DA 2022-02-03
ER

PT C
AU Imran, AS
   Haflan, V
   Shahrebabaki, AS
   Olfati, N
   Svendsen, TK
AF Imran, Ali Shariq
   Haflan, Vetle
   Shahrebabaki, Abdolreza Sabzi
   Olfati, Negar
   Svendsen, Torbjorn Karl
GP Assoc Comp Machinery
TI Evaluating Acoustic Feature Maps in 2D-CNN for Speaker Identification
SO ICMLC 2019: 2019 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND
   COMPUTING
LA English
DT Proceedings Paper
CT 11th International Conference on Machine Learning and Computing (ICMLC)
CY FEB 22-24, 2019
CL Zhuhai, PEOPLES R CHINA
DE Speaker identification; speaker classification; acoustic features; deep
   learning; machine learning; spectrogram; MFCC; MFSC; CNN
ID RECOGNITION
AB This paper presents a study evaluating different acoustic feature map representations in two-dimensional convolutional neural networks (2D-CNN) on the speech dataset for various speech-related activities. Specifically, the task involves identifying useful 2D-CNN input feature maps for enhancing speaker identification with an ultimate goal to improve speaker authentication and enabling voice as a biometric feature. Voice in contrast to fingerprints and image-based biometrics is a natural choice for hands-free communication systems where touch interfaces are inconvenient or dangerous to use. Effective input feature map representation may help CNN exploit intrinsic voice features that not only can address the instability issues of voice as an identifier for text-independent speaker authentication while preserving privacy but can also assist in developing efficacious voice-enabled interfaces. Three different acoustic features with three possible feature map representations are evaluated in this study. Results obtained on three speech corpora shows that an interpolated baseline spectrogram performs best compared to Mel frequency spectral coefficients (MFSC) and Mel frequency cepstral coefficient (MFCC) when tested on a 5-fold cross-validation method using 2D-CNN. On both text-dependent and text-independent datasets, raw spectrogram accuracy is 4% better than the traditional acoustic features.
C1 [Imran, Ali Shariq; Haflan, Vetle; Shahrebabaki, Abdolreza Sabzi; Olfati, Negar; Svendsen, Torbjorn Karl] Norwegian Univ Sci & Technol NTNU, Dept Elect Syst, Fac Informat Technol & Elect Engn, Trondheim, Norway.
C3 Norwegian University of Science & Technology (NTNU)
RP Imran, AS (corresponding author), Norwegian Univ Sci & Technol NTNU, Dept Elect Syst, Fac Informat Technol & Elect Engn, Trondheim, Norway.
EM ali.imran@ntnu.no; abdolreza.sabzi@ntnu.no; negar.olfati@ntnu.no
RI Imran, Ali Shariq/AAB-4813-2019
OI Imran, Ali Shariq/0000-0002-2416-2878; sabzi shahrebabaki,
   abdolreza/0000-0002-0877-9456
CR Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Amodei D, 2016, PR MACH LEARN RES, V48
   DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307
   Glembek O, 2009, INT CONF ACOUST SPEE, P4057, DOI 10.1109/ICASSP.2009.4960519
   Goodfellow I., 2016, DEEP LEARNING
   Huang X., 2001, SPOKEN LANGUAGE PROC, V95
   Johnson R. C., 2013, P SOC PHOTO-OPT INS, V8712
   Li Lantian, 2017, ARXIV170503670
   Lukic Y., 2016, 2016 IEEE 26 INT WOR, P1
   Nagrani A., 2017, ARXIV170608612
NR 11
TC 1
Z9 1
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
BN 978-1-4503-6600-7
PY 2019
BP 211
EP 216
DI 10.1145/3318299.3318386
PG 6
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BN2ZQ
UT WOS:000477981500037
DA 2022-02-03
ER

PT J
AU Ortega Hidalgo, MM
   Iparraguirre Bolanos, E
   Brea San-Nicolas, C
AF Ortega Hidalgo, M. M.
   Iparraguirre Bolanos, E.
   Brea San-Nicolas, C.
TI Biomass assessment in annelids: A photogrammetric method suitable for
   hatchlings and adults developed for Eisenia andrei
SO SPANISH JOURNAL OF SOIL SCIENCE
LA English
DT Article
DE Earthworm growth; morphometrics; image analysis; individual weight
   evolution
ID DRY-WEIGHT; EARTHWORMS; GROWTH; SOIL; DEMOSPONGIAE; ORGANISMS
AB A simple photogrammetric, non-destructive method for measuring individual biomass in tubular soft-bodied organisms has been developed using Eisenia andrei. Photographic procedures can be easily performed with low cost digital cameras and the number of pictures to be processed can be reduced to two per animal (Variation Coefficient <= 3.5%) even at sizes similar to 10 mg live weight. Image analysis was undertaken using CobCal 2.0 (c) software. No bias was induced by body position. Accuracy in terms of the regression coefficient of the equation (y= a*x(b)) relating portrayed area (mm(2)) to live weight (mg) was 98%. Two different procedures were designed for laboratory and field uses. No differences between methods appeared at sizes over eight mg live weight, resulting in a common function relating image area to live weight results (PS = 3.27*LW0.681). Below 8 mg, the weight exponent remained unchanged but the value for the elevation rose to 4.21 indicating an increase of surface exposure to the camera lens in newly-hatched worms: the visible part of the geometric area (cylinder shape) enlarged from 34% to 43.52%. As a conclusion this non-invasive procedure proved suitable for worms ranging from sizes of 0.2 to 3 000 mg live weight to determine biometric parameters such as length, volume, surface or body weight, which are key factors for interpreting physiological responses underlying growth patterns.
C1 [Ortega Hidalgo, M. M.; Iparraguirre Bolanos, E.; Brea San-Nicolas, C.] Univ Basque Country, Fac Ciencia & Tecnol, Dept Genet Antropol Fis & Fisiol Anim, Apdo 644, E-48080 Bilbao, Spain.
C3 University of Basque Country
RP Ortega Hidalgo, MM (corresponding author), Univ Basque Country, Fac Ciencia & Tecnol, Dept Genet Antropol Fis & Fisiol Anim, Apdo 644, E-48080 Bilbao, Spain.
EM mercedes.ortega@ehu.es
FU Basque GovernmentBasque Government
FX This work has been partially covered by Basque Government Grants to
   Esther Iparraguirre-Bolanos and Carlos Brea-San Nicolas.
CR Abdo DA, 2006, J EXP MAR BIOL ECOL, V339, P120, DOI 10.1016/j.jembe.2006.07.015
   Bernardini V, 2000, HYDROBIOLOGIA, V439, P179, DOI 10.1023/A:1004153703748
   Dalby PR, 1996, SOIL BIOL BIOCHEM, V28, P685, DOI 10.1016/0038-0717(95)00157-3
   Dominguez J, 2000, PEDOBIOLOGIA, V44, P24, DOI 10.1078/S0031-4056(04)70025-6
   Dominguez J, 1997, PEDOBIOLOGIA, V41, P566
   Dominguez JJ, 2011, VERMICULTURE TECHNOL, P27
   Eisenhauer N, 2009, PEDOBIOLOGIA, V52, P151, DOI 10.1016/j.pedobi.2008.07.002
   Florkin M., 2012, CHEM ZOOLOGY V4 ANNE
   Frund HC, 2010, PEDOBIOLOGIA, V53, P119, DOI 10.1016/j.pedobi.2009.07.002
   Gunadi B, 2002, PEDOBIOLOGIA, V46, P15, DOI 10.1078/0031-4056-00109
   Koopmans M, 2008, MAR BIOTECHNOL, V10, P502, DOI 10.1007/s10126-008-9086-9
   KRETZSCHMAR A, 1991, BIOL FERT SOILS, V12, P209, DOI 10.1007/BF00337204
   Kurth JA, 2014, J EXP BIOL, V217, P1860, DOI 10.1242/jeb.098137
   Laverack MS., 1963, INT SERIES MONOGRAPH, V15
   Littler M.M., 1985, HDB PHYCOLOGICAL MET, P161
   Lowe CN, 2005, PEDOBIOLOGIA, V49, P401, DOI 10.1016/j.pedobi.2005.04.005
   MARTIN NA, 1986, SOIL BIOL BIOCHEM, V18, P245, DOI 10.1016/0038-0717(86)90056-8
   O'BRIEN B. R. A., 1957, AUSTRALIAN JOUR EXPTL BIOL AND MED SCI, V35, P83, DOI 10.1038/icb.1957.10
   O'BRIEN B. R. A., 1957, AUSTRALIAN JOUR EXPTL BIOL AND MED SCI, V35, P373, DOI 10.1038/icb.1957.40
   Page MJ, 2005, AQUACULTURE, V250, P256, DOI 10.1016/j.aquaculture.2005.04.069
   Perea J, 2008, J MOLLUS STUD, V74, P209, DOI 10.1093/mollus/eyn008
   Ponder W, 2002, OVERVIEW CONSERVATIO
   Stovold RJ, 2003, BIOL FERT SOILS, V37, P23, DOI 10.1007/s00374-002-0563-4
   Vanaverbeke J, 2003, MAR ECOL PROG SER, V249, P157, DOI 10.3354/meps249157
NR 24
TC 2
Z9 2
U1 0
U2 0
PU UNIVERSIA
PI MADRID
PA AVDA CANTABRIA S-N, BOADILLA MONTE, MADRID, 28660, SPAIN
SN 2253-6574
J9 SPAN J SOIL SCI
JI Span. J. Soil Sci.
PY 2017
VL 7
IS 1
BP 1
EP 16
DI 10.3232/SJSS.2017.V7.N1.01
PG 16
WC Soil Science
WE Emerging Sources Citation Index (ESCI)
SC Agriculture
GA EO2SP
UT WOS:000396546500001
OA Green Submitted
DA 2022-02-03
ER

PT J
AU Andrew, W
   Gao, J
   Mullan, S
   Campbell, N
   Dowsey, AW
   Burghardt, T
AF Andrew, William
   Gao, Jing
   Mullan, Siobhan
   Campbell, Neill
   Dowsey, Andrew W.
   Burghardt, Tilo
TI Visual identification of individual Holstein-Friesian cattle via deep
   metric learning
SO COMPUTERS AND ELECTRONICS IN AGRICULTURE
LA English
DT Article
DE Automated agriculture; Computer vision; Deep learning; Metric learning;
   Animal biometrics
ID WELFARE IMPLICATIONS; PATTERN-RECOGNITION; ANIMAL BIOMETRICS; EAR TAGS;
   TRACEABILITY; GPS
AB Holstein-Friesian cattle exhibit individually-characteristic black and white coat patterns visually akin to those arising from Turing's reaction-diffusion systems. This work takes advantage of these natural markings in order to automate visual detection and biometric identification of individual Holstein-Friesians via convolutional neural networks and deep metric learning techniques. Existing approaches rely on markings, tags or wearables with a variety of maintenance requirements, whereas we present a totally hands-off method for the automated detection, localisation, and identification of individual animals from overhead imaging in an open herd setting, i.e. where new additions to the herd are identified without re-training. We find that deep metric learning systems show strong performance even when many cattle unseen during system training are to be identified and reidentified - achieving 93.8% accuracy when trained on just half of the population. This work paves the way for facilitating the non-intrusive monitoring of cattle applicable to precision farming and surveillance for automated productivity, health and welfare monitoring, and to veterinary research such as behavioural analysis, disease outbreak tracing, and more. Key parts of the source code, network weights and underpinning datasets are available publicly (OpenCows2020).
C1 [Andrew, William; Mullan, Siobhan; Dowsey, Andrew W.] Univ Bristol, Bristol Vet Sch, Langford House, Bristol BS40 5DU, Avon, England.
   [Andrew, William; Gao, Jing; Campbell, Neill; Burghardt, Tilo] Univ Bristol, Dept Comp Sci, Merchant Venturers Bldg,Woodland Rd, Bristol BS8 1UB, Avon, England.
   [Dowsey, Andrew W.] Univ Bristol, Dept Populat Hlth Sci, Oakfield House, Bristol BS8 2BN, Avon, England.
C3 University of Bristol; University of Bristol; University of Bristol
RP Dowsey, AW (corresponding author), Univ Bristol, Bristol Vet Sch, Langford House, Bristol BS40 5DU, Avon, England.
EM andrew.dowsey@bristol.ac.uk
OI Mullan, Siobhan/0000-0002-3166-6396; Dowsey, Andrew/0000-0002-7404-9128
FU Alan Turing Institute under the EPSRC grantUK Research & Innovation
   (UKRI)Engineering & Physical Sciences Research Council (EPSRC)
   [EP/N510129/1]; John Oldacre Foundation through the John Oldacre Centre
   for Sustainability and Welfare in Dairy Production, Bristol Veterinary
   School
FX This work was supported by The Alan Turing Institute under the EPSRC
   grant EP/N510129/1 and the John Oldacre Foundation through the John
   Oldacre Centre for Sustainability and Welfare in Dairy Production,
   Bristol Veterinary School. We thank Suzanne Held, David Barrett, and
   Mike Mendl of Bristol Veterinary School for fruitful discussions and
   suggestions, and Kate Robinson and the Wyndhurst Farm staff for their
   assistance with data collection. Thanks also to Miguel LagunesFortiz for
   permitting use, adaptation and redistribution of key source code.
CR Adcock SJJ, 2018, ANIMALS, V8, DOI 10.3390/ani8080137
   Allen A, 2008, LIVEST SCI, V116, P42, DOI 10.1016/j.livsci.2007.08.018
   Andrew W., 2019, THESIS
   Andrew W, 2019, IEEE INT C INT ROBOT, P237, DOI 10.1109/IROS40897.2019.8968555
   Andrew W, 2017, IEEE INT CONF COMP V, P2850, DOI 10.1109/ICCVW.2017.336
   Andrew W, 2016, IEEE IMAGE PROC, P484, DOI 10.1109/ICIP.2016.7532404
   Barbedo JGA, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19245436
   Arslan AC, 2014, SIG PROCESS COMMUN, P1347, DOI 10.1109/SIU.2014.6830487
   Awad AI, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9224914
   Awad AI, 2016, COMPUT ELECTRON AGR, V123, P423, DOI 10.1016/j.compag.2016.03.014
   Balntas V., 2016, LEARNING LOCAL FEATU, DOI 10.5244/C.30.119
   Barry B, 2007, T ASABE, V50, P1073, DOI 10.13031/2013.23121
   Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173
   Bendale A, 2015, PROC CVPR IEEE, P1893, DOI 10.1109/CVPR.2015.7298799
   Bertram L., 2006, FREEZE BRANDING
   Beyer L., 2017, ARXIV170307737
   Bhole A, 2019, LECT NOTES COMPUT SC, V11679, P108, DOI 10.1007/978-3-030-29891-3_10
   Bowling M. B., 2008, Professional Animal Scientist, V24, P287
   Buick W., 2004, DEFRA VET J, V15, P20
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Caporale V, 2001, REV SCI TECH OIE, V20, P372, DOI 10.20506/rst.20.2.1279
   Chan, 2018, ARXIV180204365
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Edwards DS, 1999, VET REC, V144, P603, DOI 10.1136/vr.144.22.603
   Edwards DS, 2001, ANIM WELFARE, V10, P141
   El Hadad HM, 2015, PROCEDIA COMPUT SCI, V65, P864, DOI 10.1016/j.procs.2015.09.044
   Everingham M., 2012, PASCAL VISUAL OBJECT
   Food A.O. of the United Nations, 2020, GAT DAIR PROD PROD
   Fosgate GT, 2006, PREV VET MED, V73, P287, DOI 10.1016/j.prevetmed.2005.09.006
   Gallant S I, 1990, IEEE Trans Neural Netw, V1, P179, DOI 10.1109/72.80230
   Ge Z., 2017, ARXIV170707418
   Geng C., 2018, ARXIV PREPRINT ARXIV
   GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
   Hadsell R., P 2006 IEEE COMP VIS, V2, P1735
   Hansen MF, 2018, COMPUT IND, V98, P14, DOI 10.1016/j.compind.2018.02.011
   Hodan T, 2017, IEEE WINT CONF APPL, P880, DOI 10.1109/WACV.2017.103
   Houston R, 2001, REV SCI TECH OIE, V20, P652, DOI 10.20506/rst.20.2.1293
   Hu HQ, 2020, BIOSYST ENG, V192, P245, DOI 10.1016/j.biosystemseng.2020.02.001
   Jain S, 2014, PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON RELIABILTY, OPTIMIZATION, & INFORMATION TECHNOLOGY (ICROIT 2014), P393, DOI 10.1109/ICROIT.2014.6798361
   Jinior Pedro Ribeiro Mendes, 2016, ARXIV160603802
   Johnston AM, 1996, VET REC, V138, P612, DOI 10.1136/vr.138.25.612
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kimura A, 2004, ELECTRON COMM JPN 2, V87, P54, DOI 10.1002/ecjb.20076
   Klindtworth M, 1999, COMPUT ELECTRON AGR, V24, P65, DOI 10.1016/S0168-1699(99)00037-X
   Krizhevsky A., 2012, INT C NEUR INF PROC, V25, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
   Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
   Kumar S, 2017, MULTIMED TOOLS APPL, V76, P26551, DOI 10.1007/s11042-016-4181-9
   Kumar S, 2017, J REAL-TIME IMAGE PR, V13, P505, DOI 10.1007/s11554-016-0645-4
   Lagunes-Fortiz M, 2019, IEEE INT CONF ROBOT, P2932, DOI 10.1109/ICRA.2019.8793715
   Li WY, 2017, COMPUT ELECTRON AGR, V142, P622, DOI 10.1016/j.compag.2017.10.029
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI [10.1109/TPAMI.2018.2858826, 10.1109/ICCV.2017.324]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Martinez-Ortiz C.A., VIDEO TRACKING DAIRY
   Masullo A., 2019, P IEEE INT C COMP VI
   Mendes PR, 2017, MACH LEARN, V106, P359, DOI 10.1007/s10994-016-5610-8
   Meyer BJ, 2019, IEEE INT CONF ROBOT, P2924, DOI 10.1109/ICRA.2019.8794188
   Neal L, 2018, LECT NOTES COMPUT SC, V11210, P620, DOI 10.1007/978-3-030-01231-1_38
   Opitz M, 2020, IEEE T PATTERN ANAL, V42, P276, DOI 10.1109/TPAMI.2018.2848925
   Oza P., 2019, DEEP CNN BASED MULTI
   Pennington J.A., 2007, AGR NATURAL RESOURCE
   Petersen W.E., 1922, J DAIRY SCI, V5, P249, DOI [10.3168/jds.s0022-0302(22)94150-5, DOI 10.3168/JDS.S0022-0302(22)94150-5]
   Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
   Qiao YL, 2019, IFAC PAPERSONLINE, V52, P318, DOI 10.1016/j.ifacol.2019.12.558
   Redmon J., 2018, ARXIV180402767
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Rudd EM, 2018, IEEE T PATTERN ANAL, V40, P762, DOI 10.1109/TPAMI.2017.2707495
   Scheirer WJ, 2014, IEEE T PATTERN ANAL, V36, P2317, DOI 10.1109/TPAMI.2014.2321392
   Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256
   Schmid F, 2013, LECT NOTES GEOINF CA, P3, DOI 10.1007/978-3-319-00615-4_1
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sermanet P., 2013, COMPUT VIS PATTERN R, V1312, P6229
   Shanahan C, 2009, COMPUT ELECTRON AGR, V66, P62, DOI 10.1016/j.compag.2008.12.002
   Shu L., 2017, ARXIV170908716
   Smith GC, 2005, MEAT SCI, V71, P174, DOI 10.1016/j.meatsci.2005.04.002
   Song H. O., 2017, P IEEE C COMP VIS PA, P5382
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Tadesse M., 2003, Livestock Research for Rural Development, V15, P1
   Tharwat A, 2014, ADV INTELL SYST, V303, P217, DOI 10.1007/978-3-319-08156-4_22
   Turner LW, 2000, CAN J ANIM SCI, V80, P405, DOI 10.4141/A99-093
   U.S.D. of Agriculture (USDA) Animal, 2018, P H I SERV CATT ID
   Ungar ED, 2005, RANGELAND ECOL MANAG, V58, P256, DOI 10.2111/1551-5028(2005)58[256:IOAAFG]2.0.CO;2
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Velez JF, 2013, SPAN J AGRIC RES, V11, P945, DOI 10.5424/sjar/2013114-3924
   W.H.F. Federation, 2020, ANN STAT
   Wang XH, 2017, IEEE INT CONF COMP V, P2364, DOI 10.1109/ICCVW.2017.279
   Wardrope DD, 1995, VET REC, V137, P675
   Yoshihashi R, 2019, PROC CVPR IEEE, P4011, DOI 10.1109/CVPR.2019.00414
NR 91
TC 1
Z9 1
U1 9
U2 11
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0168-1699
EI 1872-7107
J9 COMPUT ELECTRON AGR
JI Comput. Electron. Agric.
PD JUN
PY 2021
VL 185
AR 106133
DI 10.1016/j.compag.2021.106133
PG 14
WC Agriculture, Multidisciplinary; Computer Science, Interdisciplinary
   Applications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture; Computer Science
GA SA9UP
UT WOS:000649648600003
OA Green Submitted
DA 2022-02-03
ER

PT J
AU Zeinstra, C
   Veldhuis, R
   Spreeuwers, L
AF Zeinstra, Chris
   Veldhuis, Raymond
   Spreeuwers, Luuk
TI Grid-Based Likelihood Ratio Classifiers for the Comparison of Facial
   Marks
SO IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY
LA English
DT Article
DE Facial marks; discriminating power; calibration loss; design aspects;
   forensic biometrics
ID VASCULAR SKIN MARKS; IDENTIFICATION; RETRIEVAL
AB Facial marks have been studied before, either as a complement to face recognition systems or for their suitability as a single biometric modality. In this paper, we use a subset of the FRGCv2 data set (12307 images and 568 subjects) to study the properties of facial marks, their spatial patterns, and classifiers acting upon these patterns. We observe differences between age and ethnic groups in the number of facial marks. Also, facial marks tend to be clustered. We present six forensically relevant aspects with respect to the design and evaluation of classifiers. These aspects help to systematically study factors that influence performance characteristics (discriminating power and calibration loss) of these classifiers. Calibration loss is of particular forensic importance; it essentially measures how well the classifier output can be used as strength of evidence in a court of law. We use various facial mark grids to which the facial mark spatial patterns are assigned. We find that a classifier that utilizes the facial mark grid of a specific subject outperforms all other classifiers. We also observe that the calibration loss of such subject-based classifier indicates that small grid cell sizes should be avoided.
C1 [Zeinstra, Chris] Univ Twente, NL-7500 AE Enschede, Netherlands.
   [Veldhuis, Raymond] Univ Twente, Biometr Pattern Recognit, NL-7500 AE Enschede, Netherlands.
   [Spreeuwers, Luuk] Univ Twente, NL-7500 AE Enschede, Netherlands.
   [Spreeuwers, Luuk] Univ Twente, Grp R Veldhuis, NL-7500 AE Enschede, Netherlands.
C3 University of Twente; University of Twente; University of Twente;
   University of Twente
RP Zeinstra, C (corresponding author), Univ Twente, NL-7500 AE Enschede, Netherlands.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Ali T., 2014, THESIS
   Barber, 2012, BAYESIAN REASONING M
   Bertillon A., 1893, IDENTIFICATION ANTHR
   Bishop C.M., 2006, PATTERN RECOGN
   Biswas S., 2011, INF FOR SEC WIFS 201, P1, DOI DOI 10.1109/WIFS.2011.6123126
   Brummer N, 2006, COMPUT SPEECH LANG, V20, P230, DOI 10.1016/j.csl.2005.08.001
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Dantcheva A, 2016, IEEE T INF FOREN SEC, V11, P441, DOI 10.1109/TIFS.2015.2480381
   Davis JP, 2010, FORENSIC SCI INT, V200, P165, DOI 10.1016/j.forsciint.2010.04.012
   Evisn M. P., 2014, FORENSIC FACIAL ANAL, P1713
   Evison MP, 2010, COMPUTER-AIDED FORENSIC FACIAL COMPARISON, P1, DOI 10.1201/9781439811344
   Fawcett T, 2007, MACH LEARN, V68, P97, DOI 10.1007/s10994-007-5011-0
   Finn J., 2009, CAPTURING CRIMINAL I
   Jain AK, 2012, IEEE MULTIMEDIA, V19, P20, DOI 10.1109/MMUL.2012.4
   Klare B., 2010, P 4 IEEE INT C BIOM, P1, DOI DOI 10.1109/BTAS.2010.5634533
   Kleinberg Krista F., 2008, THESIS
   Lin D., 2006, 2006 IEEE COMP SOC C, V2, P1355
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Loy G, 2003, IEEE T PATTERN ANAL, V25, P959, DOI 10.1109/TPAMI.2003.1217601
   Meuwly D, 2017, FORENSIC SCI INT, V276, P142, DOI 10.1016/j.forsciint.2016.03.048
   Nurhudatiana A, 2016, J FORENSIC SCI, V61, P52, DOI 10.1111/1556-4029.12872
   Nurhudatiana A, 2015, IEEE T INF FOREN SEC, V10, P916, DOI 10.1109/TIFS.2014.2387575
   Nurhudatiana A, 2013, IEEE T INF FOREN SEC, V8, P998, DOI 10.1109/TIFS.2013.2258338
   Park U, 2010, IEEE T INF FOREN SEC, V5, P406, DOI 10.1109/TIFS.2010.2049842
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Pierrard J.S., 2007, SC 07, P1, DOI DOI 10.1145/1362622.1362660
   Prince J., 2012, EXAMINE EMERGING POL
   Przybocki M. A, 1997, P EUR, P1895
   Ramos D, 2013, J FORENSIC SCI, V58, P1503, DOI 10.1111/1556-4029.12233
   Ramos D, 2013, FORENSIC SCI INT, V230, P156, DOI 10.1016/j.forsciint.2013.04.014
   Roelofse MM, 2008, FORENSIC SCI INT, V177, P168, DOI 10.1016/j.forsciint.2007.12.003
   Shalin S. E., 2015, P INT C SOFT COMP NE, P1
   Spaun N. A., 2007, P 1 IEEE INT C BIOM, P1
   Srinivas N., 2011, CVPR 2011 WORKSHOPS, P106
   Srinivas N, 2016, J FORENSIC SCI, V61, pS117, DOI 10.1111/1556-4029.12923
   Srinivas N, 2012, IEEE T INF FOREN SEC, V7, P1536, DOI 10.1109/TIFS.2012.2206027
   Susyanto N., 2016, THESIS
   Tukey J. W., 1977, EXPLORATORY DATA ANA
   Zeinstra C., 2017, P INT WORKSH BIOM FO, P1
   Zeinstra C., 2016, LNI, P171
NR 41
TC 3
Z9 3
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1556-6013
EI 1556-6021
J9 IEEE T INF FOREN SEC
JI IEEE Trans. Inf. Forensic Secur.
PD JAN
PY 2018
VL 13
IS 1
BP 253
EP 264
DI 10.1109/TIFS.2017.2746013
PG 12
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA FP6HI
UT WOS:000417725500019
DA 2022-02-03
ER

PT J
AU Esteban, LG
   de Palacios, P
   Conde, M
   Fernandez, FG
   Garcia-Iruela, A
   Gonzalez-Alonso, M
AF Esteban, Luis G.
   de Palacios, Paloma
   Conde, Maria
   Fernandez, Francisco G.
   Garcia-Iruela, Alberto
   Gonzalez-Alonso, Marta
TI Application of artificial neural networks as a predictive method to
   differentiate the wood of Pinus sylvestris L. and Pinus nigra Arn subsp
   salzmannii (Dunal) Franco
SO WOOD SCIENCE AND TECHNOLOGY
LA English
DT Article
ID SUPPORT VECTOR MACHINES; WATER SORPTION ISOTHERM; SPECIES RECOGNITION;
   IDENTIFICATION; CLASSIFICATION; IMAGES; CITES; WEED; DISCRIMINATION;
   DEFECTS
AB The wood structure of conifers in general and the Pinus genus in particular makes species differentiation by traditional qualitative or quantitative methods complicated or even impossible at times. Pinus sylvestris L. and Pinus nigra Arn subsp. salzmannii (Dunal) Franco are a clear example of this because they cannot be differentiated by traditional methods. However, correctly identifying these species is very important in some cases as they are extensively used in a large variety of fields because of their wide distribution range in the forests of Europe and Asia. Using trees selected from the same forest to minimise the influence of site and performing a biometric study of 10 growth rings from the same climate period, a feedforward multilayer perceptron network trained by the resilient backpropagation algorithm was designed to determine whether the network could be used to differentiate these species with a high degree of probability. The artificial neural network achieved 90.4% accuracy in the training set, 81.6% in the validation set and 81.2% in the testing set. This result justifies the use of this tool for wood identification at anatomical level.
C1 [Esteban, Luis G.; de Palacios, Paloma; Conde, Maria; Fernandez, Francisco G.; Garcia-Iruela, Alberto; Gonzalez-Alonso, Marta] Univ Politecn Madrid, Escuela Tecn Super Ingenieros Montes, Dept Sistemas & Recursos Nat, E-28040 Madrid, Spain.
   [Conde, Maria] CIFOR INIA, Dept Prod Forestales, Madrid 28040, Spain.
C3 Universidad Politecnica de Madrid; Instituto Nacional Investigacion
   Tecnologia Agraria Alimentaria (INIA)
RP Esteban, LG (corresponding author), Univ Politecn Madrid, Escuela Tecn Super Ingenieros Montes, Dept Sistemas & Recursos Nat, E-28040 Madrid, Spain.
EM luis.garcia@upm.es
RI de Palacios, Paloma/L-2235-2017; Garcia-Iruela, Alberto/ABF-8597-2021;
   Esteban, Luis G./L-2042-2017; Fernandez, Francisco Garcia/L-1156-2017
OI de Palacios, Paloma/0000-0003-1907-0992; Garcia-Iruela,
   Alberto/0000-0002-4778-7651; Esteban, Luis G./0000-0002-7771-9123;
   Fernandez, Francisco Garcia/0000-0002-1180-9660; Conde Garcia,
   Maria/0000-0003-0194-2617
CR Avramidis S, 2005, WOOD FIBER SCI, V37, P682
   Avramidis S, 2005, HOLZFORSCHUNG, V59, P336, DOI 10.1515/HF.2005.055
   Baas P, 2004, IAWA J, V25, P1, DOI 10.1163/22941932-90000349
   Batista JW, 2011, IAWA J, V32, P285
   Burks TF, 2005, BIOSYST ENG, V91, P293, DOI 10.1016/j.biosystemseng.2004.12.012
   Catalan G., 1991, REGIONES PROCEDENCIA
   Clark J., 2012, P IEEE S COMP INT BI
   Clark JY, 2003, BIOSYSTEMS, V72, P131, DOI 10.1016/S0303-2647(03)00139-4
   Cook DF, 1992, P 1 IND ENG RES C CH
   Demuth H, 2002, US GUID
   Diamantopoulou MJ, 2005, COMPUT ELECTRON AGR, V48, P235, DOI 10.1016/j.compag.2005.04.002
   Esteban LG, 2009, INVEST AGRAR-SIST R, V18, P92
   Esteban L.G., 1988, ANATOMIA IDENTIFICAC
   Esteban LG, 2010, WOOD FIBER SCI, V42, P335
   Fernandez FG, 2008, INVEST AGRAR-SIST R, V17, P178
   Esteban LG, 2009, IAWA J, V30, P87, DOI 10.1163/22941932-90000206
   Gasson P, 2011, IAWA J, V32, P137, DOI 10.1163/22941932-90000049
   Gasson P, 2010, ANN BOT-LONDON, V105, P45, DOI 10.1093/aob/mcp270
   Ghasemloo N, 2011, J AGR SCI TECH-IRAN, V13, P1223
   Gong P, 1997, REMOTE SENS ENVIRON, V62, P189, DOI 10.1016/S0034-4257(97)00094-1
   Granitto PM, 2002, COMPUT ELECTRON AGR, V33, P91, DOI 10.1016/S0168-1699(02)00004-2
   Gu IYH, 2009, LECT NOTES COMPUT SC, V5337, P356, DOI 10.1007/978-3-642-02345-3_35
   HAGMAN POG, 1995, HOLZ ROH WERKST, V53, P75, DOI 10.1007/BF02716393
   Hanssen F, 2011, IAWA J, V32, P273, DOI 10.1163/22941932-90000057
   Hermanson JC, 2011, IAWA J, V32, P233, DOI 10.1163/22941932-90000054
   Hernandez-Perez J. A., 2004, Innovative Food Science & Emerging Technologies, V5, P57, DOI 10.1016/j.ifset.2003.10.004
   Isasi P., 2004, REDES NEURONALES ART
   Jane FW, 1970, STRUCTURE WOOD
   Jiang W, 2013, TURK J BOT, V37, P1093, DOI 10.3906/bot-1210-21
   Jordan R, 1998, ULTRASONICS, V36, P219, DOI 10.1016/S0041-624X(97)00148-0
   Kavdir I, 2004, COMPUT ELECTRON AGR, V44, P153, DOI 10.1016/j.compag.2004.03.006
   Kiani S, 2012, J AGR SCI TECH-IRAN, V14, P755
   Koch G, 2011, IAWA J, V32, P213, DOI 10.1163/22941932-90000052
   LADELL J. L., 1959, FORESTRY, V32, P124, DOI 10.1093/forestry/32.2.124
   Lycken A, 2006, SCAND J FOREST RES, V21, P167, DOI 10.1080/02827580600642050
   Moshou D, 2001, COMPUT ELECTRON AGR, V31, P5, DOI 10.1016/S0168-1699(00)00170-8
   Myhara RM, 2001, DRY TECHNOL, V19, P1543, DOI 10.1081/DRT-100107258
   Nordmark U, 2002, SCAND J FOREST RES, V17, P72, DOI 10.1080/028275802317221109
   Nordmark U, 2003, SCAND J FOREST RES, V18, P168, DOI 10.1080/02827580310003740
   Ozsahin S, 2013, EUR J WOOD WOOD PROD, V71, P769, DOI 10.1007/s00107-013-0737-9
   Panchariya PC, 2002, DRY TECHNOL, V20, P351, DOI 10.1081/DRT-120002546
   Peng GL, 2007, J FOOD ENG, V80, P562, DOI 10.1016/j.jfoodeng.2006.04.063
   Perez M.L., 2003, APLICACIONES REDES N
   Ramírez Alonso Graciela María de Jesús, 2005, Comp. y Sist., V9, P17
   Reby D, 1997, BEHAV PROCESS, V40, P35, DOI 10.1016/S0376-6357(96)00766-8
   Sarle W.S., 1997, NEURAL NETWORK FAQ 1
   Sha W, 2007, APPL CATAL A-GEN, V324, P87, DOI 10.1016/j.apcata.2007.02.053
   Turhan K, 2013, TURK J AGRIC FOR, V37, P249, DOI 10.3906/tar-1205-47
   Wheeler EA, 2011, IAWA J, V32, P199, DOI 10.1163/22941932-90000051
   White H., 1989, J NEURAL NETWORKS, V2, P359, DOI DOI 10.1016/0893-6080(89)90020-8
   Wong WK, 2009, EXPERT SYST APPL, V36, P3845, DOI 10.1016/j.eswa.2008.02.066
   Yuen CWM, 2009, EXPERT SYST APPL, V36, P2037, DOI 10.1016/j.eswa.2007.12.009
   Yusof R, 2013, MACH VISION APPL, V24, P1589, DOI 10.1007/s00138-013-0526-9
NR 53
TC 14
Z9 14
U1 1
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0043-7719
EI 1432-5225
J9 WOOD SCI TECHNOL
JI Wood Sci. Technol.
PD SEP
PY 2017
VL 51
IS 5
BP 1249
EP 1258
DI 10.1007/s00226-017-0932-7
PG 10
WC Forestry; Materials Science, Paper & Wood
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Forestry; Materials Science
GA FD8FF
UT WOS:000407759900016
DA 2022-02-03
ER

PT J
AU Koretz, JF
   Cook, CA
AF Koretz, JF
   Cook, CA
TI Aging of the optics of the human eye: Lens refraction models and
   principal plane locations
SO OPTOMETRY AND VISION SCIENCE
LA English
DT Article
DE principal planes; refractive power; gradient refractive index; aging;
   human eye; crystalline lens; paraxial ray-tracing
ID HUMAN CRYSTALLINE LENS; AGE-RELATED-CHANGES; SLIT-LAMP IMAGES; ANTERIOR
   SEGMENT; ACCOMMODATION; PRESBYOPIA; INDEX; CURVATURE; GRADIENT; PROTEIN
AB Biometric data describing the geometry and spacings of emmetropic human eyes were combined with lens shape and placement within the globe to generate paraxial models of image formation as a function of age. Three different representations of the shape of the internal refractive index gradient of the lens were evaluated-a Gullstrand-type model consisting of cortical and nuclear regions with different refractive indices, a power series model, and a linear-gradient model. All three refractive models satisfy the requirements for focus for all the data sets, indicating that lenticular refractive index gradient shape is essentially underdetermined in the paraxial limit. Lens refractive power decreases by almost 2 D during a 50-year period, and the concomitant decrease in system refractive power is due almost entirely to this effect. The reduction in spacing between the lens principal planes is a function of this, as is their anterior movement with age, and suggests that the compensatory processes maintaining far focus at the expense of near are not exactly balanced. Despite these changes in the lens contribution and their effect on the location of the system principal planes, which also move anteriorly, the spacing between the system principal planes remains constant. However, the trend toward reduced overall system power with age indicates the primary role of the lens in mediating image formation onto the retina over time.
C1 Rensselaer Polytech Inst, Ctr Biophys, Ctr Sci, Troy, NY 12180 USA.
C3 Rensselaer Polytechnic Institute
RP Koretz, JF (corresponding author), Rensselaer Polytech Inst, Ctr Biophys, Ctr Sci, Troy, NY 12180 USA.
EM koretj@rpi.edu
FU NEI NIH HHSUnited States Department of Health & Human ServicesNational
   Institutes of Health (NIH) - USANIH National Eye Institute (NEI)
   [EY02195] Funding Source: Medline; NATIONAL EYE INSTITUTEUnited States
   Department of Health & Human ServicesNational Institutes of Health (NIH)
   - USANIH National Eye Institute (NEI) [R01EY002195] Funding Source: NIH
   RePORTER
CR ATCHISON DA, 1995, VISION RES, V35, P2529, DOI 10.1016/0042-6989(95)00019-V
   Attebo K, 1999, OPHTHALMOLOGY, V106, P1066, DOI 10.1016/S0161-6420(99)90251-8
   BITO LZ, 1987, EYE-T OPHTH SOC UK, V1, P222, DOI 10.1038/eye.1987.41
   Blaker J. W., 1971, GEOMETRIC OPTICS MAT
   Bron AJ, 2000, OPHTHALMOLOGICA, V214, P86, DOI 10.1159/000027475
   BROWN N, 1974, EXP EYE RES, V19, P175, DOI 10.1016/0014-4835(74)90034-7
   Brown NP, 1999, EYE, V13, P83, DOI 10.1038/eye.1999.16
   COOK CA, 1991, APPL OPTICS, V30, P2088, DOI 10.1364/AO.30.002088
   COOK CA, 1994, VISION RES, V34, P2945, DOI 10.1016/0042-6989(94)90266-6
   Cook CA, 1998, J OPT SOC AM A, V15, P1473, DOI 10.1364/JOSAA.15.001473
   COOK CA, 1995, OSA TECHNICAL DIGEST, P138
   FAGERHOLM PP, 1981, EXP EYE RES, V33, P615, DOI 10.1016/S0014-4835(81)80101-7
   FISHER RF, 1973, J PHYSIOL-LONDON, V234, P443, DOI 10.1113/jphysiol.1973.sp010353
   FLEDELIUS HC, 1986, ACTA OPHTHALMOL, V64, P487
   HOWLAND HC, 1993, PRINCIPLES PRACTICE, P261
   Klein MV, 1986, OPTICS
   Koretz J F, 1995, Trans Am Ophthalmol Soc, V93, P105
   KORETZ JF, 1984, VISION RES, V24, P1141, DOI 10.1016/0042-6989(84)90168-8
   KORETZ JF, 1988, SCI AM, V259, P92, DOI 10.1038/scientificamerican0788-92
   Koretz JF, 2000, OSA TRENDS OPT PHOTO, V35, P246
   KORETZ JF, 1989, APPL OPTICS, V28, P1097, DOI 10.1364/AO.28.001097
   KORETZ JF, 1989, VISION RES, V29, P1685, DOI 10.1016/0042-6989(89)90150-8
   KORETZ JF, 2001, MODELS VISUAL SYSTEM
   Koretz JF, 1986, TOPICS AGING RES EUR, P57
   KORETZ JF, 1999, OSA TECHNICAL DIGEST, P106
   Koretz LF, 2001, J OPT SOC AM A, V18, P265, DOI 10.1364/JOSAA.18.000265
   Kuszak JR, 1995, INT REV CYTOL, V163, P305, DOI 10.1016/S0074-7696(08)62213-5
   LAHM D, 1985, INVEST OPHTH VIS SCI, V26, P1162
   Montes-Mico Robert, 2000, Documenta Ophthalmologica, V101, P25, DOI 10.1023/A:1002762724601
   PIERSCIONEK BK, 1989, OPTOMETRY VISION SCI, V66, P822, DOI 10.1097/00006324-198912000-00004
   PIERSCIONEK BK, 1994, OPHTHALMIC RES, V26, P32, DOI 10.1159/000267371
   POMERANTZEFF O, 1986, ADV DIAGN VIS OPT P, P12
   SIEBINGA I, 1991, EXP EYE RES, V53, P233, DOI 10.1016/0014-4835(91)90079-T
   SMITH G, 1992, J OPT SOC AM A, V9, P2111, DOI 10.1364/JOSAA.9.002111
   SMITH G, 1991, OPHTHAL PHYSL OPT, V11, P359, DOI 10.1111/j.1475-1313.1991.tb00237.x
   Sorsby A., 1957, EMMETROPIA ITS ABERR
   SORSBY A, 1961, REFRACTION ITS COMPO
   Strenk SA, 1999, INVEST OPHTH VIS SCI, V40, P1162
   WILLEKENS B, 1987, LENS RES, V4, P207
   Wong TY, 2001, INVEST OPHTH VIS SCI, V42, P73
NR 40
TC 22
Z9 22
U1 0
U2 2
PU LIPPINCOTT WILLIAMS & WILKINS
PI PHILADELPHIA
PA TWO COMMERCE SQ, 2001 MARKET ST, PHILADELPHIA, PA 19103 USA
SN 1040-5488
EI 1538-9235
J9 OPTOMETRY VISION SCI
JI Optom. Vis. Sci.
PD JUN
PY 2001
VL 78
IS 6
BP 396
EP 404
DI 10.1097/00006324-200106000-00011
PG 9
WC Ophthalmology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Ophthalmology
GA 450XX
UT WOS:000169771100019
PM 11444628
DA 2022-02-03
ER

PT J
AU Meenakshi, S
   Suganthi, M
   Kumar, PS
AF Meenakshi, S.
   Suganthi, M.
   Kumar, P. Suresh
TI An approach for automatic detection of fetal gestational age at the
   third trimester using kidney length and biparietal diameter
SO SOFT COMPUTING
LA English
DT Article
DE Biparietal diameter; Kidney length; Circular Hough transform;
   Generalized fuzzy Hough transform; Active contour model
ID GROWTH; ACCURATE; SIZE
AB To improve the quality of maternity care and the processing speed, an accurate determination of gestational age from ultrasound images is very essential. The biometric parameters at second semester vary in accuracy from 2 to 3weeks. The manual placement of calibers and probe during measurement also introduces user variability in prediction. In this paper, an attempt has been made to estimate the gestational age at third trimester by biparietal diameter (BPD) and kidney length (KL). The kernel-based fuzzy C means clustering, circular Hough transform (CHT) and generalized fuzzy Hough transform have been used for the determination fetal skull and kidney shape. The use of gradient vector in CHT works only in relevant orientation and reduces the computation time. An active contour model is then imposed on the structure to detect features such as BPD and KL. The pixel-by-pixel scanning technique has been used to extract features of an image which is simulated using MATLAB program, and the measured results were very close to real measurements. The performance of the present work was evaluated with 50 ultrasound B mode images in gestational age range from 28 to 36weeks; each has different shapes and sizes. The obtained results have been compared with the standard measurements, and it is found that the measurement error is limited to +/- 8.7%. It can also be improved with the help of kidney length using indirect measurements. It reduces the measurement error to +/- 3.08%.
C1 [Meenakshi, S.; Suganthi, M.] Mahendra Coll Engn, Salem 636106, India.
   [Kumar, P. Suresh] Mahendra Engn Coll, Namakkal 637503, India.
C3 Mahendra Engineering College, Namakkal
RP Meenakshi, S (corresponding author), Mahendra Coll Engn, Salem 636106, India.
EM meenakshisece@gmail.com
OI perumal, suresh kumar/0000-0002-2453-5929
CR Akkasalgar Prema T., 2015, INT J ENG COMPUTER S, P13151
   Butt K, 2014, J OBSTET GYNAECOL CA, V36, P171, DOI 10.1016/S1701-2163(15)30664-2
   Chalana V, 1997, IEEE T MED IMAGING, V16, P642, DOI 10.1109/42.640755
   Chatterjee S, 2016, INT J REPROD CONTRAC, V5, P1949
   Chervenak FA, 1998, AM J OBSTET GYNECOL, V178, P678, DOI 10.1016/S0002-9378(98)70477-6
   COHEN HL, 1991, AM J ROENTGENOL, V157, P545, DOI 10.2214/ajr.157.3.1872242
   DINKEL E, 1985, PEDIATR RADIOL, V15, P38, DOI 10.1007/BF02387851
   Falatah HA, 2014, OPEN J MED IMAGING, V4, P126, DOI DOI 10.4236/OJMI.2014.43018
   Gupta DP, 2013, INDIAN J CLIN PRACTI, V24, P459
   HADLOCK FP, 1991, J ULTRAS MED, V10, P557
   HADLOCK FP, 1984, RADIOLOGY, V152, P497, DOI 10.1148/radiology.152.2.6739822
   Kansaria J, 2009, BOMBAY HOSP J, V51, P155
   Konje JC, 2002, ULTRASOUND OBST GYN, V19, P592, DOI 10.1046/j.1469-0705.2002.00704.x
   Kumar K, 2013, J ANAT SOC INDIA, V62, P33
   Lu W, 2005, ULTRASOUND MED BIOL, V31, P929, DOI 10.1016/j.ultrasmedbio.2005.04.002
   Mukhopadhyay P, 2015, PATTERN RECOGN, V48, P993, DOI 10.1016/j.patcog.2014.08.027
   NYBERG DA, 1987, J ULTRAS MED, V6, P23
   Pathak M, 2014, INT J COMPUT SCI ENG, V5, P160
   Petkar CM, 2017, METHODS ESTIMATING D
   SAGI J, 1987, GYNECOL OBSTET INVES, V23, P1, DOI 10.1159/000298825
   Shivalingaiah N, 2014, INT J REPROD CONTRAC, V3, P424
   Suetake N, 2006, SOFT COMPUT, V10, P1161, DOI 10.1007/s00500-005-0038-2
   Vasanthaselvakumar R., 2017, INT J PURE APPL MATH, V117, P635
NR 23
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 1432-7643
EI 1433-7479
J9 SOFT COMPUT
JI Soft Comput.
PD APR
PY 2019
VL 23
IS 8
SI SI
BP 2839
EP 2848
DI 10.1007/s00500-019-03913-8
PG 10
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HQ6HL
UT WOS:000462514000030
DA 2022-02-03
ER

PT J
AU Tariq, SA
   Iqbal, S
   Ghafoor, M
   Taj, IA
   Jafri, NM
   Razzaq, S
   Zia, T
AF Tariq, Syed Ali
   Iqbal, Shahzaib
   Ghafoor, Mubeen
   Taj, Imtiaz A.
   Jafri, Noman M.
   Razzaq, Saad
   Zia, Tehseen
TI Massively parallel palmprint identification system using GPU
SO CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND
   APPLICATIONS
LA English
DT Article
DE Palmprint identification; Minutia quality; Parallel processing; GPU;
   CUDA
ID DISCRETE WAVELET TRANSFORM; FINGERPRINT; IMPLEMENTATION; FILTER
AB Automated human authentication is becoming increasingly important in today's world due to increased need of security and surveillance applications deployed in almost all premises and installations. In this regard, palmprint biometric based identification has gained a lot of attention in recent years. However, due to large size of palmprint images and presence of principal lines, wrinkles, creases, and other noises, there are large number of inaccurate minutiae present. The computational requirement of palmprint identification is also quite large and it takes a lot of time to find identity of a palmprint in large database. In this study, a novel palmprint identification solution has been proposed that increases the accuracy of minutia detection based on improved frequency estimation and a novel region-quality based minutia extraction algorithm. Furthermore, a novel, efficient and highly accurate minutiae based encoding and matching algorithm is proposed that is designed to achieve maximum parallelism, and it is further accelerated using graphical processing unit. The results of the proposed palmprint identification demonstrate high accuracy and much faster identification speeds in comparison with current state of the art. Therefore, it can be considered as a robust, efficient and practical solution for palmprint based identification systems.
C1 [Tariq, Syed Ali] Abasyn Univ, Dept Comp & Technol, Islamabad, Pakistan.
   [Iqbal, Shahzaib; Jafri, Noman M.] Abasyn Univ, Dept Elect Engn, Islamabad, Pakistan.
   [Ghafoor, Mubeen; Zia, Tehseen] COMSATS Inst Informat Technol, Dept Comp Sci, Islamabad, Pakistan.
   [Taj, Imtiaz A.] Capital Univ Sci & Technol, Dept Elect Engn, Islamabad, Pakistan.
   [Razzaq, Saad] Univ Sargodha, Dept Comp Sci & IT, Sargodha, Pakistan.
C3 COMSATS University Islamabad (CUI); University of Sargodha
RP Tariq, SA (corresponding author), Abasyn Univ, Dept Comp & Technol, Islamabad, Pakistan.
EM s.alitariq1@gmail.com
CR Bolz J, 2003, ACM T GRAPHIC, V22, P917, DOI 10.1145/882262.882364
   Broussard RP, 2008, 2008 IEEE SECOND INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS (BTAS), P30
   Cappelli R, 2012, IEEE T SYST MAN CY B, V42, P956, DOI 10.1109/TSMCB.2012.2183635
   Cappelli R, 2010, IEEE T PATTERN ANAL, V32, P2128, DOI 10.1109/TPAMI.2010.52
   Chen FL, 2013, IEEE T IMAGE PROCESS, V22, P4964, DOI 10.1109/TIP.2013.2280187
   Chikkerur S, 2006, LECT NOTES COMPUT SC, V3832, P309
   Crookes D, 2009, 2009 13TH INTERNATIONAL MACHINE VISION AND IMAGE PROCESSING CONFERENCE, P151, DOI 10.1109/IMVIP.2009.34
   Dai JF, 2012, IEEE T PATTERN ANAL, V34, P1618, DOI 10.1109/TPAMI.2011.237
   Dai JF, 2011, IEEE T PATTERN ANAL, V33, P945, DOI 10.1109/TPAMI.2010.164
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   Gutierrez PD, 2014, IEEE T INF FOREN SEC, V9, P62, DOI 10.1109/TIFS.2013.2291220
   Duta N, 2002, PATTERN RECOGN LETT, V23, P477, DOI 10.1016/S0167-8655(01)00179-9
   Gajdos Petr, 2010, Proceedings 10th International Conference on Intelligent Systems Design and Applications (ISDA 2010), P894, DOI 10.1109/ISDA.2010.5687076
   Ghafoor M, 2016, IET COMPUT VIS, V10, P806, DOI 10.1049/iet-cvi.2016.0005
   Ghafoor M, 2014, IET IMAGE PROCESS, V8, P417, DOI 10.1049/iet-ipr.2013.0528
   Hong L, 1998, IEEE T PATTERN ANAL, V20, P777, DOI 10.1109/34.709565
   Jain Anil K, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563117
   Jain AK, 2009, IEEE T PATTERN ANAL, V31, P1032, DOI 10.1109/TPAMI.2008.242
   Jea TY, 2005, PATTERN RECOGN, V38, P1672, DOI 10.1016/j.patcog.2005.03.016
   Jiang XD, 2000, INT C PATT RECOG, P1038, DOI 10.1109/ICPR.2000.906252
   Kong A, 2009, PATTERN RECOGN, V42, P1408, DOI 10.1016/j.patcog.2009.01.018
   Kruger J, 2003, ACM T GRAPHIC, V22, P908, DOI 10.1145/882262.882363
   Maltoni D., 2009, HDB FINGERPRINT RECO, V2nd
   Moreland K., 2003, P ACM SIGGRAPH EUROG, P112
   Nvidia C. U. D. A., 2011, NVIDIA CUDA C PROGRA, V120, P8
   Rakvic RN, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/764838
   Rakvic RN, 2009, IEEE T INF FOREN SEC, V4, P812, DOI 10.1109/TIFS.2009.2032012
   RATHA NK, 1995, PATTERN RECOGN, V28, P1657, DOI 10.1016/0031-3203(95)00039-3
   Tenllado C, 2008, IEEE T PARALL DISTR, V19, P299, DOI 10.1109/TPDS.2007.70716
   Vandal N.A., 2010, P 4 IEEE INT C BIOM, P1
   Wang RF, 2014, IET BIOMETRICS, V3, P94, DOI 10.1049/iet-bmt.2013.0067
   Wang W, 2008, PATTERN RECOGN LETT, V29, P301, DOI 10.1016/j.patrec.2007.10.004
   Wong TT, 2007, IEEE T MULTIMEDIA, V9, P668, DOI 10.1109/TMM.2006.887994
   WONG TT, 2003, GRAPHICS PROGRAMMING, P375
   Zhang KN, 2017, PATTERN RECOGN LETT, V85, P65, DOI 10.1016/j.patrec.2016.11.014
   Zheng Q, 2016, IEEE T INF FOREN SEC, V11, P633, DOI 10.1109/TIFS.2015.2503265
NR 36
TC 1
Z9 1
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 1386-7857
EI 1573-7543
J9 CLUSTER COMPUT
JI Cluster Comput.
PD MAY
PY 2019
VL 22
SU 3
BP S7201
EP S7216
DI 10.1007/s10586-017-1121-z
PG 16
WC Computer Science, Information Systems; Computer Science, Theory &
   Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JR8MJ
UT WOS:000499872200176
DA 2022-02-03
ER

PT J
AU Plasquy, E
   Garcia, JM
   Florido, MC
   Sola-Guirado, RR
AF Plasquy, Eddy
   Garcia, Jose M.
   Florido, Maria C.
   Sola-Guirado, Rafael R.
TI Estimation of the Cooling Rate of Six Olive Cultivars Using Thermal
   Imaging
SO AGRICULTURE-BASEL
LA English
DT Article
DE harvesting; storage; biothermal characteristics; refrigeration
   temperature; half time
AB Bringing the olive harvest period forward leads to storing fruit in field temperatures that risk jeopardizing its quality. Knowledge about the bio-thermal characteristics of olives is crucial when considering their cooling, although published research on the subject is limited. In this work, the cooling rate of the fruit of six olive cultivars has been empirically determined by measuring the evolution of their low temperature under controlled conditions by thermal imaging. Based on these data, the cooling time needed to cool the fruit to 22 degrees C was estimated, considering the biometric characteristics of the individual fruit, a field temperature from 26 to 42 degrees C, and a room cooling temperature from -8 to -20 degrees C. The results showed differences among the cultivars and the need to further investigate the specific heat requirements for small varieties and the impact of the conduction factor on the heavier ones. The simulation suggests that between 2 min (for the light Arbequina and Koroneiki cultivars) and 5 min (for the heavier Verdial and Gordal cultivars) suffice to cool the fruit to the desired temperature with a room temperature of -16 degrees C. These results show the feasibility of developing technological solutions for cooling olives before their industrial processing with industrial applications such as cooling tunnels on individual fruit.
C1 [Plasquy, Eddy; Garcia, Jose M.] Inst Grasa, CSIC, Dept Biochem & Mol Biol Plant Prod, Seville 41092, Spain.
   [Florido, Maria C.] Univ Seville, Dept Crystallog Mineral & Agr Chem, Seville 41089, Spain.
   [Sola-Guirado, Rafael R.] Univ Cordoba, Dept Mech, Cordoba 14014, Spain.
C3 Consejo Superior de Investigaciones Cientificas (CSIC); CSIC - Instituto
   de la Grasa (IG); University of Sevilla; Universidad de Cordoba
RP Sola-Guirado, RR (corresponding author), Univ Cordoba, Dept Mech, Cordoba 14014, Spain.
EM eddy.plasquy@telenet.be; jmgarcia@cica.es; florido@us.es;
   ir2sogur@uco.es
RI ; Florido Fernandez, Maria del Carmen/E-2890-2013
OI Sola Guirado, Rafael Ruben/0000-0002-2004-8023; Florido Fernandez, Maria
   del Carmen/0000-0001-7613-8565
CR Al-Widyan MI, 2010, J FOOD PROCESS ENG, V33, P257, DOI 10.1111/j.1745-4530.2008.00273.x
   Arenas-Castro S, 2020, SCI TOTAL ENVIRON, V709, DOI 10.1016/j.scitotenv.2019.136161
   ASHRAE Chapter 9: Methods of Precooling Fruits and Vegetables American Society of Heating, 2006, REFRIGERATION AIR CO
   Barranco D, 2008, CULTIVO OLIVO
   Becker B.R., 1996, INT J HVAC R RES, V2, P215
   Benlloch-Gonzalez M, 2019, SCI HORTIC-AMSTERDAM, V249, P162, DOI 10.1016/j.scienta.2019.01.046
   Biedermann M, 2008, EUR FOOD RES TECHNOL, V228, P65, DOI 10.1007/s00217-008-0907-x
   Boletin Oficial de la Junta de Andalucia, 2020, BOJA, V123, P598
   Cabezas JM, 2020, AGR SYST, V185, DOI 10.1016/j.agsy.2020.102937
   Caponio F, 2003, EUR J LIPID SCI TECH, V105, P201, DOI 10.1002/ejlt.200390041
   Chen QS, 2013, TRAC-TREND ANAL CHEM, V52, P261, DOI 10.1016/j.trac.2013.09.007
   Clodoveo ML, 2007, FOOD CHEM, V102, P571, DOI 10.1016/j.foodchem.2006.05.035
   Connor DJ, 2014, SCI HORTIC-AMSTERDAM, V169, P71, DOI 10.1016/j.scienta.2014.02.010
   Cuesta FJ, 2017, INT J REFRIG, V80, P120, DOI 10.1016/j.ijrefrig.2017.05.012
   Di Serio MG, 2017, FOOD CHEM, V219, P33, DOI 10.1016/j.foodchem.2016.09.109
   Dourou AM, 2020, FOOD RES INT, V129, DOI 10.1016/j.foodres.2019.108861
   Edeogu I, 1997, CAN AGR ENG, V39, P107
   Fraga H, 2019, CLIMATIC CHANGE, V152, P179, DOI 10.1007/s10584-018-2337-5
   Gabaldon-Leal C, 2017, INT J CLIMATOL, V37, P940, DOI 10.1002/joc.5048
   Galan C, 2005, INT J BIOMETEOROL, V49, P184, DOI 10.1007/s00484-004-0223-5
   Garcia JM, 1996, J AGR FOOD CHEM, V44, P590, DOI 10.1021/jf950479s
   Garcia-Mozo H, 2015, ANN AGR ENV MED, V22, P421, DOI 10.5604/12321966.1167706
   Gowen AA, 2010, TRENDS FOOD SCI TECH, V21, P190, DOI 10.1016/j.tifs.2009.12.002
   Guzman E, 2015, J FOOD SCI TECH MYS, V52, P1462, DOI 10.1007/s13197-013-1123-7
   Jabeur H, 2015, FOOD CHEM, V169, P289, DOI 10.1016/j.foodchem.2014.07.118
   Kalua CM, 2008, J AGR FOOD CHEM, V56, P2415, DOI 10.1021/jf073027b
   Kiritsakis A, 1998, J AM OIL CHEM SOC, V75, P721, DOI 10.1007/s11746-998-0212-7
   Lorite IJ, 2018, AGR WATER MANAGE, V204, P247, DOI 10.1016/j.agwat.2018.04.008
   Mihailescu E, 2020, FRONT SUSTAIN FOOD S, V4, DOI 10.3389/fsufs.2020.00064
   Rallo L, 2018, ACTA HORTIC, V1199, P21, DOI [10.17660/ActaHortic.2018.1199.4, 10.17660/actahortic.2018.1199.4]
   Jimenez MR, 2017, POSTHARVEST BIOL TEC, V132, P130, DOI 10.1016/j.postharvbio.2017.06.003
   Sola-Guirado RR, 2019, BIOSYST ENG, V184, P81, DOI 10.1016/j.biosystemseng.2019.06.009
   Sola-Guirado RR, 2017, COMPUT ELECTRON AGR, V143, P139, DOI 10.1016/j.compag.2017.10.011
   Veneziani G, 2017, FOOD CHEM, V221, P107, DOI 10.1016/j.foodchem.2016.10.067
   Vichi S, 2015, EUR J LIPID SCI TECH, V117, P2015, DOI 10.1002/ejlt.201500066
   Wang S, 2001, POSTHARVEST BIOL TEC, V22, P257, DOI 10.1016/S0925-5214(01)00085-0
NR 36
TC 3
Z9 3
U1 2
U2 3
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2077-0472
J9 AGRICULTURE-BASEL
JI Agriculture-Basel
PD FEB
PY 2021
VL 11
IS 2
AR 164
DI 10.3390/agriculture11020164
PG 13
WC Agronomy
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture
GA QM7YI
UT WOS:000621990100001
OA gold, Green Published
DA 2022-02-03
ER

PT C
AU Soegner, PFW
   Helweg, G
   Holzer, H
   zur Nedden, D
AF Soegner, PFW
   Helweg, G
   Holzer, H
   zur Nedden, D
BE Blaine, GJ
   Siegel, EL
TI Evaluation of the feasibility of security technologies in teleradiology
   as biometric fingerprint scanners for data-exchange over a satellite WAN
SO MEDICAL IMAGING 2000: PACS DESIGN AND EVALUATION - ENGINEERING AND
   CLINICAL ISSUES
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT Medical Imaging 2000 Conference
CY FEB 13-17, 2000
CL SAN DIEGO, CA
DE teleradiology; data-security; personal identification; smart card;
   fingerprint scanner; user-friendly
AB We evaluated the feasibility of fingerprint-scanners in combination with smart cards for personal identification and transmission of encrypted TCP/IP-data-packages via satellite between the university-hospital of Innsbruck and the rural hospital of Reutte. The aim of our study was the proof of the userfriendliness of the Skymed(TM) technology for security purpose in teleradiology. We examined the time of the personal identification process, the time for the necessary training and the personal satisfaction. The images were sent from the local PACS in Reutte via a Data-Encryption-and-Transmission-Box via satellite from Reutte to Innsbruck. We used an asymmetric bandwidth of 512 kbit/s from Reutte to Innsbruck and 128 kbit/s in the opposite direction. Windows NT 4.0-operating PCs were used for the electronical patient record, the medical inquiry of the referring physician and the final report of the radiologist. The images were reported on an UNIX-PACS viewing station. After identification through fingerprint-scanners in combination with the smart card the radiologist was able to open the electronic patient record (EPR) from Reutte and sign with his digital signature his confirmed final report before it was send back to Reutte. The used security technology enables encrypted communication over a WAN, which fulfil data-protection.
C1 Univ Innsbruck Hosp, Dept Radiol 2, A-6020 Innsbruck, Austria.
C3 Medical University of Innsbruck
RP Soegner, PFW (corresponding author), Univ Innsbruck Hosp, Dept Radiol 2, Anichstr 35, A-6020 Innsbruck, Austria.
EM peter.soegner@uibk.ac.at
CR BERGER R, 1998, TELEMATIK GESUNDHEIT, P104
   SOEGNER P, 1999, 6 KIS RIS PACS WORKS
   SOEGNER P, 1999, J TELEMEDICINE TE S1, V5, P134
   SOEGNER P, 1998, TELEMEDICINE PROMED, V2, P22
   Wendler T, 1998, INT CONGR SER, V1165, P364
NR 5
TC 0
Z9 0
U1 1
U2 2
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
EI 1996-756X
BN 0-8194-3597-X
J9 PROC SPIE
PY 2000
VL 3980
BP 327
EP 334
DI 10.1117/12.386419
PG 8
WC Engineering, Biomedical; Optics; Radiology, Nuclear Medicine & Medical
   Imaging
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Optics; Radiology, Nuclear Medicine & Medical Imaging
GA BQ36A
UT WOS:000088119600037
DA 2022-02-03
ER

PT C
AU Gouveia, F
   Filipe, V
   Reis, M
   Couto, C
   Bulas-Cruz, J
AF Gouveia, F
   Filipe, V
   Reis, M
   Couto, C
   Bulas-Cruz, J
GP IEEE
TI Biometry: the characterisation of chestnut-tree leaves using computer
   vision
SO ISIE '97 - PROCEEDINGS OF THE IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL
   ELECTRONICS, VOLS 1-3
LA English
DT Proceedings Paper
CT IEEE International Symposium on Industrial Electronics
CY JUL 07-11, 1997
CL UNIV MINHO, GUIMARAES, PORTUGAL
HO UNIV MINHO
AB The Department of Biology of the University of Tras-os-Montes e Alto Douro analyses every year a large number of chestnut-tree leaves, in order to measure their biometric characteristics, namely the leaf area, dimensions of the enclosing rectangle, number of teeth and secondary veins. Because for a human operator this is a time consuming and error prone task, a computer vision system has been set up to improve the process. The task of measuring the leaf presents no major problems, while counting the number of teeth and secondary veins has proved to be complex at the resolutions used.
   This paper describes the state of the project, going into some detail on the algorithms. A complete system has been assembled, based on an PC connected to an imaging system. A windows-based application has been developed, which integrates the control of the operations to grab, store and analyse images of different varieties of chestnut-tree leaves in an organised way. Because the accuracy of the computer vision algorithms used is not sufficient for the system to be completely autonomous, a semi-automatic solution has been adopted. The operator validates or corrects the results of the automatic analysis. This solution leads to a significant improvement in the performance of the human operator, both in terms of speed and quality of the results.
C1 Univ Tras Os Montes & Alto Douro, Seccao Engn, P-5001 Vila Real, Portugal.
C3 University of Tras-os-Montes & Alto Douro
RP Gouveia, F (corresponding author), Univ Tras Os Montes & Alto Douro, Seccao Engn, Apartado 202, P-5001 Vila Real, Portugal.
RI Reis, Manuel J.C.S./G-2410-2012
OI Reis, Manuel J.C.S./0000-0002-8872-5721; Filipe,
   Vitor/0000-0002-3747-6577
NR 0
TC 13
Z9 14
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 0-7803-3936-3
PY 1997
BP 757
EP 760
DI 10.1109/ISIE.1997.648634
PG 4
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Automation & Control Systems; Computer Science; Engineering
GA BK48L
UT WOS:000072310300199
DA 2022-02-03
ER

PT C
AU Ahmed, M
   Viriri, S
AF Ahmed, Marwa
   Viriri, Serestina
BE Karray, F
   Campilho, A
   Yu, A
TI Deep Learning Using Bayesian Optimization for Facial Age Estimation
SO IMAGE ANALYSIS AND RECOGNITION (ICIAR 2019), PT II
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 16th International Conference on Image Analysis and Recognition (ICIAR)
CY AUG 27-29, 2019
CL Waterloo, CANADA
DE Age estimation; Feature learning; Convolutional Neural Networks;
   Bayesian optimization
AB Age Estimation plays a significant role in many real-world applications. Age estimation is a process of determining the exact age or age group of a person depending on his biometric features. Recent research demonstrates that the deeply learned features for age estimation from large-scale data result in significant improvement of the age estimation performance for facial images. This paper propose a Convolutional Neural Network (CNN) - approach using Bayesian Optimization for facial age estimation. Bayesian Optimization is applied to minimize the classification error on the validation set for CNN model. Extensive experiments are done for evaluating Deep Learning using Bayesian Optimization (DLOB) on three datasets: MORPH, FG-NET and FERET. The results show that using Bayesian Optimization for CNN outperforms the state of the arts on FG-NET and FERET datasets with a Mean Absolute Error (MAE) of 2.88 and 1.3, and achieves comparable results compared to the most of the state-of-the-art methods on MORPH dataset with a 3.01 MAE.
C1 [Ahmed, Marwa] Sudan Univ Sci & Technol, Coll Comp Sci & Informat Technol, Khartoum, Sudan.
   [Viriri, Serestina] Univ KwaZulu Natal, Sch Math Stat & Comp Sci, Durban, South Africa.
C3 University of Kwazulu Natal
RP Viriri, S (corresponding author), Univ KwaZulu Natal, Sch Math Stat & Comp Sci, Durban, South Africa.
EM jamal.marwa@gmail.com; viriris@ukzn.ac.za
RI Viriri, Serestina/AAN-3882-2020
OI Viriri, Serestina/0000-0002-2850-8645
CR Brochu E., 2010, ARXIV10122599, DOI DOI 10.HTTPS://UI.ADSABS.HARVARD.EDU/ABS/2010ARXIV1012.2599B
   Chen K, 2013, PROC CVPR IEEE, P2467, DOI 10.1109/CVPR.2013.319
   Chen SX, 2018, IEEE T MULTIMEDIA, V20, P2209, DOI 10.1109/TMM.2017.2786869
   Dong Y, 2016, NEUROCOMPUTING, V187, P4, DOI 10.1016/j.neucom.2015.09.115
   Hu ZZ, 2017, IEEE T IMAGE PROCESS, V26, P3087, DOI 10.1109/TIP.2016.2633868
   Jones DR, 2001, J GLOBAL OPTIM, V21, P345, DOI 10.1023/A:1012771025575
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin CT, 2012, INT J ADV ROBOT SYST, V9, DOI 10.5772/52862
   Liu H, 2019, IEEE T CIRC SYST VID, V29, P486, DOI 10.1109/TCSVT.2017.2782709
   Liu H, 2017, PATTERN RECOGN, V66, P82, DOI 10.1016/j.patcog.2016.10.026
   Mockus J., 1978, APPL BAYESIAN METHOD, V2
   Ng CC, 2018, IMAGE VISION COMPUT, V69, P92, DOI 10.1016/j.imavis.2017.08.005
   Ng CC, 2017, PROCEEDINGS OF THE FIFTEENTH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS - MVA2017, P294, DOI 10.23919/MVA.2017.7986859
   Ng CC, 2015, IEEE SYS MAN CYBERN, P2418, DOI 10.1109/SMC.2015.423
   NIU ZX, 2016, PROC CVPR IEEE, P4920, DOI DOI 10.1109/CVPR.2016.532
   Panis G, 2016, IET BIOMETRICS, V5, P37, DOI 10.1049/iet-bmt.2014.0053
   Petra G, 2013, RES PAPERS FACULTY M, V21, P24
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Qiu J., 2016, CONVOLUTIONAL NEURAL
   Rasmussen C. E., 2006, GAUSSIAN PROCESSES M, V2, P4, DOI DOI 10.1142/50129065704001899
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3
   Salah A., 2016, P IEEE C COMP VIS PA
   Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218
   Shen W, 2018, PROC CVPR IEEE, P2304, DOI 10.1109/CVPR.2018.00245
   Snoek J., 2012, P 25 INT C NEURAL IN, P2951
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Swersky K., 2013, ADV NEURAL INFORM PR, P2004
   Wan J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P157, DOI 10.1145/2647868.2654948
   Wang XL, 2015, IEEE WINT CONF APPL, P534, DOI 10.1109/WACV.2015.77
   Zeiler M.D., 2013, THESIS
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58
NR 34
TC 2
Z9 2
U1 0
U2 1
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-030-27272-2; 978-3-030-27271-5
J9 LECT NOTES COMPUT SC
PY 2019
VL 11663
BP 243
EP 254
DI 10.1007/978-3-030-27272-2_21
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Imaging Science &
   Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BP7EY
UT WOS:000561796800021
DA 2022-02-03
ER

PT C
AU Goncalves, GR
   Nazare, AC
   Diniz, MA
   Lima, LEC
   Schwartz, WR
AF Goncalves, Gabriel Resende
   Nazare, Antonio Carlos
   Diniz, Matheus Alves
   Coelho Lima, Luiz Eduardo
   Schwartz, William Robson
GP IEEE
TI AVSS Challenges 2018 Soft Biometric Retrieval Using Deep Multi-Task
   Network
SO 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL
   BASED SURVEILLANCE (AVSS)
LA English
DT Proceedings Paper
CT 15th IEEE International Conference on Advanced Video and Signal Based
   Surveillance (AVSS)
CY NOV 27-30, 2018
CL Auckalnd Univ Technol, Auckland, NEW ZEALAND
HO Auckalnd Univ Technol
AB In surveillance, humans are the agents performing actions to change the states in the scene. They are the main focus in the surveillance systems and, therefore, the design of processing methods focusing on humans is extremely important to identify a person and determine his/her role in the scene. Thus, one of the goals of smart surveillance systems is to address the automatic video understanding by applying computer vision techniques to automatically detect specific humans in video streams based on attributes, such as soft biometrics. For that purpose, this work proposes an approach that receives a set of textual attributes as a query and searches for people by matching those attributes in a gallery of images, as defined in the challenge Semantic Person Retrieval in Surveillance Using Soft Biometrics Challenge, proposed in AVSS 2018. We address this problem with a multi-task learning approach hypothesizing that the attributes available for the query are highly related to each other that could be learned together in the same network as different tasks. Finally, we performed both qualitative and quantitative experimental evaluations, indicating a promising direction for the proposed approach.
C1 [Goncalves, Gabriel Resende; Nazare, Antonio Carlos; Diniz, Matheus Alves; Coelho Lima, Luiz Eduardo; Schwartz, William Robson] Univ Fed Minas Gerais, Dept Comp Sci, Smart Sense Lab, Belo Horizonte, MG, Brazil.
C3 Universidade Federal de Minas Gerais
RP Goncalves, GR (corresponding author), Univ Fed Minas Gerais, Dept Comp Sci, Smart Sense Lab, Belo Horizonte, MG, Brazil.
EM gabrielrg@dcc.ufmg.br; acnazare@dcc.ufmg.br; matheusad@dcc.ufmg.br;
   luizduducoelho@ufmg.br; william@dcc.ufmg.br
FU Coordination for the Improvement of Higher Education Personnel - CAPES
   (DeepEyes Project); PetrobrasFundacao de Amparo a Pesquisa do Amapa
   (FAPEAP)Petrobras [2017/00643-0]
FX The authors would like to thank the Coordination for the Improvement of
   Higher Education Personnel - CAPES (DeepEyes Project) and Petrobras
   (Grant 2017/00643-0).
CR Dantcheva A, 2011, MULTIMED TOOLS APPL, V51, P739, DOI 10.1007/s11042-010-0635-7
   Davies A. C., 2007, P IEEE INT DAT ACQ A, P417
   Ghaleb E, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P455, DOI 10.1145/2671188.2749296
   Halstead M, 2014, INT C PATT RECOG, P4501, DOI 10.1109/ICPR.2014.770
   Hampapur A, 2008, IEEE SIGNAL PROC MAG, V25, P136, DOI 10.1109/MSP.2008.924957
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   LAYNE R, 2012, BMVC, DOI DOI 10.5244/C.26.24
   Li Y, 2015, PROC CVPR IEEE, P4758, DOI 10.1109/CVPR.2015.7299108
   Moeskops P, 2016, INT C MED IM COMP CO
   Ranjan  R., 2017, T PATTERN ANAL MACHI
   Sivic J, 2005, LECT NOTES COMPUT SC, V3568, P226
   Smith G. J., 2004, SURVEILLANCE SOC, V2
   Vaquero DA, 2009, P WORKSH APPL COMP V, P1
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   2013, IEEE SIGNAL PROCESSI, V30, P190, DOI DOI 10.1109/MSP.2013.2241312
NR 15
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-5386-9294-3
PY 2018
BP 447
EP 452
PG 6
WC Engineering, Electrical & Electronic; Imaging Science & Photographic
   Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Imaging Science & Photographic Technology
GA BM7MI
UT WOS:000468081400075
DA 2022-02-03
ER

PT J
AU Alpar, O
AF Alpar, Orcan
TI Frequency spectrograms for biometric keystroke authentication using
   neural network based classifier
SO KNOWLEDGE-BASED SYSTEMS
LA English
DT Article
DE Biometrics; Spectrogram; Frequency; Keystroke authentication; Short-time
   Fourier transformation; Gauss-Newton based neural networks
ID USER AUTHENTICATION; DYNAMICS; RECOGNITION; SYSTEMS
AB Keystroke recognition is one of the branch of biometrics that is designed to strengthen regular passwords through inter-key times to protect the password owner from fraud attacks. The signals of keystrokes are usually evaluated only in the time domain since the applied systems collect and analyze only the time values. In addition to these kinds of algorithms, we introduce the extraction of novel frequency feature and a keystroke authentication system which has a classifier operating in frequency domain. The frequency extraction is a new approach that will enhance the authentication protocols and shed light on the keystroke authentication by providing a hidden security level. Above all, instead of inter-key times, the exact key press times are extracted and binarized in time domain. Subsequently, the spectrograms are generated by regular short time Fourier transform with the optimized window size. Since the spectrograms include both frequency and time data, represented as images, low frequencies under a threshold are erased and the high frequencies are collected in bins after the digitization. Consequently the average bin values are used as the inputs to train the Gauss-Newton based Neural Network classifier to validate the attempts. The results are highly promising that we obtained 4.1% Equal Error Rate (EER) after 60 real attempts of the password owner and 60 fraud attacks from 12 different users. The outcomes of this research enhance our understanding of knowledge-based classifiers for authentication as well as the Gauss-Newton based optimization for vectorial inputs of spectrogram analysis. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Alpar, Orcan] Univ Hradec Kralove, Fac Informat & Management, Ctr Basic & Appl Res, Rokitanskeho 62, Hradec Kralove 50003, Czech Republic.
C3 University of Hradec Kralove
RP Alpar, O (corresponding author), Univ Hradec Kralove, Fac Informat & Management, Ctr Basic & Appl Res, Rokitanskeho 62, Hradec Kralove 50003, Czech Republic.
EM orcanalpar@hotmail.com
RI Alpar, Orcan/C-5443-2013
OI Alpar, Orcan/0000-0001-7651-345X
FU project "Smart Solutions in Ubiquitous Computing Environments", Grant
   Agency of Excellence, University of Hradec Kralove, Faculty of
   Informatics and Management
FX The work and the contribution were supported by the project "Smart
   Solutions in Ubiquitous Computing Environments", Grant Agency of
   Excellence, University of Hradec Kralove, Faculty of Informatics and
   Management.
CR Ahmed AAE, 2008, P 2 INT S HUM ASP IN, P94
   Ahmed AA, 2014, IEEE T CYBERNETICS, V44, P458, DOI 10.1109/TCYB.2013.2257745
   Alpar O, 2015, LECT NOTES COMPUT SC, V9375, P395, DOI 10.1007/978-3-319-24834-9_46
   Alpar O, 2015, LECT NOTES COMPUT SC, V9339, P193, DOI 10.1007/978-3-319-24369-6_16
   Alpar O, 2015, EXPERT SYST APPL, V42, P6286, DOI 10.1016/j.eswa.2015.04.052
   Alpar O, 2014, ENG APPL ARTIF INTEL, V32, P213, DOI 10.1016/j.engappai.2013.11.009
   Alsultan A, 2013, IEEE SYS MAN CYBERN, P4658, DOI 10.1109/SMC.2013.793
   Angulo J., 2012, IFIP ADV INFORM COMM, V375, P130
   Araujo LCF, 2005, IEEE T SIGNAL PROCES, V53, P851, DOI 10.1109/TSP.2004.839903
   Buschek D, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1393, DOI 10.1145/2702123.2702252
   Campisi P, 2009, IET SIGNAL PROCESS, V3, P333, DOI 10.1049/iet-spr.2008.0171
   Chang TY, 2012, J SYST SOFTWARE, V85, P1157, DOI 10.1016/j.jss.2011.12.044
   Clarke NL, 2007, INT J INF SECUR, V6, P1, DOI 10.1007/s10207-006-0006-6
   Clarke NL, 2007, COMPUT SECUR, V26, P109, DOI 10.1016/j.cose.2006.08.008
   Crawford H, 2010, ANN CONF PRIV SECUR, P205, DOI 10.1109/PST.2010.5593258
   Dholi P.R., 2013, COMMUN COMPUT INF SC, V296, P275
   Garg U, 2013, ADV INTELL SYST, V174, P131
   Gunetti D., 2005, ACM Transactions on Information and Systems Security, V8, P312, DOI 10.1145/1085126.1085129
   Hwang SS, 2009, EXPERT SYST APPL, V36, P10649, DOI 10.1016/j.eswa.2009.02.075
   Jamil D., 2011, INT J ENG SCI TECHNO, V3, P1953
   Kambourakis G, 2016, SECUR COMMUN NETW, V9, P542, DOI 10.1002/sec.1061
   Kang P, 2007, LECT NOTES COMPUT SC, V4642, P1203
   Kang P, 2015, INFORM SCIENCES, V308, P72, DOI 10.1016/j.ins.2014.08.070
   Karatzouni S, 2007, INT FED INFO PROC, V232, P253
   Karnan M, 2011, APPL SOFT COMPUT, V11, P1565, DOI 10.1016/j.asoc.2010.08.003
   Li YW, 2011, IEEE CUST INTEGR CIR, DOI 10.1109/CICC.2011.6055295
   Maiorana E., 2011, P ACM S APPL COMP SA, P21
   Messerman A., 2011, INT JOINT C BIOM IJC
   Montalvao J, 2015, PATTERN RECOGN LETT, V52, P80, DOI 10.1016/j.patrec.2014.09.016
   Rudrapal D., 2013, LECT NOTES ELECT ENG, V221, P375
   Sae-Bae N., 2012, P SIGCHI C HUM FACT, P977, DOI DOI 10.1145/2207676.2208543
   Saevanee H, 2009, CONS COMM NETW C CCN, P10
   Shahzad M, 2013, P 19 ANN INT C MOB C, P39
   Spillane R., 1975, IBM TECHNICAL DISCLO, P17
   Syed Z., 2011, Proceedings of the 2011 IEEE 13th International Symposium on High-Assurance Systems Engineering (HASE 2011), P352, DOI 10.1109/HASE.2011.16
   Tasia CJ, 2014, SECUR COMMUN NETW, V7, P750, DOI 10.1002/sec.776
   Teh PS, 2010, EXPERT SYST APPL, V37, P8618, DOI 10.1016/j.eswa.2010.06.097
   Zheng N., 2012, TECHNICAL REPORT
   Zhong Y., 2012, 2012 IEEE COMP VIS P, P117, DOI [DOI 10.1109/CVPRW.2012.6239225, 10.1109/CVPRW.2012.6239225]
NR 39
TC 24
Z9 26
U1 0
U2 11
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0950-7051
EI 1872-7409
J9 KNOWL-BASED SYST
JI Knowledge-Based Syst.
PD JAN 15
PY 2017
VL 116
BP 163
EP 171
DI 10.1016/j.knosys.2016.11.006
PG 9
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EI8PR
UT WOS:000392770400015
DA 2022-02-03
ER

PT C
AU Bekele, E
   Lawson, WE
   Horne, Z
   Khemlani, S
AF Bekele, Esube
   Lawson, Wallace E.
   Horne, Zachary
   Khemlani, Sangeet
GP IEEE
TI Implementing a Robust Explanatory Bias in a Person Re-identification
   Network
SO PROCEEDINGS 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN
   RECOGNITION WORKSHOPS (CVPRW)
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
LA English
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-22, 2018
CL Salt Lake City, UT
AB Deep learning improved attributes recognition significantly in recent years. However, many of these networks remain "black boxes" and providing a meaningful explanation of their decisions is a major challenge. When these networks misidentify a person, they should be able to explain this mistake. The ability to generate explanations compelling enough to serve as useful accounts of the system's operations at a very high human-level is still in its infancy. In this paper, we utilize person re-identification (re-ID) networks as a platform to generate explanations. We propose and implement a framework that can be used to explain person re-ID using soft-biometric attributes. In particular, the resulting framework embodies a cognitively validated explanatory bias: people prefer and produce explanations that concern inherent properties instead of extrinsic influences. This bias is pervasive in that it affects the fitness of explanations across a broad swath of contexts, particularly those that concern conflicting or anomalous observations. To explain person re-ID, we developed a multi-attribute residual network that treats a subset of its features as either inherent or extrinsic. Using these attributes, the system generates explanations based on inherent properties when the similarity of two input images is low, and it generates explanations based on extrinsic properties when the similarity is high. We argue that such a framework provides a blueprint for how to make the decisions of deep networks comprehensible to human operators. As an intermediate step, we demonstrate state-of-the-art attribute recognition performance on two pedestrian datasets (PETA and PA100K) and a face-based attribute dataset (CelebA). The VIPeR dataset is then used to generate explanations for reID with a network trained on PETA attributes.
C1 [Bekele, Esube] CNR, Washington, DC 20418 USA.
   [Lawson, Wallace E.; Khemlani, Sangeet] Naval Res Lab, Washington, DC 20375 USA.
   [Horne, Zachary] Arizona State Univ, Phoenix, AZ USA.
C3 United States Department of Defense; United States Navy; Naval Research
   Laboratory; Arizona State University; Arizona State University-Downtown
   Phoenix
RP Bekele, E (corresponding author), CNR, Washington, DC 20418 USA.
EM esube.bekele.ctr@nrl.navy.mil; ed.lawson@nrl.navy.mil;
   zachary.horne@asu.edu; sunny.khemlani@nrl.navy.mil
OI Horne, Zachary/0000-0001-6629-2040
FU National Research Council Postdoctoral Fellowships; Office of Naval
   ResearchOffice of Naval Research
FX This research was supported by National Research Council Postdoctoral
   Fellowships to EB and ZH, as well as a grant from the Office of Naval
   Research to WL and SK.
CR Bekele E, 2017, IEEE INT CONF AUTOMA, P386, DOI 10.1109/FG.2017.55
   Cimpian A, 2014, BEHAV BRAIN SCI, V37, P461, DOI 10.1017/S0140525X13002197
   Deng YB, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P789, DOI 10.1145/2647868.2654966
   Gray D., 2007, P IEEE INT WORKSH PE, V3
   He K., 2016, ARXIV160305027
   Huang G., 2016, ARXIV160806993
   Khemlani S., 2018, STEVENS HDB EXPT PSY, P385
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li D., 2015, MULTIATTRIBUTE LEARN
   Li D, 2016, ARXIV160307054
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liu X., 2017, ARXIV170909930
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Miller T., 2017, P INT JOINT C 2017 W, P36, DOI DOI 10.1016/J.FOODCHEM.2017.11.091
   Sarfraz M Saquib, 2017, ARXIV170706089
   Sudowe Patrick, 2015, P IEEE INT C COMP VI, P87
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Yu K., 2016, ARXIV161105603
   Zhu JQ, 2015, INT CONF BIOMETR, P535, DOI 10.1109/ICB.2015.7139070
NR 19
TC 2
Z9 2
U1 0
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2160-7508
BN 978-1-5386-6100-0
J9 IEEE COMPUT SOC CONF
PY 2018
BP 2246
EP 2253
DI 10.1109/CVPRW.2018.00291
PG 8
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BL9LT
UT WOS:000457636800284
DA 2022-02-03
ER

PT J
AU Umar, Z
   Qureshi, AS
   Rehan, S
   Ijaz, M
   Faisal, T
   Umar, S
AF Umar, Zaima
   Qureshi, Anas Sarwar
   Rehan, Sarmad
   Ijaz, Misbah
   Faisal, Tanzeela
   Umar, Saqib
TI Effects of oral administration of black seed (Nigella sativa) oil on
   histomorphometric dynamics of testes and testosterone profile in rabbits
SO PAKISTAN JOURNAL OF PHARMACEUTICAL SCIENCES
LA English
DT Article
DE Black seed (Nigella sativa) oil; rabbits; testes; testosterone level;
   histomorphometry
ID FERTILITY; SIZE
AB The present study was conducted to evaluate the influence of oral administration of black seed (Nigella sativa) oil on histomorphometrical characteristics of testes and testosterone profile in adult rabbits. Twenty adult male rabbits aged seven months were divided into two groups: control and treated. Black seed oil was administrated orally for 60 days at 5ml/kg body weight/day on daily basis in addition to the food and water ad lib to the treated group. Biometric parameters of the testes were recorded immediately after their removal. Tissue samples of testes were processed with paraffin tissue preparation technique. Histometrical parameters of testes were measured with the help of automated image analysis software Image J (R). Serum testosterone concentration was determined with Radioimmunoassay technique. Statistical analysis revealed significant (P<0.05) rise in weight, length, circumference and volume of testis in treated group than control group. The values of histometrical parameters studied viz., thickness of spermatogenic epithelium, diameter and area of seminiferous tubules, diameter of lumen of seminiferous tubules, number of spermatogenic layers of testes and serum testosterone concentration were found significantly (P<0.05) higher in treated group than control group. Based on the data it is conceivable that the oral administration of black seed oil has potential to stimulate testicular function in adult rabbits.
C1 [Umar, Zaima; Qureshi, Anas Sarwar; Rehan, Sarmad; Faisal, Tanzeela] Univ Agr Faisalabad, Dept Anat, Faisalabad, Pakistan.
   [Ijaz, Misbah] Univ Agr Faisalabad, Dept Clin Med & Surg, Faisalabad, Pakistan.
   [Umar, Saqib] Univ Agr Faisalabad, Dept Theriogenol, Faisalabad, Pakistan.
C3 University of Agriculture Faisalabad; University of Agriculture
   Faisalabad; University of Agriculture Faisalabad
RP Qureshi, AS (corresponding author), Univ Agr Faisalabad, Dept Anat, Faisalabad, Pakistan.
EM anas-sarwar@hotmail.com
RI Rehan, Sarmad/E-9996-2017; Umer, Saqib/U-2942-2019
OI Rehan, Sarmad/0000-0003-3840-2710; 
CR Abu-Elawa MEM, 1995, THESIS
   Al-Ali Amein, 2008, J Ayub Med Coll Abbottabad, V20, P25
   Al-Bukhari MI, 1976, THESIS
   Al-Helali IAH, 2002, THESIS
   Al-Sa'aidi J. A. A., 2009, Iraqi Journal of Veterinary Sciences, V23, pEn123
   Al-Taee AA, 2008, J KERBALA U, V6, P246
   Ali BH, 2003, PHYTOTHER RES, V17, P299, DOI 10.1002/ptr.1309
   Ashraf HA, 2013, J AM SCI, V9, P24
   Ayan M, 2015, ANDROLOGIA
   Bancroft JD, 2018, THEORY PRACTICE HIST, P303
   Calixto JB, 2005, J ETHNOPHARMACOL, V100, P131, DOI 10.1016/j.jep.2005.06.004
   El-Tohamy M.M., 2010, AM J SCI, V6, P1247
   Farah IO, 2003, BIOMED SCI INSTRUM, V39, P359
   Gokce A, 2010, EUR UROL SUPPL, V9, P585, DOI 10.1016/S1569-9056(10)61395-4
   Gokce A, 2011, HUM EXP TOXICOL, V30, P897, DOI 10.1177/0960327110382564
   Gromadzka-Ostrowska Joanna, 2002, Reprod Biol, V2, P277
   Hamdon HAM, 2005, THESIS
   Heinrich M, 2000, PHYTOTHER RES, V14, P479, DOI 10.1002/1099-1573(200011)14:7&lt;479::AID-PTR958&gt;3.0.CO;2-2
   Isidori AM, 2006, REPROD BIOMED ONLINE, V12, P704, DOI 10.1016/S1472-6483(10)61082-6
   Kassab AY, 2007, THESIS
   Mansour Sherif W., 2013, Asian Pacific Journal of Tropical Biomedicine, V3, P563, DOI 10.1016/S2221-1691(13)60114-8
   MOSHER WD, 1991, FERTIL STERIL, V56, P192
   Moura AA, 2011, ANIM REPROD SCI, V124, P39, DOI 10.1016/j.anireprosci.2011.01.016
   Mukhalad A.M., 2009, RES J MED SCI, V4, P386
   Ogunlade J. T., 2006, World Applied Sciences Journal, V1, P35
   Ozturk A, 2002, INDIAN J ANIM SCI, V72, P9
   Parandin R, 2012, IRAN J REPROD MED, V10, P355
   Preston BT, 2012, J ANIM ECOL, V81, P296, DOI 10.1111/j.1365-2656.2011.01907.x
   PRINS GS, 1991, ENDOCRINOLOGY, V129, P3187, DOI 10.1210/endo-129-6-3187
   Salhab SA, 2001, SMALL RUMINANT RES, V40, P187, DOI 10.1016/S0921-4488(00)00224-8
   Samir BAE, 2007, INT J PHARMACEUT, V3, P27, DOI DOI 10.3923/IJP.2007.27.33
   Sumalatha K., 2010, INT J PHARM PRACT, V1, P6
   WHO, 2002, TRAD MED STRAT 2002
   Yakubu MT, 2007, PHCOG REV, V1, p[1, 49]
   Zanouny A. I., 2013, Egyptian Journal of Sheep and Goat Sciences, V8, P47
NR 35
TC 6
Z9 6
U1 0
U2 4
PU UNIV KARACHI
PI KARACHI
PA UNIV CAMPUS, FAC PHARMACY, KARACHI, 75270, PAKISTAN
SN 1011-601X
J9 PAK J PHARM SCI
JI Pak. J. Pharm. Sci.
PD MAR
PY 2017
VL 30
IS 2
BP 531
EP 536
PG 6
WC Pharmacology & Pharmacy
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Pharmacology & Pharmacy
GA ET4CG
UT WOS:000400226200027
PM 28649080
DA 2022-02-03
ER

PT J
AU Juels, A
   Sudan, M
AF Juels, A
   Sudan, M
TI A fuzzy vault scheme
SO DESIGNS CODES AND CRYPTOGRAPHY
LA English
DT Article
DE authentication; cryptography; error-correting codes
AB We describe a simple and novel cryptographic construction that we refer to as a fuzzy vault. A player Alice may place a secret value kappa degrees in a fuzzy vault and "lock" it using a set A of elements from some public universe U. If Bob tries to "unlock" the vault using a set B of similar length, he obtains kappa degrees only if B is close to A, i.e., only if A and B overlap substantially. In constrast to previous constructions of this flavor, ours possesses the useful feature of order invariance, meaning that the ordering of A and B is immaterial to the functioning of the vault. As we show, our scheme enjoys provable security against a computationally unbounded attacker. Fuzzy vaults have potential application to the problem of protecting data in a number of real-world, error-prone environments. These include systems in which personal information serves to authenticate users for, e.g., the purposes of password recovery, and also to biometric authentication systems, in which readings are inherently noisy as a result of the refractory nature of image capture and processing.
C1 RSA Labs, Bedford, MA 01730 USA.
   MIT, Cambridge, MA 02139 USA.
C3 Massachusetts Institute of Technology (MIT)
RP Juels, A (corresponding author), RSA Labs, 174 Middlesex Turnpike, Bedford, MA 01730 USA.
EM ajuels@rsasecurity.com
CR ALABBADI M, 1994, LNCS, V917, P238
   Bennett C. H., 1992, Journal of Cryptology, V5, P3
   BENNETT CH, 1991, CRIPTO 91, V576, P351
   Berlekamp Elwyn R., 1968, ALGEBRAIC CODING THE
   Bleichenbacher D, 2000, LECT NOTES COMPUT SC, V1807, P53
   Boudot F, 2001, DISCRETE APPL MATH, V111, P23, DOI 10.1016/S0166-218X(00)00342-5
   Boyko V, 2000, LECT NOTES COMPUT SC, V1807, P156
   Crepeau C., 1997, Advances in Cryptology - EUROCRYPT '97. International Conference on the Theory and Application of Cryptographic Techniques Proceedings, P306
   Davida G. I., 1999, P WCC99 WORKSH COD C
   DAVIDA GI, 1998, IEEE S PRIV SEC
   Dumer I., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P475, DOI 10.1109/SFFCS.1999.814620
   *EL FRONT FDN, 1998, CRACK DES SECR ENCR
   Ellison C, 2000, FUTURE GENER COMP SY, V16, P311, DOI 10.1016/S0167-739X(99)00055-2
   FRYKHOLM N, 2001, 8 ACM C COMP COMM SE, P1
   Guruswami V, 1998, ANN IEEE SYMP FOUND, P28, DOI 10.1109/SFCS.1998.743426
   Jakobsen T, 1998, LECT NOTES COMPUT SC, V1462, P212
   JAKOBSSON M, 1996, LECT NOTES COMPUTER, V1109, P186
   Jermyn I, 1999, USENIX ASSOCIATION PROCEEDINGS OF THE EIGHTH USENIX SECURITY SYMPOSIUM (SECURITY '99), P1
   Juels A, 1999, 6TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P28
   Juels A, 2002, ISIT: 2002 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P408, DOI 10.1109/ISIT.2002.1023680
   Kilian J., 1988, P 29 ANN IEEE S FDN, P42
   MASSEY JL, 1969, IEEE T INFORM THEORY, V15, P122, DOI 10.1109/TIT.1969.1054260
   MCELIECE RJ, 1978, 4244 CALTECH JET PRO, P42
   Monrose F, 1999, 6TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P73
   PEDERSEN TP, 1992, LECT NOTES COMPUT SC, V576, P129
   PETERSON WW, 1960, IRE T INFORM THEOR, V6, P459, DOI 10.1109/TIT.1960.1057586
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   SOUTAR C, 1998 RSA DAT SEC C
   SOUTAR C, 1996, CARDTECH SECURTECH C, V1, P245
   Stern J., 1993, P ANN INT CYRPT C CA, DOI DOI 10.1007/3-540-48329-2
NR 30
TC 357
Z9 380
U1 1
U2 16
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0925-1022
EI 1573-7586
J9 DESIGN CODE CRYPTOGR
JI Designs Codes Cryptogr.
PD FEB
PY 2006
VL 38
IS 2
BP 237
EP 257
DI 10.1007/s10623-005-6343-z
PG 21
WC Computer Science, Theory & Methods; Mathematics, Applied
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Mathematics
GA 002HN
UT WOS:000234602200006
DA 2022-02-03
ER

PT J
AU Lavdas, AA
   Salingaros, NA
   Sussman, A
AF Lavdas, Alexandros A.
   Salingaros, Nikos A.
   Sussman, Ann
TI Visual Attention Software: A New Tool for Understanding the "Subliminal"
   Experience of the Built Environment
SO APPLIED SCIENCES-BASEL
LA English
DT Article
DE eye-tracking; visual attention; predictive engagement; subconscious
   attraction; design tools; coherence; architecture
AB Eye-tracking technology is a biometric tool that has found many commercial and research applications. The recent advent of affordable wearable sensors has considerably expanded the range of these possibilities to fields such as computer gaming, education, entertainment, health, neuromarketing, psychology, etc. The Visual Attention Software by 3M (3M-VAS) is an artificial intelligence application that was formulated using experimental data from eye-tracking. It can be used to predict viewer reactions to images, generating fixation point probability maps and fixation point sequence estimations, thus revealing pre-attentive processing of visual stimuli with a very high degree of accuracy. We have used 3M-VAS software in an innovative implementation to analyze images of different buildings, either in their original state or photographically manipulated, as well as various geometric patterns. The software not only reveals non-obvious fixation points, but also overall relative design coherence, a key element of Christopher Alexander's theory of geometrical order. A more evenly distributed field of attention seen in some structures contrasts with other buildings being ignored, those showing instead unconnected points of splintered attention. Our findings are non-intuitive and surprising. We link these results to both Alexander's theory and Neuroscience, identify potential pitfalls in the software's use, and also suggest ways to avoid them.
C1 [Lavdas, Alexandros A.] Univ Lubeck, Affiliated Inst, Inst Biomed, Eurac Res, Via Galvani 31, I-39100 Bolzano, Italy.
   [Salingaros, Nikos A.] Univ Texas San Antonio, Dept Math & Architecture, San Antonio, TX 78249 USA.
   [Sussman, Ann] Human Architecture & Planning Inst Inc, 43 Bradford St, Concord, MA 01742 USA.
C3 European Academy of Bozen-Bolzano; University of Texas System;
   University of Texas at San Antonio (UTSA)
RP Salingaros, NA (corresponding author), Univ Texas San Antonio, Dept Math & Architecture, San Antonio, TX 78249 USA.
EM alexandros.lavdas@eurac.edu; yxk833@my.utsa.edu; annsmail4@gmail.com
OI Salingaros, Nikos/0000-0002-8856-9175; Lavdas,
   Alexandros/0000-0002-4206-6724
CR 3M, 2020, VIS ATT SOFTW VIS ATT SOFTW
   Alexander C., 2001, NATURE ORDER BOOK 1
   Chang L, 2017, CELL, V169, P1013, DOI 10.1016/j.cell.2017.05.011
   Compton R. J, 2018, COGN NEUROSCI-UK
   Ellard C., 2015, PLACES HEART PSYCHOG
   Ergan S, 2019, J COMPUT CIVIL ENG, V33, DOI 10.1061/(ASCE)CP.1943-5487.0000812
   Expoze, 2021, AI BAS EYE TRACK AI BAS EYE TRACK
   EyeQuant, 2021, DAT DRIV DES DAT DRIV DES
   Fischmeister FP, 2017, CORTEX, V97, P183, DOI 10.1016/j.cortex.2016.08.016
   Hofflinger B., 2007, SPRINGER SERIES ADV SPRINGER SERIES ADV
   Hollander JB, 2021, ARCHIT SCI REV, V64, P383, DOI 10.1080/00038628.2021.1929055
   Hollander JB, 2020, J PHYS ACT HEALTH, V17, P1153, DOI 10.1123/jpah.2020-0127
   Hollander JB, 2020, PLAN PRACT RES, V35, P485, DOI 10.1080/02697459.2020.1768332
   iMotions, 2021, EYE TRACKING
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Jiang B, 2016, PHYSICA A, V463, P475, DOI 10.1016/j.physa.2016.07.038
   Jiang B, 2015, INT J GEOGR INF SCI, V29, P1632, DOI 10.1080/13658816.2015.1038542
   Joye Y, 2007, REV GEN PSYCHOL, V11, P305, DOI 10.1037/1089-2680.11.4.305
   Kandel E., 2012, AGE INSIGHT QUEST UN
   Lesser S., 2021, NOTES NATURE ORDER
   Mandelbrot B B, 1982, FRACTAL GEOMETRY NAT, P206
   Martins MJ, 2014, NEUROIMAGE, V96, P300, DOI 10.1016/j.neuroimage.2014.03.064
   McFadyen J, 2019, J EXP NEUROSCI, V13, DOI 10.1177/1179069519846445
   Merrifield C, 2014, EXP BRAIN RES, V232, P481, DOI 10.1007/s00221-013-3755-2
   Ruggles D.H., 2018, BEAUTY NEUROSCIENCE
   Salingaros N., 2003, COMMUN COGNITION, V36, P331
   Salingaros N., 2013, UNIFIED ARCHITECTURA
   Salingaros N.A., 2012, J BIOURBANISM, V2, P11
   Salingaros N.A., 2016, OFF COMMON BOOKS OFF COMMON BOOKS
   Salingaros N.A., 2020, J BIOURBANISM, V8, P13
   Salingaros NA, 1997, PHYS ESSAYS, V10, P165, DOI 10.4006/1.3028694
   Salingaros NA, 2020, SHE JI, V6, P455, DOI 10.1016/j.sheji.2020.08.005
   Salingaros NA, 2020, URBAN SCI, V4, DOI 10.3390/urbansci4020026
   Sussman A, 2015, COGNITIVE ARCHITECTURE: DESIGNING FOR HOW WE RESPOND TO THE BUILT ENVIRONMENT, P1
   Sussman A., 2021, COMMON EDGE
   Sussman A., 2017, COMMON EDGE
   Sussman A., 2016, PLANNING SUBCONSCIOU
   Sussman A., 2019, NEW DESIGN IDEAS NEW DES IDEAS, V3, P53
   Taylor RP, 2006, LEONARDO, V39, P245, DOI 10.1162/leon.2006.39.3.245
   Taylor RP, 2021, SUSTAINABILITY-BASEL, V13, DOI 10.3390/su13020823
   Tollner T, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0016276
   Valtchanov D, 2010, CYBERPSYCH BEH SOC N, V13, P503, DOI 10.1089/cyber.2009.0308
   Zou Z., 2019, ADV INFORMATICS COMP, P439
   Zou Z., 2019, P ENV DES RES ASS ED P ENV DES RES ASS ED
NR 44
TC 0
Z9 0
U1 6
U2 6
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2076-3417
J9 APPL SCI-BASEL
JI Appl. Sci.-Basel
PD JUL
PY 2021
VL 11
IS 13
AR 6197
DI 10.3390/app11136197
PG 29
WC Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Materials Science; Physics
GA TH7ZE
UT WOS:000672302600001
OA gold
DA 2022-02-03
ER

PT C
AU Simhadri, S
   Steel, J
   Fuller, B
AF Simhadri, Sailesh
   Steel, James
   Fuller, Benjamin
BE Lin, Z
   Papamanthou, C
   Polychronakis, M
TI Cryptographic Authentication from the Iris
SO INFORMATION SECURITY, ISC 2019
SE Lecture Notes in Computer Science
LA English
DT Proceedings Paper
CT 22nd International Conference on Information Security (ISC)
CY SEP 16-18, 2019
CL New York, NY
ID ROBUST FUZZY EXTRACTORS; GENERATE STRONG KEYS; BIOMETRICS; AGREEMENT
AB Biometrics exhibit noise between repeated readings. Due to the noise, devices store a plaintext template of the biometric. This stored template is an appetizing target for an attacker.
   Fuzzy extractors derive a stable cryptographic key from biometrics (Dodis et al., Eurocrypt 2004). Despite many attempts, there are no iris key derivation systems that prove lower bounds on key strength.
   Our starting point is a fuzzy extractor due to Canetti et al. (Eurocrypt 2016). We modify and couple the image processing and cryptographic algorithms. We then present a sufficient condition on the iris distribution for security, and analysis this condition using the ND0405 Iris dataset.
   We build an iris key derivation system with 32 bits of security even when multiple keys are derived from the same iris. We acknowledge 32 bits of security is insufficient for a secure system. Multifactor systems hold the most promise for cryptographic authentication. Our scheme is suited for incorporation of additional noiseless factors such as a password.
   Our scheme is implemented in C and Python and is open-sourced.
C1 [Simhadri, Sailesh] Google Inc, Cambridge, MA USA.
   [Steel, James; Fuller, Benjamin] Univ Connecticut, Storrs, CT 06269 USA.
C3 Google Incorporated; University of Connecticut
RP Fuller, B (corresponding author), Univ Connecticut, Storrs, CT 06269 USA.
EM saileshsimhadri@gmail.com; james.steel@uconn.edu;
   benjamin.fuller@uconn.edu
OI Fuller, Benjamin/0000-0001-6450-0088
FU Comcast Inc.
FX We thank the anonymous reviews for their helpful suggestions and
   comments. Mariem Ouni and Tyler Cromwell contributed to software
   described in this work. We thank Leonid Reyzin and Alexander Russell for
   helpful discussions and insights. This work was supported in part
   through a grant with Comcast Inc. Work of S. Simhadri was done while at
   University of Connecticut.
CR Alamelou Q., 2018, ASIACCS
   Apon D, 2017, LECT NOTES COMPUT SC, V10332, P1, DOI 10.1007/978-3-319-60080-2_1
   Bellare M., 1993, P ACM CCS, P62, DOI DOI 10.1145/168588.168596
   Bernstein Daniel J., 2012, Progress in Cryptology - LATINCRYPT 2012. Proceedings of the 2nd International Conference on Cryptology and Information Security in Latin America, P159, DOI 10.1007/978-3-642-33481-8_9
   Bitansky N, 2010, LECT NOTES COMPUT SC, V6223, P520, DOI 10.1007/978-3-642-14623-7_28
   Blanton M., 2012, IACR CRYPTOLOGY EPRI, V2012, P608
   Blanton M, 2013, IEEE T INF FOREN SEC, V8, P1433, DOI 10.1109/TIFS.2013.2272786
   Blanton M, 2011, LECT NOTES COMPUT SC, V6879, P190, DOI 10.1007/978-3-642-23822-2_11
   Blundo C., 2013, LNCS, P89, DOI DOI 10.1007/978-3-642-35890-6
   Bonneau J, 2012, P IEEE S SECUR PRIV, P553, DOI 10.1109/SP.2012.44
   Bonneau J, 2012, P IEEE S SECUR PRIV, P538, DOI 10.1109/SP.2012.49
   Bowyer K.W., 2016, HDB IRIS RECOGNITION, DOI [10.1007/978-1-4471-4402-1_2, 10.1007/978-1-4471-4402-1]
   Bowyer KW, 2008, COMPUT VIS IMAGE UND, V110, P281, DOI 10.1016/j.cviu.2007.08.005
   Bowyer Kevin W., 2016, ARXIV160604853
   Boyen X, 2005, LECT NOTES COMPUT SC, V3494, P147
   Boyen X., 2004, ACM C COMP COMM SEC, P82, DOI DOI 10.1145/1030083.1030096
   Bringer J, 2007, 2007 FIRST IEEE INTERNATIONAL CONFERENCE ON BIOMETRICS: THEORY, APPLICATIONS AND SYSTEMS, P1, DOI 10.1109/BTAS.2007.4401904
   Bringer J, 2013, LECT NOTES COMPUT SC, V7862, P164, DOI 10.1007/978-3-642-41320-9_11
   Canetti R, 2008, LECT NOTES COMPUT SC, V4965, P489
   Canetti R, 2016, LECT NOTES COMPUT SC, V9665, P117, DOI 10.1007/978-3-662-49890-3_5
   Carter F., 2008, EBF BIOM ENCR SEM JU
   Cheon JH, 2018, LECT NOTES COMPUT SC, V10946, P28, DOI 10.1007/978-3-319-93638-3_3
   Dakdouk R.R., 2009, THESIS
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   Delvaux J, 2016, LECT NOTES COMPUT SC, V9813, P412, DOI 10.1007/978-3-662-53140-2_20
   Deshmukh S, 2016, IEEE CONF COMM NETW, P480, DOI 10.1109/CNS.2016.7860539
   Dodis Y, 2004, LECT NOTES COMPUT SC, V3027, P523
   Dodis Y, 2008, SIAM J COMPUT, V38, P97, DOI 10.1137/060651380
   Dodis Y, 2006, LECT NOTES COMPUT SC, V4117, P232
   Dodis Y, 2012, IEEE T INFORM THEORY, V58, P6207, DOI 10.1109/TIT.2012.2200290
   Dupont PA, 2018, LECT NOTES COMPUT SC, V10822, P393, DOI 10.1007/978-3-319-78372-7_13
   Evans D., 2011, P 17 C NETW DISTR SY
   Fuller B., 2017, 20171177 CRYPT EPRIN
   Fuller B., 2018, COMPUTATIONAL FUZZY
   Fuller B, 2016, LECT NOTES COMPUT SC, V10031, P277, DOI 10.1007/978-3-662-53887-6_10
   Fuller B, 2013, LECT NOTES COMPUT SC, V8269, P174, DOI 10.1007/978-3-642-42033-7_10
   Goldreich O., 2011, LNCS, V6650, P302
   GROSSMANN A, 1984, SIAM J MATH ANAL, V15, P723, DOI 10.1137/0515056
   Guo ZM, 2016, IEEE INT SYMP CIRC S, P1318, DOI 10.1109/ISCAS.2016.7527491
   Hao F, 2006, IEEE T COMPUT, V55, P1081, DOI 10.1109/TC.2006.138
   Holenstein T, 2005, LECT NOTES COMPUT SC, V3621, P478
   Hollingsworth KP, 2009, IEEE T PATTERN ANAL, V31, P964, DOI 10.1109/TPAMI.2008.185
   Itkis G, 2015, IEEE SIGNAL PROC MAG, V32, P42, DOI 10.1109/MSP.2015.2439717
   Josefsson S., 2015, MEMORY
   Juels A, 1999, 6TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P28
   Kanade S, 2008, 2008 BIOMETRICS SYMPOSIUM (BSYM), P59, DOI 10.1109/BSYM.2008.4655523
   Kelkboom EJC, 2011, IEEE T INF FOREN SEC, V6, P107, DOI 10.1109/TIFS.2010.2091637
   Komanduri S, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2595
   Krichen E., 2017, OSIRIS OPEN SOURCE I
   Lynn B, 2004, LECT NOTES COMPUT SC, V3027, P20
   Nisan N, 1996, J COMPUT SYST SCI, V52, P43, DOI 10.1006/jcss.1996.0004
   Pass R., 2013, 2013781 CRYPT EPRINT
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P54, DOI 10.1109/MSP.2015.2434151
   Percival C, 2016, TECHNICAL REPORT
   Phillips JL, 2008, IEEE INT C BIO BIO W, P17, DOI 10.1109/BIBMW.2008.4686204
   Phillips P.J., 2006, IEEE T PATTERN ANAL
   Prabhakar S., 2003, IEEE Security & Privacy, V1, P33, DOI 10.1109/MSECP.2003.1193209
   Simoens K, 2009, P IEEE S SECUR PRIV, P188, DOI 10.1109/SP.2009.24
   Valiant, 2010, EL C COMP COML ECCC, V17, P9
   Valiant G, 2011, ACM S THEORY COMPUT, P685
   Wang D, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1242, DOI 10.1145/2976749.2978339
   Wen YH, 2018, DESIGN CODE CRYPTOGR, V86, P2495, DOI 10.1007/s10623-018-0459-4
   Woodage J, 2017, LECT NOTES COMPUT SC, V10403, P682, DOI 10.1007/978-3-319-63697-9_23
   Yunhua Wen, 2018, Advances in Cryptology - ASIACRYPT 2018. 24th International Conference on the Theory and Application of Cryptology and Information Security. Proceedings: Lecture Notes in Computer Science (LNCS 11274), P459, DOI 10.1007/978-3-030-03332-3_17
NR 64
TC 6
Z9 6
U1 0
U2 0
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-030-30215-3; 978-3-030-30214-6
J9 LECT NOTES COMPUT SC
PY 2019
VL 11723
BP 465
EP 485
DI 10.1007/978-3-030-30215-3_23
PG 21
WC Computer Science, Information Systems; Computer Science, Theory &
   Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BP8GD
UT WOS:000565241600023
DA 2022-02-03
ER

PT J
AU Jan, F
   Min-Allah, N
AF Jan, Farmanullah
   Min-Allah, Nasro
TI An effective iris segmentation scheme for noisy images
SO BIOCYBERNETICS AND BIOMEDICAL ENGINEERING
LA English
DT Article
DE Iris biometrics; Pupil localization; Iris localization; Iris
   segmentation
ID LOCALIZATION; RECOGNITION; BIOMETRICS
AB Iris segmentation plays a critical role in the iris biometric systems. It has two modules: iris localization and noise detection. The first module demarcates the actual iris' inner and outer boundaries in input eyeimages. The second module detects and removes noise in the valid iris part. Researchers devised numerous iris segmentation and/or localization schemes, which are based on the histogram and thresholding, circular Hough transform (CHT), Integro- differential operator (IDO), active contour models, graph-cuts, or deep learning. It is observed that most contemporary schemes perform poorly when confronted with images containing noisy factors such as the eyebrows, eyelashes, contact lenses, non-uniform illumination, light reflections, defocus and/or eyeglasses. The performance of CHT and IDO against noise is found robust, but these operators are computationally expensive. On the other hand, the histogram and thresholding-based schemes are considered fast, but these are less robust against noise. Besides, most contemporary schemes mark iris contours with a circle approximation and offer no noise removal strategy. To address these issues, this study offers an effective iris segmentation algorithm. First, it applies an optimized coarse-to-fine scheme based on an adaptive threshold to mark iris inner boundary. Next, it detects and marks eyelashes adaptively. After that, it marks iris outer boundary via an optimized coarseto-fine scheme. Then, it regularizes the non-circular iris' contours using the Fourier series. Finally, eyelids and reflections are marked in the iris polar form. The proposed scheme shows better results on the CASIA-Iris-Interval V3.0, IITD V1.0, and MMU V1.0 iris databases. (C) 2020 Nalecz Institute of Biocybernetics and Biomedical Engineering of the Polish Academy of Sciences. Published by Elsevier B.V. All rights reserved.
C1 [Jan, Farmanullah; Min-Allah, Nasro] Imam Abdulrahman Bin Faisal Univ, Coll Comp Sci & Informat Technol, Dept Comp Sci, POB 1982, Dammam, Saudi Arabia.
C3 Imam Abdulrahman Bin Faisal University
RP Jan, F (corresponding author), Imam Abdulrahman Bin Faisal Univ, Coll Comp Sci & Informat Technol, Dept Comp Sci, POB 1982, Dammam, Saudi Arabia.
EM fzmjan@iau.edu.sa
RI Min-Allah, Nasro/O-3147-2019
OI Jan, Farmanullah/0000-0002-9118-3652
FU Malaysia Multimedia University (MMU), Department of Computer Science
   SOCIA Lab. Malaysia; Biometrics Research Laboratory, Indian Institute of
   Technology Delhi (IITD), New Delhi, India; Chinese Academy of Sciences'
   Institute of Automation (CASIA)Chinese Academy of Sciences
FX Authors are thankful to the Malaysia Multimedia University (MMU),
   Department of Computer Science SOCIA Lab. Malaysia; the Biometrics
   Research Laboratory, Indian Institute of Technology Delhi (IITD), New
   Delhi, India; and the Chinese Academy of Sciences' Institute of
   Automation (CASIA) for granting a free access to their relevant iris
   databases.
CR Abdullah MAM, 2017, IEEE T SYST MAN CY-S, V47, P3128, DOI 10.1109/TSMC.2016.2562500
   Ahad Md., 2018, APPL SOFT COMPUT
   Basit A, 2007, MACH VIS 2007 INT C
   Boonchuan T., 2018, ELECT LETT COMPUTER, V17, P16, DOI [10.5565/rev/elcvia.1044, DOI 10.5565/REV/ELCVIA.1044]
   Bowyer KW, 2008, COMPUT VIS IMAGE UND, V110, P281, DOI 10.1016/j.cviu.2007.08.005
   DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676
   Donida Labati R, COMPUT VIS IMAGE UND
   Gawande U, 2011, BIOM TECHNOL TODAY J, V2011, P8
   Gonzalez RC, 1992, PRENTICE HALL PROFES
   Ibrahim MT, 2012, OPT LASER ENG, V50, P645, DOI 10.1016/j.optlaseng.2011.11.008
   Jan F., 2014, THESIS
   Jan F, 2017, COMPUT ELECTR ENG, V62, P166, DOI 10.1016/j.compeleceng.2016.11.031
   Jan F, 2017, SIGNAL PROCESS, V133, P192, DOI 10.1016/j.sigpro.2016.11.007
   Jan F, 2014, OPTIK, V125, P4274, DOI 10.1016/j.ijleo.2014.04.009
   Jan F, 2012, DIGIT SIGNAL PROCESS, V22, P971, DOI 10.1016/j.dsp.2012.06.001
   Kadi KL, 2016, BIOCYBERN BIOMED ENG, V36, P233, DOI 10.1016/j.bbe.2015.11.004
   Khakzar M, 2017, BIOCYBERN BIOMED ENG, V37, P742, DOI 10.1016/j.bbe.2017.09.001
   Khan TM, 2011, OPT LASER ENG, V49, P177, DOI 10.1016/j.optlaseng.2010.08.020
   Li YH, 2019, MOB INF SYST, V2019, DOI 10.1155/2019/6142839
   Ma L, 2020, DIGIT SIGNAL PROCESS, V100, DOI 10.1016/j.dsp.2020.102682
   MASEK L, RECOGNITION HUMAN IR
   Masek L, 2003, MATLAB SOURCE CODE B
   Mehrotra H, 2013, MATH COMPUT MODEL, V58, P132, DOI 10.1016/j.mcm.2012.06.034
   Min TH, 2009, PATTERN RECOGN LETT, V30, P1138, DOI 10.1016/j.patrec.2009.03.017
   Mire A, 2010, INT J COMPUT APPL, V7
   Nguyen K, 2017, PATTERN RECOGN, V72, P123, DOI 10.1016/j.patcog.2017.05.021
   Ross A, 2006, 2006 BIOM S SPEC SES
   Sardar M, 2018, APPL SOFT COMPUT, V67, P61, DOI 10.1016/j.asoc.2018.02.047
   Soliman NF, 2017, OPTIK, V140, P469, DOI 10.1016/j.ijleo.2016.11.150
   Vyas R, 2019, INT J BIOMETRICS, V11, P274, DOI 10.1504/IJBM.2019.100842
   Wan HL, 2013, IET IMAGE PROCESS, V7, P111, DOI 10.1049/iet-ipr.2012.0084
   Wang CY, 2020, IEEE T INF FOREN SEC, V15, P2944, DOI 10.1109/TIFS.2020.2980791
   Wildes RP, 1997, P IEEE, V85, P1348, DOI 10.1109/5.628669
NR 33
TC 1
Z9 1
U1 4
U2 7
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0208-5216
J9 BIOCYBERN BIOMED ENG
JI Biocybern. Biomed. Eng.
PD JUL-SEP
PY 2020
VL 40
IS 3
BP 1064
EP 1080
DI 10.1016/j.bbe.2020.06.002
PG 17
WC Engineering, Biomedical
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering
GA OE6SF
UT WOS:000580657600015
DA 2022-02-03
ER

PT J
AU Luo, Y
   Cheung, SCS
   Lazzeretti, R
   Pignata, T
   Barni, M
AF Luo, Ying
   Cheung, Sen-ching S.
   Lazzeretti, Riccardo
   Pignata, Tommaso
   Barni, Mauro
TI Anonymous subject identification and privacy information management in
   video surveillance
SO INTERNATIONAL JOURNAL OF INFORMATION SECURITY
LA English
DT Article
DE Anonymous subject identification; Privacy information management;
   Privacy protection; Video surveillance; Garbled circuit
ID IMPROVED GARBLED CIRCUIT; FACE
AB The widespread deployment of surveillance cameras has raised serious privacy concerns, and many privacy-enhancing schemes have been recently proposed to automatically redact images of selected individuals in the surveillance video for protection. Of equal importance are the privacy and efficiency of techniques to first, identify those individuals for privacy protection and second, provide access to original surveillance video contents for security analysis. In this paper, we propose an anonymous subject identification and privacy data management system to be used in privacy-aware video surveillance. The anonymous subject identification system uses iris patterns to identify individuals for privacy protection. Anonymity of the iris-matching process is guaranteed through the use of a garbled-circuit (GC)-based iris matching protocol. A novel GC complexity reduction scheme is proposed by simplifying the iris masking process in the protocol. A user-centric privacy information management system is also proposed that allows subjects to anonymously access their privacy information via their iris patterns. The system is composed of two encrypted-domain protocols: The privacy information encryption protocol encrypts the original video records using the iris pattern acquired during the subject identification phase; the privacy information retrieval protocol allows the video records to be anonymously retrieved through a GC-based iris pattern matching process. Experimental results on a public iris biometric database demonstrate the validity of our framework.
C1 [Luo, Ying] Purdue Univ Northwest, Dept Comp Informat Technol & Graph, Hammond, IN 46323 USA.
   [Cheung, Sen-ching S.] Univ Kentucky, Dept Elect & Comp Engn, Lexington, KY 40506 USA.
   [Lazzeretti, Riccardo] Univ Padua, Dept Math, Padua, Italy.
   [Pignata, Tommaso; Barni, Mauro] Univ Siena, Dept Informat Engn, Siena, Italy.
C3 University of Kentucky; University of Padua; University of Siena
RP Cheung, SCS (corresponding author), Univ Kentucky, Dept Elect & Comp Engn, Lexington, KY 40506 USA.
EM ying.luo@pnw.edu; sccheung@ieee.org; riccardo.lazzeretti@math.unipd.com;
   pignata.tommaso@gmail.com; barni@dii.unisi.it
RI Lazzeretti, Riccardo/U-2247-2019
OI Cheung, Sen-ching/0000-0002-9207-5514
FU National Science FoundationNational Science Foundation (NSF) [1018241]
FX The first two authors acknowledge the support by the National Science
   Foundation under Grant No. 1018241. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the author(s) and do not necessarily reflect the views of the National
   Science Foundation.
CR Avidan S, 2006, LECT NOTES COMPUT SC, V3953, P1
   Barker E., 2012, NIST SPECIAL PUBLICA, V800, P1, DOI DOI 10.6028/NIST.SP.800-57PT1R4
   Barnett M., 2010, BIOM THEOR APPL SYST, P1, DOI [DOI 10.1109/WIFS.2010.5711460, DOI 10.1109/BTAS.2010.5634527]
   Barni M, 2015, IEEE SIGNAL PROC MAG, V32, P66, DOI 10.1109/MSP.2015.2438131
   Beaver D., 1995, LECT NOTES COMPUTER, V963
   Blanton M., 2010, TECHNICAL REPORT
   Cavoukian Ann, 2007, BIOMETRIC ENCRYPTION
   Cheung S. C., 2009, PROTECTING PRIVACY V
   Chinomi K, 2008, LECT NOTES COMPUT SC, V4903, P144
   Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
   De Marsico M, 2015, PATTERN RECOGN LETT, V57, P17, DOI 10.1016/j.patrec.2015.02.009
   Devore J., 1991, PROBABILITY STAT ENG, V704
   Dufaux F., 2006, 2006 C COMP VIS PATT, P160, DOI DOI 10.1109/CVPRW.2006.184
   Elezovikj S., 2013, 2013 IEEE INT C MULT, P1
   Erkin Z, 2009, LECT NOTES COMPUT SC, V5672, P235, DOI 10.1007/978-3-642-03168-7_14
   Fidaleo Douglas A., 2004, VSSN 04, P46, DOI DOI 10.1145/1026799.1026809
   Fontaine C, 2007, EURASIP J INF SECUR, DOI 10.1155/2007/13801
   Gentry C., 2013, IACR CRYPTOLOGY EPRI, V2013, P687
   Goldwasser S, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P555
   Grother P, 2014, NIST INTERAGENCY REP, V8009, P2
   Grother P., 2012, 7836 INT, V7836
   Hao F, 2006, IEEE T COMPUT, V55, P1081, DOI 10.1109/TC.2006.138
   Hazay C, 2010, INFORM SEC CRYPT TEX, P3, DOI 10.1007/978-3-642-14303-8
   Kolesnikov V., 2009, SIGN PROC ENCR DOM 1
   Kolesnikov V, 2008, LECT NOTES COMPUT SC, V5126, P486, DOI 10.1007/978-3-540-70583-3_40
   Kolesnikov V, 2009, LECT NOTES COMPUT SC, V5888, P1, DOI 10.1007/978-3-642-10433-6_1
   Lazzeretti R, 2011, 2011 IEEE INT WORKSH
   Lazzeretti R, 2012, THESIS
   Lee Y, 2013, J RES NATL INST STAN, V118, P218, DOI 10.6028/jres.118.011
   Lewis SM, 2002, MICH LAW REV, V101, P273, DOI 10.2307/1290421
   Li Y., 2008, 2008 37 IEEE APPL IM
   Linnartz J., 2003, LECT NOTES COMPUTER, P1059
   Lioudakis GV, 2007, COMPUT NETW, V51, P4679, DOI 10.1016/j.comnet.2007.06.010
   Liu C, 2015, P IEEE S SECUR PRIV, P359, DOI 10.1109/SP.2015.29
   Luo Y., 2012, IEEE INT C IM PROC I
   Luo Y, 2013, PROC SPIE, V8712, DOI 10.1117/12.2015999
   Luo Y, 2009, IEEE INT CON MULTI, P1046, DOI 10.1109/ICME.2009.5202677
   MALKHI D, 2004, USENIX
   Martin K, 2008, IEEE T CIRC SYST VID, V18, P1152, DOI 10.1109/TCSVT.2008.927110
   Masek L., 2003, TECHNICAL REPORT
   Nakashima Y., 2010, P ACM INT C MULT, P1135, DOI DOI 10.1145/1873951.1874169
   Nakashima Yuta, 2011, P IEEE GLOB TEL C GL, P1, DOI [10.1109/ICME.2011.6011955, DOI 10.1109/GLOCOM.2011.6133675]
   Newton EM, 2005, IEEE T KNOWL DATA EN, V17, P232, DOI 10.1109/TKDE.2005.32
   Organisations C. C. P. S., 2012, CCIMB201209002 ORG C
   Paruchuri JK, 2009, EURASIP J INF SECUR, DOI 10.1155/2009/236139
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Pinkas B, 2009, LECT NOTES COMPUT SC, V5912, P250, DOI 10.1007/978-3-642-10366-7_15
   Qureshi FZ, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P442, DOI 10.1109/AVSS.2009.97
   Padilla-Lopez JR, 2015, SENSORS-BASEL, V15, P12959, DOI 10.3390/s150612959
   Rashwan HA, 2016, INT J INF SECUR, V15, P225, DOI 10.1007/s10207-015-0286-9
   Sadeghi AR, 2010, LECT NOTES COMPUT SC, V5984, P229
   Saini M, 2012, ADV MULTIMED, V2012, DOI 10.1155/2012/639649
   Sajid H, 2015, IEEE IMAGE PROC, P4530, DOI 10.1109/ICIP.2015.7351664
   Schiff Jeremy, 2007, 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, P971, DOI 10.1109/IROS.2007.4399122
   Senior A, 2005, IEEE SECUR PRIV, V3, P50, DOI 10.1109/MSP.2005.65
   Songhori EM, 2015, P IEEE S SECUR PRIV, P411, DOI 10.1109/SP.2015.32
   SUNDSTROM E, 1980, ACAD MANAGE J, V23, P101, DOI 10.2307/255498
   Tan T., 2005, TECHNICAL REPORT
   Uhl A., 2004, IMAGE VIDEO ENCRYPTI, V15
   Uludag U, 2004, P IEEE, V92, P948, DOI 10.1109/JPROC.2004.827372
   Uludag U., 2006, P CVPR WORKSH PRIV R
   US Department of Health and Human Services, 2003, SUMM HIPAA PRIV RUL
   Venkatesh MV, 2009, PATTERN RECOGN LETT, V30, P168, DOI 10.1016/j.patrec.2008.03.011
   Wactlar H., ENABLING PERSONAL PR
   Wada J., 2001, MONITOR CAMERA SYSTE
   Wasson, 2003, N C L REV, V81, P1348
   Wickramasuriya J., 2004, ACM INT C MULT
   William S., 2015, COMPUTER SECURITY PR
   YAO AC, 1982, P 23 ANN IEEE S FDN
   Ye S., 2009, EURASIP J INF SECUR, V2009, P17
   Yu XY, 2007, LECT NOTES COMPUT SC, V4844, P651
   Zhao J., 2007, P ACM IEEE INT C DIS
   Zhao J, 2008, IEEE J-STSP, V2, P464, DOI 10.1109/JSTSP.2008.2001430
NR 73
TC 5
Z9 5
U1 0
U2 17
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 1615-5262
EI 1615-5270
J9 INT J INF SECUR
JI Int. J. Inf. Secur.
PD JUN
PY 2018
VL 17
IS 3
BP 261
EP 278
DI 10.1007/s10207-017-0380-2
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GC3CT
UT WOS:000429662500002
OA Green Submitted
DA 2022-02-03
ER

PT J
AU Malian, A
   Azizi, A
AF Malian, A
   Azizi, A
TI Development of a robust photogrammetric metrology system for monitoring
   the healing of bedsores
SO PHOTOGRAMMETRIC RECORD
LA English
DT Article
DE camera calibration; close range photogrammetry; geometric constraints;
   image processing; medical photogrammetry; three-dimensional surface
   reconstruction; wound measurement
ID VOLUME
AB A three-camera close range photogrammetric system for robust and precise measurement of bedsores has been designed and constructed. MEDPHOS (MEDical PHOtogrammetric System) consists of three synchronised cameras with convergent optical axes. A light projector is fixed in the centre of the rig that holds the cameras. A special dot pattern is projected onto the surface to be measured, to compensate for the lack of natural texture on the wound surface. The proposed algorithm consists of the following steps: the cameras and projector are calibrated so that all interior and exterior parameters are known; tailored image segmentation procedures are developed and applied for the detection of the projected pattern dots from the uneven background of the images using morphologic operators; and watershed transformation is used to tackle the problem of overlapping pattern dots. To reduce the effects of non-uniform illumination and specular reflection of light due to humidity (often the case with wounds), a homomorphic transformation is developed and applied to the images. After segmentation of the images, a connected-component labelling procedure is used to establish the points for matching. The centroids of these components were precisely calculated. Intensity-based image matching has been tested without yielding satisfactory results due to the significant deviation from the Lambertian reflection assumption used for solving the correspondence problem. This problem is reliably solved by developing a new algorithm based on geometric constraints that allow feature-based matching and do not need approximate values of the location of the targets in the images. This robust three-focal constraint is found to be very effective for matching provided the necessary conditions for the system configuration are met. Auxiliary photometric constraints together with the calibrated projector (which is treated like an active camera) also serve as additional sources of information for reducing the number of remaining ambiguities and checking the consistency of the results. Almost all of the required biometric information can be obtained rapidly, robustly and easily using MALIAN et al. Robust photogrammetric metrology system for monitoring the healing of bedsores MEDPHOS. Experimental results showed the effectiveness of the proposed technique.
C1 Univ Tehran, Tehran 14174, Iran.
   Delft Univ Technol, NL-2600 AA Delft, Netherlands.
   Amirkabir Univ Technol, Tehran, Iran.
C3 University of Tehran; Delft University of Technology; Amirkabir
   University of Technology
RP Malian, A (corresponding author), Univ Tehran, Tehran 14174, Iran.
EM mahan@ut.ac.ir; aazizi@ut.ac.ir
OI Malian, Abbass/0000-0002-4793-0657
CR Alvarez L, 2002, J VIS COMMUN IMAGE R, V13, P3, DOI 10.1006/jvci.2001.0482
   ARIYAWANSA DD, 1999, SPIE, V3641, P92
   Boersma SM, 2000, INT ARCH PHOTOGRAMME, V33, P84
   Faugeras O, 2001, GEOMETRY MULTIPLE IM
   Fraser C.S., 2001, CLOSE RANGE PHOTOGRA, P256
   Gonzalez RC., 2003, DIGITAL IMAGE PROCES
   Gooch MJ, 2001, COMPUT GEOSCI-UK, V27, P913, DOI 10.1016/S0098-3004(00)00129-1
   Hartley Richard, 2004, MULTIPLE VIEW GEOMET
   JONES TD, 1999, THESIS U GLAMORGAN
   Kasser M, 2002, DIGITAL PHOTOGRAMMETRY, P1
   Kraus K., 1997, PHOTOGRAMMETRY, V2
   KRUCK E, 1998, USERS MANUAL VERSION
   MAAS HG, 1997, SCHRIFTENREIHE I GEO
   Malian A., 2004, INT ARCH PHOTOGRAMME, V35, P311
   Malian A., 2002, INT ARCH PHOTOGRAMME, V34, P264
   Marjanovic D, 1998, PHYSIOL MEAS, V19, P535, DOI 10.1088/0967-3334/19/4/008
   Maver R W, 1991, J Insur Med, V23, P120
   Mitchell HL, 2002, ISPRS J PHOTOGRAMM, V56, P286, DOI 10.1016/S0924-2716(02)00065-5
   Patias P, 2002, ISPRS J PHOTOGRAMM, V56, P295, DOI 10.1016/S0924-2716(02)00066-7
   PLASSMANN P, 1995, PHOTOGRAMM REC, V15, P197, DOI 10.1111/0031-868X.00025
   Plassmann P, 1998, MED ENG PHYS, V20, P332, DOI 10.1016/S1350-4533(98)00034-4
   Santamaria N, 2000, Collegian, V7, P14, DOI 10.1016/S1322-7696(08)60385-6
   Shao J, 1999, IMAGE VISION COMPUT, V17, P1021, DOI 10.1016/S0262-8856(99)00004-9
   TAO CV, 1999, SURVEYING LAND INFOR, V59, P187
   TRINDER JC, 1995, ISPRS J PHOTOGRAMM, V50, P12, DOI 10.1016/0924-2716(95)98211-H
   TUYTELAARS T, 1999, P 3 INT C VIS INF IN, V1614, P493
   van den Heuvel FA, 2002, ISPRS J PHOTOGRAMM, V56, P283, DOI 10.1016/S0924-2716(02)00063-1
   ZHANG ZY, 1995, ARTIF INTELL, V78, P87, DOI 10.1016/0004-3702(95)00022-4
   ZOLFAGHARI M, 2000, INT ARCH PHOTOGRAMME, V33, P501
NR 29
TC 18
Z9 18
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0031-868X
EI 1477-9730
J9 PHOTOGRAMM REC
JI Photogramm. Rec.
PD SEP
PY 2005
VL 20
IS 111
BP 241
EP 273
DI 10.1111/j.1477-9730.2005.00319.x
PG 33
WC Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Physical Geography; Geology; Remote Sensing; Imaging Science &
   Photographic Technology
GA 963LZ
UT WOS:000231807100004
DA 2022-02-03
ER

PT J
AU Sam, BB
   Saravanan, M
AF Sam, B. Baron
   Saravanan, M.
TI A combined score level fusion approach for multi model biometric system
   using left and right palm print
SO INTERNATIONAL JOURNAL OF ADVANCED AND APPLIED SCIENCES
LA English
DT Article
DE Biometrics; Palm print; Matching score
ID PALMPRINT VERIFICATION; IDENTIFICATION
AB Biometrics is the art of setting up the character of an individual in view of the physical, concoction or behavioral properties of the individual. Palm Print recognizable proof is an imperative individual ID innovation and it has pulled in much consideration. The palm print contains standard bends and wrinkles as well as rich surface and miniscule focuses, so the palm print ID can accomplish a high exactness. We propose a novel structure of joining the left with right palm print at the coordinating (matching) score level. In the system, three sorts of coordinating (matching) scores, which are separately acquired by the left palm print coordinating, right palm print coordinating and crossing coordinating between the left query and right training palm print, are fused to make the final decision. The structure not only combines the left and right palm print images for identification, additionally appropriately abuses the comparability between the left and right palm print of a similar subject. The proposed strategy accurately takes the method for the left and right palm print pictures into record, and plans estimation to survey the relation between them. In additament, by using this nearness, the proposed weighted cumulation scheme uses a system to facilitate the three sorts of scores incited from the left and right palm print pictures. (c) 2017 The Authors. Published by IASE. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
C1 [Sam, B. Baron; Saravanan, M.] Sathyabama Univ, Sch Comp, Chennai, Tamil Nadu, India.
C3 Sathyabama Institute of Science & Technology
RP Sam, BB (corresponding author), Sathyabama Univ, Sch Comp, Chennai, Tamil Nadu, India.
EM baronsam1988@gmail.com
RI B, SAM/X-9226-2018
OI B, SAM/0000-0002-3571-4535
CR Ananth Christo, 2014, INT J INNOVATIVE RES, V2, P3716
   Ananth Christo, 2014, AM J SUSTAINABLE CIT, V1, P274
   Dai JF, 2011, IEEE T PATTERN ANAL, V33, P945, DOI 10.1109/TPAMI.2010.164
   Du F, 2011, COMM COM INF SC, V159, P230
   Huang DS, 2008, PATTERN RECOGN, V41, P1316, DOI 10.1016/j.patcog.2007.08.016
   Jain A. K., 2007, HDB BIOMETRICS
   Jain AK, 2004, COMMUN ACM, V47, P34, DOI 10.1145/962081.962102
   Kong AWK, 2004, INT C PATT RECOG, P520, DOI 10.1109/ICPR.2004.1334184
   Palanikumar P, 2015, INT J APPL ENG RES, V10, P42472
   Ribaric S, 2005, IEEE T PATTERN ANAL, V27, P1698, DOI 10.1109/TPAMI.2005.209
   Wu XQ, 2014, PATTERN RECOGN, V47, P3314, DOI 10.1016/j.patcog.2014.04.008
   Xu Y., 2015, P IEEE T IMAGE PROCE, V24, P549, DOI DOI 10.1109/TIP.2014.2380171
   Zhang D, 2003, IEEE T PATTERN ANAL, V25, P1041, DOI 10.1109/TPAMI.2003.1227981
NR 13
TC 0
Z9 0
U1 0
U2 1
PU INST ADVANCED SCIENCE EXTENSION
PI TAIPEI
PA PO BOX 23-31,, TAIPEI, 00000, TAIWAN
SN 2313-626X
EI 2313-3724
J9 INT J ADV APPL SCI
JI Int. J. Adv. Appl. Sci.
PD FEB
PY 2018
VL 5
IS 2
BP 103
EP 107
DI 10.21833/ijaas.2018.02.017
PG 5
WC Multidisciplinary Sciences
WE Emerging Sources Citation Index (ESCI)
SC Science & Technology - Other Topics
GA GA2AR
UT WOS:000428119500017
OA gold
DA 2022-02-03
ER

PT J
AU van Mastrigt, NM
   Celie, K
   Mieremet, AL
   Ruifrok, ACC
   Geradts, Z
AF van Mastrigt, Nina M.
   Celie, Kevin
   Mieremet, Arjan L.
   Ruifrok, Arnout C. C.
   Geradts, Zeno
TI Critical review of the use and scientific basis of forensic gait
   analysis
SO FORENSIC SCIENCES RESEARCH
LA English
DT Review
DE Forensic science; forensic gait analysis; validation; biometric
   characteristics; image analysis; video analysis; survey; gait
   recognition
AB This review summarizes the scientific basis of forensic gait analysis and evaluates its use in the Netherlands, United Kingdom and Denmark, following recent critique on the admission of gait evidence in Canada. A useful forensic feature is (1) measurable, (2) consistent within and (3) different between individuals. Reviewing the academic literature, this article found that (1) forensic gait features can be quantified or observed from surveillance video, but research into accuracy, validity and reliability of these methods is needed; (2) gait is variable within individuals under differing and constant circumstances, with speed having major influence; (3) the discriminative strength of gait features needs more research, although clearly variation exists between individuals. Nevertheless, forensic gait analysis has contributed to several criminal trials in Europe in the past 15 years. The admission of gait evidence differs between courts. The methods are mainly observer-based: multiple gait analysts (independently) assess gait features on video footage of a perpetrator and suspect. Using gait feature databases, likelihood ratios of the hypotheses that the observed individuals have the same or another identity can be calculated. Automated gait recognition algorithms calculate a difference measure between video clips, which is compared with a threshold value derived from a video gait recognition database to indicate likelihood. However, only partly automated algorithms have been used in practice. We argue that the scientific basis of forensic gait analysis is limited. However, gait feature databases enable its use in court for supportive evidence with relatively low evidential value. The recommendations made in this review are (1) to expand knowledge on inter- and intra-subject gait variabilities, discriminative strength and interdependency of gait features, method accuracies, gait feature databases and likelihood ratio estimations; (2) to compare automated and observer-based gait recognition methods; to design (3) an international standard method with known validity, reliability and proficiency tests for analysts; (4) an international standard gait feature data collection method resulting in database(s); (5) (inter)national guidelines for the admission of gait evidence in court; and (6) to decrease the risk for cognitive and contextual bias in forensic gait analysis. This is expected to improve admission of gait evidence in court and judgment of its evidential value. Several ongoing research projects focus on parts of these recommendations.
C1 [van Mastrigt, Nina M.; Celie, Kevin; Mieremet, Arjan L.; Ruifrok, Arnout C. C.; Geradts, Zeno] Netherlands Forens Inst, Dept Digital Technol & Biometry, The Hague, Netherlands.
   [Geradts, Zeno] Univ Amsterdam, Intelligent Informat Syst, Amsterdam, Netherlands.
C3 University of Amsterdam
RP Geradts, Z (corresponding author), Netherlands Forens Inst, Dept Digital Technol & Biometry, The Hague, Netherlands.; Geradts, Z (corresponding author), Univ Amsterdam, Intelligent Informat Syst, Amsterdam, Netherlands.
EM geradts@uva.nl
RI Geradts, Zeno/H-5365-2019
OI Geradts, Zeno/0000-0001-5912-5295
CR Abboud R, 2017, FORENSIC GAIT SNALYS
   [Anonymous], 2014, REEKS WAARSCH HET BA
   Arnhem-Leeuwarden Gerechtshof, 2017, ECLINLGHARL20179969
   BEARDSWORTH T, 1981, B PSYCHONOMIC SOC, V18, P19
   Bejek Z, 2006, KNEE SURG SPORT TR A, V14, P612, DOI 10.1007/s00167-005-0005-6
   Birch I, 2016, SCI JUSTICE, V56, P426, DOI 10.1016/j.scijus.2016.06.009
   Birch I, 2016, SCI JUSTICE, V56, P351, DOI 10.1016/j.scijus.2016.05.006
   Birch I, 2015, SCI JUSTICE, V55, P279, DOI 10.1016/j.scijus.2015.03.002
   Birch I, 2014, SCI JUSTICE, V54, P159, DOI 10.1016/j.scijus.2013.10.002
   Birch I, 2013, J FORENSIC LEG MED, V20, P915, DOI 10.1016/j.jflm.2013.07.005
   Birch I, 2013, SCI JUSTICE, V53, P339, DOI 10.1016/j.scijus.2013.04.005
   Bouchrika I, 2017, INTEL SYST REF LIBR, V115, P307, DOI 10.1007/978-3-319-44270-9_13
   Bouchrika I, 2016, MULTIMED TOOLS APPL, V75, P1201, DOI 10.1007/s11042-014-2364-9
   Bouchrika I, 2011, J FORENSIC SCI, V56, P882, DOI 10.1111/j.1556-4029.2011.01793.x
   Branco Marco, 2014, ScientificWorldJournal, V2014, P527940, DOI 10.1155/2014/527940
   Celie K., 2016, THESIS
   Chien J. H., 2015, J PHYS ACT NUTR REHA, V2015, P1
   Chung MJ, 2010, GAIT POSTURE, V31, P131, DOI 10.1016/j.gaitpost.2009.09.013
   Couper F. J., 2004, DRUGS HUMAN PERFORMA
   Cunliffe E., 2013, CAN BAR REV, V92, P327
   CUTTING JE, 1977, B PSYCHONOMIC SOC, V9, P353, DOI 10.3758/BF03337021
   Doekhie G., 2012, THESIS
   Edmond G, 2016, J CRIM LAW CRIM, V106, P219
   Edmond G, 2014, LAW PROBAB RISK, V13, P1, DOI [10.1093/lpr/mgt011, 10.1093/lpr/mgu018]
   Gafurov D., 2007, ANN NORW COMP SCI C
   Geradts Z, 2002, P SOC PHOTO-OPT INS, V4709, P16, DOI 10.1117/12.474735
   Gerechtshof Amsterdam, 2017, ECLINLGHAMS20175494
   Goffredo M, 2008, BTAS 2008 IEEE 2 INT
   Gorton GE, 2009, GAIT POSTURE, V29, P398, DOI 10.1016/j.gaitpost.2008.10.060
   Growney E, 1997, GAIT POSTURE, V6, P147, DOI 10.1016/S0966-6362(97)01114-4
   Guan Y, 2015, IEEE T PATTERN ANAL, V37, P1521, DOI 10.1109/TPAMI.2014.2366766
   Han J, 2006, IEEE T PATTERN ANAL
   Hayfron-Acquah JB, 2003, PATTERN RECOGN LETT, V24, P2175, DOI 10.1016/S0167-8655(03)00086-2
   Hoogeboom B, 2009, J FORENSIC SCI, V54, P1365, DOI 10.1111/j.1556-4029.2009.01179.x
   Hora M, 2017, PLOS ONE, V12, P1
   Iwama H, 2013, IWRCV
   Jamil N, 2015, PROCEDIA COMPUT SCI, V76, P342, DOI 10.1016/j.procs.2015.12.305
   Jarolav M., 2013, STAT RES LETT, V2, P69
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   KADABA MP, 1989, J ORTHOPAED RES, V7, P849, DOI 10.1002/jor.1100070611
   KIRTLEY C, 1985, J BIOMED ENG, V7, P282, DOI 10.1016/0141-5425(85)90055-X
   Kirtley C., 2006, CLIN GAIT ANAL THEOR
   Koster O, 2012, INT J SPEECH LANG LA, V19, P51, DOI 10.1558/ijsll.v19i1.51
   Kwon JW, 2015, J PHYS THER SCI, V27, P477, DOI 10.1589/jpts.27.477
   Larsen PK, 2008, J FORENSIC SCI, V53, P1149, DOI 10.1111/j.1556-4029.2008.00807.x
   Larsen PK, 2007, SPIE IS T, P6491
   Larsen PK, 2010, PUBL J FORENSIC BIOM, V1
   Li S.Z, 2015, ENCY BIOMETRICS, V2nd
   Loula F, 2005, J EXP PSYCHOL HUMAN, V31, P210, DOI 10.1037/0096-1523.31.1.210
   Lynnerup N, 2014, IET BIOMETRICS, V3, P47, DOI 10.1049/iet-bmt.2013.0090
   Matthis JS, 2015, J VISION, V15, DOI 10.1167/15.3.10
   McGinley JL, 2009, GAIT POSTURE, V29, P360, DOI 10.1016/j.gaitpost.2008.09.003
   Nirenberg M, 2018, SCI JUSTICE, V58, P292, DOI 10.1016/j.scijus.2018.03.002
   Nixon M. S., 2006, HUMAN IDENTIFICATION
   Nixon MS, 2010, EUR SIGNAL PR CONF, P1655
   OBERG T, 1993, J REHABIL RES DEV, V30, P210
   Page M, 2011, FORENSIC SCI INT, V206, P12, DOI 10.1016/j.forsciint.2010.08.004
   Rathinam C, 2014, GAIT POSTURE, V40, P279, DOI 10.1016/j.gaitpost.2014.04.187
   Rechtbank Noord-Nederland, 2015, ECLINLRBNNE20151785
   Samson MM, 2001, AGING CLIN EXP RES, V13, P16, DOI 10.1007/BF03351489
   Sanders Richard D, 2010, Psychiatry (Edgmont), V7, P38
   Sangeux M, 2016, GAIT POSTURE, V46, P194, DOI 10.1016/j.gaitpost.2016.03.015
   Sarkar S, 2005, IEEE T PATTERN ANAL, V27, P162, DOI 10.1109/TPAMI.2005.39
   Thakurta A. G., 2016, ADV APPL PHYSL, V1, P24, DOI [10.11648/j.aap.20160102.12, DOI 10.11648/J.AAP.20160102.12]
   Towler AK, 2016, THESIS
   Venture G, 2014, INT J SOC ROBOT, V6, P621, DOI 10.1007/s12369-014-0243-1
   VERNON DW, 2017, FORENSIC PODIATRY PR, V2nd
   Wilken JM, 2012, GAIT POSTURE, V35, P301, DOI 10.1016/j.gaitpost.2011.09.105
   WINTER DA, 1984, HUM MOVEMENT SCI, V3, P51, DOI 10.1016/0167-9457(84)90005-8
   Yang SXM, 2014, J FORENSIC SCI, V59, P1242, DOI 10.1111/1556-4029.12490
   Yang SXM, 2014, J FORENSIC SCI, V59, P494, DOI 10.1111/1556-4029.12322
   Yeoh TW, 2017, APPL SOFT COMPUT, V61, P42, DOI 10.1016/j.asoc.2017.07.041
NR 72
TC 6
Z9 6
U1 1
U2 2
PU TAYLOR & FRANCIS INC
PI PHILADELPHIA
PA 530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA
SN 2096-1790
EI 2471-1411
J9 FOREN SCI RES
JI Foren. Sci. Res.
PY 2018
VL 3
IS 3
SI SI
BP 183
EP 193
DI 10.1080/20961790.2018.1503579
PG 11
WC Medicine, Legal
WE Emerging Sources Citation Index (ESCI)
SC Legal Medicine
GA VJ0PO
UT WOS:000526125900002
PM 30483668
OA Green Published, gold
DA 2022-02-03
ER

PT J
AU Usha, K
   Ezhilarasan, M
AF Usha, K.
   Ezhilarasan, M.
TI Fusion of geometric and texture features for finger knuckle surface
   recognition
SO ALEXANDRIA ENGINEERING JOURNAL
LA English
DT Article
DE Finger back knuckle surface; Binarization; Angular geometric analysis;
   Contourlet transform; Principle component analysis
ID BIOMETRIC IDENTIFICATION
AB Hand-based biometrics plays a significant role in establishing security for real-time environments involving human interaction and is found to be more successful in terms of high speed and accuracy. This paper investigates on an integrated approach for personal authentication using Finger Back Knuckle Surface (FBKS) based on two methodologies viz., Angular Geometric Analysis based Feature Extraction Method (AGFEM) and Contourlet Transform based Feature Extraction Method (CTFEM). Based on these methods, this personal authentication system simultaneously extracts shape oriented feature information and textural pattern information of FBKS for authenticating an individual. Furthermore, the proposed geometric and textural analysis methods extract feature information from both proximal phalanx and distal phalanx knuckle regions (FBKS), while the existing works of the literature concentrate only on the features of proximal phalanx knuckle region. The finger joint region found nearer to the tip of the finger is called distal phalanx region of FBKS, which is a unique feature and has greater potentiality toward identification. Extensive experiments conducted using newly created database with 5400 FBKS images and the obtained results infer that the integration of shape oriented features with texture feature information yields excellent accuracy rate of 99.12% with lowest equal error rate of 1.04%. (C) 2015 Faculty of Engineering, Alexandria University. Production and hosting by Elsevier B.V.
C1 [Usha, K.] Pondicherry Engn Coll, Dept Comp Sci & Engn, Pondicherry, India.
   [Ezhilarasan, M.] Pondicherry Engn Coll, Dept Informat Technol, Pondicherry, India.
C3 Pondicherry Engineering College; Pondicherry Engineering College
RP Usha, K (corresponding author), Pondicherry Engn Coll, Dept Comp Sci & Engn, Pondicherry, India.
CR Arun R., 1999, C AUD VID BAS BIOM P, P166
   Bao P, 2005, IEEE T PATTERN ANAL, V27, P1485, DOI 10.1109/TPAMI.2005.173
   Biometrics Hand-Based, 2003, BIOMETRIC TECHNOLOGY, V11, P9, DOI [10.1016/S0969-4765(03)07018-8, DOI 10.1016/S0969-4765(03)07018-8]
   Do M.N., 2005, IEEE T IMAGE PROCESS, V14, P1
   Gao GW, 2014, NEUROCOMPUTING, V135, P180, DOI 10.1016/j.neucom.2013.12.036
   Gao GW, 2013, IEEE T IMAGE PROCESS, V22, P5050, DOI 10.1109/TIP.2013.2281429
   Gonzalez S., 2003, 37 IEEE INT CARN C S, P39
   Han CC, 2003, PATTERN RECOGN, V36, P371
   Hegde C, 2013, SIGNAL IMAGE VIDEO P, V7, P633, DOI 10.1007/s11760-013-0469-7
   Jain AK, 2004, IEEE T CIRC SYST VID, V14, P4, DOI 10.1109/TCSVT.2003.818349
   Karamizadeh S., 2013, J SIGNAL INFORM PROC, V4, P173, DOI 10.4236/jsip.2013.43B031
   Kumar A, 2003, LECT NOTES COMPUT SC, V2688, P668
   Kumar A, 2006, IEEE T IMAGE PROCESS, V15, P2454, DOI 10.1109/TIP.2006.875214
   Kumar A, 2010, IEEE T INSTRUM MEAS, V59, P730, DOI 10.1109/TIM.2009.2028773
   Kumar A, 2009, IEEE T IMAGE PROCESS, V18, P2127, DOI 10.1109/TIP.2009.2023153
   Kumar A, 2009, IEEE T INF FOREN SEC, V4, P98, DOI 10.1109/TIFS.2008.2011089
   Masaebi Saeed, 2012, INT J ELECT COMPUT E, V2, P699
   MEGIDDO N, 1983, MATH OPER RES, V8, P498, DOI 10.1287/moor.8.4.498
   Meraoumia A., 2011, 2011 IEEE INT C COMM, P1
   Michael G., 2010, P INT S INF TECHN, V1, P1
   Ribaric S, 2005, IEEE T PATTERN ANAL, V27, P1698, DOI 10.1109/TPAMI.2005.209
   Saigaa M., 2012, P ICITES, V2, P1
   Sanchez-Reillo R, 2000, IEEE T PATTERN ANAL, V22, P1168, DOI 10.1109/34.879796
   Shariatmadar Zahra S., 2013, J CIRCUIT SYST COMP, V3, P135
   Singh T. Romen, 2011, IJCSI, V8, P271
   Woodard DL, 2005, COMPUT VIS IMAGE UND, V100, P357, DOI 10.1016/j.cviu.2005.06.003
   Yin Jun, 2010, P IEEE INT WORKSH EM, V1, P200
   Yu PF, 2014, APPL MECH MATER, V441, P703, DOI 10.4028/www.scientific.net/AMM.441.703
   Zhang L, 2012, IMAGE VISION COMPUT, V30, P1043, DOI 10.1016/j.imavis.2012.09.003
   Zhang L, 2010, PATTERN RECOGN, V43, P2560, DOI 10.1016/j.patcog.2010.01.020
   Zhang YB, 2007, LECT NOTES COMPUT SC, V4781, P154
   Zhu HQ, 2005, INT CONF ACOUST SPEE, P469
NR 38
TC 11
Z9 12
U1 0
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1110-0168
EI 2090-2670
J9 ALEX ENG J
JI Alex. Eng. J.
PD MAR
PY 2016
VL 55
IS 1
BP 683
EP 697
DI 10.1016/j.aej.2015.10.003
PG 15
WC Engineering, Multidisciplinary
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering
GA DG2HP
UT WOS:000371888200066
OA gold
DA 2022-02-03
ER

PT C
AU Masood, H
   Asim, M
   Mumtaz, M
   Bin Mansoor, A
AF Masood, Hassan
   Asim, Mohammad
   Mumtaz, Mustafa
   Bin Mansoor, Atif
BE Shi, H
   Zhang, YC
   Bottema, MJ
   Lovell, BC
   Maeder, AJ
TI Combined Contourlet and Non-subsampled Contourlet Transforms Based
   Approach for Personal Identification using Palmprint
SO 2009 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA 2009)
LA English
DT Proceedings Paper
CT 11th Conference on Digital Image Computing: Techniques and Applications
CY DEC 01-03, 2009
CL Melbourne, AUSTRALIA
DE Biometrics; Palmprint Identification; Texture Analysis; Directional
   Filterbanks; Contourlet Transform; Non-subsampled Contourlet Transform
ID EXTRACTION
AB Palmprint based personal verification is an accepted biometric modality due to its reliability, ease of acquisition and user acceptance. This paper presents a novel palmprint based identification approach which draw on the textural information available on the palmprint by utilizing a combination of Contour let and Non Subsampled Contour let Transforms. Center of the palm is computed using the Distance Transform whereas the parameters of best fitting ellipse help determine the alignment of the palmprint. ROI of 256x256 pixels is cropped around the center, and subsequently it is divided into fine slices, using iterated directional filterbanks. Next, directional energy components for each block of the decomposed subband outputs are computed using Contourlet and Non Subsampled Contourlet Transforms. The proposed algorithm captures global details in a palmprint as compact fixed length palm codes for Contourlet and NSCT respectively which are further concatenated at feature level for identification using normalized Euclidean distance classifier. The proposed algorithm is tested on a total of 500 palm images of GPDS Hand database, acquired from University of Las Palmas de Gran Canaria. The experimental results were compiled for individual transforms as well as for their optimized combination at feature level. CT based approach demonstrated the Decidability Index of 2.6212 and Equal Error Rate (EER) of 0.7082% while NSCT based approach depicted Decidability Index of 2.7278 and EER of 0.5082%. The feature level fusion achieved Decidability Index of 2.7956 and EER of 0.3112%.
C1 [Masood, Hassan; Asim, Mohammad; Mumtaz, Mustafa; Bin Mansoor, Atif] Natl Univ Sci & Technol, Rawalpindi, Pakistan.
C3 National University of Sciences & Technology - Pakistan
RP Masood, H (corresponding author), Natl Univ Sci & Technol, Rawalpindi, Pakistan.
EM hassan13204@yahoo.com; axim.saeed@gmail.com; mustafa672@ieee.org;
   atif.mansoor@gmail.com
CR Candes E., 2000, P SPIE, V4119
   CUNHA AL, 2005, IEEE T IMAGE PRO MAY
   EKINCI M, 2007, MACHINE LEARNING DAT, V4571
   *GPDS, GPDS HAND DAT
   KUMAR A, 2003, P 4 INT C AUD AND VI
   Kumar A, 2006, IEEE T IMAGE PROCESS, V15, P2454, DOI 10.1109/TIP.2006.875214
   Li WX, 2002, INT J PATTERN RECOGN, V16, P417, DOI 10.1142/S0218001402001757
   MANSOOR AB, 2008, P INT S VIS COMP ISV
   Masood H., 2008, P IEEE INT S BIOM SE
   Michael GKO, 2008, IMAGE VISION COMPUT, V26, P1551, DOI 10.1016/j.imavis.2008.06.010
   MUMTAZ M, IEEE INT C IN PRESS
   Park CH, 2004, IEEE T CIRC SYST VID, V14, P74, DOI 10.1109/TCSVT.2003.818355
   Sanchez-Reillo R, 2000, IEEE T PATTERN ANAL, V22, P1168, DOI 10.1109/34.879796
   SHI W, 2001, INT J IMAGE GRAPHICS, V1, P135
   SHU W, 1998, P 14 INT C PATT REC
   SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725
   Wang JG, 2008, IEEE IMAGE PROC, P253, DOI 10.1109/ICIP.2008.4711739
   Wu XQ, 2006, IEEE T SYST MAN CY A, V36, P978, DOI 10.1109/TSMCA.2006.871797
   Xue F., 2008, 19 INT C PATT REC, P1
   You J, 2002, PATTERN RECOGN, V35, P847, DOI 10.1016/S0031-3203(01)00100-5
   Zhang D, 2003, IEEE T PATTERN ANAL, V25, P1041, DOI 10.1109/TPAMI.2003.1227981
   Zuo W., 2008, 19 INT C PATT REC IC, P1, DOI 10.1109/ICPR.2008.4761868
   [No title captured]
NR 23
TC 4
Z9 4
U1 0
U2 2
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-4244-5297-2
PY 2009
BP 408
EP 415
DI 10.1109/DICTA.2009.73
PG 8
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Imaging Science & Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Imaging Science & Photographic Technology
GA BUV70
UT WOS:000290469200060
DA 2022-02-03
ER

PT C
AU Plesh, R
   Bahmani, K
   Jang, G
   Yambay, D
   Brownlee, K
   Swyka, T
   Johnson, P
   Ross, A
   Schuckers, S
AF Plesh, Richard
   Bahmani, Keivan
   Jang, Ganghee
   Yambay, David
   Brownlee, Ken
   Swyka, Timothy
   Johnson, Peter
   Ross, Arun
   Schuckers, Stephanie
GP IEEE
TI Fingerprint Presentation Attack Detection utilizing Time-Series, Color
   Fingerprint Captures
SO 2019 INTERNATIONAL CONFERENCE ON BIOMETRICS (ICB)
SE International Conference on Biometrics
LA English
DT Proceedings Paper
CT International Conference on Biometrics (ICB)
CY JUN 04-07, 2019
CL Crete, GREECE
AB Fingerprint capture systems can be fooled by widely accessible methods to spoof the system using fake fingers, known as presentation attacks. As biometric recognition systems become more extensively relied upon at international borders and in consumer electronics, presentation attacks are becoming an increasingly serious issue. A robust solution is needed that can handle the increased variability and complexity of spoofing techniques. This paper demonstrates the viability of utilizing a sensor with time-series and color-sensing capabilities to improve the robustness of a traditional fingerprint sensor and introduces a comprehensive fingerprint dataset with over 36,000 image sequences and a state-of-the-art set of spoofing techniques. The specific sensor used in this research captures a traditional gray-scale static capture and a time-series color capture simultaneously. Two different methods for Presentation Attack Detection (PAD) are used to assess the benefit of a color dynamic capture. The first algorithm utilizes Static-Temporal Feature Engineering on the fingerprint capture to generate a classification decision. The second generates its classification decision using features extracted by way of the Inception V3 CNN trained on ImageNet. Classification performance is evaluated using features extracted exclusively from the static capture, exclusively from the dynamic capture, and on a fusion of the two feature sets. With both PAD approaches we find that the fusion of the dynamic and static feature-set is shown to improve performance to a level not individually achievable.
C1 [Plesh, Richard; Bahmani, Keivan; Jang, Ganghee; Yambay, David; Schuckers, Stephanie] Clarkson Univ, Potsdam, NY 13699 USA.
   [Brownlee, Ken] Silk ID Syst Inc, Santa Clara, CA USA.
   [Swyka, Timothy; Johnson, Peter] Precise Biometr, Potsdam, NY USA.
   [Ross, Arun] Michigan State Univ, E Lansing, MI 48824 USA.
C3 Clarkson University; Michigan State University
RP Plesh, R (corresponding author), Clarkson Univ, Potsdam, NY 13699 USA.
EM pleshro@clarkson.edu; bahmank@clarkson.edu; gjang@clarkson.edu;
   yambayda@clarkson.edu; ken@silkid.com; tim.swyka@precisebiometrics.com;
   peter.johnson@precisebiometrics.com; rossarun@cse.msu.edu;
   sschucke@clarkson.edu
RI Schuckers, Stephanie/F-6197-2017
OI Schuckers, Stephanie/0000-0002-9365-9642
FU Office of the Director of National Intelligence (ODNI), Intelligence
   Advanced Research Projects Activity (IARPA), via IARPA RD [2017
   -17020200004]
FX This research is based upon work supported in part by the Office of the
   Director of National Intelligence (ODNI), Intelligence Advanced Research
   Projects Activity (IARPA), via IARPA R&D Contract No. 2017 -17020200004.
   The views and conclusions contained herein are those of the authors and
   should not be interpreted as necessarily representing the official
   policies, either expressed or implied, of ODNI, IARPA, or the U.S.
   Government. The U.S. Government is authorized to reproduce and
   distribute reprints for governmental purposes notwithstanding any
   copyright annotation therein.
CR Antonelli A, 2006, IEEE T INF FOREN SEC, V1, P360, DOI 10.1109/TIFS.2006.879289
   Cappelli R., 2001, P INT C ADV PATT REC, P371, DOI DOI 10.1007/3-540-44732-6_38
   Chollet F., 2015, KERAS
   DeCann B, 2009, LECT NOTES COMPUT SC, V5558, P627, DOI 10.1007/978-3-642-01793-3_64
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ghiani L., 2015, THESIS
   Glorot X., 2010, P 13 INT C ART INT S, P249
   Kingma D., 2015, ADAM METHOD STOCHAST
   Mascaro S., 2006, COMMON PATTERNS BLOO
   Nixon K. Adair, 2007, SPOOF DETECTION SCHE, P403
   Parthasaradhi STV, 2005, IEEE T SYST MAN CY C, V35, P335, DOI 10.1109/TSMCC.2005.848192
   Schuckers S, 2016, IMAGE VISION COMPUT, V55, P26, DOI 10.1016/j.imavis.2016.03.016
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan B., 2005, LIVENESS DETECTION U
   Tan B., 2006, 2006 C COMP VIS PATT, DOI DOI 10.1109/CVPRW.2006.120
   Yau WY, 2007, LECT NOTES COMPUT SC, V4642, P888
   Zhang YY, 2007, LECT NOTES COMPUT SC, V4642, P742
NR 17
TC 6
Z9 6
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2376-4201
BN 978-1-7281-3640-0
J9 INT CONF BIOMETR
PY 2019
PG 8
WC Computer Science, Theory & Methods; Engineering, Multidisciplinary;
   Mathematical & Computational Biology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Mathematical & Computational Biology
GA BP2EI
UT WOS:000542138900036
DA 2022-02-03
ER

PT C
AU Dantcheva, A
   Singh, A
   Elia, P
   Dugelay, JL
AF Dantcheva, Antitza
   Singh, Arun
   Elia, Petros
   Dugelay, Jean-Luc
GP IEEE
TI Search Pruning in Video Surveillance Systems: Efficiency-Reliability
   Tradeoff
SO 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV
   WORKSHOPS)
LA English
DT Proceedings Paper
CT IEEE International Conference on Computer Vision (ICCV)
CY NOV 06-13, 2011
CL Barcelona, SPAIN
AB In the setting of computer vision, algorithmic searches often aim to identify an object of interest inside large sets of images or videos. Towards reducing the often astronomical complexity of this search, one can use pruning to filter out objects that are sufficiently distinct from the object of interest pruning gain of an overall reduced objects that are s est, thus resulting search space.
   Motivated by practical computer vision based scenarios such as time-constrained human identification in biometric-based video surveillance systems, we analyze the stochastic behavior of time-restricted search pruning, over large and unstructured data sets which are firrthermore random and varying, and where in addition, pruning itself is not fully reliable but is instead prone to errors. In this stochastic setting we apply the information theoretic method of types as well as information divergence techniques to explore the natural tradeoff that appears' between pruning gain and reliability, and proceed to study the typical and atypical gain-reliability behavior; giving insight on how often pruning might fail to substantially reduce the search space. The result, as is, applies to a plethora of computer vision based applications where efficiency and reliability are intertwined bottlenecks in the overall system performance, and the simplicity of the obtained expressions allows for rigorous and insight/id assessment of the pruning gain-reliability behavior in such, applications, as well as for intuition into designing general object recognition systems.
C1 [Dantcheva, Antitza; Singh, Arun; Elia, Petros; Dugelay, Jean-Luc] EURECOM, Sophia Antipolis, France.
C3 IMT - Institut Mines-Telecom; EURECOM
RP Dantcheva, A (corresponding author), EURECOM, Sophia Antipolis, France.
EM Antitza.Dantcheva@eurecom.fr; Arun.Singh@eurecom.fr;
   Petros.Elia@eurecom.fr; Jean-Luc.Dugelay@eurecom.fr
RI Singh, Arun Kumar/O-6025-2019; DUGELAY, Jean-Luc/ABE-7096-2021
OI Singh, Arun Kumar/0000-0003-2305-7045; 
CR Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108
   Agrell E., 2002, IEEE T INFO THEORY, V48
   Coughlan JM, 2003, IMA V MATH, V133, P1
   Cover Thomas A., 2002, ELEMENTS INFORM THEO, V2nd
   Dantcheva A., 2010, P IEEE MMSP
   Dantcheva A, 2011, MULTIMED TOOLS APPL, V51, P739, DOI 10.1007/s11042-010-0635-7
   Escolano F, 2009, INFORMATION THEORY IN COMPUTER VISION AND PATTERN RECOGNITION, P1, DOI 10.1007/978-1-84882-297-9
   Givens G., 2003, P IEEE CVPR
   Jain A. K., 2004, P ICBA
   Jain AK, 2004, PROC SPIE, V5404, P561, DOI 10.1117/12.542890
   Kumar N., 2008, P ECCV
   Kumar N., 2009, P IEEE ICCV
   NEWHAM E, 1995, BIOMETRIC REPORT
   Nowak E., 2006, P ECCV
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Zheng L., 2003, IEEE T INFO THEORY, V49
NR 16
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-4673-0063-6
PY 2011
PG 8
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BYS96
UT WOS:000300056700195
DA 2022-02-03
ER

PT J
AU Lucas, BL
   Bernardino, R
   Goncalves, LC
   Gomes, VL
AF Lucas, B. L.
   Bernardino-Junior, R.
   Goncalves, L. C.
   Gomes, V. L.
TI Distance between the medialis angles of the eyes as an anatomical
   parameter for tooth selection
SO JOURNAL OF ORAL REHABILITATION
LA English
DT Article
DE anthropometry; artificial tooth; mouth rehabilitation; prosthesis;
   dental anatomy; aesthetics
ID MAXILLARY CENTRAL INCISOR; 4 RACIAL GROUPS; ANTERIOR TEETH;
   INTERPUPILLARY DISTANCE; DENTAL PROPORTIONS; INTERALAR WIDTH; INNER
   CANTHAL; GUIDE; FACE; INTERCANTHAL
AB P>During the construction of a removable prosthesis, the lack of pre-extractions records turns the selection of artificial maxillary anterior teeth into a complex procedure. The aim of this study was to identify a mathematical relation between the anterior dental segment and the distance between the medialis angles of the eyes, for selecting the suitable width of the six maxillary anterior teeth. Standardized digital images of 80 dentate Brazilian subjects were used to measure both facial and oral structures when viewed from the frontal aspect through an image processing program. Accurate casts were made to measure on a curve the distance between the maxillary canines. Parametric statistics was performed to analyse the results (P < 0 center dot 05). The distance between the medialis angles of the eyes showed no significant difference according to gender (P < 0 center dot 108). The Pearson Product Moment Correlation Coefficient showed significant positive correlation between the distance between the medialis angles of the eyes and all variables compared. After the linear regression analysis, mathematical formulae and biometric ratios were concluded to estimate the combined width of the six maxillary anterior teeth from the measurement of the distance between the medialis angles of the eyes. The distance between the medialis angles of the eyes measured through photogrammetry can be a reliable guide for tooth selection.
C1 [Lucas, B. L.; Bernardino-Junior, R.] Univ Fed Uberlandia, Dept Morphol, Inst Biomed Sci, BR-38405320 Uberlandia, MG, Brazil.
   [Goncalves, L. C.; Gomes, V. L.] Univ Fed Uberlandia, Fac Dent, Dept Removable Prosthodont & Dent Mat, BR-38405320 Uberlandia, MG, Brazil.
C3 Universidade Federal de Uberlandia; Universidade Federal de Uberlandia
RP Lucas, BL (corresponding author), Univ Fed Uberlandia, Dept Morphol, Inst Biomed Sci, Rua Para,1720,Bl 2E Sl 36, BR-38405320 Uberlandia, MG, Brazil.
EM lucas.barbara@gmail.com
RI Lucas, Barbara L/E-2853-2010; Lucas, Barbara/M-6529-2019; de Lima Lucas,
   Barbara/AAC-3355-2021
OI Lucas, Barbara L/0000-0003-2829-0508; Lucas,
   Barbara/0000-0003-2829-0508; 
FU CAPES (Coordenacao de Aperfeicoamento de Pessoal de Nivel
   Superior)Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior
   (CAPES)
FX BLL gratefully acknowledges CAPES (Coordenacao de Aperfeicoamento de
   Pessoal de Nivel Superior) for financial support in the form of an M.
   Phil studentship. The authors would like to thank Mr J. S. Jong-A-Tjauw
   and Mr. B. Arthur for reviewing the English grammar of this manuscript.
CR Abdullah MA, 2002, J PROSTHET DENT, V88, P16, DOI 10.1067/mpr.2002.126095
   Al Wazzan KA, 2001, J PROSTHET DENT, V86, P608, DOI 10.1067/mpr.2001.119682
   Bernabe E, 2004, ANGLE ORTHOD, V74, P765
   Berry FH, 1905, DENT MAG, V12, P405
   CESARIO VA, 1984, J PROSTHET DENT, V52, P641, DOI 10.1016/0022-3913(84)90133-1
   FARIABY J, 2005, BR J ORAL MAXILLOFAC, V44, P393
   FARKAS LG, 1992, CLEFT PALATE-CRAN J, V29, P315, DOI 10.1597/1545-1569(1992)029<0315:GPITOR>2.3.CO;2
   FREIHOFER HPM, 1980, J MAXILLOFAC SURG, V8, P324, DOI 10.1016/S0301-0503(80)80121-4
   Frush JP., 1955, J PROSTHET DENT, V5, P586
   Gomes VL, 2009, J ESTHET RESTOR DENT, V21, P26, DOI 10.1111/j.1708-8240.2008.00227.x
   Gomes Vanderlei Luiz, 2006, J Esthet Restor Dent, V18, P196, DOI 10.1111/j.1708-8240.2006.00019_1.x
   GOMES VL, 2008, EUR J ESTHET DENT, V3, P14
   Hasanreisoglu U, 2005, J PROSTHET DENT, V94, P530, DOI 10.1016/j.prosdent.2005.10.007
   HOFFMAN W, 1986, J PROSTHET DENT, V55, P219, DOI 10.1016/0022-3913(86)90348-3
   House MM, 1939, FORM COLOR HARMONY D, P3
   JOHNSON PF, 1992, J PROSTHET DENT, V67, P502, DOI 10.1016/0022-3913(92)90081-K
   Krajicek DD, 1960, J PROSTHET DENT, V10, P205, DOI 10.1016/0022-3913(60)90041-X
   LAESTADIUS ND, 1969, J PEDIATR-US, V74, P465, DOI 10.1016/S0022-3476(69)80206-4
   LATTA GH, 1991, J PROSTHET DENT, V65, P250, DOI 10.1016/0022-3913(91)90170-2
   LAVERE AM, 1992, J PROSTHET DENT, V67, P810, DOI 10.1016/0022-3913(92)90589-3
   LAVERE AM, 1992, J PROSTHET DENT, V67, P661, DOI 10.1016/0022-3913(92)90166-8
   MAVROSKOUFIS F, 1981, J PROSTHET DENT, V45, P592, DOI 10.1016/0022-3913(81)90417-0
   MCARTHUR DR, 1985, J PROSTHET DENT, V53, P216, DOI 10.1016/0022-3913(85)90113-1
   McCord JF, 2000, BRIT DENT J, V188, P660, DOI 10.1038/sj.bdj.4800569a
   MENDESJUNIOR CT, 2001, IJHG, V1, P249
   SCANDRETT FR, 1982, J PROSTHET DENT, V48, P15, DOI 10.1016/0022-3913(82)90041-5
   SILVA JA, 2000, AM J HUM GENET, V67, P444
   SILVERMAN SI, 1967, DENT CLIN N AM, P115
   SMITH BJ, 1975, J PROSTHET DENT, V34, P562, DOI 10.1016/0022-3913(75)90044-X
   Toolson LB, 2006, J PROSTHET DENT, V95, P335, DOI 10.1016/j.prosdent.2006.03.013
   Varjao FM, 2006, QUINTESSENCE INT, V37, P767
   Varjao FM, 2005, INT J PROSTHODONT, V18, P513
   WOODHEAD CM, 1977, J DENT, V5, P93, DOI 10.1016/0300-5712(77)90066-5
   Zlataric DK, 2007, INT J PROSTHODONT, V20, P313
NR 34
TC 8
Z9 9
U1 0
U2 5
PU WILEY-BLACKWELL PUBLISHING, INC
PI MALDEN
PA COMMERCE PLACE, 350 MAIN ST, MALDEN 02148, MA USA
SN 0305-182X
J9 J ORAL REHABIL
JI J. Oral Rehabil.
PD NOV
PY 2009
VL 36
IS 11
BP 840
EP 847
DI 10.1111/j.1365-2842.2009.02002.x
PG 8
WC Dentistry, Oral Surgery & Medicine
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Dentistry, Oral Surgery & Medicine
GA 508BL
UT WOS:000270901700009
PM 19765195
DA 2022-02-03
ER

PT J
AU Singureanu, V
   Ungur, R
   Onac, I
   Kovacs, MH
   Moldovan, G
   Singureanu, V
AF Singureanu, Valentin
   Ungur, Rodica
   Onac, Ioan
   Kovacs, Melinda Haydee
   Moldovan, Gelu
   Singureanu, Victoria
TI Automatic Germination Evaluation and Qualitative Analysis of Essential
   Oil of Mentha x piperita L. under the Influence of High Frequency
   Pulsatile Electromagnetic and Ultrasound Pulsatile Fields
SO NOTULAE BOTANICAE HORTI AGROBOTANICI CLUJ-NAPOCA
LA English
DT Article
DE open source; catalyst; terpenes; raster image; vector image
ID MAGNETIC-FIELD; MOMORDICA-CHARANTIA; SEED-GERMINATION; GROWTH
AB The study illustrates the influence of high frequency pulsatile electromagnetic fields and ultrasound pulsatile fields on Mentha x piperita L. seed germination and the quality of its essential oil. The physiological role of the above mentioned experimental factors was considered to be a catalyticall base point, improving germination percent, SVI (seedling vigor index), GVI (germination velocity index). All the biometric aspects of the germination process (seed area, seed perimeter, seed development on x and y radius, radicele length, hypocotyl length) where determined using open free software, consolidating the general idea that scientific communities can improve and perfect open source projects. High frequency pulsatile electromagnetic fields (91.75%) and ultrasound pulsatile fields (64.75%) experimental variants gave higher germination percent compared to control (47.00%). Following the main terpenes determination, the same experimental variants obtained high accumulations of menthol, eugenol, thymol, eucalyptol, linalool and other components. These aspects can be scientifically sustained by the seedling vigor index marks obtained at high frequency pulsatile electromagnetic fields (1985.47) and ultrasound pulsatile fields (1480.09), creating the general premises for better development stages in the nursery sector. Raised accumulation of main therapeutical terpenes in Mentha x piperita L. must be supervised in further studies, when microscopically imagery of glandular trichomes and their density may lead to more profound conclusions.
C1 [Singureanu, Valentin; Moldovan, Gelu] Univ Agr Sci & Vet Med, Cluj Napoca 400372, Romania.
   [Ungur, Rodica; Onac, Ioan] Iuliu Hatieganu Univ Med & Pharm Cluj Napoca, Cluj Napoca, Romania.
   [Kovacs, Melinda Haydee] INCDO INOE2000, Res Inst Analyt Instrumentat ICLA, Cluj Napoca 400293, Romania.
   [Singureanu, Victoria] Rehabil Clin Hosp Cluj Napoca, Cluj Napoca, Romania.
C3 University of Agricultural Sciences & Veterinary Medicine Cluj Napoca;
   Iuliu Hatieganu University of Medicine & Pharmacy; National Research &
   Development Institute Optoelectronics INOE 2000
RP Singureanu, V (corresponding author), Univ Agr Sci & Vet Med, 3-5 Manastur St, Cluj Napoca 400372, Romania.
EM valentin.singureanu@usamvcluj.ro; ungurmed@yahoo.com;
   ioan.onac@umfcluj.ro; melinda.kovacs@icia.ro; gelumoldo@yahoo.com;
   vickituns@yahoo.com
RI Ana, Rodica/AAW-1422-2020
FU European Social Fund, Human Resources Development Operational Programme
   [POSDRU/159/1.5/S/132765]
FX This paper was published under the frame of European Social Fund, Human
   Resources Development Operational Programme 2007-2013,project no.
   POSDRU/159/1.5/S/132765.
CR Adkins S., 2007, SEEDS BIOL DEV ECOLO
   Ahamed M. E. M., 2013, Asian Journal of Crop Science, V5, P286
   Alexander C, 2012, ARCHITECTURAL DESIGN
   Bilalis D, 2012, ACTA AGR SCAND B-S P, V62, P94, DOI 10.1080/09064710.2011.570374
   Chiu KY, 2014, INT J FOOD SCI TECH, V49, P1699, DOI 10.1111/ijfs.12476
   D'Ancuono LF, 2006, ACTA HORTIC, V723, P33
   Estrelles E., 2004, 4 EUR C CONS WILD PL
   Feizi H., 2012, Notulae Scientia Biologicae, V4, P116
   FITTER AH, 1994, J ECOL, V82, P415, DOI 10.2307/2261309
   Freire MM, 2012, J FOOD SAFETY, V32, P29, DOI 10.1111/j.1745-4565.2011.00341.x
   Gordon A.G., 1963, Ultrasonics, V1, P70, DOI 10.1016/0041-624X(63)90057-X
   Goussous SJ, 2010, EXP AGR, V46, P231, DOI 10.1017/S0014479709991062
   Hirota N, 1999, J APPL PHYS, V85, P5717, DOI 10.1063/1.370262
   Ilis RA, 1981, SEED SCI TECHNOL, V9, P373
   Jedlicka J., 2014, Potravinarstvo: Scientific Journal for Food Industry, V8, P150
   Jeong MJ, 2013, J KOREAN SOC APPL BI, V56, P377, DOI 10.1007/s13765-013-3088-7
   Kew, 2008, SEED INFORM DATABASE
   Kubisz L, 2012, ACTA PHYS POL A, V121, pA49
   Kumar B, 2014, BIOMED RES INT, V42, P1
   [刘建成 Liu Jiancheng], 2012, [中草药, Chinese Traditional and Herbal Drugs], V43, P1000
   MAGUIRE JAMES D., 1962, CROP SCI, V2, P176, DOI 10.2135/cropsci1962.0011183X000200020033x
   Mahajan TS, 2015, INT J AGRIC BIOL, V17, P351
   Mahajan TS, 2014, INT AGROPHYS, V28, P57, DOI 10.2478/intag-2013-0027
   MURRAY MJ, 1972, CAN J GENET CYTOL, V14, P13, DOI 10.1139/g72-002
   Onofri A., 2007, P 6 NAT C IT BIOM SO, P93
   Parra A, 2014, CHEMOSPHERE, V113, P132, DOI 10.1016/j.chemosphere.2014.04.090
   Pascual B, 2004, SEED SCI TECHNOL, V32, P637, DOI 10.15258/sst.2004.32.2.33
   Pietruszewski S, 2010, INT AGROPHYS, V24, P297
   Pistrick K, 2006, ACTA HORTIC, P133, DOI 10.17660/ActaHortic.2006.723.14
   Rico Martinez F., 2014, Annual Research & Review in Biology, V4, P3627, DOI 10.9734/ARRB/2014/10689
   Rokhina EV, 2009, TRENDS BIOTECHNOL, V27, P298, DOI 10.1016/j.tibtech.2009.02.001
   Rosalba Z, 2014, AFRICAN J OFBIOTECHN, V13, P76
   Saeed S, 2005, PAK J BOT, V37, P997
   Toth I, 2012, LUCRARI STIINTIFICE, V45, P422
   Vashisth A, 2008, BIOELECTROMAGNETICS, V29, P571, DOI 10.1002/bem.20426
   Vashisth A, 2010, J PLANT PHYSIOL, V167, P149, DOI 10.1016/j.jplph.2009.08.011
   Wang QZ, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0047204
   Yaldagard M, 2008, J I BREWING, V114, P14, DOI 10.1002/j.2050-0416.2008.tb00300.x
NR 38
TC 0
Z9 0
U1 0
U2 8
PU UNIV AGR SCI & VETERINARY MED CLUJ-NAPOCA
PI CLUJ-NAPOCA
PA 3-5 MANASTUR ST, CLUJ-NAPOCA, 400372, ROMANIA
SN 0255-965X
J9 NOT BOT HORTI AGROBO
JI Not. Bot. Horti Agrobot. Cluj-Na.
PD JAN-JUN
PY 2015
VL 43
IS 1
BP 146
EP 152
DI 10.15835/nbha4319973
PG 7
WC Plant Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Plant Sciences
GA CL5LE
UT WOS:000357000600022
OA hybrid
DA 2022-02-03
ER

PT J
AU Speijer, RP
   Van Loo, D
   Masschaele, B
   Vlassenbroeck, J
   Cnudde, V
   Jacobs, P
AF Speijer, Robert P.
   Van Loo, Denis
   Masschaele, Bert
   Vlassenbroeck, Jelle
   Cnudde, Veerle
   Jacobs, Patric
TI Quantifying foraminiferal growth with high-resolution X-ray computed
   tomography: New opportunities in foraminiferal ontogeny, phylogeny, and
   paleoceanographic applications
SO GEOSPHERE
LA English
DT Article
DE foraminifera; X-ray analysis; computed tomography; biometry; phylogeny;
   paleoceanography
ID PLANKTONIC-FORAMINIFERA
AB The latest generation of high-resolution X-ray computed tomography (HRXCT), with a submicron resolution, enables for the first time three-dimensional (3D) imaging and bio-metric quantification of foraminiferal interiors. Here we exemplify the basic possibilities and opportunities of this new technique by means of an analysis on a fossil specimen of Pseudouvigerina sp. from the basal Paleocene of the Brazos River, Texas. The total scan consists of 1200 X-ray radiographs generated during stepwise rotation (0.3 degrees) of the specimen. These radiographs were processed and reconstructed to build cross-sectional images of the object. After 3D rendering of the data, the specimens' chambers could be segmented, showing an exponential ontogenetic growth rate. From the second chamber onward (i.e., after the megalospheric proloculus with a volume of 10(4) mu m(3)), the size of the chambers steadily increases by a factor of similar to 1.5. Various other dimensions can also be calculated from the scan, such as the total volume of shell calcite or the size of the foramen. The technological improvements with HRXCT could open up a new era in fundamental biometric-evolutionary research and provide a means of morphologic evaluation of phylogenies based on molecular data. Eventually, the accuracy of paleoceanographic and paleoclimatic reconstructions could also benefit from the possibility of morphological differentiation between cryptic planktic species.
C1 [Speijer, Robert P.] Katholieke Univ Leuven, Dept Earth & Environm Sci, B-3001 Heverlee, Belgium.
   [Van Loo, Denis; Masschaele, Bert; Vlassenbroeck, Jelle] Univ Ghent, Dept Subat & Radiat Phys, Ctr Xray Tomog, B-9000 Ghent, Belgium.
   [Cnudde, Veerle; Jacobs, Patric] Univ Ghent, Dept Geol & Soil Sci, B-9000 Ghent, Belgium.
C3 KU Leuven; Ghent University; Ghent University
RP Speijer, RP (corresponding author), Katholieke Univ Leuven, Dept Earth & Environm Sci, B-3001 Heverlee, Belgium.
RI Cnudde, Veerle/K-2026-2016; Speijer, Robert/H-5073-2016; Jacobs, Patric
   JS/E-4067-2012
OI Cnudde, Veerle/0000-0002-3269-5914; Speijer, Robert/0000-0002-5873-7203;
   Jacobs, Patric JS/0000-0001-9154-385X
CR BANNER F. T., 1967, MIC ROP ALE ONTOLOGY [NEW YORK], V13, P133, DOI 10.2307/1484667
   HEMLEBEN C, 1989, MODERN PLANKTONIC FO
   HOOPER K, 1959, J PALEONTOL, V33, P631
   HUANG C-Y, 1981, Journal of Foraminiferal Research, V11, P173
   HUBER BT, 1994, SMITHSONIAN CONTRIBU, V77
   Kucera M, 2002, PHILOS T ROY SOC A, V360, P695, DOI 10.1098/rsta.2001.0962
   Masschaele BC, 2007, NUCL INSTRUM METH A, V580, P266, DOI 10.1016/j.nima.2007.05.099
   Murray JW, 2006, ECOLOGY AND APPLICATIONS OF BENTHIC FORAMINIFERA, P1, DOI 10.1017/CBO9780511535529
   Petrizzo MR, 2006, J FORAMIN RES, V36, P233, DOI 10.2113/gsjfr.36.3.233
   Schulte P, 2006, SEDIMENT GEOL, V184, P77, DOI 10.1016/j.sedgeo.2005.09.021
   SVERDLOVE MS, 1985, J FORAMIN RES, V15, P235, DOI 10.2113/gsjfr.15.4.235
   Tyszka J, 2005, PALEOBIOLOGY, V31, P522, DOI 10.1666/0094-8373(2005)031[0522:ANATMO]2.0.CO;2
   Vlassenbroeck J, 2007, NUCL INSTRUM METH A, V580, P442, DOI 10.1016/j.nima.2007.05.073
NR 13
TC 43
Z9 44
U1 0
U2 15
PU GEOLOGICAL SOC AMER, INC
PI BOULDER
PA PO BOX 9140, BOULDER, CO 80301-9140 USA
SN 1553-040X
J9 GEOSPHERE
JI Geosphere
PD AUG
PY 2008
VL 4
IS 4
BP 760
EP 763
DI 10.1130/GES00176.1
PG 4
WC Geosciences, Multidisciplinary
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Geology
GA 331GM
UT WOS:000258000700006
OA Bronze, Green Published
DA 2022-02-03
ER

PT J
AU Canovas, F
   Roussanne, Y
   Captier, G
   Bonnel, F
AF Canovas, F
   Roussanne, Y
   Captier, G
   Bonnel, F
TI Study of carpal bone morphology and position in three dimensions by
   image analysis from computed tomography scans of the wrist
SO SURGICAL AND RADIOLOGIC ANATOMY
LA English
DT Article
DE image analysis; anatomy; carpal bone; three dimensions
ID MATURATION
AB The morphology and positioning of the carpal bones were studied in three dimensions in 18 normal adults on computed tomography (CT) scans of the wrist. The digital data from each CT scan were processed to extract the carpal bones and to automatically characterize their geometry (geometric centroid, principal axes of inertia) using specific software tools. Biometric and angular parameters were defined for this purpose, and most of these parameters showed a normal distribution. The mean distance between the geometric centroid of the capitate and that of the triquetrum, expressed as a relationship to the length of the first principal axis of inertia of the capitate, was found to be the greatest (157.6%+/-8.4%), whereas the smallest mean distance was between the hamate and triquetrum (91.4%+/-7.3%). In the sagittal plane, the first principal axis of inertia of the bones of the first carpal row projected in front of the vertical axis of the orthogonal reference system, whereas the first principal axis of the capitate projected behind it. Measurements using this methodology are far more numerous than those from standard plain radiographs and have the additional advantage of being independent of the examiner. Future investigations on normal wrists should provide a normal range for each quantitative parameter, and comparative study of normal and pathologic wrist measurements should help to define the most relevant parameters for specific traumatic pathologies of the wrist.
C1 CHU Montpellier, Hop Lapeyronie, F-34295 Montpellier 5, France.
   Fac Med Montpellier, Lab Anat, F-34060 Montpellier 1, France.
C3 Universite de Montpellier; CHU de Montpellier
RP Canovas, F (corresponding author), CHU Montpellier, Hop Lapeyronie, Ave Doyen G Giraud, F-34295 Montpellier 5, France.
EM f-canovas@chu-montpellier.fr
OI Captier, Guillaume/0000-0002-1705-2339
CR Aufauvre B, 1999, SURG RADIOL ANAT, V21, P383, DOI 10.1007/BF01631347
   BERGER RA, 1982, CLIN ORTHOP RELAT R, P303
   BOUMAN HW, 1994, J HAND SURG-BRIT EUR, V19B, P325, DOI 10.1016/0266-7681(94)90081-7
   Canovas F, 1997, SURG RADIOL ANAT, V19, P395, DOI 10.1007/BF01628507
   Canovas F, 2000, ARCH PEDIATRIE, V7, P976, DOI 10.1016/S0929-693X(00)90014-9
   Canovas F, 2000, HORM RES, V54, P6
   CANOVAS F, 2000, THESIS U MONTPELLIER
   Feipel V, 1999, SURG RADIOL ANAT, V21, P175
   Feipel V, 1999, SURG RADIOL ANAT, V21, P207, DOI 10.1007/s00276-999-0207-6
   GARCIAELIAS M, 1989, J HAND SURG-AM, V14A, P1017, DOI 10.1016/S0363-5023(89)80053-X
   GILULA LA, 1978, RADIOLOGY, V129, P641, DOI 10.1148/129.3.641
   LINSCHEID RL, 1972, J BONE JOINT SURG AM, VA 54, P1612, DOI 10.2106/00004623-197254080-00003
   PATTERSON RM, 1995, J HAND SURG-AM, V20A, P923, DOI 10.1016/S0363-5023(05)80138-8
   SCHUIND FA, 1992, J BONE JOINT SURG AM, V74A, P1418, DOI 10.2106/00004623-199274090-00018
   SEMPE M, 1979, PEDIATRIE, V34, P833
   SMITH DK, 1993, RADIOLOGY, V187, P187, DOI 10.1148/radiology.187.1.8451410
   VIEGAS SF, 1993, J HAND SURG-AM, V18A, P341, DOI 10.1016/0363-5023(93)90372-A
   YOUM Y, 1978, J BONE JOINT SURG AM, V60, P423, DOI 10.2106/00004623-197860040-00001
NR 18
TC 13
Z9 15
U1 0
U2 1
PU SPRINGER FRANCE
PI PARIS
PA 22 RUE DE PALESTRO, PARIS, 75002, FRANCE
SN 0930-1038
EI 1279-8517
J9 SURG RADIOL ANAT
JI Surg. Radiol. Anat.
PD JUN
PY 2004
VL 26
IS 3
BP 186
EP 190
DI 10.1007/s00276-003-0207-x
PG 5
WC Anatomy & Morphology; Radiology, Nuclear Medicine & Medical Imaging;
   Surgery
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Anatomy & Morphology; Radiology, Nuclear Medicine & Medical Imaging;
   Surgery
GA 825JG
UT WOS:000221752100006
PM 15173959
DA 2022-02-03
ER

PT J
AU Nicot, R
   Roland-Billecart, T
   Srouji, A
   Barry, F
   Ferri, J
   Schlund, M
AF Nicot, Romain
   Roland-Billecart, Thomas
   Srouji, Alexandre
   Barry, Florent
   Ferri, Joel
   Schlund, Matthias
TI Lateral Pterygoid Muscle Biometric Modifications in Pterygoid Process
   Fractures Associated With Mandibular Fractures
SO JOURNAL OF ORAL AND MAXILLOFACIAL SURGERY
LA English
DT Article
ID PLATE FRACTURES
AB Purpose: Pterygoid process fractures (PPFs) are classically associated with Lefort fractures but can also be encountered in association with other facial fractures such as mandibular fractures. The aim of this study was to estimate the frequency of PPFs associated with mandibular fractures and identify factors associated with PPF.
   Materials and Methods: We conducted a retrospective cross-sectional study using computed tomography scanning of patients having a mandibular fracture between November 2018 and April 2020. PPFs were classified using the classification by An et al. Volume, length, and width of both lateral pterygoid muscles have been evaluated by using an image processing software. Study population has been divided into 2 groups: fracture of pterygoid process or the absence of PPF. To evaluate the implication of lateral pterygoid muscle in the pathophysiology of PPF, we compared lateral pterygoid muscle volume, its maximal length, and width between both groups. Patients with bilateral fractures were excluded from this analysis.
   Results: About 304 patients with at least 1 mandibular fracture have been included in this study. About 18 patients presenting an association of mandibular fracture and PPF were finally selected. About 83.33% of the patients were concerned by a fracture of the posterior part of the mandible. The PPF was classified as type IIA by the classification of An et al for 94.4% of patients. The lateral pterygoid muscle volumes were significantly larger on the side of the PPF (P = .02). However, there were no significant differences in the maximum length (P = .49) and width (P = .1) of lateral pterygoid muscle.
   Conclusions: Our study showed an association between mandibular fractures (mainly ipsilateral posterior) and isolated PPF through a lateral pterygoid muscle volume increase. (C) 2020 American Association of Oral and Maxillofacial Surgeons
C1 [Nicot, Romain; Ferri, Joel; Schlund, Matthias] Univ Lille, CHU Lille, Dept Oral & Maxillofacial Surg, INSERM,Controlled Drug Delivery Syst & Biomat, Lille, France.
   [Roland-Billecart, Thomas; Barry, Florent] Univ Lille, Dept Oral & Maxillofacial Surg, CHU Lille, Lille, France.
   [Srouji, Alexandre] Univ Bourgogne, CHU Dijon, Dept Plast Oral & Maxillofacial & Hand Surg, Dijon, France.
C3 Fondation I-SITE ULNE; CHU Lille; Universite de Lille; Institut National
   de la Sante et de la Recherche Medicale (Inserm); Fondation I-SITE ULNE;
   CHU Lille; Universite de Lille; CHU Dijon Bourgogne; Universite de
   Bourgogne
RP Nicot, R (corresponding author), CHU Lille, Dept Oral & Maxillofacial Surg, Roger Salengro Hosp, Rue Emile Laine, F-59037 Lille, France.
EM romain.nicot@gmail.com
RI Schlund, Matthias/AAP-7875-2021; Nicot, Romain/N-3926-2013
OI Nicot, Romain/0000-0001-7391-2121; BARRY, Florent/0000-0002-3391-1025;
   Schlund, Matthias/0000-0002-5836-7169
CR An JG, 2014, OR SURG OR MED OR PA, V117, P243, DOI 10.1016/j.oooo.2013.11.500
   Defabianis Patrizia, 2002, Funct Orthod, V19, P34
   ERIKSSON L, 1979, ORAL SURG ORAL MED O, V47, P127, DOI 10.1016/0030-4220(79)90165-8
   Garg RK, 2015, J CRANIOFAC SURG, V26, P1823, DOI 10.1097/SCS.0000000000001901
   Imai T, 2014, DENTOMAXILLOFAC RAD, V43, DOI 10.1259/dmfr.20130355
   Kolk A, 2015, J CRANIO MAXILL SURG, V43, P452, DOI 10.1016/j.jcms.2015.02.004
   Rhea JT, 2005, AM J ROENTGENOL, V184, P1700, DOI 10.2214/ajr.184.5.01841700
   Simonds JS, 2011, AM J NEURORADIOL, V32, P468, DOI 10.3174/ajnr.A2337
   Talwar RM, 1998, J ORAL MAXIL SURG, V56, P430, DOI 10.1016/S0278-2391(98)90707-8
   Tessier P, 1972, Plast Reconstr Surg, V50, P600
   Tessier P, 1972, Plast Reconstr Surg, V50, P497
   Throckmorton GS, 1999, J ORAL MAXIL SURG, V57, P500, DOI 10.1016/S0278-2391(99)90061-7
   Toure G, 2018, PLAST RECONSTR SURG, V141, p718E, DOI 10.1097/PRS.0000000000004295
   Truong AQ, 2014, JAMA FACIAL PLAST SU, V16, P437, DOI 10.1001/jamafacial.2014.645
   Walker Cameron J, 2017, Atlas Oral Maxillofac Surg Clin North Am, V25, P11, DOI 10.1016/j.cxom.2016.10.002
   White EA, 2018, EMERG RADIOL, V25, P235, DOI 10.1007/s10140-018-1589-8
NR 16
TC 0
Z9 0
U1 0
U2 0
PU W B SAUNDERS CO-ELSEVIER INC
PI PHILADELPHIA
PA 1600 JOHN F KENNEDY BOULEVARD, STE 1800, PHILADELPHIA, PA 19103-2899 USA
SN 0278-2391
EI 1531-5053
J9 J ORAL MAXIL SURG
JI J. Oral Maxillofac. Surg.
PD DEC
PY 2020
VL 78
IS 12
BP 2258
EP 2266
DI 10.1016/j.joms.2020.07.219
PG 9
WC Dentistry, Oral Surgery & Medicine
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Dentistry, Oral Surgery & Medicine
GA PA2PG
UT WOS:000595475200027
PM 32866485
DA 2022-02-03
ER

PT J
AU Aleynikov, YG
   Konstantinovich, AV
AF Aleynikov, Yury G.
   Konstantinovich, Anastasia, V
TI Creation of 3D Cloud Models for Plants Using A Scanner and Walking
   Machine with Dynamic Stability
SO BIOSCIENCE BIOTECHNOLOGY RESEARCH COMMUNICATIONS
LA English
DT Article
DE WALKING MACHINE; WALKING MACHINE SENSORS; 3D POINT CLOUD; 3D SCANNING;
   COMPUTER VISION
AB Processes research of plant organisms involves many biometric parameters. Latest 3D scanning technologies and computer vision systems makes it possible to evaluate such plant parameters as leaf width, length, plant height, green mass and volume. Thus, the chief purpose of the study is to examine the creation of 3D cloud models for plants using a scanner and a walking machine with dynamic stability. To fulfil that aim, for scanning plants in laboratory and in field conditions, a 3D scanner and stereo cameras are mounted on a walking machine with dynamic stability. To automate the collection of biometric parameters of plants, it is proposed to use a walking machine with dynamic stability equipped with high-resolution cameras and a 3D scanner. For scanning plants in the laboratory and in the field, a 3D scanner and stereo cameras are installed on a walking machine with dynamic stability. A 3D scanner creates a point cloud depth map by illuminating the surrounding space with an infrared laser with a structured light. Depth data is converted to 3D image by software. Laboratory experiments and field tests of a 3D scanner installed on a walking machine with dynamic stability, were created 3D point clouds models of plants. In the course of the research were revealed the features of scanning plants with aperture light. Small leaves and thin stems are difficult to scan if their size are less than 8 mm. Due wind condition imperfect scan algorithms create duplicated identical leaves. To reduce the distortion of the point cloud, it is necessary to apply stabilization methods, based on vibrations and orientation in space parameters of the walking machine body. And make scan action at exactly moment in time when the machine body has the lowest oscillatory speed.
C1 [Aleynikov, Yury G.; Konstantinovich, Anastasia, V] Russian State Agr Univ, Moscow Timiryazev Agr Acad, Timiryazevskavast 49, Moscow 127550, Russia.
C3 Russian State Agrarian University - Moscow Timiryazev Agricultural
   Academy
RP Aleynikov, YG (corresponding author), Russian State Agr Univ, Moscow Timiryazev Agr Acad, Timiryazevskavast 49, Moscow 127550, Russia.
EM yuri@AleyRobotics.com
OI Aleynikov, Yury/0000-0001-6586-9741
CR Asgari Bajgirani M, 2021, PROGR CHEM BIOCHEMIC, P11
   Asgharzadeh M. R., 2021, International Journal of Advanced Biological and Biomedical Research, V9, P147, DOI 10.22034/ijabbr.2021.241642
   Chaudhury Ayan, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12540), P244, DOI 10.1007/978-3-030-65414-6_18
   Kjaer KH, 2015, SENSORS-BASEL, V15, P13533, DOI 10.3390/s150613533
   Nguyen CV, 2016, 2016 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P148
   Panjvani K, 2019, FRONT PLANT SCI, V10, DOI 10.3389/fpls.2019.00147
   Paulus S, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0490-0
   Petrov NB, 2019, POLYTECHNIC YOUTH MA, V9, P7
   Sheikhshoaie I, 2018, CHEM METHODOL, V2, P103, DOI 10.22631/chemm.2018.112913.1031
   Wang YJ, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11010063
NR 10
TC 0
Z9 0
U1 2
U2 2
PU SOC SCIENCE & NATURE
PI BHOPAL
PA C-52 HOUSING BOARD COLONY, KOHE FIZA, BHOPAL, MADHYA PRADESH 462 001,
   INDIA
SN 0974-6455
J9 BIOSCI BIOTECH RES C
JI Biosci. Biotechnol. Res. Commun.
PD APR-JUN
PY 2021
VL 14
IS 2
BP 505
EP 508
DI 10.21786/bbrc/14.2.9
PG 4
WC Biotechnology & Applied Microbiology
WE Emerging Sources Citation Index (ESCI)
SC Biotechnology & Applied Microbiology
GA TM2RK
UT WOS:000675398800008
DA 2022-02-03
ER

PT J
AU Bechtel, JH
   Svacek, JF
AF Bechtel, JH
   Svacek, JF
TI Metro optical networks for homeland security
SO FIBER AND INTEGRATED OPTICS
LA English
DT Article
DE metro optical networks; video-centric multiservice; homeland security
AB Metro optical networks provide an enticing opportunity for strengthening homeland security Many existing and emerging fiber-optic networks can be adapted for enhanced security applications. Applications include airports, theme parks, sports venues, and border surveillance systems. Here real-time high-quality video and captured images can be collected, transported, processed, and stored for security applications. Video and data collection are important also at correctional facilities, courts, infrastructure (e.g., dams, bridges, railroads, reservoirs, power stations), and at military and other government locations. The scaling of DWDM-based networks allows vast amounts Of data to be collected and transported including biometric features of individuals at security check points. Here applications will be discussed along with potential solutions and challenges. Examples of solutions to these problems are given. This includes a discussion of metropolitan aggregation platforms for voice, video, and data that are SONET compliant for use in SONET networks and the use of DWDM technology for scaling and transporting a variety of protocols. Element management software allows not only network status monitoring, but also provides optimized allocation of network resources through the use of optical switches or electrical cross connects.
C1 IPITEK, Carlsbad, CA 92008 USA.
RP Bechtel, JH (corresponding author), IPITEK, 2330 Faraday Ave, Carlsbad, CA 92008 USA.
NR 0
TC 0
Z9 0
U1 0
U2 7
PU TAYLOR & FRANCIS INC
PI PHILADELPHIA
PA 325 CHESTNUT ST, SUITE 800, PHILADELPHIA, PA 19106 USA
SN 0146-8030
J9 FIBER INTEGRATED OPT
JI Fiber Integrated Opt.
PY 2003
VL 22
IS 2
BP 131
EP 139
DI 10.1080/01468030390111940
PG 9
WC Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Optics
GA 662TB
UT WOS:000181963500006
DA 2022-02-03
ER

PT J
AU Ortega, M
   Alarcon-Munoz, E
   Ulloa, S
   Cordova, J
   Vidal, N
   Olave, E
AF Ortega, M.
   Alarcon-Munoz, E.
   Ulloa, S.
   Cordova, J.
   Vidal, N.
   Olave, E.
TI Presence and Biometry of Accessory Spleen in Chile Individuals: Study by
   Computerized Tomography
SO INTERNATIONAL JOURNAL OF MORPHOLOGY
LA Spanish
DT Article
DE Anatomy; Accessory Spleen; Imaging; Biometry
ID PREVALENCE; CT
AB The spleen is located in the upper left quadrant of the abdomen, subsequently related to the 9th to 11th rib, from which it is separated by the diaphragm and the cost-diaphragmatic recess, it is located behind the stomach and laterally to the left kidney. Due to alterations in its development, accessory spleens (AS) can be generated, being considered an ectopic tissue of the spleen. The AS are considered normal tissue, with the same physiological processes as the main spleen. With the purpose of locating and determining biometric aspects of them, a cross-sectional and descriptive study was carried out on a sample of 220 CT scans belonging to patients over 18 years of age at the Herman Henriquez Aravena Regional Hospital. Temuco, Chile . For this study, all CT scans with a history of splenectomy and spleen or peri-splenic lesions were excluded. The analysis of the data showed a prevalence of 32.3 % of AS, being able to be of a single presence. two and even three AS per patient. Of a total of 71 people who have at least one AS, 34 (47.89 %) were female and 37 (52.11 %) male. There were 56 patients (78.9 %) with a one AS, 29 (40.85 %) of the female sex and 27 (38.03 %) of the male; 15 (21.1 %) presented more than one AS, 5 (7.04 %) female and 10 (14.08 %) male, although variation in the amount ofAS according to sex can be observed, no there is a statistically significant relationship between these variables. The most frequent location found in the axial plane was the anteromedial zone with 59 cases (66.29 %); also, in the sagittal plane, the most frequent location was in the lower pole with 40 cases (44.44 %). Biometric data of these AS are shown in tables. This information will be ofgreat morphological and medical value due to the limited existing literature on this subject in Chilean individuals.
C1 [Ortega, M.] Tecnol Med Con Menc Imagenol, Temuco, Chile.
   [Alarcon-Munoz, E.; Ulloa, S.; Cordova, J.] Univ La Frontera, Tecnol Med, Temuco, Chile.
   [Vidal, N.] Univ Santo Tomas, Fac Salud, Escuela Enfermeria, Santiago, Chile.
   [Olave, E.] Univ La Frontera, Fac Med, Temuco, Chile.
C3 Universidad de La Frontera; Universidad Santo Tomas; Universidad de La
   Frontera
RP Olave, E (corresponding author), Univ La Frontera, Fac Med, Temuco, Chile.
EM enrique.olave@ufrontera.cl
CR Agur A, 2013, ANATOMIA CON ORIENTA
   Bontrager KL., 2014, TXB RADIOGRAPHIC POS, V8th ed
   Castillo OA, 2013, REV CHIL CIR, V65, P162, DOI 10.4067/S0718-40262013000200010
   Izzo L, 2004, LIVER INT, V24, P216, DOI 10.1111/j.1478-3231.2004.00915.x
   Kim SH, 2008, KOREAN J RADIOL, V9, P162, DOI 10.3348/kjr.2008.9.2.162
   Mortele KJ, 2004, AM J ROENTGENOL, V183, P1653, DOI 10.2214/ajr.183.6.01831653
   Quah C, 2011, SURG ENDOSC, V25, P261, DOI 10.1007/s00464-010-1171-2
   Rashid SA, 2014, MALAYS J MED SCI, V21, P18
   Romer T, 2012, JBR-BTR, V95, P61, DOI 10.5334/jbr-btr.75
   Standring S., 2008, GRAYS ANATOMY
   Vikse J, 2017, INT J SURG, V45, P18, DOI 10.1016/j.ijsu.2017.07.045
   Yang B, 2017, MEDICINE, V96, DOI 10.1097/MD.0000000000009040
   Yildiz AE, 2013, SCI WORLD J, DOI 10.1155/2013/321810
NR 13
TC 0
Z9 0
U1 0
U2 2
PU SOC CHILENA ANATOMIA
PI TEMUCO
PA CASILLA 54-D, TEMUCO, 00000, CHILE
SN 0717-9502
EI 0717-9367
J9 INT J MORPHOL
JI Int. J. Morphol.
PD JUN
PY 2020
VL 38
IS 3
BP 787
EP 792
PG 6
WC Anatomy & Morphology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Anatomy & Morphology
GA LD0WX
UT WOS:000525755300039
DA 2022-02-03
ER

PT C
AU Munzer, B
   Leibetseder, A
   Kletz, S
   Primus, MJ
   Schoeffmann, K
AF Muenzer, Bernd
   Leibetseder, Andreas
   Kletz, Sabrina
   Primus, Manfred Juergen
   Schoeffmann, Klaus
GP Assoc Comp Machinery
TI lifeXplore at the Lifelog Search Challenge 2018
SO LSC '18: PROCEEDINGS OF THE 2018 ACM WORKSHOP ON THE LIFELOG SEARCH
   CHALLENGE
LA English
DT Proceedings Paper
CT ACM Workshop on Multimedia for Real Estate Tech (RETech) / 1st ACM
   Workshop on The Lifelog Search Challenge (LSC)
CY JUN 11, 2018
CL Yokohama, JAPAN
DE Lifelogging; evaluation campaign; interactive image retrieval; video
   browsing
AB With the growing hype for wearable devices recording biometric data comes the readiness to capture and combine even more personal information as a form of digital diary - lifelogging today is practiced ever more and can be categorized anywhere between an informative hobby and a life-changing experience. From an information processing point of view, analyzing the entirety of such multi-source data is immensely challenging, which is why the first Lifelog Search Challenge 2018 competition is brought into being, as to encourage the development of efficient interactive data retrieval systems. Answering this call, we present a retrieval system based on our video search system diveXplore, which has successfully been used in the Video Browser Showdown 2017 and 2018. Due to the different task definition and available data corpus, the base system was adapted and extended to this new challenge. The resulting lifeXplore system is a flexible retrieval and exploration tool that offers various easy-to-use, yet still powerful search and browsing features that have been optimized for lifelog data and for usage by novice users. Besides efficient presentation and summarization of lifelog data, it includes searchable feature maps, concept and metadata filters, similarity search and sketch search.
C1 [Muenzer, Bernd; Leibetseder, Andreas; Kletz, Sabrina; Primus, Manfred Juergen; Schoeffmann, Klaus] Alpen Adria Univ, Inst Informat Technol, A-9020 Klagenfurt, Austria.
C3 University of Klagenfurt
RP Munzer, B (corresponding author), Alpen Adria Univ, Inst Informat Technol, A-9020 Klagenfurt, Austria.
EM bernd@itec.aau.at; aleibets@itec.aau.at; skletz@itec.aau.at;
   mprimus@itec.aau.at; ks@itec.aau.at
RI Leibetseder, Andreas/AAI-2725-2020
OI Leibetseder, Andreas/0000-0002-9535-966X
FU Universitat Klagenfurt; Lakeside Labs GmbH, Klagenfurt, Austria;
   European Regional Development FundEuropean Commission [KWF 20214 u.
   3520/26336/38165]; Carinthian Economic Promotion Fund (KWF) [KWF 20214
   u. 3520/26336/38165]
FX This work is supported by Universitat Klagenfurt and Lakeside Labs GmbH,
   Klagenfurt, Austria and funding from the European Regional Development
   Fund and the Carinthian Economic Promotion Fund (KWF) under grant KWF
   20214 u. 3520/26336/38165.
CR Barthel Kai Uwe, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P237, DOI 10.1007/978-3-319-14442-9_21
   Barthel K. U., 2016, LNCS, V9517, P418, DOI [10.1007/978-3-319-27674-843, DOI 10.1007/978-3-319-27674-8]
   Cobarzan C, 2017, MULTIMED TOOLS APPL, V76, P5539, DOI 10.1007/s11042-016-3661-2
   Dang-Nguyen DT, 2017, PROCEEDINGS OF THE 15TH INTERNATIONAL WORKSHOP ON CONTENT-BASED MULTIMEDIA INDEXING (CBMI), DOI 10.1145/3095713.3095736
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Doherty AR, 2011, COMPUT HUM BEHAV, V27, P1948, DOI 10.1016/j.chb.2011.05.002
   Duane A, 2018, LECT NOTES COMPUT SC, V10705, P377, DOI 10.1007/978-3-319-73600-6_36
   Gurrin Cathal, 2014, Foundations and Trends in Information Retrieval, V8, P5, DOI 10.1561/1500000033
   Hopfgartner Frank, 2013, SEMANTIC MODELS ADAP, P187
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kohonen T., 1998, Neurocomputing, V21, P1, DOI 10.1016/S0925-2312(98)00030-7
   Krizhevsky A., 2012, INT C NEUR INF PROC, V25, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
   MICO ML, 1994, PATTERN RECOGN LETT, V15, P9, DOI 10.1016/0167-8655(94)90095-7
   Primus MJ, 2018, LECT NOTES COMPUT SC, V10705, P438, DOI 10.1007/978-3-319-73600-6_47
   Schoeffmann K, 2017, LECT NOTES COMPUT SC, V10133, P457, DOI 10.1007/978-3-319-51814-5_41
   Schoeffmann K, 2014, IEEE MULTIMEDIA, V21, P8, DOI 10.1109/MMUL.2014.56
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/CVPR.2015.7298594
NR 17
TC 16
Z9 16
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
BN 978-1-4503-5796-8
PY 2018
BP 3
EP 8
DI 10.1145/3210539.3210541
PG 6
WC Computer Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BO0NS
UT WOS:000492677200002
DA 2022-02-03
ER

PT J
AU Fischer, MCM
   Tokunaga, K
   Okamoto, M
   Habor, J
   Radermacher, K
AF Fischer, Maximilian C. M.
   Tokunaga, Kunihiko
   Okamoto, Masashi
   Habor, Juliana
   Radermacher, Klaus
TI Preoperative factors improving the prediction of the postoperative
   sagittal orientation of the pelvis in standing position after total hip
   arthroplasty
SO SCIENTIFIC REPORTS
LA English
DT Article
ID ACETABULAR COMPONENT POSITION; CUP ORIENTATION; TILT; SUPINE; ALIGNMENT;
   PARAMETERS; MOTION; ANGLE; INCLINATION; MORPHOLOGY
AB The aims of this study were to investigate if the sagittal orientation of the pelvis (SOP) in the standing position changes after total hip arthroplasty (THA) and evaluate what preoperative factors may improve the prediction of the postoperative standing SOP in the context of a patient-specific functional cup orientation. 196 primary THA patients from Japan were retrospectively selected for this study. Computed tomography imaging of the pelvis, EOS imaging of the lower body and lateral radiographs of the lumbar spine in the standing position were taken preoperatively. Common biometrics and preoperative Harris Hip Score were recorded. The EOS imaging in the standing position was repeated three months following THA. A 3D/2.5D registration process was used to determine the standing SOP. Thirty-three preoperative biometric, morphological and functional parameters were measured. Important preoperative parameters were identified that significantly improve the prediction of the postoperative standing SOP by using multiple linear LASSO regression. On average, the SOP changed significantly (p<0.001) between the preoperative and postoperative standing position three months after THA by 3 degrees +/- 4 degrees in the posterior direction. The age, standing lumbar lordosis angle (LLA) and preoperative supine and standing SOP significantly (p<0.001) improve the prediction of the postoperative standing SOP. The linear regression model for the prediction of the postoperative standing SOP is significantly (p<0.001) improved by adding the parameters preoperative standing SOP and LLA, in addition to the preoperative supine SOP, reducing the root mean square error derived from a leave-one-out cross-validation by more than 1 degrees. The mean standing SOP in Japanese patients changes already three months after THA in comparison to the preoperative value. The preoperative factors age, LLA, supine and standing SOP can significantly improve the prediction of the postoperative standing SOP and should be considered within the preoperative planning process of a patient-specific functional cup orientation.
C1 [Fischer, Maximilian C. M.; Habor, Juliana; Radermacher, Klaus] Rhein Westfal TH Aachen, Helmholtz Inst Biomed Engn, Chair Med Engn, Aachen, Germany.
   [Tokunaga, Kunihiko] Kameda Daiichi Hosp, Niigata Hip Joint Ctr, Niigata, Japan.
   [Okamoto, Masashi] Kameda Daiichi Hosp, Dept Radiol, Niigata, Japan.
C3 Helmholtz Association; RWTH Aachen University
RP Fischer, MCM (corresponding author), Rhein Westfal TH Aachen, Helmholtz Inst Biomed Engn, Chair Med Engn, Aachen, Germany.
EM m.fischer@hia.rwth-aachen.de
RI Radermacher, Klaus/V-5850-2017
OI Radermacher, Klaus/0000-0003-4395-3197; Fischer,
   Maximilian/0000-0001-7832-5671
FU Projekt DEAL
FX Open Access funding enabled and organized by Projekt DEAL.
CR Akiyama H, 2012, J ORTHOP SCI, V17, P358, DOI 10.1007/s00776-012-0229-5
   Altman DG, 2006, BRIT MED J, V332, P1080, DOI 10.1136/bmj.332.7549.1080
   Barbier O, 2017, ACTA ORTHOP BELG, V83, P360
   Berliner JL, 2018, BONE JOINT J, V100B, P1289, DOI 10.1302/0301-620X.100B10.BJJ-2017-1336.R2
   Blondel B, 2009, ORTHOP TRAUMATOL-SUR, V95, P568, DOI 10.1016/j.otsr.2009.08.004
   Cerveri P, 2011, ANN BIOMED ENG, V39, P2791, DOI 10.1007/s10439-011-0375-5
   Chaibi Y, 2012, COMPUT METHOD BIOMEC, V15, P457, DOI 10.1080/10255842.2010.540758
   Diebo BG, 2017, SPINE, V42, pE234, DOI 10.1097/BRS.0000000000001744
   DiGioia AM, 2006, CLIN ORTHOP RELAT R, P272, DOI 10.1097/01.blo.0000238862.92356.45
   Dorr LD, 2019, J ARTHROPLASTY, V34, P1, DOI 10.1016/j.arth.2018.10.035
   Eilander W, 2013, BONE JOINT J, V95B, P1326, DOI 10.1302/0301-620X.95B10.31446
   Endo K, 2012, J ORTHOP SCI, V17, P682, DOI 10.1007/s00776-012-0281-1
   Fedorov V, 2009, PHARM STAT, V8, P50, DOI 10.1002/pst.331
   Fischer MCM, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-49573-4
   Halawi MJ, 2018, BONE JOINT J, V100B, P1262, DOI 10.1302/0301-620X.100B10.BJJ-2018-0542
   Hsu J, 2019, J BIOMECH, V82, P193, DOI 10.1016/j.jbiomech.2018.10.020
   Imai N, 2013, CLIN ORTHOP RELAT R, V471, P1271, DOI 10.1007/s11999-012-2700-1
   Ishida T, 2011, J ORTHOP SCI, V16, P682, DOI 10.1007/s00776-011-0153-0
   Kanawade V, 2014, J BONE JOINT SURG AM, V96A, P978, DOI 10.2106/JBJS.M.00765
   KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572
   Kyo T, 2013, ORTHOPEDICS, V36, pE753, DOI 10.3928/01477447-20130523-20
   Langston J, 2018, BONE JOINT J, V100B, P845, DOI 10.1302/0301-620X.100B7.BJJ-2017-1599.R1
   Lazennec J.Y., 2012, SEMIN ARTHROPLAST, V23, P220, DOI DOI 10.1053/j.sart.2013.01.005
   Lazennec JY, 2017, J ARTHROPLASTY, V32, P3550, DOI 10.1016/j.arth.2017.06.023
   Le Huec JC, 2016, EUR SPINE J, V25, P3630, DOI 10.1007/s00586-016-4485-5
   Legaye J, 2011, HIP INT, V21, P87, DOI 10.5301/HIP.2011.6283
   Maillot C, 2019, ORTHOP TRAUMATOL-SUR, V105, P907, DOI 10.1016/j.otsr.2019.03.015
   Maratt JD, 2015, J ARTHROPLASTY, V30, P387, DOI 10.1016/j.arth.2014.10.014
   Marques CJ, 2018, J ULTRAS MED, V37, P2333, DOI 10.1002/jum.14581
   McBride A, 2013, ANZ J SURG, V83, P171, DOI 10.1111/j.1445-2197.2012.06176.x
   McMahon S., 2016, RECON REV, V6, P25, DOI [10.15438/rr.6.4.148, DOI 10.15438/RR.6.4.148]
   Meftah M, 2013, J ARTHROPLASTY, V28, P1200, DOI 10.1016/j.arth.2012.09.018
   Merrill RK, 2018, CLIN SPINE SURG, V31, pE109, DOI 10.1097/BSD.0000000000000555
   Murphy WS, 2013, CLIN ORTHOP RELAT R, V471, P417, DOI 10.1007/s11999-012-2581-3
   Murtaugh PA, 2014, ECOLOGY, V95, P611, DOI 10.1890/13-0590.1
   Nam D, 2017, J ARTHROPLASTY, V32, P1200, DOI 10.1016/j.arth.2016.11.008
   Nishihara S, 2003, CLIN ORTHOP RELAT R, P140, DOI 10.1097/01.blo.0000069891.31220.fd
   Ochi H, 2016, EUR SPINE J, V25, P3699, DOI 10.1007/s00586-015-4217-2
   Oe S, 2015, SPINE, V40, P1487, DOI 10.1097/BRS.0000000000001071
   Parratte S, 2009, CLIN ORTHOP RELAT R, V467, P43, DOI 10.1007/s11999-008-0521-z
   Philippot R, 2009, ORTHOP TRAUMATOL-SUR, V95, P70, DOI 10.1016/j.otsr.2008.01.001
   Ranawat CS, 2016, J ARTHROPLASTY, V31, P1222, DOI 10.1016/j.arth.2015.11.035
   Riviere C, 2019, SEM ART, DOI [10.1053/j.sart. 2019.05.008., DOI 10.1053/j.sart.2019.05.008]
   Riviere C, 2019, ORTHOP TRAUMATOL-SUR, V105, P895, DOI 10.1016/j.otsr.2019.02.012
   Sautet P, 2018, ORTHOP TRAUMATOL-SUR, V104, P347, DOI 10.1016/j.otsr.2017.10.006
   Shimizu M, 2019, SPINE J, V19, P1202, DOI 10.1016/j.spinee.2019.02.006
   Suzuki H, 2016, J ARTHROPLASTY, V31, P317, DOI 10.1016/j.arth.2015.07.026
   Taki N, 2012, J ARTHROPLASTY, V27, P940, DOI 10.1016/j.arth.2011.10.003
   Tamura S, 2017, J ARTHROPLASTY, V32, P877, DOI 10.1016/j.arth.2016.08.035
   Tamura S, 2015, J ORTHOP RES, V33, P542, DOI 10.1002/jor.22799
   Teeter MG, 2018, J ARTHROPLASTY, V33, P263, DOI 10.1016/j.arth.2017.08.016
   Tezuka T, 2019, J ARTHROPLASTY, V34, P3, DOI 10.1016/j.arth.2018.10.034
   Tiberi JV, 2015, J ARTHROPLASTY, V30, P1555, DOI 10.1016/j.arth.2015.03.025
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Uemura K, 2019, J ORTHOP SURG-HONG K, V27, DOI 10.1177/2309499019828515
   Uemura K, 2018, J ARTHROPLASTY, V33, P595, DOI 10.1016/j.arth.2017.09.027
   Vrtovec T, 2012, SPINE J, V12, P433, DOI 10.1016/j.spinee.2012.02.013
   Weng WJ, 2016, EUR SPINE J, V25, P3608, DOI 10.1007/s00586-016-4444-1
   Widmer KH, 2007, INT ORTHOP, V31, pS29, DOI 10.1007/s00264-007-0429-3
   Yun H, 2018, J ARTHROPLASTY, V33, P1442, DOI 10.1016/j.arth.2017.11.069
   Zheng GY, 2009, COMPUT METH PROG BIO, V95, P236, DOI 10.1016/j.cmpb.2009.02.009
NR 61
TC 2
Z9 2
U1 1
U2 3
PU NATURE RESEARCH
PI BERLIN
PA HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY
SN 2045-2322
J9 SCI REP-UK
JI Sci Rep
PD SEP 29
PY 2020
VL 10
IS 1
AR 15944
DI 10.1038/s41598-020-72782-1
PG 10
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA NZ5GX
UT WOS:000577129100007
PM 32994419
OA gold, Green Published
DA 2022-02-03
ER

PT C
AU Soares, J
   Gaikwad, AN
AF Soares, Joyce
   Gaikwad, A. N.
GP IEEE
TI A Self Banking Biometric Machine with Fake Detection Applied to
   Fingerprint and Iris along with GSM Technology for OTP
SO 2016 INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING
   (ICCSP), VOL. 1
LA English
DT Proceedings Paper
CT IEEE International Conference on Communication and Signal Processing
   (ICCSP)
CY APR 06-08, 2016
CL Adhiparasakthi Engn Coll, Dept Elect & Commun Engn, Melmaruvathur, INDIA
HO Adhiparasakthi Engn Coll, Dept Elect & Commun Engn
DE Biometrics; Fingerprint Matching; Image Quality; Iris Recognition
AB The growing direct or spoofing fraudulent attacks of thieves has motivated us to focus our prime concern on the security over money transaction. The accuracy of biometrics in identification is increasing its usage extensively. The method proposed in this paper focuses on how the money transaction in an ATM machine will be secured by providing personal identification by analyzing biometrics like fingerprints and iris patterns which are known for their steadiness and diversity. Use of biometrics provides a paperless banking environment along with the smart ATM access. In this system the samples of the fingerprint and iris along with the registered mobile number of the customer needs to be collected and saved in the database by the banker if the customer is to access the ATM. The actual operation of the system begins when the customer is to access the ATM to make a money transaction. The fingerprint and iris samples will be captured and matched. The system will distinguish between the real legitimate trait and fake self manufactured synthetic or reconstructed samples by comparing it with the samples saved in the database during enrollment. After finding valid samples the system generates a 3 digit code which is received by the customer on his/her registered mobile number. This process is carried out using a GSM modem interfaced with the ARM7. The entered OTP will be checked, after the OTP is found valid the customer is allowed to make further transactions otherwise the account is blocked. The ATM terminal will also be secured from fire and thieves by including a thermistor and a tilt sensor in the system. The experiments were conducted in real time operation by first performing enrollment and then authentication for two individuals.
C1 [Soares, Joyce; Gaikwad, A. N.] Zeal Coll Engn, Elect & Telecommun Engn Dept, Pune, Maharashtra, India.
RP Soares, J (corresponding author), Zeal Coll Engn, Elect & Telecommun Engn Dept, Pune, Maharashtra, India.
EM joycejas@rediffmail.com; arungkwd47@gmail.com
CR Balwir S.P., 2014, INT J ADV RES COMPUT, V4
   Bigun J., 2006, COMP VIS PATT REC WO, P30
   Bowyler Kelvin. W., 2008, COMPUTER VISION IMAG, V110, P281
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Galbally J, 2012, FUTURE GENER COMP SY, V28, P311, DOI 10.1016/j.future.2010.11.024
   Goud D. Shelkar, 2012, GLOBAL J ADV ENG TEC
   Karovaliya Mohsin, INTERNATIONALCONFERE
   Khatmode Ranjit P, 2014, INT J EMERGING TECHN, V4
   Monaheng Matsoso Samuel, 2013, INT J INNOVATIVE RES, V2
   Moravec P, 2009, CEUR WORKSHOP PROCEE, V471, P80
   Nandakumar Karthik, 2015, IEEE SIGNAL PROCESSI
   Ne'ma B., 2009, INT ARAB J INFORM TE, V6
   Prabhakar S., 2003, IEEE Security & Privacy, V1, P33, DOI 10.1109/MSECP.2003.1193209
   Rowe R.K., ADV BIOMETRICS, V08, P3
   Sharma K. K., 2014, INT J ADV RES COMPUT, V4
   Wildes R., 2011, P IEEE WORKSH APPL C, P121
NR 16
TC 4
Z9 4
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-5090-0396-9
PY 2016
BP 508
EP 512
PG 5
WC Engineering, Electrical & Electronic; Telecommunications
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering; Telecommunications
GA BG7LT
UT WOS:000391430100108
DA 2022-02-03
ER

PT J
AU Convertini, N
   Dentamaro, V
   Impedovo, D
   Pirlo, G
AF Convertini, Nicola
   Dentamaro, Vincenzo
   Impedovo, Donato
   Pirlo, Giuseppe
TI Sit-to-Stand Test for Neurodegenerative Diseases Video Classification
SO INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE
LA English
DT Article
DE Computer vision; image processing; machine learning; behavioral
   biometric; sit-to-stand; neurodegenerative diseases classification
ID GAIT ANALYSIS; KINEMATIC THEORY; PERFORMANCE
AB In this extended version of this paper, an automatic video diagnosis system for dementia classification is presented. Starting from video recordings of patients and control subjects, performing sit-to-stand test, the designed system is capable of extracting relevant patterns for binary discern patients with dementia from healthy subjects. The original system achieved an accuracy 0.808 by using the rigorous inter-patient separation scheme especially suited for medical purposes. This separation scheme provides the use of some people for training and others, different, people for testing. The implementation of features from the kinematic theory of rapid human movement and its sigma-lognormal model together with classic features increased the overall accuracy of the system to 0.947 F1 score. In addition, multi-class classification was performed with the aim of classifying neurodegenerative disease severities. This work is an original and pioneering work on sit-to-stand video classification for neurodegenerative diseases, its novelties are on phases segmentation, experimental setup and the application of kinematic theory of rapid human movements to sit-to-stand videos for neurodegenerative disease assessment.
C1 [Convertini, Nicola; Dentamaro, Vincenzo; Impedovo, Donato; Pirlo, Giuseppe] Univ Bari Aldo Moro, Dept Comp, Via Orabona 4, Bari, Italy.
C3 Universita degli Studi di Bari Aldo Moro
RP Dentamaro, V (corresponding author), Univ Bari Aldo Moro, Dept Comp, Via Orabona 4, Bari, Italy.
EM nicola.convertini@uniba.it; vincenzo.dentamaro@uniba.it;
   donato.impedovo@uniba.it; giuseppe.pirlo@uniba.it
FU Regione Puglia POR Puglia FESR-FSE 2014-2020. Fondo Europeo Sviluppo
   Regionale. Azione 1.6-Avviso pubblico \InnoNetwork" [YJTGRA7]
FX This work is within the BESIDE project (No. YJTGRA7) funded by the
   Regione Puglia POR Puglia FESR-FSE 2014-2020. Fondo Europeo Sviluppo
   Regionale. Azione 1.6-Avviso pubblico \InnoNetwork".
CR Alzheimers Assoc, 2015, ALZHEIMERS DEMENT, V11, P332, DOI 10.1016/j.jalz.2015.02.003
   Applebaum EV, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0176946
   Bertram L, 2005, J CLIN INVEST, V115, P1449, DOI 10.1172/JCI24761
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Dauer W, 2003, NEURON, V39, P889, DOI 10.1016/S0896-6273(03)00568-3
   De Stefano C, 2019, PATTERN RECOGN LETT, V121, P37, DOI 10.1016/j.patrec.2018.05.013
   Dentamaro Vincenzo, 2020, Pattern Recognition and Artificial Intelligence. International Conference, ICPRAI 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12068), P596, DOI 10.1007/978-3-030-59830-3_52
   Dentamaro V, 2020, IEEE ACCESS, V8, P193966, DOI 10.1109/ACCESS.2020.3032202
   Dentamaro V, 2019, LECT NOTES COMPUT SC, V11752, P618, DOI 10.1007/978-3-030-30645-8_56
   Dentamaro V, 2018, INFORMATION, V9, DOI 10.3390/info9120317
   Duncan RP, 2011, ARCH PHYS MED REHAB, V92, P1431, DOI 10.1016/j.apmr.2011.04.008
   Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1
   Impedovo D, 2019, IEEE SIGNAL PROC LET, V26, P632, DOI 10.1109/LSP.2019.2902936
   Impedovo D, 2019, IEEE REV BIOMED ENG, V12, P209, DOI 10.1109/RBME.2018.2840679
   Li TP, 2018, IEEE T NEUR SYS REH, V26, P2189, DOI 10.1109/TNSRE.2018.2875738
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Matthew RP, 2017, P ANN INT IEEE EMBS, P1893, DOI 10.1109/EMBC.2017.8037217
   Muro-de-la-Herran A, 2014, SENSORS-BASEL, V14, P3362, DOI 10.3390/s140203362
   O'Reilly C., 2012, 2012 11th International Conference on Information Sciences, Signal Processing and their Applications (ISSPA), P787, DOI 10.1109/ISSPA.2012.6310660
   PLAMONDON R, 1995, BIOL CYBERN, V72, P295, DOI 10.1007/s004220050131
   Plamondon R, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00945
   PODSIADLO D, 1991, J AM GERIATR SOC, V39, P142, DOI 10.1111/j.1532-5415.1991.tb01616.x
   Ross BC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0087357
   Tsukahara A, 2010, ADV ROBOTICS, V24, P1615, DOI 10.1163/016918610X512622
   Welch G., 1994, INTRO KALMAN FILTER
   Whitney SL, 2005, PHYS THER, V85, P1034, DOI 10.1093/ptj/85.10.1034
   Whittle MW, 1996, HUM MOVEMENT SCI, V15, P369, DOI 10.1016/0167-9457(96)00006-1
   Zheng EH, 2014, INT J ADV ROBOT SYST, V11, DOI 10.5772/57788
NR 28
TC 0
Z9 0
U1 4
U2 4
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 0218-0014
EI 1793-6381
J9 INT J PATTERN RECOGN
JI Int. J. Pattern Recognit. Artif. Intell.
PD SEP 30
PY 2021
VL 35
IS 12
AR 2160003
DI 10.1142/S021800142160003X
PG 22
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WQ8UL
UT WOS:000714085600010
DA 2022-02-03
ER

PT C
AU Loegering, J
   Krause, K
   Ahlquist, J
   Webb, K
   Xu, KR
   Tran, N
   Greenhalgh, D
   Palmieri, T
AF Loegering, Julia
   Krause, Kevin
   Ahlquist, Jesse
   Webb, Kevin
   Xu, Karen
   Nam Tran
   Greenhalgh, David
   Palmieri, Tina
GP IEEE
TI Point-of-care 3D body-mapping for determining total body surface area of
   severely burned patients
SO 2019 IEEE HEALTHCARE INNOVATIONS AND POINT OF CARE TECHNOLOGIES
   (HI-POCT)
LA English
DT Proceedings Paper
CT IEEE-EMB-NIH Healthcare Innovations and Point-of-Care Technologies
   (HI-POCT)
CY NOV 20-22, 2019
CL Bethesda, MD
AB Total body surface area (TBSA) is a critical biometric for accurate body fluid restoration and drug dosing in medical treatments. However, current clinical equation calculations of TBSA are highly inaccurate, resulting in error up to 25%. Within burn care, this error leads to misinformed fluid resuscitation that result in increased medical complications. Our team sought to combine recently developed mathematical equations that are clinically unutilized with 3D scanning methods to better the accuracy of TBSA calculations in treatment. To bridge the gap between modern TBSA equations and the clinic, we developed an algorithm that indexes an equation best suited to a patient according to inputs such as age, height and weight. For patients that cannot be matched to an appropriate equation, our team developed a time-of-flight scanning protocol to capture 3D models of the human body. From these models, TBSA can be extrapolated finite analysis deconstruction and image processing tools. Our scanning device reduced error of TBSA to an average of 4% across all scanned subjects and it proved to be one of the first 3D scanning devices compatible to the clinic workflow.
C1 [Loegering, Julia; Nam Tran] UC Davis Hlth, Dept Pathol & Lab Med, Davis, CA 95616 USA.
   [Krause, Kevin; Ahlquist, Jesse] Univ Calif Davis, Davis, CA 95616 USA.
   [Webb, Kevin] Mayo Clin, Coll Med & Sci, Rochester, MN USA.
   [Xu, Karen] Indiana Unvers Sch Med, Indianapolis, IN USA.
   [Greenhalgh, David; Palmieri, Tina] UC Davis Hlth, Dept Surg, Davis, CA USA.
C3 University of California System; University of California Davis; Mayo
   Clinic
RP Loegering, J (corresponding author), UC Davis Hlth, Dept Pathol & Lab Med, Davis, CA 95616 USA.
EM jmloegeirng@ucdavis.edu; kjkrause@ucdavis.edu; jtahlquist@ucdavis.edu;
   kevwebb@ucdavis.edu; kakxu@ucdavis.edu; nktran@ucdavis.edu;
   dggreenhalgh@ucdavis.edu; tlpalmieri@ucdavis.edu
FU Department of Biomedical Engineering at University of California, Davis
FX This project was made possible by funding from the Department of
   Biomedical Engineering at University of California, Davis. Special
   thanks to Armaun Emami (teaching assistant), Alireza Tafazzol (teaching
   assistant), and Dr. Randy Carney from the Department of Biomedical
   Engineering for their guidance; Dr. David Greenhalgh, Leonard Sterling
   (R.N), and Dr. Soman Sen for their collaboration; Dr. Jennifer Choi for
   this opportunity; and Dr. Nam Tran for his support.
CR Haberal M, 2010, INDIAN J PLAST SURG, V43, pS29, DOI 10.4103/0970-0358.70715
   Klein MB, 2009, JAMA-J AM MED ASSOC, V302, P1774, DOI 10.1001/jama.2009.1548
   Kuehnapfel A, 2017, EUR J APPL PHYSIOL, V117, P371, DOI 10.1007/s00421-016-3525-5
   Livingston Lee, AM J PHYSL ENDOCRINO
   Redlarski G, 2016, SCI REP-UK, V6, DOI 10.1038/srep27966
   Schmid David M., 2015, PERSONAL INJURY LAW
   Weavind Liza, BURN SHOCK RESUSCITA
NR 7
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-7281-3812-1
PY 2019
BP 111
EP 114
PG 4
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Medical Informatics
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering; Medical Informatics
GA BP1WZ
UT WOS:000541005600029
DA 2022-02-03
ER

PT J
AU Khurana, V
   Gahalawat, M
   Kumar, P
   Roy, PP
   Dogra, DP
   Scheme, E
   Soleymani, M
AF Khurana, Vaishali
   Gahalawat, Monika
   Kumar, Pradeep
   Roy, Partha Pratim
   Dogra, Debi Prosad
   Scheme, Erik
   Soleymani, Mohammad
TI A Survey on Neuromarketing Using EEG Signals
SO IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS
LA English
DT Article
DE Electroencephalography; Neuroscience; Brain; Functional magnetic
   resonance imaging; Consumer behavior; Neural activity; Pricing;
   E-commerce; electroencephalography (EEG); neuromarketing; neuroscience
ID REMOVE MUSCLE ARTIFACTS; HIGH-RESOLUTION EEG; LONG-TERM-MEMORY; CONSUMER
   NEUROSCIENCE; ELECTROENCEPHALOGRAM EEG; SUBJECTIVE VALUE;
   SEX-DIFFERENCES; BRAIN; PREFERENCE; PATTERNS
AB Neuromarketing is the application of neuroscience to the understanding of consumer preferences toward products and services. As such, it studies the neural activity associated with preference and purchase intent. Neuromarketing is considered an emerging area of research, driven in part by the approximately 400 billion dollars spent annually on advertisement and promotion. Given the size of this market, even a slight improvement in performance can have an immense impact. Traditional approaches to marketing consider a posteriori user feedback in the form of questionnaires, product ratings, or review comments, but these approaches do not fully capture or explain the real-time decision-making process of consumers. Various physiological measurement techniques have been proposed to facilitate the recording of this crucial aspect of the decision-making process, including brain imaging techniques [functional magnetic resonance imaging (fMRI), electroencephalography (EEG), steady state topography (SST)], and various biometric sensors. The use of EEG in neuromarketing is especially promising. EEG detects the sequential changes of brain activity, without appreciable time delay, needed to assess both the unconscious reaction and sensory reaction of the customer. Several types of EEG devices are now available in the market, each with its own advantages and disadvantages. Researchers have conducted experiments using many of these devices, across different age groups and different categories of products. Because of the deep insights that can be gained, the field of neuromarketing research is carefully monitored by consumer and research protection groups to ensure that subjects are properly protected. This article surveys a range of considerations for EEG-based neuromarketing strategies, including the types of information that can be gathered, how marketing stimuli are presented to consumers, how such strategies may affect the consumer in terms of appeal and memory, machine learning techniques applied in the field, and the variety of challenges faced, including ethics, in this emerging field.
C1 [Khurana, Vaishali; Gahalawat, Monika; Roy, Partha Pratim] Indian Inst Technol Roorkee, Dept Comp Sci & Engn, Roorkee 247667, Uttar Pradesh, India.
   [Kumar, Pradeep; Scheme, Erik] Univ New Brunswick, Inst Biomed Engn, Fredericton, NB E3B 5A3, Canada.
   [Dogra, Debi Prosad] Indian Inst Technol Bhubaneswar, Sch Elect Sci, Bhubaneswar 752050, India.
   [Soleymani, Mohammad] Univ Southern Calif, Inst Creat Technol, Los Angeles, CA 90007 USA.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Roorkee; University of New Brunswick; Indian
   Institute of Technology System (IIT System); Indian Institute of
   Technology Bhubaneswar; University of Southern California
RP Kumar, P (corresponding author), Univ New Brunswick, Inst Biomed Engn, Fredericton, NB E3B 5A3, Canada.
EM vaishali.khurana27@gmail.com; monikagahlawat62@gmail.com;
   pkumar1@unb.ca; proy.fcs@iitr.ac.in; dpdogra@iitbbs.ac.in;
   escheme@unb.ca; soleymani@ict.usc.edu
RI Scheme, Erik/Q-6808-2017
OI Scheme, Erik/0000-0002-4421-1016; Roy, Partha Pratim/0000-0002-5735-5254
FU Army Research Office [W911NF-20-2-0053]
FX The work of Mohammad Soleymani was supported by the Army Research Office
   under Cooperative Agreement W911NF-20-2-0053.
CR Abdul-Latif AA, 2004, PROCEEDINGS OF THE 2004 INTELLIGENT SENSORS, SENSOR NETWORKS & INFORMATION PROCESSING CONFERENCE, P531
   Adhami M., 2013, INT J MOBILE MARKET, V8, P95
   Agarwal S, 2015, DECISION, V42, P457, DOI 10.1007/s40622-015-0113-1
   Aggarwal P, 2012, J CONSUM PSYCHOL, V22, P114, DOI 10.1016/j.jcps.2011.11.009
   Ahn M, 2014, SENSORS-BASEL, V14, P14601, DOI 10.3390/s140814601
   Alfadda Assim A, 2014, Int J Endocrinol, V2014, P794943, DOI [10.1155/2014/730218, 10.1155/2014/794943]
   Al-Qammaz A. Y., 2018, INT J ENG TECHNOL, V7, P146
   Al-Shargie F, 2016, BIOMED OPT EXPRESS, V7, P3882, DOI 10.1364/BOE.7.003882
   Alnemari M., 2017, THESIS UC IRVINE CA
   Dos Santos MA, 2018, INT J SPORT MARK SPO, V19, P25, DOI 10.1108/IJSMS-09-2016-0067
   Alvino L., 2018, INT J MARK STUD, V10, P90, DOI [10.5539/ijms.v10n1p90, DOI 10.5539/IJMS.V10N1P90, DOI 10.5539/ijms.v10n1p90]
   AMBLER T, 2000, BUS STRAT REV, V11, P17, DOI DOI 10.1111/1467-8616.00144
   An X, 2014, LECT N BIOINFORMAT, V8590, P203, DOI 10.1007/978-3-319-09330-7_25
   Anderer P, 1999, NEUROPSYCHOBIOLOGY, V40, P150, DOI 10.1159/000026613
   Andrew J., 1999, INT J RES ANAL REV, V16, P545
   Uriguen JA, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/3/031001
   Ariely D, 2010, NAT REV NEUROSCI, V11, P284, DOI 10.1038/nrn2795
   Astolfi Laura, 2009, Comput Intell Neurosci, P652078, DOI 10.1155/2009/652078
   Babiloni F, 2005, NEUROIMAGE, V24, P118, DOI 10.1016/j.neuroimage.2004.09.036
   Bagozzi RP, 2019, ORGAN RES METHODS, V22, P299, DOI 10.1177/1094428117697042
   Bakardjieva E, 2017, ETHICS BEHAV, V27, P179, DOI 10.1080/10508422.2016.1162719
   Balconi M., 2014, NEUROPSYCHOL TRENDS, V16, P15, DOI [DOI 10.7358/NEUR-2014-016-BALC, 10.7358/neur-2014-016-balc]
   Baraybar-Fernandez A, 2017, COMUNICAR, V25, P19, DOI 10.3916/C52-2017-02
   Barbosa I. B., 2015, P NIK, P1
   Bashivan P., 2015, ARXIV 2015 151106448
   Bastiaansen M, 2018, J DESTIN MARK MANAGE, V7, P76, DOI 10.1016/j.jdmm.2016.09.003
   Bercik J., 2015, P 143 JOINT EAAE AAE
   Berns GS, 2012, J CONSUM PSYCHOL, V22, P154, DOI 10.1016/j.jcps.2011.05.001
   Bhardwaj A, 2015, 2ND INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED NETWORKS (SPIN) 2015, P180, DOI 10.1109/SPIN.2015.7095376
   Bothma C, 2016, RETAIL MARK REV, V12, P92
   Boz H, 2017, TOUR MANAG PERSPECT, V23, P119, DOI 10.1016/j.tmp.2017.06.002
   Braeutigam S, 2004, EUR J NEUROSCI, V20, P293, DOI 10.1111/j.1460-9568.2004.03467.x
   Cao T, 2014, BIOMED ENG ONLINE, V13, DOI 10.1186/1475-925X-13-28
   Cartocci G, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3795325
   Catrambone V, 2019, IEEE T NEUR SYS REH, V27, P411, DOI 10.1109/TNSRE.2019.2898469
   Chan HL, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00066
   Chark R., 2018, INNOVATIVE RES METHO, P179
   Chen X, 2018, IEEE T INSTRUM MEAS, V67, P359, DOI 10.1109/TIM.2017.2759398
   Chen X, 2017, IEEE T INSTRUM MEAS, V66, P1770, DOI 10.1109/TIM.2016.2608479
   Chew LH, 2016, COGN NEURODYNAMICS, V10, P165, DOI 10.1007/s11571-015-9363-z
   Cho ZH, 1998, P NATL ACAD SCI USA, V95, P2670, DOI 10.1073/pnas.95.5.2670
   Chu C, 2012, NEUROIMAGE, V60, P59, DOI 10.1016/j.neuroimage.2011.11.066
   Costa J. V., 2015, MARKET REV, V15, P1
   Cote-Allard U, 2020, FRONT BIOENG BIOTECH, V8, DOI 10.3389/fbioe.2020.00158
   Dapkevicius A., 2009, MOKSLAS LIETUVOS ATE, V1, P17
   Darabi M., 2018, J ADV SPORT TECH, V2, P15
   Das Ratan, 2015, 2015 IEEE Power & Energy Society General Meeting, DOI 10.1109/PESGM.2015.7286018
   Das R, 2016, IEEE SIGNAL PROC LET, V23, P341, DOI 10.1109/LSP.2016.2516043
   DAVIDSON RJ, 1976, BIOL PSYCHOL, V4, P119, DOI 10.1016/0301-0511(76)90012-0
   de Beeck H. O., 2019, INTRO HUMAN NEUROIMA
   De Clercq W, 2006, IEEE T BIO-MED ENG, V53, P2583, DOI 10.1109/TBME.2006.879459
   Deitz GD, 2016, J ADVERTISING RES, V56, P217, DOI 10.2501/JAR-2016-030
   Delorme A, 2004, COGNITIVE BRAIN RES, V19, P103, DOI 10.1016/j.cogbrainres.2003.11.010
   Delorme A, 2004, J NEUROSCI METH, V134, P9, DOI 10.1016/j.jneumeth.2003.10.009
   Djamal E. C., 2017, J TELECOMMUN ELECT C, V9, P105
   Dmochowski JP, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5567
   Dmochowski JP, 2012, FRONT HUM NEUROSCI, V6, DOI 10.3389/fnhum.2012.00112
   Dushanova J, 2014, ADV MED SCI-POLAND, V59, P61, DOI 10.1016/j.advms.2013.08.002
   Eckert MA, 2008, JARO-J ASSOC RES OTO, V9, P252, DOI 10.1007/s10162-008-0113-3
   Erk S, 2002, NEUROREPORT, V13, P2499, DOI 10.1097/00001756-200212200-00024
   Esch FR, 2012, J CONSUM PSYCHOL, V22, P75, DOI 10.1016/j.jcps.2010.08.004
   Eser Z, 2011, J MARKET MANAG-UK, V27, P854, DOI 10.1080/02672571003719070
   Estes Z, 2012, J CONSUM PSYCHOL, V22, P86, DOI 10.1016/j.jcps.2011.11.002
   Fatourechi M, 2007, CLIN NEUROPHYSIOL, V118, P480, DOI 10.1016/j.clinph.2006.10.019
   Faw B, 2003, CONSCIOUS COGN, V12, P83, DOI 10.1016/S1053-8100(02)00030-2
   Fehr E, 2005, AM ECON REV, V95, P346, DOI 10.1257/000282805774669736
   Ferrier D., 1886, FUNCTIONS BRAIN, V2nd
   Fulcher E, 2016, WOODHEAD PUBL FOOD S, V296, P121, DOI 10.1016/B978-0-08-100356-5.00006-1
   Garcia J. R., 2008, J CONSUM BEHAV, V7, P397, DOI [10.1002/cb.259, DOI 10.1002/CB.259]
   Gauba H, 2017, NEURAL NETWORKS, V92, P77, DOI 10.1016/j.neunet.2017.01.013
   Gentsch A., 2016, AFFECTIVE TOUCH NEUR, P355, DOI DOI 10.1007/978-1-4939-6418-5_21
   Golnar-Nik P, 2019, PHYSIOL BEHAV, V207, P90, DOI 10.1016/j.physbeh.2019.04.025
   Guger C, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00060
   Guo G., 2013, J ADV MANAG SCI, P61
   Gupta A, 2017, IOP CONF SER-MAT SCI, V225, DOI 10.1088/1757-899X/225/1/012129
   Gurbuz F., 2018, P IEEE 2 INT S MULT, P1
   Hershey N, DETECTING EPILEPTIC
   Hillenbrand P, 2013, J PROD BRAND MANAG, V22, P300, DOI 10.1108/JPBM-04-2012-0120
   Hosseini M.-P., 2017, CLOUD BASED DEEP LEA
   Hubert M, 2008, J CONSUMER BEHAV, V7, P272, DOI DOI 10.1002/CB.251
   Hussin SS, 2013, PROCEDIA ENGINEER, V53, P288, DOI 10.1016/j.proeng.2013.02.038
   Hynes CA, 2006, NEUROPSYCHOLOGIA, V44, P374, DOI 10.1016/j.neuropsychologia.2005.06.011
   Ioannides AA, 2000, BRAIN TOPOGR, V13, P11, DOI 10.1023/A:1007878001388
   Javor A, 2013, BMC NEUROL, V13, DOI 10.1186/1471-2377-13-13
   Jirayucharoensak S, 2014, SCI WORLD J, DOI 10.1155/2014/627892
   Johal PK, 2016, 2016 INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS, AND OPTIMIZATION TECHNIQUES (ICEEOT), P2088, DOI 10.1109/ICEEOT.2016.7755056
   Joy M. M., 2018, P NAT C NEW AG MARK
   Jung TP, 2000, PSYCHOPHYSIOLOGY, V37, P163, DOI 10.1017/S0048577200980259
   Jung TP, 1998, NEURAL NETWORKS FOR SIGNAL PROCESSING VIII, P63, DOI 10.1109/NNSP.1998.710633
   Kable JW, 2007, NAT NEUROSCI, V10, P1625, DOI 10.1038/nn2007
   Kawasaki M, 2012, NEUROIMAGE, V59, P808, DOI 10.1016/j.neuroimage.2011.07.042
   Kenning P, 2011, J VERBRAUCH LEBENSM, V6, P111, DOI 10.1007/s00003-010-0652-5
   Khurana V, 2018, COGN SYST RES, V49, P33, DOI 10.1016/j.cogsys.2017.11.003
   Khushaba R. N., 2012, P INT JOINT C NEUR N, P1
   Khushaba RN, 2013, EXPERT SYST APPL, V40, P3803, DOI 10.1016/j.eswa.2012.12.095
   Khushaba RN, 2012, EXPERT SYST APPL, V39, P12378, DOI 10.1016/j.eswa.2012.04.084
   Krauss P, 2018, FRONT HUM NEUROSCI, V12, DOI 10.3389/fnhum.2018.00121
   Krishnaveni V., 2006, MEAS SCI REV, V6, P45
   Kropotov J, 2016, FRONT AGING NEUROSCI, V8, DOI 10.3389/fnagi.2016.00069
   Kumar S, 2015, UNIVERSAL J MANAGEME, V3, P524, DOI [10.13189/ujm.2015.031208, DOI 10.13189/UJM.2015.031208, DOI 10.13189/ujm.2015.031208]
   Kumar S, 2019, INFORM FUSION, V52, P41, DOI 10.1016/j.inffus.2018.11.001
   Lee EJ, 2014, J BUS ETHICS, V122, P511, DOI 10.1007/s10551-013-1775-2
   Lee N, 2007, INT J PSYCHOPHYSIOL, V63, P199, DOI 10.1016/j.ijpsycho.2006.03.007
   Lee N, 2017, J MARKET MANAG-UK, V33, P878, DOI 10.1080/0267257X.2017.1327249
   Lees M, 2015, REV PRODUCT INTEGRAT
   Levallois C, 2012, NAT REV NEUROSCI, V13, P789, DOI 10.1038/nrn3354
   Levy DJ, 2011, J NEUROSCI, V31, P14693, DOI 10.1523/JNEUROSCI.2218-11.2011
   Li R, 2021, IEEE T PATTERN ANAL, V43, P316, DOI 10.1109/TPAMI.2020.2973153
   Li YT, 2017, ADV NEUR IN, V30
   Light Gregory A, 2010, Curr Protoc Neurosci, VChapter 6, DOI 10.1002/0471142301.ns0625s52
   Lipton ZC, 2018, COMMUN ACM, V61, P36, DOI 10.1145/3233231
   Litt A, 2012, J CONSUM PSYCHOL, V22, P55, DOI 10.1016/j.jcps.2011.11.007
   Luck SJ, 2014, INTRODUCTION TO THE EVENT-RELATED POTENTIAL TECHNIQUE, 2ND EDITION, P1
   Ludwig KA, 2009, J NEUROPHYSIOL, V101, P1679, DOI 10.1152/jn.90989.2008
   Lundberg SM, 2017, ADV NEUR IN, V30
   Madan CR, 2010, EUREKA, V1, P34
   Marco S., 2008, J CONSUM BEHAV, V7, P342, DOI DOI 10.1002/CB.256
   Marcut L., 2018, P INT EC C SIB, P143
   McCabe C, 2008, SOC COGN AFFECT NEUR, V3, P97, DOI 10.1093/scan/nsn005
   McClure SM, 2004, NEURON, V44, P379, DOI 10.1016/j.neuron.2004.09.019
   McFarland D. J., 2010, CURRENT OPIN BIOMED, V21, P451
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Milosavljevic M, 2012, J CONSUM PSYCHOL, V22, P67, DOI 10.1016/j.jcps.2011.10.002
   Minguillon J, 2017, BIOMED SIGNAL PROCES, V31, P407, DOI 10.1016/j.bspc.2016.09.005
   Morin C, 2011, SOCIETY, V48, P131, DOI 10.1007/s12115-010-9408-1
   Murugappan M, 2014, 2014 IEEE 10TH INTERNATIONAL COLLOQUIUM ON SIGNAL PROCESSING & ITS APPLICATIONS (CSPA 2014), P25, DOI 10.1109/CSPA.2014.6805714
   Murugappan M., 2011, Proceedings of the 2011 International Conference on Pattern Analysis and Intelligent Robotics (ICPAIR 2011), P148, DOI 10.1109/ICPAIR.2011.5976886
   Noreika V, 2020, INFANT BEHAV DEV, V58, DOI 10.1016/j.infbeh.2019.101393
   Ohme R, 2009, J NEUROSCI PSYCHOL E, V2, P21, DOI DOI 10.1037/A0015462
   Ohme R, 2010, J ECON PSYCHOL, V31, P785, DOI 10.1016/j.joep.2010.03.008
   Pani D, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00090
   Pawar Dipti, 2020, IAENG International Journal of Computer Science, V47
   Pereda E, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0201660
   Perrier J, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0116864
   Phillips LH, 2002, J GERONTOL B-PSYCHOL, V57, pP526, DOI 10.1093/geronb/57.6.P526
   Plakhin A., 2018, P MATEC WEB C, V184
   Plassmann H., 2011, INT ENCY MARKETING
   Plassmann H, 2007, J NEUROSCI, V27, P9984, DOI 10.1523/JNEUROSCI.2131-07.2007
   Quevedo WX, 2018, LECT NOTES COMPUT SC, V10851, P176, DOI 10.1007/978-3-319-95282-6_13
   Rakshit A., 2016, P IEEE 1 INT C PROC, P1
   Ralchle ME, 2007, NEUROIMAGE, V37, P1083, DOI 10.1016/j.neuroimage.2007.02.041
   Rashid M, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.00025
   Reimann M, 2012, J CONSUM PSYCHOL, V22, P128, DOI 10.1016/j.jcps.2011.11.003
   Reimann M, 2010, J CONSUM PSYCHOL, V20, P431, DOI 10.1016/j.jcps.2010.06.009
   Rolls ET, 2010, NEUROSCI BIOBEHAV R, V34, P237, DOI 10.1016/j.neubiorev.2008.03.010
   Rossiter JR, 2001, J ADVERTISING RES, V41, P13, DOI 10.2501/JAR-41-2-13-21
   Saad G, 2012, J CONSUM PSYCHOL, V22, P102, DOI 10.1016/j.jcps.2011.10.001
   SAMSON S, 1991, J EXP PSYCHOL LEARN, V17, P793, DOI 10.1037/0278-7393.17.4.793
   Schirrmeister RT, 2017, HUM BRAIN MAPP, V38, P5391, DOI 10.1002/hbm.23730
   Schulz E, 2012, CEREB CORTEX, V22, P1118, DOI 10.1093/cercor/bhr186
   Schwarzkopf S., 2010, ERNEST DICHTER MOTIV, P269
   Senior C, 2003, NEURON, V38, P525, DOI 10.1016/S0896-6273(03)00293-9
   Seric N, 2015, TOUR S E EUROPE, V3, P429
   Simor P, 2016, J SLEEP RES, V25, P269, DOI 10.1111/jsr.12376
   Solomon P. R., BIOMED J SCI TECH RE, V12, P9136
   Morillo LMS, 2016, BIOMED ENG ONLINE, V15, DOI 10.1186/s12938-016-0181-2
   Morillo LMS, 2015, LECT N BIOINFORMAT, V9044, P701, DOI 10.1007/978-3-319-16480-9_68
   Spence C., 2019, MULTISENSORY PACKAGI, P319
   Srinivasan, 2006, ELECT FIELDS BRAIN N
   Stanton SJ, 2017, J BUS ETHICS, V144, P799, DOI 10.1007/s10551-016-3059-0
   Stipp H, 2015, J ADVERTISING RES, V55, P120, DOI 10.2501/JAR-55-2-120-122
   Sweeney KT, 2012, IEEE T INF TECHNOL B, V16, P488, DOI 10.1109/TITB.2012.2188536
   Talukdar U., 2016, P INT C INT HUM COMP, P122
   Tantanatewin W, 2016, J ENVIRON PSYCHOL, V46, P197, DOI 10.1016/j.jenvp.2016.04.015
   Teo J, 2017, AIP CONF PROC, V1891, DOI 10.1063/1.5005474
   Tomescu MI, 2018, DEV COGN NEUROS-NETH, V31, P58, DOI 10.1016/j.dcn.2018.04.011
   Treleaven-Hassard S, 2010, J ECON PSYCHOL, V31, P777, DOI 10.1016/j.joep.2010.03.007
   Tusche A, 2010, J NEUROSCI, V30, P8024, DOI 10.1523/JNEUROSCI.0064-10.2010
   Ueda K., 2017, EMOTIONAL ENG, V5, P31
   Vecchiato G, 2010, J NEUROSCI METH, V191, P283, DOI 10.1016/j.jneumeth.2010.07.009
   Vecchiato G, 2011, MED BIOL ENG COMPUT, V49, P579, DOI 10.1007/s11517-011-0747-x
   Venkatraman V, 2012, J CONSUM PSYCHOL, V22, P143, DOI 10.1016/j.jcps.2011.11.008
   Viola FC, 2009, CLIN NEUROPHYSIOL, V120, P868, DOI 10.1016/j.clinph.2009.01.015
   WADA Y, 1994, CLIN ELECTROENCEPHAL, V25, P81, DOI 10.1177/155005949402500209
   Wang Y., 2018, P IEEE C REC IMT MAY, P1, DOI [10.1109/ijcnn.2018.8489715, DOI 10.1109/IJCNN.2018.8489715]
   Wang YZ, 2015, IEEE T AUTON MENT DE, V7, P248, DOI 10.1109/TAMD.2015.2434733
   Weber B, 2016, STUD NEUROSCI, P333, DOI 10.1007/978-3-642-35923-1_17
   Wei Z, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00076
   Yadava M, 2017, MULTIMED TOOLS APPL, V76, P19087, DOI 10.1007/s11042-017-4580-6
   Yanagimoto M, 2016, 2016 IEEE 9TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL INTELLIGENCE AND APPLICATIONS (IWCIA), P27, DOI 10.1109/IWCIA.2016.7805744
   Yang SM, 2020, IEEE T NEUR NET LEAR, V31, P148, DOI 10.1109/TNNLS.2019.2899936
   Yang SH, 2022, IEEE T ANTENN PROPAG, V70, P197, DOI [10.1109/TAP.2021.3098589, 10.1109/TNNLS.2020.3045492]
   Yilmaz B, 2014, COMPUT METH PROG BIO, V113, P705, DOI 10.1016/j.cmpb.2013.11.010
   Yu WJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00779
   Yuan H, 2016, BRAIN CONNECT, V6, P122, DOI 10.1089/brain.2014.0336
   Yuanfang Ren, 2014, 2014 International Joint Conference on Neural Networks (IJCNN), P2850, DOI 10.1109/IJCNN.2014.6889383
   Zarei R, 2017, COMPUT METH PROG BIO, V146, P47, DOI 10.1016/j.cmpb.2017.05.009
   Zhang Dalin, 2017, ARXIV170806578
   Zhang Y, 2016, IEEE T NEUR NET LEAR, V27, P2256, DOI 10.1109/TNNLS.2015.2476656
NR 189
TC 0
Z9 0
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2379-8920
EI 2379-8939
J9 IEEE T COGN DEV SYST
JI IEEE Trans. Cogn. Dev. Syst.
PD DEC
PY 2021
VL 13
IS 4
BP 732
EP 749
DI 10.1109/TCDS.2021.3065200
PG 18
WC Computer Science, Artificial Intelligence; Robotics; Neurosciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Robotics; Neurosciences & Neurology
GA XM6HH
UT WOS:000728925200005
DA 2022-02-03
ER

PT J
AU Sofka, M
   Zhang, JD
   Good, S
   Zhou, SK
   Comaniciu, D
AF Sofka, Michal
   Zhang, Jingdan
   Good, Sara
   Zhou, S. Kevin
   Comaniciu, Dorin
TI Automatic Detection and Measurement of Structures in Fetal Head
   Ultrasound Volumes Using Sequential Estimation and Integrated Detection
   Network (IDN)
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Fetal brain measurements; fetal head measurements; fetal utrasound;
   object detection; sequential sampling; three-dimensional (3-D)
   ultrasound
ID SEGMENTATION; IMAGES; RECOGNITION; SONOGRAPHY; OBJECTS; FACE
AB Routine ultrasound exam in the second and third trimesters of pregnancy involves manually measuring fetal head and brain structures in 2-D scans. The procedure requires a sonographer to find the standardized visualization planes with a probe and manually place measurement calipers on the structures of interest. The process is tedious, time consuming, and introduces user variability into the measurements. This paper proposes an automatic fetal head and brain (AFHB) system for automatically measuring anatomical structures from 3-D ultrasound volumes. The system searches the 3-D volume in a hierarchy of resolutions and by focusing on regions that are likely to be the measured anatomy. The output is a standardized visualization of the plane with correct orientation and centering as well as the biometric measurement of the anatomy. The system is based on a novel framework for detecting multiple structures in 3-D volumes. Since a joint model is difficult to obtain in most practical situations, the structures are detected in a sequence, one-by-one. The detection relies on Sequential Estimation techniques, frequently applied to visual tracking. The interdependence of structure poses and strong prior information embedded in our domain yields faster and more accurate results than detecting the objects individually. The posterior distribution of the structure pose is approximated at each step by sequential Monte Carlo. The samples are propagated within the sequence across multiple structures and hierarchical levels. The probabilistic model helps solve many challenges present in the ultrasound images of the fetus such as speckle noise, signal drop-out, shadows caused by bones, and appearance variations caused by the differences in the fetus gestational age. This is possible by discriminative learning on an extensive database of scans comprising more than two thousand volumes and more than thirteen thousand annotations. The average difference between ground truth and automatic measurements is below 2 mm with a running time of 6.9 s (GPU) or 14.7 s (CPU). The accuracy of the AFHB system is within inter-user variability and the running time is fast, which meets the requirements for clinical use.
C1 [Sofka, Michal; Zhang, Jingdan] Siemens Corp, Corp Technol, Princeton, NJ 08540 USA.
   [Good, Sara] Siemens Healthcare, Ultrasound Div, Mountain View, CA 94043 USA.
   [Zhou, S. Kevin; Comaniciu, Dorin] Siemens Corp, Corp Technol, Imaging & Comp Vis, Princeton, NJ 08540 USA.
C3 Siemens AG; Siemens AG; Siemens AG
RP Sofka, M (corresponding author), Cisco Syst, Prague 12000 2, Czech Republic.
EM msofka@cisco.com
OI Comaniciu, Dorin/0000-0002-5238-8647
CR Anquez J, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: FROM NANO TO MACRO, VOLS 1 AND 2, P109, DOI 10.1109/ISBI.2009.5192995
   Benacerraf BR, 2005, J ULTRAS MED, V24, P371, DOI 10.7863/jum.2005.24.3.371
   Benacerraf BR, 2002, J ULTRAS MED, V21, P1063, DOI 10.7863/jum.2002.21.10.1063
   Butko NJ, 2009, PROC CVPR IEEE, P2743
   Carneiro G, 2008, PROC CVPR IEEE, P133
   Carneiro G, 2008, IEEE T MED IMAGING, V27, P1342, DOI 10.1109/TMI.2008.928917
   Caughey A. B., 2006, OBSTET GYNECOLOGY
   Chalana V, 1997, IEEE T MED IMAGING, V16, P642, DOI 10.1109/42.640755
   Chan LW, 2009, ULTRASOUND OBST GYN, V33, P447, DOI 10.1002/uog.6321
   Chen T, 2009, IEEE I CONF COMP VIS, P795, DOI 10.1109/ICCV.2009.5459243
   Crandall D, 2005, PROC CVPR IEEE, P10
   Criminisi A, 2013, MED IMAGE ANAL, V17, P1293, DOI 10.1016/j.media.2013.01.001
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Desai C, 2009, IEEE I CONF COMP VIS, P229, DOI 10.1109/ICCV.2009.5459256
   Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532
   Doucet A., 2001, SEQUENTIAL MONTE CAR
   Efron B., 1993, INTRO BOOTSTRAP, V57
   Feng SL, 2009, PROC CVPR IEEE, P2480
   Gallage C. P., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587799
   Goncalves LF, 2005, J ULTRAS MED, V24, P1599, DOI 10.7863/jum.2005.24.12.1599
   Gooding MJ, 2008, ULTRASOUND MED BIOL, V34, P183, DOI 10.1016/j.ultrasmedbio.2007.07.023
   Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5
   Hong W, 2006, LECT NOTES COMPUT SC, V3954, P397
   Joachims T, 2009, COMMUN ACM, V52, P97, DOI 10.1145/1592761.1592783
   Juang R, 2011, I S BIOMED IMAGING, P606, DOI 10.1109/ISBI.2011.5872480
   Kumar S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1150, DOI 10.1109/ICCV.2003.1238478
   Liu D, 2010, PROC CVPR IEEE, P2831, DOI 10.1109/CVPR.2010.5540016
   Liu JS, 2001, STAT ENG IN, P225
   Lu W, 2005, ULTRASOUND MED BIOL, V31, P929, DOI 10.1016/j.ultrasmedbio.2005.04.002
   Malinger G, 2007, ULTRASOUND OBST GYN, V29, P109, DOI 10.1002/uog.3909
   Noble JA, 2006, IEEE T MED IMAGING, V25, P987, DOI 10.1109/TMI.2006.877092
   Pauly O., 2012, MED IM COMP COMP ASS, V7512, P443
   Pauly O, 2012, LECT NOTES COMPUT SC, V7512, P443, DOI 10.1007/978-3-642-33454-2_55
   Quattoni A., 2004, NIPS, P1097
   Rahmatullah B, 2012, LECT NOTES COMPUT SC, V7512, P402, DOI 10.1007/978-3-642-33454-2_50
   Rahmatullah B, 2011, I S BIOMED IMAGING, P6, DOI 10.1109/ISBI.2011.5872342
   Romeny BMT, 1999, LECT NOTES COMPUT SC, V1613, P56
   Rousseau F, 2005, LECT NOTES COMPUT SC, V3749, P548
   Sapp B, 2010, LECT NOTES COMPUT SC, V6312, P406, DOI 10.1007/978-3-642-15552-9_30
   Sauerbrei E. E., 1998, PRACTICAL GUIDE ULTR
   Shen DG, 2003, IEEE T MED IMAGING, V22, P539, DOI 10.1109/TMI.2003.809057
   Sofka M, 2012, METHOD INFORM MED, V51, P268, DOI 10.3414/ME11-02-0017
   Sofka M, 2011, I S BIOMED IMAGING, P294, DOI 10.1109/ISBI.2011.5872409
   Sofka M, 2010, PROC CVPR IEEE, P1735, DOI 10.1109/CVPR.2010.5539842
   Taskar B., 2005, P 22 INT C MACH LEAR, P896, DOI DOI 10.1145/1102351.1102464
   Torralba A, 2004, PROC CVPR IEEE, P762
   Triggs B., 2005, PROC CVPR IEEE, V1, P886, DOI DOI 10.1109/CVPR.2005.177
   Tu Z, 2008, IEEE T MED IMAGING, V27, P495, DOI 10.1109/TMI.2007.908121
   Tu ZW, 2005, IEEE I CONF COMP VIS, P1589
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Yang L, 2011, IEEE T MED IMAGING, V30, P1921, DOI 10.1109/TMI.2011.2158440
   Yaqub M, 2012, 2012 9TH IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING (ISBI), P1555, DOI 10.1109/ISBI.2012.6235870
   Zhan YQ, 2008, LECT NOTES COMPUT SC, V5241, P313, DOI 10.1007/978-3-540-85988-8_38
   Zhan YQ, 2003, LECT NOTES COMPUT SC, V2878, P688
   Zhang J., 2007, P COMP VIS PATT REC, P1
   Zhang L, 2012, MED PHYS, V39, P5015, DOI 10.1118/1.4736415
NR 56
TC 20
Z9 20
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2014
VL 33
IS 5
BP 1054
EP 1070
DI 10.1109/TMI.2014.2301936
PG 17
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA AG4HF
UT WOS:000335379500005
PM 24770911
OA Green Published
DA 2022-02-03
ER

PT J
AU Einy, S
   Oz, C
   Navaei, YD
AF Einy, Sajad
   Oz, Cemil
   Navaei, Yahya Dorostkar
TI IoT Cloud-Based Framework for Face Spoofing Detection with Deep
   Multicolor Feature Learning Model
SO JOURNAL OF SENSORS
LA English
DT Article
ID MUTUAL INFORMATION; PATTERN; SYSTEM
AB A face-based authentication system has become an important topic in various fields of IoT applications such as identity validation for social care, crime detection, ATM access, computer security, etc. However, these authentication systems are vulnerable to different attacks. Presentation attacks have become a clear threat for facial biometric-based authentication and security applications. To address this issue, we proposed a deep learning approach for face spoofing detection systems in IoT cloud-based environment. The deep learning approach extracted features from multicolor space to obtain more information from the input face image regarding luminance and chrominance data. These features are combined and selected by the Minimum Redundancy Maximum Relevance (mRMR) algorithm to provide an efficient and discriminate feature set. Finally, the extracted deep color-based features of the face image are used for face spoofing detection in a cloud environment. The proposed method achieves stable results with less training data compared to conventional deep learning methods. This advantage of the proposed approach reduces the time of processing in the training phase and optimizes resource management in storing training data on the cloud. The proposed system was tested and evaluated based on two challenging public access face spoofing databases, namely, Replay-Attack and ROSE-Youtu. The experimental results based on these databases showed that the proposed method achieved satisfactory results compared to the state-of-the-art methods based on an equal error rate (EER) of 0.2% and 3.8%, respectively, for the Replay-Attack and ROSE-Youtu databases.
C1 [Einy, Sajad; Oz, Cemil] Sakarya Univ, Dept Comp Engn, Sakarya, Turkey.
   [Einy, Sajad] Istanbul Aydin Univ, Applicat & Res Ctr Adv Studies, Istanbul, Turkey.
   [Navaei, Yahya Dorostkar] Islamic Azad Univ, Qazvin Branch, Comp & Informat Technol Engn, Qazvin, Iran.
C3 Sakarya University; Istanbul Aydin University; Islamic Azad University
RP Navaei, YD (corresponding author), Islamic Azad Univ, Qazvin Branch, Comp & Informat Technol Engn, Qazvin, Iran.
EM sajad.einy@ogr.sakarya.edu.tr; coz@sakarya.edu.tr;
   y.dorostkar@qiau.ac.ir
OI EINY, SAJAD/0000-0002-7923-5253
CR Ali Z, 2018, FUTURE GENER COMP SY, V85, P76, DOI 10.1016/j.future.2018.02.040
   Aloysius N, 2017, 2017 INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), P588, DOI 10.1109/ICCSP.2017.8286426
   Arora S, 2021, VISUAL COMPUT, DOI 10.1007/s00371-021-02123-4
   Boulkenafet Z, 2018, IMAGE VISION COMPUT, V77, P1, DOI 10.1016/j.imavis.2018.04.007
   Boulkenafet Z, 2016, IEEE T INF FOREN SEC, V11, P1818, DOI 10.1109/TIFS.2016.2555286
   Boulkenafet Z, 2015, IEEE IMAGE PROC, P2636, DOI 10.1109/ICIP.2015.7351280
   Chingovska I., P INT C BIOM SPEC IN, P1
   Nguyen DT, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18030699
   de Souza GB, 2017, IEEE T CIRCUITS-II, V64, P1397, DOI 10.1109/TCSII.2017.2764460
   Di Martino JM, 2021, IEEE T IMAGE PROCESS, V30, P1086, DOI 10.1109/TIP.2020.3042082
   Ding C, 2003, PROCEEDINGS OF THE 2003 IEEE BIOINFORMATICS CONFERENCE, P523, DOI 10.1109/CSB.2003.1227396
   Edmunds T, 2018, J VIS COMMUN IMAGE R, V50, P314, DOI 10.1016/j.jvcir.2017.12.004
   Erdogmus N., 2013, SPOOFING 2D FACE REC, P1
   Feng LT, 2016, J VIS COMMUN IMAGE R, V38, P451, DOI 10.1016/j.jvcir.2016.03.019
   George A, 2021, IEEE T INF FOREN SEC, V16, P361, DOI 10.1109/TIFS.2020.3013214
   Gomez-Barrero M, 2017, PATTERN RECOGN, V67, P149, DOI 10.1016/j.patcog.2017.01.024
   Gumaei A, 2019, J PARALLEL DISTR COM, V124, P27, DOI 10.1016/j.jpdc.2018.10.005
   Hasan M.M., 2019 4 INT C EL INF, P20, DOI [10.1109/EICT48899.2019.9068813, DOI 10.1109/EICT48899.2019.9068813]
   Jia S, 2021, PATTERN RECOGN LETT, V145, P103, DOI 10.1016/j.patrec.2021.01.021
   Komulainen J, 2013, INT CONF BIOMETR
   Kumari P, 2020, MICROPROCESS MICROSY, V79, DOI 10.1016/j.micpro.2020.103277
   Li HL, 2018, IEEE T INF FOREN SEC, V13, P2639, DOI 10.1109/TIFS.2018.2825949
   Li L, 2020, NEUROCOMPUTING, V409, P191, DOI 10.1016/j.neucom.2020.05.017
   Li L, 2018, J VIS COMMUN IMAGE R, V54, P182, DOI 10.1016/j.jvcir.2018.05.009
   Masud M, 2020, COMPUT COMMUN, V152, P215, DOI 10.1016/j.comcom.2020.01.050
   Pan G, 2007, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2007.4409068
   Parkhi OM, 2015, P BR MACH VIS, P1, DOI DOI 10.5244/C.29.41
   Peng F, 2020, J VIS COMMUN IMAGE R, V66, DOI 10.1016/j.jvcir.2019.102746
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Perez C, 2012, INT J OPTOMECHATRONI, V6, P92, DOI 10.1080/15599612.2012.663463
   Pinto A, 2015, IEEE T IMAGE PROCESS, V24, P4726, DOI 10.1109/TIP.2015.2466088
   Powers D. M. W., 2007, J MACH LEARN TECHNOL
   Raghavendra RJ, 2020, J INF SECUR APPL, V52, DOI 10.1016/j.jisa.2020.102482
   Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI [10.1162/NECO_a_00990, 10.1162/neco_a_00990]
   Rehman YAU, 2020, IMAGE VISION COMPUT, V94, DOI 10.1016/j.imavis.2019.103858
   Rehman YAU, 2020, EXPERT SYST APPL, V142, DOI 10.1016/j.eswa.2019.113002
   Rehman YAU, 2019, J VIS COMMUN IMAGE R, V59, P574, DOI 10.1016/j.jvcir.2019.02.014
   Shaki KA, 2020, J KING SAUD UNIV-COM, V32, P57, DOI 10.1016/j.jksuci.2017.07.001
   Simonyan K., 2015, P ICLR
   Song X, 2019, PATTERN RECOGN, V85, P220, DOI 10.1016/j.patcog.2018.08.019
   Szegedy C, 2017, THIRTY-FIRST AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4278
   Tirunagari S, 2015, IEEE T INF FOREN SEC, V10, P762, DOI 10.1109/TIFS.2015.2406533
   Togacar M, 2020, IRBM, V41, P212, DOI 10.1016/j.irbm.2019.10.006
   Vidya BS, 2019, ALEX ENG J, V58, P103, DOI 10.1016/j.aej.2018.12.008
   Wang GQ, 2021, IEEE T INF FOREN SEC, V16, P56, DOI 10.1109/TIFS.2020.3002390
   Wang YQ, 2014, IMAGE PROCESS ON LIN, V4, P128, DOI 10.5201/ipol.2014.104
   Wen D, 2015, IEEE T INF FOREN SEC, V10, P746, DOI 10.1109/TIFS.2015.2400395
   Yang Z., 2018 24 INT C PATT R, P338, DOI [10.1109/ICPR.2018.8546053, DOI 10.1109/ICPR.2018.8546053]
   Zhao PZ, 2016, 2016 IEEE INTERNATIONAL INSTRUMENTATION AND MEASUREMENT TECHNOLOGY CONFERENCE PROCEEDINGS, P1
NR 49
TC 0
Z9 0
U1 3
U2 3
PU HINDAWI LTD
PI LONDON
PA ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND
SN 1687-725X
EI 1687-7268
J9 J SENSORS
JI J. Sens.
PD AUG 31
PY 2021
VL 2021
AR 5047808
DI 10.1155/2021/5047808
PG 18
WC Engineering, Electrical & Electronic; Instruments & Instrumentation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering; Instruments & Instrumentation
GA UP3YT
UT WOS:000695319900003
OA gold
DA 2022-02-03
ER

PT C
AU Agnihotri, M
   Rathod, A
   Thapar, D
   Jaswal, G
   Tiwari, K
   Nigam, A
AF Agnihotri, Manish
   Rathod, Aditya
   Thapar, Daksh
   Jaswal, Gaurav
   Tiwari, Kamlesh
   Nigam, Aditya
BE DeMarsico, M
   DiBaja, GS
   Fred, A
TI Learning Domain Specific Features using Convolutional Autoencoder: A
   Vein Authentication Case Study using Siamese Triplet Loss Network
SO ICPRAM: PROCEEDINGS OF THE 8TH INTERNATIONAL CONFERENCE ON PATTERN
   RECOGNITION APPLICATIONS AND METHODS
LA English
DT Proceedings Paper
CT 8th International Conference on Pattern Recognition Applications and
   Methods (ICPRAM)
CY FEB 19-21, 2019
CL Prague, CZECH REPUBLIC
DE Siamese Network; Vascular Biometrics
ID FINGER-VEIN; FEATURE-EXTRACTION; PATTERNS
AB Recently, deep hierarchically learned models (such as CNN) have achieved superior performance in various computer vision tasks but limited attention has been paid to biometrics till now. This is major because of the number of samples available in biometrics are limited and are not enough to train CNN efficiently. However, deep learning often requires a lot of training data because of the huge number of parameters to be tuned by the learning algorithm. How about designing an end-to-end deep learning network to match the biometric features when the number of training samples is limited? To address this problem, we propose a new way to design an end-to-end deep neural network that works in two major steps: first an auto-encoder has been trained for learning domain specific features followed by a Siamese network trained via. triplet loss function for matching. A publicly available vein image data set has been utilized as a case study to justify our proposal. We observed that transformations learned from such a network provide domain specific and most discriminative vascular features. Subsequently, the corresponding traits are matched using multimodal pipelined end-to-end network in which the convolutional layers are pre-trained in an unsupervised fashion as an autoencoder. Thorough experimental studies suggest that the proposed framework consistently outperforms several state-of-the-art vein recognition approaches.
C1 [Agnihotri, Manish; Rathod, Aditya] Manipal Inst Technol Manipal, Dept Informat & Commun Technol, Manipal, India.
   [Thapar, Daksh; Jaswal, Gaurav; Nigam, Aditya] Indian Inst Technol Mandi, Sch Comp & Elect Engn, Suran, India.
   [Tiwari, Kamlesh] Birla Inst Technol & Sci Pilani, Dept Comp Sci & Informat Syst, Pilani, Rajasthan, India.
C3 Manipal Academy of Higher Education (MAHE); Indian Institute of
   Technology System (IIT System); Indian Institute of Technology (IIT) -
   Mandi; Birla Institute of Technology & Science Pilani (BITS Pilani)
RP Agnihotri, M (corresponding author), Manipal Inst Technol Manipal, Dept Informat & Commun Technol, Manipal, India.
RI Jaswal, Gaurav/AAE-4016-2021
OI Jaswal, Gaurav/0000-0002-3971-0160
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Beining Huang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1269, DOI 10.1109/ICPR.2010.316
   Bhilare S, 2018, MACH VISION APPL, V29, P1269, DOI 10.1007/s00138-018-0959-2
   Cummings AH, 2011, PATTERN RECOGN LETT, V32, P2053, DOI 10.1016/j.patrec.2011.08.020
   Fang Y., 2018, NEUROCOMPUTING
   Jaswal G, 2017, MULTIMED TOOLS APPL, V76, P18955, DOI 10.1007/s11042-017-4475-6
   Joon Hwan Choi, 2009, Proceedings of the SPIE - The International Society for Optical Engineering, V7251, DOI 10.1117/12.810458
   Kharola A, 2016, IEEE I C COMP INT CO, P13
   Kimura T, 2015, INT CONF BIOMETR, P519, DOI 10.1109/ICB.2015.7139068
   Miura N, 2004, MACH VISION APPL, V15, P194, DOI 10.1007/s00138-004-0149-2
   Nigam A, 2016, NEUROCOMPUTING, V188, P190, DOI 10.1016/j.neucom.2015.04.126
   Qin HF, 2017, IEEE T INF FOREN SEC, V12, P1816, DOI 10.1109/TIFS.2017.2689724
   Ramakrishnan M., 2011, 2011 IEEE EUROCON IN, P1
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Umer S, 2016, PATTERN ANAL APPL, V19, P283, DOI 10.1007/s10044-015-0482-2
   Veluchamy S, 2017, IET BIOMETRICS, V6, P232, DOI 10.1049/iet-bmt.2016.0112
   Wang HG, 2008, PATTERN RECOGN, V41, P1514, DOI 10.1016/j.patcog.2007.10.021
   Xie CH, 2017, ADV COMPUT VIS PATT, P109, DOI 10.1007/978-3-319-61657-5_5
   Yang JF, 2012, PATTERN RECOGN LETT, V33, P623, DOI 10.1016/j.patrec.2011.11.002
   Yang WM, 2014, INFORM SCIENCES, V268, P20, DOI 10.1016/j.ins.2013.10.010
   Zhou Y., 2010, 2010 4 IEEE INT C BI, P1, DOI DOI 10.1109/BTAS.2010.5634470
   Zhou YB, 2011, IEEE T INF FOREN SEC, V6, P1259, DOI 10.1109/TIFS.2011.2158423
NR 22
TC 1
Z9 1
U1 1
U2 1
PU SCITEPRESS
PI SETUBAL
PA AV D MANUELL, 27A 2 ESQ, SETUBAL, 2910-595, PORTUGAL
BN 978-989-758-351-3
PY 2019
BP 778
EP 785
DI 10.5220/0007568007780785
PG 8
WC Computer Science, Artificial Intelligence; Imaging Science &
   Photographic Technology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Imaging Science & Photographic Technology
GA BR5ZQ
UT WOS:000659174900088
OA hybrid
DA 2022-02-03
ER

PT J
AU Xing, S
   Nikolis, VC
   Kublitski, J
   Guo, EJ
   Jia, XK
   Wang, YZ
   Spoltore, D
   Vandewal, K
   Kleemann, H
   Benduhn, J
   Leo, K
AF Xing, Shen
   Nikolis, Vasileios Christos
   Kublitski, Jonas
   Guo, Erjuan
   Jia, Xiangkun
   Wang, Yazhong
   Spoltore, Donato
   Vandewal, Koen
   Kleemann, Hans
   Benduhn, Johannes
   Leo, Karl
TI Miniaturized VIS-NIR Spectrometers Based on Narrowband and Tunable
   Transmission Cavity Organic Photodetectors with Ultrahigh Specific
   Detectivity above 10(14) Jones
SO ADVANCED MATERIALS
LA English
DT Article
DE miniaturized spectrometers; organic photodetectors; transmission
   cavities; tunable spectra; wavelength selectivity
ID POLYMER; PHOTODIODES; DEVICES; FUTURE; LAYERS
AB Spectroscopic photodetection plays a key role in many emerging applications such as context-aware optical sensing, wearable biometric monitoring, and biomedical imaging. Photodetectors based on organic semiconductors open many new possibilities in this field. However, ease of processing, tailorable optoelectronic properties, and sensitivity for faint light are still significant challenges. Here, the authors report a novel concept for a tunable spectral detector by combining an innovative transmission cavity structure with organic absorbers to yield narrowband organic photodetection in the wavelength range of 400-1100 nm, fabricated in a full-vacuum process. Benefiting from this strategy, one of the best performed narrowband organic photodetectors is achieved with a finely wavelength-selective photoresponse (full-width-at-half-maximum of approximate to 40 nm), ultrahigh specific detectivity above 10(14) Jones, the maximum response speed of 555 kHz, and a large dynamic range up to 168 dB. Particularly, an array of transmission cavity organic photodetectors is monolithically integrated on a small substrate to showcase a miniaturized spectrometer application, and a true proof-of-concept transmission spectrum measurement is successfully demonstrated. The excellent performance, the simple device fabrication as well as the possibility of high integration of this new concept challenge state-of-the-art low-noise silicon photodetectors and will mature the spectroscopic photodetection into technological realities.
C1 [Xing, Shen; Nikolis, Vasileios Christos; Kublitski, Jonas; Guo, Erjuan; Jia, Xiangkun; Wang, Yazhong; Spoltore, Donato; Kleemann, Hans; Benduhn, Johannes; Leo, Karl] Tech Univ Dresden, Dresden Integrated Ctr Appl Phys & Photon Mat IAP, Nothnitzer Str 61, D-01187 Dresden, Germany.
   [Xing, Shen; Nikolis, Vasileios Christos; Kublitski, Jonas; Guo, Erjuan; Jia, Xiangkun; Wang, Yazhong; Spoltore, Donato; Kleemann, Hans; Benduhn, Johannes; Leo, Karl] Tech Univ Dresden, Inst Appl Phys, Nothnitzer Str 61, D-01187 Dresden, Germany.
   [Nikolis, Vasileios Christos] Heliatek GmbH, Treidler Str 3, D-01139 Dresden, Germany.
   [Vandewal, Koen] Hasselt Univ, Inst Mat Res IMO IMOMEC, Wetenschapspk 1, B-3590 Diepenbeek, Belgium.
C3 Technische Universitat Dresden; Technische Universitat Dresden; Hasselt
   University
RP Xing, S; Benduhn, J (corresponding author), Tech Univ Dresden, Dresden Integrated Ctr Appl Phys & Photon Mat IAP, Nothnitzer Str 61, D-01187 Dresden, Germany.; Xing, S; Benduhn, J (corresponding author), Tech Univ Dresden, Inst Appl Phys, Nothnitzer Str 61, D-01187 Dresden, Germany.
EM shen.xing@tu-dresden.de; johannes.benduhn@tu-dresden.de
RI Spoltore, Donato/I-9290-2012
OI Spoltore, Donato/0000-0002-2922-9293; , Shen/0000-0002-0637-3962;
   Kublitski, Jonas/0000-0003-0558-9152
FU China Scholarship CouncilChina Scholarship Council [201706070125,
   201706890003, 201706140127]; DFGGerman Research Foundation (DFG)European
   Commission [VA 1035/5-1]; Sachsische Aufbaubank [100325708]
FX S.X., E.G., and X.J. acknowledge the financial support from China
   Scholarship Council (nos. 201706070125, 201706890003, and 201706140127,
   respectively). Moreover, the authors thank for funding through the DFG
   project VA 1035/5-1 (Photogen) and the Sachsische Aufbaubank project no.
   100325708 (InfraKart). Additionally, they acknowledge Louis Conrad
   Winkler for discussion about the noise analysis, Dr. David Wynands for
   discussion about the evaporation mask design and the Senorics GmbH to
   support the miniaturized spectrometer fabrication.; Open access funding
   enabled and organized by Projekt DEAL.
CR Armin A, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7343
   Armin A, 2014, LASER PHOTONICS REV, V8, P924, DOI 10.1002/lpor.201400081
   de Arquer FPG, 2017, NAT REV MATER, V2, DOI 10.1038/natrevmats.2016.100
   Drechsel J., 2006, SID INT S, V37, P1692, DOI DOI 10.1889/1.2433332
   Fang YJ, 2019, NAT PHOTONICS, V13, P1, DOI 10.1038/s41566-018-0288-z
   Fang YJ, 2015, NAT PHOTONICS, V9, P679, DOI [10.1038/NPHOTON.2015.156, 10.1038/nphoton.2015.156]
   Gielen S, 2020, ADV MATER, V32, DOI 10.1002/adma.202003818
   Han JF, 2017, J MATER CHEM C, V5, P159, DOI 10.1039/c6tc05031j
   Holzmuller F, 2017, ORG ELECTRON, V45, P198, DOI 10.1016/j.orgel.2017.03.009
   Jang W, 2020, ADV FUNCT MATER, V30, DOI 10.1002/adfm.202001402
   Jansen-van Vuuren RD, 2016, ADV MATER, V28, P4766, DOI 10.1002/adma.201505405
   Jia XK, 2021, NANO ENERGY, V89, DOI 10.1016/j.nanoen.2021.106404
   Johnston MB, 2015, NAT PHOTONICS, V9, P633, DOI 10.1038/nphoton.2015.180
   Kim SK, 2018, MACROMOLECULES, V51, P8241, DOI 10.1021/acs.macromol.8b01751
   Koeppe R, 2003, APPL PHYS LETT, V82, P2601, DOI 10.1063/1.1565710
   Konstantatos G, 2007, NAT PHOTONICS, V1, P531, DOI 10.1038/nphoton.2007.147
   Kublitski J, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-020-20856-z
   Lan ZJ, 2020, ADV OPT MATER, V8, DOI 10.1002/adom.202001388
   Lee CC, 2020, ADV OPT MATER, V8, DOI 10.1002/adom.202000519
   Li TY, 2017, J AM CHEM SOC, V139, P13636, DOI 10.1021/jacs.7b07887
   Li W, 2019, ADV FUNCT MATER, V29, DOI 10.1002/adfm.201808948
   Li YQ, 2004, CHEM PHYS LETT, V386, P128, DOI 10.1016/j.cplett.2004.01.049
   Lin CL, 2005, APPL PHYS LETT, V87, DOI 10.1063/1.1988985
   Lin QQ, 2015, NAT PHOTONICS, V9, P687, DOI [10.1038/nphoton.2015.175, 10.1038/NPHOTON.2015.175]
   Lin QQ, 2015, ADV MATER, V27, P2060, DOI 10.1002/adma.201405171
   Lin T, 2019, ADV MATER, V31, DOI 10.1002/adma.201901473
   Liu C, 2016, NANO ENERGY, V30, P27, DOI 10.1016/j.nanoen.2016.09.035
   Liu GH, 2020, ACS APPL MATER INTER, V12, P17781, DOI 10.1021/acsami.0c00191
   Liu JS, 2020, SOL RRL, V4, DOI 10.1002/solr.202000139
   Liu XD, 2018, J MATER CHEM C, V6, P3499, DOI 10.1039/c7tc05042a
   Liu ZX, 2019, NANO ENERGY, V63, DOI 10.1016/j.nanoen.2019.06.003
   Lukac R, 2006, J REAL-TIME IMAGE PR, V1, P45, DOI 10.1007/s11554-006-0003-z
   Nishiwaki S, 2013, NAT PHOTONICS, V7, P240, DOI [10.1038/NPHOTON.2012.345, 10.1038/nphoton.2012.345]
   Pettersson LAA, 1999, J APPL PHYS, V86, P487, DOI 10.1063/1.370757
   Sergeeva N, 2018, PHYS REV APPL, V9, DOI 10.1103/PhysRevApplied.9.024039
   Shen L, 2016, ADV MATER, V28, P2043, DOI 10.1002/adma.201503774
   Siegmund B, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15421
   Sim KM, 2018, ACS APPL MATER INTER, V10, P8405, DOI 10.1021/acsami.8b01437
   Simone G, 2020, ADV FUNCT MATER, V30, DOI 10.1002/adfm.201904205
   Tang Z, 2017, ADV MATER, V29, DOI 10.1002/adma.201702184
   Wang HY, 2018, ACS APPL MATER INTER, V10, P3856, DOI 10.1021/acsami.7b15730
   Wang J, 2019, ACS PHOTONICS, V6, P1393, DOI 10.1021/acsphotonics.9b00471
   Wang WB, 2017, NANO LETT, V17, P1995, DOI 10.1021/acs.nanolett.6b05418
   Wang Y., 2020, ADV OPT MATER, V9
   Wei YZ, 2018, ADV FUNCT MATER, V28, DOI 10.1002/adfm.201706690
   Xiao LG, 2018, J MATER CHEM C, V6, P3341, DOI 10.1039/c8tc00270c
   Xie BM, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16675-x
   Xing S, 2020, ACS APPL MATER INTER, V12, P13061, DOI 10.1021/acsami.9b22058
   Xu XF, 2018, ADV FUNCT MATER, V28, DOI 10.1002/adfm.201805570
   Xu YL, 2020, APPL PHYS REV, V7, DOI 10.1063/1.5144840
   Yazmaciyan A, 2019, ADV OPT MATER, V7, DOI 10.1002/adom.201801543
   Yeddu V, 2019, ACS PHOTONICS, V6, P2368, DOI 10.1021/acsphotonics.9b00669
   Young M, 2016, ADV OPT MATER, V4, P1028, DOI 10.1002/adom.201600102
   Zheng YC, 2019, ORG ELECTRON, V65, P82, DOI 10.1016/j.orgel.2018.10.040
NR 54
TC 3
Z9 3
U1 52
U2 52
PU WILEY-V C H VERLAG GMBH
PI WEINHEIM
PA POSTFACH 101161, 69451 WEINHEIM, GERMANY
SN 0935-9648
EI 1521-4095
J9 ADV MATER
JI Adv. Mater.
PD NOV
PY 2021
VL 33
IS 44
AR 2102967
DI 10.1002/adma.202102967
EA SEP 2021
PG 8
WC Chemistry, Multidisciplinary; Chemistry, Physical; Nanoscience &
   Nanotechnology; Materials Science, Multidisciplinary; Physics, Applied;
   Physics, Condensed Matter
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Science & Technology - Other Topics; Materials Science;
   Physics
GA WP7EJ
UT WOS:000695167400001
PM 34515381
OA hybrid
DA 2022-02-03
ER

PT J
AU El-Bakry, HM
AF El-Bakry, Hazem M.
TI An efficient algorithm for pattern detection using combined classifiers
   and data fusion
SO INFORMATION FUSION
LA English
DT Article
DE Combined neural classifiers; Image decomposition; Data fusion;
   Cross-correlation; Face detection
AB Neural networks have shown good results for detecting a certain pattern in a given image. In this paper, efficient neural networks (ENNs) for face detection are presented Such classifiers are designed based on cross-correlation in the frequency domain between the input matrix and the input weights of neural networks. This approach is developed to reduce the computation steps required by these ENNs for the searching process. The principle of divide and conquer strategy is applied through matrix decomposition Each matrix is divided into smaller in size sub-matrices and then each one is tested separately by using a single efficient neural classifier. Then, the final result is achieved by fusing results from the combined classifiers Furthermore, faster face detection is obtained by using parallel processing techniques to test the resulting sub-matrices at the same time using the same number of ENNs. Another advantage is that the speed up ratio is increased with the size of the input matrix when using ENNs and matrix decomposition. In addition. for many reasons presented here, it is proved that the equations given in previous work [S. Ben-Yacoub, B. Fasel.J. Luettin, Fast face detection using MLP and FFT, in: Proceedings of the Second International Conference on Audio and Video-based Biometric Person Authentication (AVBPA'99), 1999: B. Fasel, Fast multi-scale face detection, IDIAP-Com 98-04, 1998: S. Ben-Yacoub, Fast object detection using MLP and FFT, IDIAP-RR 11, IDIAP. 1997] for conventional and fast neural networks are not valid Correct equations for the number of computation steps required by cross-correlation in the spatial and frequency domains arc given. Moreover, the problem of local sub-matrix normalization in the frequency domain is solved. The effect of matrix normalization on the speed up ratio of face detection is discussed. Simulation results show that local sub-matrix normalization through weight normalization is faster than sub-matrix normalization in the spatial domain. The overall speed up ratio of the detection process is increased as the normalization of weights is done off line. (C) 2009 Elsevier B.V. All rights reserved.
C1 Mansoura Univ, Fac Comp Sci & Informat Syst, Mansoura, Egypt.
C3 Mansoura University
RP El-Bakry, HM (corresponding author), Mansoura Univ, Fac Comp Sci & Informat Syst, Mansoura, Egypt.
CR ANIFANTIS D, 1999, P 6 IEEE INT C EL CI, P109
   BENYACOUB S, 1997, 11 IDIAP RR
   BENYACOUB S, 1999, P 2 INT C AUD VID BA
   BRUCE J, 2003, P ICRA 03 2003 IEEE, P1
   COOLEY JW, 1965, MATH COMPUT, V19, P297, DOI 10.2307/2003354
   ELBAKRY HM, 2005, INT J INFORM TECHNOL, V2, P71
   Essannouni L., 2006, P 2006 2 EUR INT S C
   FASEL B, 1998, 9804 IDIAPCOM
   Feraud R., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P77, DOI 10.1109/AFGR.2000.840615
   Gonzalez Rafael C, 2002, DIGITAL IMAGE PROCES
   ISHAK KA, 2004, P INT S INF COMM TEC, V2, P5
   KLETTE R, 1996, ZAMPERON HDB IMAGE P
   LANG KJ, CMUCS88152
   Lewis J.P., FAST NORMALIZED CROS
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Schneiderman H, 1998, PROC CVPR IEEE, P45, DOI 10.1109/CVPR.1998.698586
   Srisuk S, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P306, DOI 10.1109/AFGR.2002.1004171
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883
   ZHU Y, 2000, P IEEE COMP SOC INT, V1, P1636
NR 19
TC 10
Z9 10
U1 0
U2 6
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1566-2535
EI 1872-6305
J9 INFORM FUSION
JI Inf. Fusion
PD APR
PY 2010
VL 11
IS 2
BP 133
EP 148
DI 10.1016/j.inffus.2009.06.001
PG 16
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 560TY
UT WOS:000274927200008
DA 2022-02-03
ER

PT C
AU Parchami, M
   Bashbaghi, S
   Granger, E
AF Parchami, Mostafa
   Bashbaghi, Saman
   Granger, Eric
GP IEEE
TI Video-Based Face Recognition Using Ensemble of Haar-Like Deep
   Convolutional Neural Networks
SO 2017 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
LA English
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY MAY 14-19, 2017
CL Anchorage, AK
AB Growing number of surveillance and biometric applications seek to recognize the face of individuals appearing in the viewpoint of video cameras. Systems for video-based FRcan be subjected to challenging operational environments, where the appearance of faces captured with video cameras varies significantly due to changes in pose, illumination, scale, blur, expression, occlusion, etc. In particular, with still-to-video FR, a limited number of high-quality facial images are typically captured for enrollment of an individual to the system, whereas an abundance facial trajectories can be captured using video cameras during operations, under different viewpoints and uncontrolled conditions. This paper presents a deep learning architecture that can learn a robust facial representation for each target individual during enrollment, and then accurately compare the facial regions of interest (ROIs) extracted from a still reference image (of the target individual) with ROIs extracted from live or archived videos. An ensemble of deep convolutional neural networks (DCNNs) named HaarNet is proposed, where a trunk network first extracts features from the global appearance of the facial ROIs (holistic representation). Then, three branch networks effectively embed asymmetrical and complex facial features (local representations) based on Haar-like features. In order to increase the discriminativness of face representations, a novel regularized triplet-loss function is proposed that reduces the intra-class variations, while increasing the inter-class variations. Given the single reference still per target individual, the robustness of the proposed DCNN is further improved by fine-tuning the HaarNet with synthetically-generated facial still ROIs that emulate capture conditions found in operational environments. The proposed system is evaluated on stills and videos from the challenging COX Face and Chokepoint datasets according to accuracy and complexity. Experimental results indicate that the proposed method can significantly improve performance with respect to state-of-the-art systems for video-based FR.
C1 [Parchami, Mostafa] Univ Texas Arlington, Comp Sci & Engn Dept, Arlington, TX 76019 USA.
   [Bashbaghi, Saman; Granger, Eric] Univ Quebec, Ecole Technol Super, Montreal, PQ, Canada.
C3 University of Texas System; University of Texas Arlington; University of
   Quebec; Ecole de Technologie Superieure - Canada; University of Quebec
   Montreal
RP Parchami, M (corresponding author), Univ Texas Arlington, Comp Sci & Engn Dept, Arlington, TX 76019 USA.
EM mostafa.parchami@mavs.uta.edu; bashbaghi@livia.etsmtl.ca;
   eric.granger@etsmtl.ca
CR Bashbaghi S., 2015, AVSS
   Bashbaghi S., 2014, ICPR
   Bashbaghi S, 2017, MACH VISION APPL, V28, P219, DOI 10.1007/s00138-016-0820-4
   Chellappa R., 2016, CORR
   De-la-Torre M, 2015, INFORM FUSION, V24, P31, DOI 10.1016/j.inffus.2014.05.006
   Deng WH, 2012, IEEE T PATTERN ANAL, V34, P1864, DOI 10.1109/TPAMI.2012.30
   Ding CX, 2018, IEEE T PATTERN ANAL, V40, P1002, DOI 10.1109/TPAMI.2017.2700390
   Gao SH, 2015, IEEE T INF FOREN SEC, V10, P2108, DOI 10.1109/TIFS.2015.2446438
   Han X., 2015, CVPR
   Hassner Tal, 2015, CVPR
   Huang G. B., 2012, CVPR
   Huang ZW, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2493448
   Huang Zhiwu, 2014, CVPR
   Jain AK, 2016, PATTERN RECOGN LETT, V79, P80, DOI 10.1016/j.patrec.2015.12.013
   Kim M., 2008, CVPR
   Learned-Miller E., 2007, TECH REP
   Mudunuri SP, 2016, IEEE T PATTERN ANAL, V38, P1034, DOI 10.1109/TPAMI.2015.2469282
   Nourbakhsh F., 2016, ACCV WORKSH HUM ID S
   Schroff Florian, 2015, CVPR
   Sun Y., 2014, NIPS
   Sun Y., 2013, ICCV
   Sun Y., 2014, CVPR
   Szegedy C., 2015, CVPR
   Taigman Y., 2014, CVPR
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang J., 2014, CVPR
   Wong Y., 2011, CVPR WORKSHOPS
   Xu H., 2016, WACV
   Zhang D., 2016, DISCRIMINATIVE LEARN, P199
NR 29
TC 14
Z9 14
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2161-4393
BN 978-1-5090-6182-2
J9 IEEE IJCNN
PY 2017
BP 4625
EP 4632
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BJ6WL
UT WOS:000426968704116
DA 2022-02-03
ER

PT J
AU Roda, MS
   Griesshaber, E
   Ziegler, A
   Rupp, U
   Yin, XF
   Henkel, D
   Haussermann, V
   Laudien, J
   Brand, U
   Eisenhauer, A
   Checa, AG
   Schmahl, WW
AF Roda, Maria Simonet
   Griesshaber, Erika
   Ziegler, Andreas
   Rupp, Ulrich
   Yin, Xiaofei
   Henkel, Daniela
   Haussermann, Vreni
   Laudien, Juergen
   Brand, Uwe
   Eisenhauer, Anton
   Checa, Antonio G.
   Schmahl, Wolfgang W.
TI Calcite fibre formation in modern brachiopod shells
SO SCIENTIFIC REPORTS
LA English
DT Article
ID OXYGEN-ISOTOPE COMPOSITION; ORGANIC MATRIX; BIOLOGICAL-MATERIALS;
   CO-ORIENTATION; NACRE; EVOLUTION; MICROSTRUCTURE; GROWTH; ARCHITECTURE;
   DELTA-O-18
AB The fibrous calcite layer of modern brachiopod shells is a hybrid composite material and forms a substantial part of the hard tissue. We investigated how cells of the outer mantle epithelium (OME) secrete calcite material and generate the characteristic fibre morphology and composite microstructure of the shell. We employed AFM, FE-SEM, and TEM imaging of embedded/etched, chemically fixed/decalcified and high-pressure frozen/freeze substituted samples. Calcite fibres are secreted by outer mantle epithelium (OME) cells. Biometric analysis of TEM micrographs indicates that about 50% of these cells are attached via hemidesmosomes to an extracellular organic membrane present at the proximal, convex surface of the fibres. At these sites, mineral secretion is not active. Instead, ion transport from OME cells to developing fibres occurs at regions of closest contact between cells and fibres, however only at sites where the extracellular membrane at the proximal fibre surface is not developed yet. Fibre formation requires the cooperation of several adjacent OME cells. It is a spatially and temporally changing process comprising of detachment of OME cells from the extracellular organic membrane, mineral secretion at detachment sites, termination of secretion with formation of the extracellular organic membrane, and attachment of cells via hemidesmosomes to this membrane.
C1 [Roda, Maria Simonet; Griesshaber, Erika; Yin, Xiaofei; Schmahl, Wolfgang W.] LMU, Dept Earth & Environm Sci, D-80333 Munich, Germany.
   [Ziegler, Andreas; Rupp, Ulrich] Univ Ulm, Cent Facil Elect Microscopy, D-89069 Ulm, Germany.
   [Henkel, Daniela; Eisenhauer, Anton] GEOMAR Helmholtz Ctr Ocean Res, Marine Biogeochem Marine Syst, D-24148 Kiel, Germany.
   [Haussermann, Vreni] Pontificia Univ Catolica Valparaiso, Fac Recursos Nat, Escuela Ciencias Mar, Avda Brasil, Valparaiso 2950, Chile.
   [Haussermann, Vreni] Huinay Sci Field Stn, Puerto Montt, Chile.
   [Laudien, Juergen] Helmholtz Zentrum Polar & Meeresforsch, Alfred Wegener Inst, D-27568 Bremerhaven, Germany.
   [Brand, Uwe] Brock Univ, Dept Earth Sci, 1812 Sir Isaac Brock Way, St Catharines, ON L2S 3A1, Canada.
   [Checa, Antonio G.] Univ Granada, Fac Ciencias, Dept Estratig & Paleontol, E-18071 Granada, Spain.
   [Checa, Antonio G.] Univ Granada, CSIC, Inst Andaluz Ciencias Tierra, Armilla 18100, Spain.
C3 University of Munich; Ulm University; Helmholtz Association; GEOMAR
   Helmholtz Center for Ocean Research Kiel; Pontificia Universidad
   Catolica de Valparaiso; Helmholtz Association; Alfred Wegener Institute,
   Helmholtz Centre for Polar & Marine Research; Brock University;
   University of Granada; Consejo Superior de Investigaciones Cientificas
   (CSIC); CSIC - Instituto Andaluz de Ciencias de la Tierra (IACT);
   University of Granada
RP Roda, MS (corresponding author), LMU, Dept Earth & Environm Sci, D-80333 Munich, Germany.
EM simonet@lrz.uni-muenchen.de
RI Schmahl, Wolfgang/AAR-7387-2021; Eisenhauer, Anton/K-6454-2012
OI Rupp, Ulrich/0000-0002-1301-1054; Yin, Xiaofei/0000-0002-9204-0901;
   Henkel, Daniela/0000-0001-6814-0198
FU European Union's Horizon 2020 research and innovation program [643084]
FX We thank Renate Kunz of the Central Facility for Electron Microscopy of
   Ulm University for technical support and assistance. This is a BASE-LINE
   Earth project supported by the European Union's Horizon 2020 research
   and innovation program under grant agreement No. 643084. This is
   publication nr. 159 of Huinay Scientific Field Station.
CR Angiolini L, 2009, J GEOL SOC LONDON, V166, P933, DOI 10.1144/0016-76492008-096R
   Barthelat F, 2013, J R SOC INTERFACE, V10, DOI 10.1098/rsif.2013.0711
   BATES NR, 1991, CHEM GEOL, V94, P67, DOI 10.1016/0168-9622(91)90041-T
   Carlson SJ, 2016, ANNU REV EARTH PL SC, V44, P409, DOI 10.1146/annurev-earth-060115-012348
   CARPENTER SJ, 1995, GEOCHIM COSMOCHIM AC, V59, P3749, DOI 10.1016/0016-7037(95)00291-7
   Carter, 1990, SKELETAL BIOMINERALI, V2, P301
   Carter, 1990, SKELETAL BIOMINERALI, V1, P832
   Cartwright JHE, 2007, J R SOC INTERFACE, V4, P491, DOI 10.1098/rsif.2006.0188
   Chec AG, 2009, P NATL ACAD SCI USA, V106, P38, DOI 10.1073/pnas.0808796106
   Checa AG, 2018, FRONT MAR SCI, V5, DOI 10.3389/fmars.2018.00353
   Checa AG, 2016, P ROY SOC B-BIOL SCI, V283, DOI 10.1098/rspb.2016.0032
   Corni I, 2012, BIOINSPIR BIOMIM, V7, DOI 10.1088/1748-3182/7/3/031001
   Deville S, 2006, SCIENCE, V311, P515, DOI 10.1126/science.1120937
   Dixon G, 1789, VOYAGE WORLD MORE PA, V1, P355
   Dunlop JWC, 2010, ANNU REV MATER RES, V40, P1, DOI 10.1146/annurev-matsci-070909-104421
   Fabritius H, 2005, J STRUCT BIOL, V150, P190, DOI 10.1016/j.jsb.2005.01.004
   Garbelli C, 2017, RIV ITAL PALEONTOL S, V123, P541
   Gaspard D, 2008, FOSS STRATA, P269
   Goetz AJ, 2014, ACTA BIOMATER, V10, P3885, DOI 10.1016/j.actbio.2014.06.012
   Goetz AJ, 2011, ACTA BIOMATER, V7, P2237, DOI 10.1016/j.actbio.2011.01.035
   Goetz AJ, 2009, EUR J MINERAL, V21, P303, DOI 10.1127/0935-1221/2009/0021-1922
   Griesshaber E., 2018, HIGHLIGHTS APPL MINE, P245, DOI [10.1515/ 9783110497342-012, DOI 10.1515/9783110497342-012]
   Griesshaber E, 2007, AM MINERAL, V92, P722, DOI 10.2138/am.2007.2220
   Griesshaber E, 2013, ACTA BIOMATER, V9, P9492, DOI 10.1016/j.actbio.2013.07.020
   Griesshaber E, 2009, EUR J MINERAL, V21, P715, DOI 10.1127/0935-1221/2009/0021-1950
   Grossman EL, 1996, J SEDIMENT RES, V66, P1011
   GROSSMAN EL, 1991, GEOL SOC AM BULL, V103, P953, DOI 10.1130/0016-7606(1991)103<0953:SISOBF>2.3.CO;2
   Henkes GA, 2018, EARTH PLANET SC LETT, V490, P40, DOI 10.1016/j.epsl.2018.02.001
   Levi-Kalisman Y, 2001, J STRUCT BIOL, V135, P8, DOI 10.1006/jsbi.2001.4372
   Logan A, 2007, TREATISE INVERTEBRAT, P2822
   Maier BJ, 2014, ACTA BIOMATER, V10, P3866, DOI 10.1016/j.actbio.2014.02.039
   MCGHEE GR, 1980, PALEOBIOLOGY, V6, P57, DOI 10.1017/S0094837300012513
   Meyers MA, 2008, PROG MATER SCI, V53, P1, DOI 10.1016/j.pmatsci.2007.05.002
   Naleway SE, 2015, ADV MATER, V27, P5455, DOI 10.1002/adma.201502403
   Niebel TP, 2016, J MECH PHYS SOLIDS, V96, P133, DOI 10.1016/j.jmps.2016.06.011
   Nielsen JK, 2013, GEOL SOC SPEC PUBL, V376, P387, DOI 10.1144/SP376.6
   Nindiyasari F, 2015, CRYST GROWTH DES, V15, P2667, DOI 10.1021/cg5018483
   Pan N, 2014, APPL PHYS REV, V1, DOI 10.1063/1.4871365
   Reddin CJ, 2018, GLOBAL ECOL BIOGEOGR, V27, P704, DOI 10.1111/geb.12732
   Richardson Joyce R., 1997, P441
   Ritchie RO, 2011, NAT MATER, V10, P817, DOI [10.1038/NMAT3115, 10.1038/nmat3115]
   Roark A, 2016, GEOL SOC AM BULL, V128, P597, DOI 10.1130/B31330.1
   Rollion-Bard C, 2016, CHEM GEOL, V423, P49, DOI 10.1016/j.chemgeo.2016.01.007
   ROSENBERG GD, 1988, LETHAIA, V21, P219
   Rowell A.J., 1987, P445
   Rudwick M. J. S, 1970, LIVING FOSSIL BRACHI, V199
   Rudwick M. J. S., 1959, GEOL MAG, VXCVI
   Schmahl WW, 2008, MINERAL MAG, V72, P541, DOI 10.1180/minmag.2008.072.2.541
   Schmahl WW, 2012, Z KRIST-CRYST MATER, V227, P604, DOI 10.1524/zkri.2012.1479
   Schmahl WW, 2004, EUR J MINERAL, V16, P693, DOI 10.1127/0935-1221/2004/0016-0693
   Harper Elizabeth M., 2012, Treatise Online, V44, P1
   Simonet-Roda M, J STRUCT BIOL
   Stigall AL, 2018, LETHAIA, V51, P165, DOI 10.1111/let.12232
   Studart AR, 2014, SOFT MATTER, V10, P1284, DOI 10.1039/c3sm51883c
   Studart AR, 2013, ADV FUNCT MATER, V23, P4423, DOI 10.1002/adfm.201300340
   Studart AR, 2012, ADV MATER, V24, P5024, DOI 10.1002/adma.201201471
   Veizer J, 2000, NATURE, V408, P698, DOI 10.1038/35047044
   VEIZER J, 1986, GEOCHIM COSMOCHIM AC, V50, P1679, DOI 10.1016/0016-7037(86)90130-4
   Veizer J, 1999, CHEM GEOL, V161, P59, DOI 10.1016/S0009-2541(99)00081-9
   Veizer J, 2015, EARTH-SCI REV, V146, P92, DOI 10.1016/j.earscirev.2015.03.008
   Wahlisch FC, 2014, ACTA BIOMATER, V10, P3978, DOI 10.1016/j.actbio.2014.05.014
   Walther P, 2002, J MICROSC-OXFORD, V208, P3, DOI 10.1046/j.1365-2818.2002.01064.x
   Walther Paul, 2008, P245, DOI 10.1007/978-0-387-72972-5_10
   Wang SN, 2015, CRYSTENGCOMM, V17, P2964, DOI 10.1039/c4ce02308k
   Wegst UGK, 2015, NAT MATER, V14, P23, DOI [10.1038/NMAT4089, 10.1038/nmat4089]
   Weiner S, 1998, ANNU REV MATER SCI, V28, P271, DOI 10.1146/annurev.matsci.28.1.271
   WEINER S, 1983, INT J BIOL MACROMOL, V5, P325, DOI 10.1016/0141-8130(83)90055-7
   Williams A., 1968, Palaeontology, V11, P486
   WILLIAMS A, 1968, LETHAIA, V1, P268, DOI 10.1111/j.1502-3931.1968.tb01741.x
   WILLIAMS A, 1966, NATURE, V211, P1146, DOI 10.1038/2111146a0
   Williams A., 1968, Special Papers in Palaeontology, VNo. 2, P1
   Williams A., 2000, LINGULIFORMEA CRANII, P1
   Williams Alwyn, 1997, P267
   WILLIAMS ALWYN, 1953, MEM GEOL SOC AMER, V56, P1
   Ye FC, 2018, DATA BRIEF, V18, P300, DOI 10.1016/j.dib.2018.02.071
   Ye FC, 2018, J STRUCT BIOL, V201, P221, DOI 10.1016/j.jsb.2017.11.011
NR 76
TC 16
Z9 16
U1 2
U2 16
PU NATURE PUBLISHING GROUP
PI LONDON
PA MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND
SN 2045-2322
J9 SCI REP-UK
JI Sci Rep
PD JAN 24
PY 2019
VL 9
AR 598
DI 10.1038/s41598-018-36959-z
PG 15
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA HI6HI
UT WOS:000456554600037
PM 30679565
OA Green Accepted, gold, Green Published
DA 2022-02-03
ER

PT C
AU Sonawane, SR
   Gande, ST
   Dhulekar, PA
   Phade, GM
AF Sonawane, Sarika Ramesh
   Gande, S. T.
   Dhulekar, P. A.
   Phade, G. M.
BE Chopde, AM
   Vatti, R
TI Design of FIR filter using WPS,CSF, and GB algorithm and it's delay
   comparison
SO 2015 IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING (ICIP)
LA English
DT Proceedings Paper
CT IEEE International Conference on Information Processing (ICIP)
CY DEC 16-19, 2015
CL BRACTs Vishwakarma Inst Technol, Dept Elect Engn, Pune, INDIA
HO BRACTs Vishwakarma Inst Technol, Dept Elect Engn
DE FIR- Finite Impulse Response; RADAR -Radio Detection and Raging;
   ILP-Integer Linear Programming; STA-Static Timing Analysis WPS-Without
   Partial Product Sharing; CSE-Common Subexpression Elimination; GB-Graph
   Based; DSP-Digital Signal Processor; PCB-Printed Circuit Board;
   MCM-Multiple Constant Multiplication
AB The FIR filter used in DSP has been focus in new product design and technical literature for decade of years. The electronic communication fields which adopt DSP include multimedia system, communication system, image processing, RADAR, medical etc. Now a day, digital signal processor becomes the heart of digital camera, cell phone, hearing devices, aid devices, audio and video players, satellite and even biometric security equipment. The main function of digital filter is to pass the desired components or to remove the undesired components of the input signal. From mathematical view, a digital filter compute the convolution of the sampled input and the weighting function of the filter. In this paper the multiplier of FIR filter design with constant coefficient. The coefficient is already designed in matlab. This multiplier is most important part of FIR filter. The multiplier design with adder, subtractor and add shift techniques. This technique is nothing but FIR filter architecture with transposed form. This technique design by three algorithm and three architecture described in next section with delay comparison.[9],[10]
C1 [Sonawane, Sarika Ramesh] SITRC, E&TC Dept, Nasik, India.
   [Gande, S. T.] SITRC, E&TC, Nasik, India.
   [Dhulekar, P. A.; Phade, G. M.] E&TC Dept, Nasik, India.
RP Sonawane, SR (corresponding author), SITRC, E&TC Dept, Nasik, India.
EM sonsarika@gmail.com; principal@sitrc.org; pravin.dhulekar@sitrc.org;
   gayatri.phade@sitrc.org
RI , Gayatri/AAS-5014-2020
CR Aksoy L, 2008, IEEE T COMPUT AID D, V27, P1013, DOI 10.1109/TCAD.2008.923242
   [Anonymous], 2013, IEEE T VERY LARGE SC, V21
   Hartley RI, 1996, IEEE T CIRCUITS-II, V43, P677, DOI 10.1109/82.539000
   Ko HJ, 2011, IEEE T CIRCUITS-II, V58, P304, DOI 10.1109/TCSII.2011.2148970
   Malini Hema, 2014, LOW COMPLEXITY DIGIT, V3, P226
   Nguyen HT, 2000, IEEE T VLSI SYST, V8, P419, DOI 10.1109/92.863621
   Shi D, 2011, IEEE T CIRCUITS-I, V58, P126, DOI 10.1109/TCSI.2010.2055290
   Yu YJ, 2007, IEEE T CIRCUITS-I, V54, P2330, DOI 10.1109/TCSI.2007.904599
   Yuen H.A.H., 2008, IEEE EXPLORE, P119
NR 9
TC 0
Z9 0
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
BN 978-1-4673-7758-4
PY 2015
BP 729
EP 732
PG 4
WC Computer Science, Information Systems
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BH1XG
UT WOS:000398588700138
DA 2022-02-03
ER

PT J
AU Singh, SP
   Ayub, S
   Saini, JP
AF Singh, Sharad Pratap
   Ayub, Shahanaz
   Saini, J. P.
TI Analysis and comparison of normal and altered fingerprint using
   artificial neural networks
SO INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING
   SYSTEMS
LA English
DT Article
DE Artificial neural network; feedforward back propagation; fingerprints;
   minutiae; biometrics
AB Fingerprint matching is based on the number of minute matches between two fingerprints. Implementation mainly includes image enhancement, segmentation, orientation histogram, etc., extraction (completeness) and corresponding minutiae. Finally, a matching score is generated that indicates whether two fingerprints coincide with the help of coding with MATLAB to find the matching score and simulation of Artificial Neural Network extending the feedback of the network. Using the artificial neural network tool, an important advantage is the similarity index between the sample data or unknown data. A neural network is a massively parallel distributed processor consisting of simple processing units that have a natural property to store knowledge and computer experiences are available for use. A fingerprint comparison essentially consists of two fingerprints to generate a fingerprint match score the match score is used to determine whether the two impressions they are of the same finger. The decision is made this study shows the comparison of normal and altered fingerprints using MATLAB coding and data used to study in the self-generated data using biometric scanner also the open source data available on the web is used for finding out matching score or similarity index, The study shows that there is hardly any matching between normal and altered fingerprints of the same person.
C1 [Singh, Sharad Pratap; Ayub, Shahanaz] Bundelkhand Inst Engn & Technol, Dept Elect & Commun, Jhansi, Uttar Pradesh, India.
   [Saini, J. P.] NSIT, New Delhi, India.
C3 Bundelkhand Institute of Engineering & Technology; Netaji Subhas
   University of Technology
RP Singh, SP (corresponding author), Bundelkhand Inst Engn & Technol, Dept Elect & Commun, Jhansi, Uttar Pradesh, India.
EM sp25singh@gmail.com
CR Bartumek J.S., 2013, DATA IEEE T IMAGE PR DATA IEEE T IMAGE PR, P123
   Bazen M., 2000, P WORKSH CIRC SYST S P WORKSH CIRC SYST S, P276
   Bolle RM, 2002, PATTERN RECOGN, V35, P2727, DOI 10.1016/S0031-3203(01)00247-3
   Campisi P., 2014, IEEE T INFORM FORENS IEEE T INFORM FORENS, P96
   Czyzewski A, 2019, IET BIOMETRICS, V8, P92, DOI 10.1049/iet-bmt.2018.5030
   Kanak A, 2018, IET BIOMETRICS, V7, P510, DOI 10.1049/iet-bmt.2018.5067
   Kim WS, 2013, INT J SECUR APPL, V7, P355, DOI 10.14257/ijsia.2013.7.5.32
   Lin CH, 2018, SENSOR MATER, V30, P385, DOI 10.18494/SAM.2018.1757
   Maltoni D., HDB FINGERPRINT RECO
   Myint O., 2014, FEED FORWARD NEURAL, P1
   National science and Technology council (NSTC) committee on technology, COMM HOM NAT SEC SUB COMM HOM NAT SEC SUB
   Othman A, 2013, IEEE T INF FOREN SEC, V8, P260, DOI 10.1109/TIFS.2012.2223676
   Paiva ARC, 2012, INT J PATTERN RECOGN, V26, DOI 10.1142/S0218001412560101
   SAYED M, 2018, J COMPUTER SCI, V14, P1064
   Schuch P, 2019, IET BIOMETRICS, V8, P1, DOI 10.1049/iet-bmt.2017.0279
   Sha LF, 2006, INT C PATT RECOG, P566
   Subhan R., 2014, LECT NOTES SOFTWARE LECT NOTES SOFTWARE, P209
   Ulery B.T., 2011, P NATL ACAD SCI, V108, P256
   Yen, 2018, 2018 IEEE C EV COMP, P1, DOI DOI 10.1109/CEC.2018.8477921
   Yoon S., 2013, IEEE T PATTERN ANAL IEEE T PATTERN ANAL, P451
NR 20
TC 0
Z9 0
U1 1
U2 1
PU IOS PRESS
PI AMSTERDAM
PA NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS
SN 1327-2314
EI 1875-8827
J9 INT J KNOWL-BASED IN
JI Int. J. Knowl.-Based Intell. Eng. Syst.
PY 2021
VL 25
IS 2
BP 243
EP 249
DI 10.3233/KES-210068
PG 7
WC Computer Science, Artificial Intelligence
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA TR6HO
UT WOS:000679063700008
DA 2022-02-03
ER

PT J
AU Moura, V
   Rossell, ECF
   Mascarenhas, ARP
AF Moura, Valdir
   Franco Rossell, Eduardo Candido
   Prazeres Mascarenhas, Adriano Reis
TI Phytosociological analysis open rainforest colonization in different
   models of the Amazon
SO NATIVA
LA Portuguese
DT Article
DE colonization of the Amazon; phytosociology; forest cuccession
ID LAND-USE; DYNAMICS
AB The aim of this study was to evaluate the influence of colonization models (fishbone and topographic) in different successional stages: Initial (SS1), Intermediate (SS2) and Advanced (SS3) in the Brazilian Amazon, through phytosociological attributes supported by satellite images, and understand the variables that affect the patterns of forest succession and the differences between biometric variables within the different settlements located in the northwestern portion of the state of Rondonia. For that were sampled 56 transects of 10 x 250m in areas of advanced successional stage, 64 transects of 10 x 10 m in areas of intermediate successional stage, and 79 of 2 x 2 m transects in areas of early initial successional stage. In the colonization model called fishbone, were identified 23 families and 43 species - in initial successional stage; in intermediate successional stage were identified 20 families and 41 species already; in advanced successional stage - were cataloged 37 families with 42 species. In the colonization model whose shape is known as topographic, were cataloged on stage SS1, 32 families and 60 species- stage SS2, 29 families and 54 species and stage SS3, 43 families and 57 species. The most important families in the process of succession were the Fabaceae, Caesalpiniaceae, Lauraceae and Bignoniaceae in both models.
C1 [Moura, Valdir] Inst Fed Educ Ciencia & Tecnol, Dept Agron, Colorado Do Oeste, RO, Brazil.
   [Franco Rossell, Eduardo Candido; Prazeres Mascarenhas, Adriano Reis] Univ Fed Rondonia, Dept Engn Florestal, Rolim De Moura, RO, Brazil.
C3 Instituto Federal do Sertao Pernambucano; Universidade Federal de
   Rondonia
RP Moura, V (corresponding author), Inst Fed Educ Ciencia & Tecnol, Dept Agron, Colorado Do Oeste, RO, Brazil.
EM valdir.moura@ifro.edu.br
RI Mascarenhas, Adriano Reis Prazeres/O-8876-2019; Mascarenhas, Adriano
   Reis Prazeres/P-3395-2018
OI Mascarenhas, Adriano Reis Prazeres/0000-0002-7554-3590; 
CR BRONDIZIO ES, 1994, HUM ECOL, V22, P249, DOI 10.1007/BF02168853
   Brower J., 1984, FIELD LAB METHODS GE
   Bussab W. O., 1990, INTRO ANALISE AGRUPA
   CAIN SA, 1956, AM J BOT, V43, P911, DOI 10.2307/2439008
   Carim Sâmyrams, 2007, Acta Bot. Bras., V21, P293, DOI 10.1590/S0102-33062007000200005
   Causton D. R., 1988, INTRO VEGETATION ANA
   Clarke L., 2007, SCENARIOS GREENHOUSE
   CORLETT RT, 1995, PROG PHYS GEOG, V19, P159, DOI 10.1177/030913339501900201
   Francez Luciana Maria de Barros, 2007, Acta Amaz., V37, P219, DOI 10.1590/S0044-59672007000200007
   de Jong W, 2001, FOREST ECOL MANAG, V150, P135, DOI 10.1016/S0378-1127(00)00687-3
   Ezzine-de-Blas D, 2011, ENVIRON SCI POLICY, V14, P188, DOI 10.1016/j.envsci.2010.11.009
   Felfili J. M., 1997, CONTRIBUICAO AO CONH, P6
   FELFILI JM, 1995, VEGETATIO, V117, P1, DOI 10.1007/BF00033255
   Gama João Ricardo Vasconcellos, 2002, Rev. Árvore, V26, P559, DOI 10.1590/S0100-67622002000500005
   Hosokawa R.T., 1981, MANEJO FLORESTAS TRO
   Karsenty A, 2008, INT FOREST REV, V10, P443, DOI 10.1505/ifor.10.3.443
   Kindermann G, 2008, P NATL ACAD SCI USA, V105, P10302, DOI 10.1073/pnas.0710616105
   KNIGHT DH, 1975, ECOL MONOGR, V45, P259, DOI 10.2307/1942424
   Lavras, 2006, INVENTARIO FLORESTAL
   Magurran A.E, 1988, ECOLOGICAL DIVERSITY
   Matteucci S. D., 1982, METODOLOGIA ESTUDIO
   Moran EF, 2000, FOREST ECOL MANAG, V139, P93, DOI 10.1016/S0378-1127(99)00337-0
   Moran Emilio F., 1994, INDIANA J GLOBAL LEG, V1, P397
   Nepstad D, 2009, SCIENCE, V326, P1350, DOI 10.1126/science.1182108
   NERA-NUCLEO DE ESTUDOS PESQUISAS E PROJETOS DE REFORMA AGRARIA, 2008, B DATA, V4
   Oliveira AT, 2000, BIOTROPICA, V32, P793, DOI 10.1646/0006-3606(2000)032[0793:POFDAA]2.0.CO;2
   OLIVEIRA-FILHO A. T., 2004, PLANT DIVERSITY BIOG
   RADAMBRASIL. Departamento Nacional de Producao Mineral, 1976, FOLH SC 20 X C MACH
   Richards P. W., 1955, Science Progress, London, V43, P45
   RONDONIA, 2000, ZON SOC EC
   RONDONIA, 2004, B CLIMATOLOGICO ROND
   SALOMAO R. P., 2012, B MUSEU PARAENSE EMI, V7, P57
   SHEPHERD G. J., 1994, FITOPAC 2 1 MANUAL U
   Souza Deoclides Ricardo de, 2006, Rev. Árvore, V30, P75, DOI 10.1590/S0100-67622006000100010
   Stern N., 2007, EC CLIMATE CHANGE
   ter Steege H, 2013, SCIENCE, V342, P325, DOI 10.1126/science.1243092
   Tucker JM, 1998, INTERCIENCIA, V23, P64
   Turner BL, 1995, LAND USE LAND COVER
   UHL C, 1982, OIKOS, V38, P313, DOI 10.2307/3544671
   UNITED NATIONS FRAMEWORK CONVENTION ON CLIMATE CHANGE (UNFCC), RED EM DEF DEV COUNT
   Van Den Berg Eduardo, 2000, Revista Brasileira de Botanica, V23, P231
   Vasconcelos Sumaia S., 2009, Acta Amaz., V39, P71, DOI 10.1590/S0044-59672009000100007
   WALKER B., 2004, ECOLOGY SOC, V9, P1
   Whitmore TC, 1998, CLIMATIC CHANGE, V39, P429, DOI 10.1023/A:1005356906898
NR 44
TC 2
Z9 2
U1 0
U2 2
PU UNIV FEDERAL MATO GROSSO
PI SINOP
PA AVE ALEXANDRE FERRONATO 1200, SETOR INDUSTRIAL, SINOP, MG 78557-267,
   BRAZIL
SN 2318-7670
J9 NATIVA
JI Nativa
PD MAR-APR
PY 2017
VL 5
IS 2
BP 118
EP 126
DI 10.5935/2318-7670.v05n02a07
PG 9
WC Agriculture, Multidisciplinary
WE Emerging Sources Citation Index (ESCI)
SC Agriculture
GA FU8BB
UT WOS:000424075700007
OA gold, Green Published
DA 2022-02-03
ER

PT J
AU Akgun, RO
   Orhan, IO
   Ekim, O
AF Akgun, Remzi Orkun
   Orhan, Ismail Onder
   Ekim, Okan
TI Three-dimensional bone modeling of forelimb joints in New Zealand
   Rabbit: A Micro-Computed Tomography study
SO ANKARA UNIVERSITESI VETERINER FAKULTESI DERGISI
LA English
DT Article
DE Forelimb joints; laboratory rabbit; micro-computed tomography;
   morphometry; three-dimensional reconstruction
ID 3D; ACCURACY; RELIABILITY
AB In this study, it was aimed to obtain 3-dimensional (3D) digital and printed models of healthy forelimb joints using micro-computed tomography (mu CT) technique in New Zealand Rabbit, which is frequently preferred in experimental orthopedic studies. Moreover, it was aimed to provide morphometric measurements on the shoulder and elbow joints over 3D digital models. A total of 14 adults (7 female, 7 male) New Zealand Rabbits were used in the study. After imaging the forelimbs with the mu CT device, 3D digital and printed models were obtained. Biometric measurements of shoulder and elbow joints were performed over 3D digital models and the data obtained from female and male rabbits were evaluated statistically. The anatomical structure on the 3D joint models was very detailed due to the low section thickness and high detector quality. 3D printed models produced as a result of the 3D printing process were quite durable, odorless, and clean. No anatomical differences were observed between 3D printed models and 3D digital models. In this study, it is thought that the anatomical and morphometric data obtained from laboratory rabbits will contribute to scientists take part both in experimental orthopedic intervention and clinical anatomy education.
C1 [Akgun, Remzi Orkun] Cankiri Karatekin Univ, Fac Dent, Dept Basic Sci, Cankiri, Turkey.
   [Orhan, Ismail Onder; Ekim, Okan] Ankara Univ, Fac Vet Med, Dept Anat, Ankara, Turkey.
C3 Cankiri Karatekin University; Ankara University
RP Akgun, RO (corresponding author), Cankiri Karatekin Univ, Fac Dent, Dept Basic Sci, Cankiri, Turkey.
EM roakgun@karatekin.edu.tr
FU Ankara University Scientific Research Projects UnitAnkara University
   [17L0239013]
FX This study was financially supported by Ankara University Scientific
   Research Projects Unit (Project no: 17L0239013) .
CR Bakici C., 2019, VETERINER HEKIMLER D, V90, P71
   Berco M, 2009, AM J ORTHOD DENTOFAC, V136, DOI 10.1016/j.ajodo.2008.08.021
   Bouxsein ML, 2010, J BONE MINER RES, V25, P1468, DOI 10.1002/jbmr.141
   Brazina D, 2014, PROCD SOC BEHV, V143, P367, DOI 10.1016/j.sbspro.2014.07.496
   Estai M, 2016, ANN ANAT, V208, P151, DOI 10.1016/j.aanat.2016.02.010
   Fasel JHD, 2016, SURG RADIOL ANAT, V38, P361, DOI 10.1007/s00276-015-1588-3
   Freitas EP, 2011, ADV APPL RAPID PROTO
   Gribel BF, 2011, ANGLE ORTHOD, V81, P26, DOI 10.2319/032210-166.1
   Jamali AA, 2007, COMPUT AIDED SURG, V12, P278, DOI 10.1080/10929080701680265
   Jiang Y., 2000, Journal of Musculoskeletal & Neuronal Interactions, V1, P45
   Jones DG, 2019, ANAT SCI EDUC, V12, P435, DOI 10.1002/ase.1851
   Keles A, 2015, TURKIYE KLIN, V1, P32
   Kim M, 2012, IMAGNG SCI DENT, V42, P25, DOI 10.5624/isd.2012.42.1.25
   Labruyere J, 2013, IN PRACTICE, V35, P546, DOI 10.1136/inp.f6720
   Lagravere MO, 2008, AM J ORTHOD DENTOFAC, V134, P112, DOI 10.1016/j.ajodo.2006.08.024
   Li FZ, 2018, ANAT SCI EDUC, V11, P73, DOI 10.1002/ase.1725
   Martini L, 2001, COMPARATIVE MED, V51, P292
   Mitsouras D, 2015, RADIOGRAPHICS, V35, P1966, DOI 10.1148/rg.2015140320
   Moreira CR, 2009, ORAL SURG ORAL MED O, V108, P430, DOI 10.1016/j.tripleo.2009.01.032
   Murgitroyd E, 2015, SURG-J R COLL SURG E, V13, P177, DOI 10.1016/j.surge.2014.10.007
   Naff KA, 2012, AM COLL LAB, P157, DOI 10.1016/B978-0-12-380920-9.00006-7
   Nomina Anatomica Veterinaria, 2017, PREP INT COMM VET GR, V6
   Ozkadif S, 2015, IRAN J VET RES, V16, P205
   Ozkadif S., 2015, Ataturk Universitesi Veteriner Bilimleri Dergisi, V10, P46
   Pazvant G, 2009, ISTANBUL U VET FAK D, V35, P23
   Preece D, 2013, ANAT SCI EDUC, V6, P216, DOI 10.1002/ase.1345
   Pujol S, 2016, ACAD RADIOL, V23, P507, DOI 10.1016/j.acra.2015.12.012
   Remzi OA, 2019, ACTA VET-BEOGRAD, V69, P192, DOI 10.2478/acve-2019-0015
   Rengier F, 2010, INT J COMPUT ASS RAD, V5, P335, DOI 10.1007/s11548-010-0476-x
   Ventola C Lee, 2014, P T, V39, P704
   von den Driesch A., 1976, DOMESTIKATIONSFORSCH, V1
NR 31
TC 0
Z9 0
U1 2
U2 2
PU ANKARA UNIV PRESS
PI ANKARA
PA INCITAS SOKAK NO. 10, BESEVLER, ANKARA, 06510, TURKEY
SN 1300-0861
EI 1308-2817
J9 ANKARA UNIV VET FAK
JI Ank. Univ. Vet. Fak. Derg.
PY 2021
VL 68
IS 4
BP 355
EP 363
DI 10.33988/auvfd.762615
PG 9
WC Veterinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Veterinary Sciences
GA WS7LV
UT WOS:000715359300005
OA Green Submitted, gold
DA 2022-02-03
ER

PT J
AU Suchocka, M
   Swoczyna, T
   Kosno-Jonczy, J
   Kalaji, HM
AF Suchocka, Marzena
   Swoczyna, Tatiana
   Kosno-Jonczy, Joanna
   Kalaji, Hazem M.
TI Impact of heavy pruning on development and photosynthesis of Tilia
   cordata Mill. trees
SO PLOS ONE
LA English
DT Article
ID CHLOROPHYLL FLUORESCENCE; PARTIAL DEFOLIATION; WATER RELATIONS;
   GAS-EXCHANGE; SHADE LEAVES; FRUIT-TREES; GROWTH; RESPONSES; FORESTS;
   BUDS
AB Tree pruning is carried out to reduce conflict with infrastructure, buildings, and any other human activity. However, heavy pruning may result in a diminished tree crown capacity for sugar production and exposure to fungal infection. This risk leads to a decrease in tree stability or vigour. In this work, we analysed the effect of heavy pruning of roadside trees on the photosynthetic performance process compared to neighbouring unpruned trees. Four years of tree crown growth was studied by terrestrial imaging. Tree vitality (Roloff's classification) and risk (Visual Tree Assessment) were evaluated. Over-pruned trees showed intensified photosynthetic efficiency during the growing season following pruning. Particularly ET0/TR0 and PIABS tended to increase in pruned trees while higher F-v/F-m was noted only in late October, suggesting delayed leaf senescence. After four years, pruned trees rebuilt their crowns, however not in their entirety. Results obtained from biometric, vitality, and risk assessment showed high differentiation in pruned tree crown recovery. Our results revealed that despite the intensified efforts of trees to recover from wounding effects, severe pruning evokes dieback occurrence and a higher risk of failure in mature trees.
C1 [Suchocka, Marzena; Kosno-Jonczy, Joanna] Warsaw Univ Life Sci SGGW, Inst Environm Engn, Dept Landscape Architecture, Warsaw, Poland.
   [Swoczyna, Tatiana] Warsaw Univ Life Sci SGGW, Inst Hort Sci, Dept Environm Protect & Dendrol, Warsaw, Poland.
   [Kalaji, Hazem M.] Warsaw Univ Life Sci SGGW, Inst Biol, Dept Plant Physiol, Warsaw, Poland.
C3 Warsaw University of Life Sciences; Research Institute of Horticulture;
   Warsaw University of Life Sciences; Warsaw University of Life Sciences
RP Suchocka, M (corresponding author), Warsaw Univ Life Sci SGGW, Inst Environm Engn, Dept Landscape Architecture, Warsaw, Poland.
EM marzena_suchocka@sggw.edu.pl
RI Kalaji, Mohamed Hazem/E-8086-2012
OI Kalaji, Mohamed Hazem/0000-0002-3833-4917; Suchocka,
   Marzena/0000-0002-0759-5348
CR ABRAMS MD, 1995, TREE PHYSIOL, V15, P361, DOI 10.1093/treephys/15.6.361
   Baba W, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0156201
   Badrulhisham N., 2006, P J, V5, P223
   Begin C, 1999, CAN J BOT, V77, P664, DOI 10.1139/cjb-77-5-664
   Bellingham PJ, 2000, OIKOS, V89, P409, DOI 10.1034/j.1600-0706.2000.890224.x
   Benson AR, 2019, URBAN FOR URBAN GREE, V38, P64, DOI 10.1016/j.ufug.2018.11.006
   Blake J., 1983, TISSUE CULTURE TREES, P29, DOI [10.1007/978-1-4684-6691-1_4, DOI 10.1007/978-1-4684-6691-1_4]
   Borowski J., 2007, ROCZNIK DENDROLGICZN, V55, P33
   Bourgery C., 1988, PLANTATIONS ALIGNEME, V650, P25
   Boutaud J., 2003, TAILLE FORMATION ARB
   Burrows GE, 2008, AUST J BOT, V56, P254, DOI 10.1071/BT07164
   Burrows N, 2010, ENVIRON MANAGE, V45, P1332, DOI 10.1007/s00267-010-9490-6
   Bussotti F, 2010, SCAND J FOREST RES, V25, P43, DOI 10.1080/02827581.2010.485777
   Campanella B, 2009, URBAN FOR URBAN GREE, V8, P49, DOI 10.1016/j.ufug.2008.11.001
   Christen D, 2007, ENVIRON EXP BOT, V60, P504, DOI 10.1016/j.envexpbot.2007.02.003
   Clark James R., 2010, Arboriculture & Urban Forestry, V36, P110
   Close DD., 2001, ARBORIC J, V27, P106
   Coder KD., 1996, ASSESSING CONSTRUCTI
   Cooper-Ellis S, 1999, ECOLOGY, V80, P2683, DOI 10.1890/0012-9658(1999)080[2683:FRTCWR]2.0.CO;2
   De Jaegere T, 2016, FORESTS, V7, DOI 10.3390/f7030056
   Deal RL, 2003, FORESTRY, V76, P401, DOI 10.1093/forestry/76.4.401
   Desotgiu R, 2012, PLANT BIOSYST, V146, P206, DOI 10.1080/11263504.2012.705350
   Dmuchowski Wojciech, 2011, Ecological Questions, V15, P97, DOI 10.2478/v10090-011-0041-4
   Dong TF, 2016, TREE PHYSIOL, V36, P807, DOI 10.1093/treephys/tpw025
   Dujesiefken D., 2016, TREES A LIFESPAN APP
   Dujesiefken Dirk, 2002, Urban Forestry & Urban Greening, V1, P75, DOI 10.1078/1618-8667-00008
   EHSEN H, 1987, Arboricultural Journal, V11, P245
   EUFORGEN, DISTRIBUTION MAP LIM
   Fazio James R., 1999, Journal of Arboriculture, V25, P193
   Fini A, 2015, URBAN FOR URBAN GREE, V14, P664, DOI 10.1016/j.ufug.2015.06.011
   Finn GA, 2007, ANN APPL BIOL, V151, P127, DOI 10.1111/j.1744-7348.2007.00159.x
   Genard M, 2008, TREES-STRUCT FUNCT, V22, P269, DOI 10.1007/s00468-007-0176-5
   Gibbons JW, 2000, BIOSCIENCE, V50, P653, DOI 10.1641/0006-3568(2000)050[0653:TGDORD]2.0.CO;2
   Gilman E. F., 2006, Arboriculture & Urban Forestry, V32, P74
   Gilman EF., 1997, ILLUSTRATED GUIDE PR
   Gilman EF., 2005, J ARBORICULT, V31, P38
   Grabosky Jason C., 2007, Arboriculture & Urban Forestry, V33, P360
   Gruber F., 1994, EFFECTS ACID RAIN FO, V8, P265
   Gruebler MU, 2013, BIODIVERS CONSERV, V22, P3233, DOI 10.1007/s10531-013-0581-6
   Harris RW., 1999, ARBORICULTURE INTEGR, V3rd ed
   Hart M, 2000, CAN J BOT, V78, P583, DOI 10.1139/cjb-78-5-583
   Hermans C, 2003, J PLANT PHYSIOL, V160, P81, DOI 10.1078/0176-1617-00917
   Hipps NA, 2014, PLANT SOIL, V382, P61, DOI 10.1007/s11104-014-2143-4
   Ishii H, 2002, FOREST ECOL MANAG, V169, P257, DOI 10.1016/S0378-1127(01)00751-4
   Kalaji M.H., 2017, CHLOROPHYLL FLUORESC
   KAUPPI A, 1987, FLORA, V179, P55
   Kempler C., 1994, ACTA HORTIC, V351, P481
   KENT RL, 1995, LANDSCAPE URBAN PLAN, V33, P341, DOI 10.1016/0169-2046(94)02027-D
   Koch W., 2001, AKTUALISIERTE GEHOLZ
   Koeser AK, 2017, URBAN FOR URBAN GREE, V24, P109, DOI 10.1016/j.ufug.2017.03.027
   Kormanik PP., 1970, USDA FOREST SERVICE, V54
   Kosmala M., 2008, HORTICULT LANDSC ARC, V29, P137
   LAYNE DR, 1993, PHYSIOL PLANTARUM, V88, P44, DOI 10.1111/j.1399-3054.1993.tb01758.x
   Lepedus H, 2010, CROAT CHEM ACTA, V83, P379
   Li KT, 2003, J HORTIC SCI BIOTECH, V78, P749, DOI 10.1080/14620316.2003.11511694
   Ma XD, 2016, SPECTROSC SPECT ANAL, V36, P3986, DOI 10.3964/j.issn.1000-0593(2016)12-3986-05
   Mahmud MH., 2013, MALAYS J COMMUN, V29, P127
   Matheny NP., 1994, PHOTOGRAPHIC GUIDE E
   Mattheck C., 1994, BODY LANGUAGE TREES
   Mattheck C., 1991, TREES MECH DESIGN, DOI [10.1515/bmte.1991.36.1-2.20, DOI 10.1515/BMTE.1991.36.1-2.20]
   Maurin V, 2013, FOREST ECOL MANAG, V304, P399, DOI 10.1016/j.foreco.2013.05.039
   Meier AR, 2012, TREE PHYSIOL, V32, P565, DOI 10.1093/treephys/tps040
   Meier U, 2003, TASK VEG SC, V39, P269
   Meier U., 2009, Journal fur Kulturpflanzen, V61, P41
   Michalcewicz Jakub, 2012, Polish Journal of Entomology, V81, P49, DOI 10.2478/v10200-011-0063-7
   Miller GW, 2004, P 14 CENTR HARDW FOR, P37
   Murad AG, 2000, THESIS U PUTRA MALAY
   NEELY D, 1970, J AM SOC HORTIC SCI, V95, P536
   Neilsen WA, 2003, CAN J FOREST RES, V33, P2067, DOI 10.1139/X03-131
   Nicholson DI., 1965, S EC RES HUM TROP VE
   Nicolini E, 2003, ANN BOT-LONDON, V92, P97, DOI 10.1093/aob/mcg119
   Nicolini E, 2001, ANN BOT-LONDON, V87, P737, DOI 10.1006/anbo.2001.1398
   Norainiratna B., 2013, THESIS U PUTRA MALAY
   O'Hara K. L., 1991, Western Journal of Applied Forestry, V6, P59
   O'Hara KL, 2009, ANN FOREST SCI, V66, DOI 10.1051/forest/2009015
   O'Hara KL, 2000, CAN J FOREST RES, V30, P324, DOI 10.1139/cjfr-30-2-324
   Oleksa A, 2007, POL J ECOL, V55, P315
   OVASKA J, 1993, J EXP BOT, V44, P1385, DOI 10.1093/jxb/44.8.1385
   Pauleit Stephan, 2002, Urban Forestry & Urban Greening, V1, P83, DOI 10.1078/1618-8667-00009
   Percival Glynn C., 2005, Journal of Arboriculture, V31, P215
   Piene H, 1996, CAN ENTOMOL, V128, P1101, DOI 10.4039/Ent1281101-6
   Pinkard EA, 1998, TREES-STRUCT FUNCT, V12, P119, DOI 10.1007/PL00009702
   Platt W, 1986, P FRUG SEED DISP DOR, P309
   Pollastrini M, 2014, PLANT BIOLOGY, V16, P323, DOI 10.1111/plb.12068
   Pradines Ch, 5 COUNC EUR C EUR LA
   Psidova E, 2018, ENVIRON EXP BOT, V152, P97, DOI 10.1016/j.envexpbot.2017.12.001
   Remm J, 2006, FOREST ECOL MANAG, V221, P267, DOI 10.1016/j.foreco.2005.10.015
   Ricek EW, 1967, APOLLO, V8, P1
   Robles H, 2011, FOREST ECOL MANAG, V261, P1428, DOI 10.1016/j.foreco.2011.01.029
   Roloff A., 2015, HDB BAUMDIAGNOSTIK B
   Schwarze F. W. M. R., 2001, Arboricultural Journal, V25, P321
   Shigo AL, SHIGO TREES, P1989
   Sieghardt M., 2005, URBAN FORESTS TREES, P281
   Sjoman H, 2012, URBAN FOR URBAN GREE, V11, P31, DOI 10.1016/j.ufug.2011.09.004
   Sjoman H, 2010, URBAN FOR URBAN GREE, V9, P281, DOI 10.1016/j.ufug.2010.04.001
   SLABAUGH PAUL E., 1957, JOUR FOREST, V55, P904
   Smiley E. Thomas, 2006, Journal of Arboriculture, V32, P33
   Smiley ET., 2012, ARBORIST NEWS, V21, P12
   SPURR SH, 1956, ECOLOGY, V37, P443, DOI 10.2307/1930166
   Stirbet A, 2011, J PHOTOCH PHOTOBIO B, V104, P236, DOI 10.1016/j.jphotobiol.2010.12.010
   Stone EL, 1943, AM J BOT, V30, P346, DOI 10.2307/2437519
   Strasser RJ, 2004, ADV PHOTO RESPIRAT, V19, P321
   Suchocka M., 2014, JAK DBAC DRZEWA ABY, DOI [10.1016/j.pharep.2014.07.017, DOI 10.1016/J.PHAREP.2014.07.017]
   Suchocka M, 2019, SUSTAINABILITY-BASEL, V11, DOI 10.3390/su11061816
   Swoczyna T, 2020, PHOTOSYNTHETICA, V58, P573, DOI 10.32615/ps.2020.006
   Swoczyna T., 2016, Acta Horticulturae et Regiotecturae, V19, P37
   Swoczyna T, 2020, PLANTS URBAN AREAS L, P59
   Swoczyna T, 2015, URBAN FOR URBAN GREE, V14, P544, DOI 10.1016/j.ufug.2015.05.005
   Toussaint Andre, 2002, Biotechnologie Agronomie Societe et Environnement, V6, P99
   TSCHAPLINSKI TJ, 1989, PHYSIOL PLANTARUM, V75, P157, DOI 10.1111/j.1399-3054.1989.tb06163.x
   Turnbull TL, 2007, TREE PHYSIOL, V27, P1481, DOI 10.1093/treephys/27.10.1481
   Ugolini F, 2012, URBAN FOR URBAN GREE, V11, P313, DOI 10.1016/j.ufug.2012.02.006
   Urban L, 2007, TREE PHYSIOL, V27, P345, DOI 10.1093/treephys/27.3.345
   von Aufsess H., 1975, FORSTW CBL, V94, P140
   Watson Gary W., 1998, Journal of Arboriculture, V24, P47
   WILSON BF, 1994, CAN J FOREST RES, V24, P149, DOI 10.1139/x94-020
   Yordanov I., 2008, PHOTOSYNTHESIS ENERG, P675
   Yordanov I, 2008, J PLANT PHYSIOL, V165, P1954, DOI 10.1016/j.jplph.2008.05.003
   Zakaria AZ., 2012, P 1 INT C INN TECHN, P175
   Zechmeister HG, 2003, AGR ECOSYST ENVIRON, V94, P159, DOI 10.1016/S0167-8809(02)00028-2
   Zhang HH, 2016, PEERJ, V4, DOI 10.7717/peerj.2125
   Zivcak M, 2014, PHOTOSYNTH RES, V119, P339, DOI 10.1007/s11120-014-9969-8
NR 122
TC 2
Z9 2
U1 7
U2 7
PU PUBLIC LIBRARY SCIENCE
PI SAN FRANCISCO
PA 1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA
SN 1932-6203
J9 PLOS ONE
JI PLoS One
PY 2021
VL 16
IS 8
AR e0256465
DI 10.1371/journal.pone.0256465
PG 22
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA UD7YU
UT WOS:000687421200004
PM 34424935
OA Green Published, gold
DA 2022-02-03
ER

PT J
AU Tudoran, GM
   Dobre, AC
   Cicsa, A
   Pascu, IS
AF Tudoran, Gheorghe Marian
   Dobre, Alexandru Claudiu
   Cicsa, Avram
   Pascu, Ionut Silviu
TI Development of Mathematical Models for the Estimation of Dendrometric
   Variables Based on Unmanned Aerial Vehicle Optical Data: A Romanian Case
   Study
SO FORESTS
LA English
DT Article
DE sessile oak; tree diameter; tree height; tree volume; crown diameter;
   stand structure; digital surface model; mathematical models
AB Research highlights: In this study, the possibility of developing predictive models for both individual trees and forest stands, based on information derived from digital surface models (DSMs), was evaluated. Background and objectives: Unmanned aerial systems (UASs) make it possible to obtain digital images with increased spectral and spatial resolution at a lower cost. Based on the variables extracted by means of the digital representation of surfaces, we aimed at generating mathematical models that would allow the prediction of the main biometric features of both individual trees and forest stands. Materials and methods: Forest stands are characterized by various structures. As such, measurements may address upper-level trees, but most often are oriented towards those belonging to the mean tree category, randomly selected from those identifiable from digital models. In the case of grouped trees, it is the best practice to measure the projected area of the entire canopy. Tree and stand volumes can be determined using models based on features measured in UAS-derived digital models. For the current study, 170-year-old mixed sessile oak stands were examined. Results: Mathematical models were developed based on variables (i.e., crown diameter and tree height) extracted from digital models. In this way, we obtained results characterized by root mean square error (RMSE) values of 18.37% for crown diameter, 10.95% for tree height, and 8.70% for volume. The simplified process allowed for the estimates of the stand volume using crown diameter or diameter at breast height, producing results with RMSE values of 9%. Conclusions: The accuracy of the evaluation of the main biometric features depends on the structural complexity of the studied plots, and on the quality of the DSM. In turn, this leads to the necessity to parametrize the used models in such a manner that can explain the variation induced by the stand structure.
C1 [Tudoran, Gheorghe Marian; Dobre, Alexandru Claudiu; Cicsa, Avram] Transilvania Univ, Fac Silviculture & Forest Engn, Dept Forest Engn, Forest Management Planning & Terr Measurements, 1 Ludwig van Beethoven Str, Brasov 500123, Romania.
   [Dobre, Alexandru Claudiu; Cicsa, Avram; Pascu, Ionut Silviu] Marin Dracea Romanian Natl Inst Res & Dev Forestr, Dept Forest Monitoring, 128 Eroilor Blvd, Voluntari 077190, Romania.
C3 Transylvania University of Brasov
RP Dobre, AC (corresponding author), Transilvania Univ, Fac Silviculture & Forest Engn, Dept Forest Engn, Forest Management Planning & Terr Measurements, 1 Ludwig van Beethoven Str, Brasov 500123, Romania.; Dobre, AC (corresponding author), Marin Dracea Romanian Natl Inst Res & Dev Forestr, Dept Forest Monitoring, 128 Eroilor Blvd, Voluntari 077190, Romania.
EM tudoran.george@unitbv.ro; dobre.alexandruclaudiu@gmail.com;
   cicsa_avram@yahoo.co.uk; ionut.pascu@icas.ro
RI Pascu, Ionut Silviu/AAI-6133-2020; Dobre,
   Alexandru-Claudiu/AAZ-2636-2021
OI Pascu, Ionut Silviu/0000-0003-2998-603X; Avram,
   Cicsa/0000-0002-3656-4663
CR Chisholm RA, 2013, J UNMANNED VEH SYST, V1, P61, DOI 10.1139/juvs-2013-0017
   Dandois JP, 2013, REMOTE SENS ENVIRON, V136, P259, DOI 10.1016/j.rse.2013.04.005
   Fritz A, 2013, INT ARCH PHOTOGRAMM, P141
   Giurgiu V, 1979, DENDROMETRIE I AUXOL
   Giurgiu V., 2004, METODE SI TABELE DEN
   Heinzel J, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10071056
   Iizuka K, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10010013
   Jaakkola A, 2010, ISPRS J PHOTOGRAMM, V65, P514, DOI 10.1016/j.isprsjprs.2010.08.002
   Jayathunga S, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10020187
   Kameyama S, 2020, DRONES-BASEL, V4, DOI 10.3390/drones4020019
   Khosravipour A, 2016, INT J APPL EARTH OBS, V52, P104, DOI 10.1016/j.jag.2016.06.005
   Liang XL, 2019, FOR ECOSYST, V6, DOI 10.1186/s40663-019-0173-3
   Lisein J, 2013, FORESTS, V4, P922, DOI 10.3390/f4040922
   Mohan M, 2017, FORESTS, V8, DOI 10.3390/f8090340
   Ota T, 2017, FORESTS, V8, DOI 10.3390/f8090343
   Otero V, 2018, FOREST ECOL MANAG, V411, P35, DOI 10.1016/j.foreco.2017.12.049
   Panagiotidis D, 2017, INT J REMOTE SENS, V38, P2392, DOI 10.1080/01431161.2016.1264028
   Pascu IS, 2020, FORESTS, V11, DOI 10.3390/f11040392
   Popescu SC, 2003, CAN J REMOTE SENS, V29, P564, DOI 10.5589/m03-027
   Puliti S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12081245
   Puliti S, 2017, REMOTE SENS ENVIRON, V194, P115, DOI 10.1016/j.rse.2017.03.019
   Puliti S, 2015, REMOTE SENS-BASEL, V7, P9632, DOI 10.3390/rs70809632
   Salami E, 2014, REMOTE SENS-BASEL, V6, P11051, DOI 10.3390/rs61111051
   Vorovencii I, 2010, FOTOGRAMETRIE
   Vorovencii I, 2017, ENVIRON MONIT ASSESS, V189, DOI 10.1007/s10661-017-6234-6
   Vorovencii I, 2015, ENVIRON MONIT ASSESS, V187, DOI 10.1007/s10661-015-4909-4
   Vorovencii I, 2015, ENVIRON MONIT ASSESS, V187, DOI 10.1007/s10661-015-4428-3
   Vorovencii I, 2014, ENVIRON MONIT ASSESS, V186, P5951, DOI 10.1007/s10661-014-3831-5
   Wallace L, 2014, IEEE T GEOSCI REMOTE, V52, P7619, DOI 10.1109/TGRS.2014.2315649
   Wang YS, 2019, REMOTE SENS ENVIRON, V232, DOI 10.1016/j.rse.2019.111309
   Wieser M, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9111154
   Yurtseven H, 2019, ISPRS INT J GEO-INF, V8, DOI 10.3390/ijgi8040175
   Zagalikis G, 2005, CAN J FOREST RES, V35, P1224, DOI 10.1139/X05-030
   Zhang J, 2016, BIOL CONSERV, V198, P60, DOI 10.1016/j.biocon.2016.03.027
NR 34
TC 0
Z9 0
U1 2
U2 2
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1999-4907
J9 FORESTS
JI Forests
PD FEB
PY 2021
VL 12
IS 2
AR 200
DI 10.3390/f12020200
PG 17
WC Forestry
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Forestry
GA QN5LV
UT WOS:000622501600001
OA gold
DA 2022-02-03
ER

PT C
AU Kuang, HF
   Ji, RR
   Liu, H
   Zhang, SC
   Sun, XS
   Zhang, BC
   Huang, FY
AF Kuang, Huafeng
   Ji, Rongrong
   Liu, Hong
   Zhang, Shengchuan
   Sun, Xiaoshuai
   Zhang, Baochang
   Huang, Feiyue
GP ACM
TI Multi-modal Multi-layer Fusion Network with Average Binary Center Loss
   for Face Anti-spoofing
SO PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA
   (MM'19)
LA English
DT Proceedings Paper
CT 27th ACM International Conference on Multimedia (MM)
CY OCT 21-25, 2019
CL Nice, FRANCE
DE Face Anti-spoofing; Multi-modal Feature Fusion; Convolutional Neural
   Network
AB Face anti-spoofing detection is critical to guarantee the security of biometric face recognition systems. Despite extensive advances in facial anti-spoofing based on single-model image, little work has been devoted to multi-modal anti-spoofing, which is however widely encountered in real-world scenarios. Following the recent progress, this paper mainly focuses on multi-modal face anti-spoofing and aims to solve the following two challenges: (1) how to effectively fuse multi-modal information; and (2) how to effectively learn distinguishable features despite single cross-entropy loss. We propose a novel Multi-modal Multi-layer Fusion Convolutional Neural Network (mmfCNN), which targets at finding a discriminative model for recognizing the subtle differences between live and spoof faces. The mmfCNN can fully use different information provided by diverse modalities, which is based on a weight-adaptation aggregation approach. Specifically, we utilize a multi-layer fusion model to further aggregate the features from different layers, which fuses the low-, mid- and high-level information from different modalities in a unified framework. Moreover, a novel Average Binary Center (ABC) loss is proposed to maximize the dissimilarity between the features of live and spoof faces, which helps to stabilize the training to generate a robust and discriminative model. Extensive experiments conducted on the CISIA-SURF and 3DMAD datasets verify the significance and generalization capability of the proposed method for the face anti-spoofing task. Code is available at: https://github.com/SkyKuang/Face-anti-spoofing.
C1 [Kuang, Huafeng; Ji, Rongrong; Liu, Hong; Zhang, Shengchuan; Sun, Xiaoshuai] Xiamen Univ, Sch Informat, Dept Artificia Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.
   [Zhang, Baochang] Beihang Univ, Beijing, Peoples R China.
   [Huang, Feiyue] Tencent Technol Shanghai Co Ltd, Tencent Youtu Lab, Shanghai, Peoples R China.
C3 Xiamen University; Beihang University; Tencent
RP Ji, RR (corresponding author), Xiamen Univ, Sch Informat, Dept Artificia Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.
EM rrji@xmu.edu.cn
FU Nature Science Foundation of ChinaNational Natural Science Foundation of
   China (NSFC) [U1705262, 61772443, 61572410]; National Key RD Program
   [2017YFC0113000, 2016YFB1001503]; Post Doctoral Innovative Talent
   Support Program [BX201600094]; China Post-Doctoral Science
   FoundationChina Postdoctoral Science Foundation [2017M612134];
   Scientific Research Project of National Language Committee of China
   [YB135-49]; Nature Science Foundation of Fujian Province, ChinaNatural
   Science Foundation of Fujian Province [2017J01125, 2018J01106]
FX This work is supported by the Nature Science Foundation of China
   (No.U1705262, No.61772443, and No.61572410), National Key R&D Program
   (No.2017YFC0113000, and No.2016YFB1001503), Post Doctoral Innovative
   Talent Support Program under Grant BX201600094, China Post-Doctoral
   Science Foundation under Grant 2017M612134, Scientific Research Project
   of National Language Committee of China (Grant No. YB135-49), and Nature
   Science Foundation of Fujian Province, China (No. 2017J01125 and No.
   2018J01106).
CR Atoum Yousef, 2018, P IFCB
   Boulkenafet Z, 2017, IEEE SIGNAL PROC LET, V24, P141, DOI 10.1109/LSP.2016.2630740
   Boulkenafet Z, 2016, IEEE T INF FOREN SEC, V11, P1818, DOI 10.1109/TIFS.2016.2555286
   Boulkenafet Z, 2015, IEEE IMAGE PROC, P2636, DOI 10.1109/ICIP.2015.7351280
   Chingovska I., 2012, P INT C BIOM SPEC IN, DOI DOI 10.1109/VTCFALL.2012.6399116
   D'Mello SK, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2682899
   de Freitas Pereira Tiago, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P121, DOI 10.1007/978-3-642-37410-4_11
   DiWen Hu Han, 2015, IEEE T INF FOREN SEC
   Edmunds T, 2018, J VIS COMMUN IMAGE R, V50, P314, DOI 10.1016/j.jvcir.2017.12.004
   Erdogmus N., 2013, SPOOFING 2D FACE REC, P1
   Feng Litong, 2016, J VISUAL COMMUNICATI
   Galbally Javier, 2017, IEEE ACCESS
   Ghiass Reza Shoja, 2014, PATTERN RECOGN
   Hadid A., 2011, BIOM IJCB 2011 INT J, P1, DOI [DOI 10.1109/IJCB.2011.6117510, 10.1109/IJCB.2011.6117510]
   He K., 2016, PROC CVPR IEEE, P770, DOI DOI 10.1109/CVPR.2016.90
   Hinton G.E., 2010, ICML, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Kollreider Klaus, 2007, IEEE T INF FOREN SEC
   Komulainen J., 2013, P IEEE 6 INT C BIOM, P1
   Laage Dominic P, 2005, US Patent, Patent No. [6,931,382, 6931382]
   Li Haoliang, 2018, IEEE T INF FOREN SEC
   Li JW, 2004, P SOC PHOTO-OPT INS, V5404, P296, DOI 10.1117/12.541955
   Li L., 2016, 6 INT C IM PROC THEO, P1
   Li Stan Z, 2007, IEEE T PATTERN ANAL
   Liu Ajian, 2019, P CVPR
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu YJ, 2018, PROC CVPR IEEE, P389, DOI 10.1109/CVPR.2018.00048
   Menotti D, 2015, IEEE T INF FOREN SEC, V10, P864, DOI 10.1109/TIFS.2015.2398817
   Pan G, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P117
   Pan G, 2007, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2007.4409068
   Parkin A, 2019, IEEE COMPUT SOC CONF, P1617, DOI 10.1109/CVPRW.2019.00204
   Patel K, 2016, LECT NOTES COMPUT SC, V9967, P611, DOI 10.1007/978-3-319-46654-5_67
   Ramachandra R, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3038924
   Sandhu R.S, 1998, ADV COMPUTERS, V46, P234
   Saxena N, 2011, INT CONF PERVAS COMP, P181, DOI 10.1109/PERCOM.2011.5767583
   Siddiqui TA, 2016, INT C PATT RECOG, P1035, DOI 10.1109/ICPR.2016.7899772
   Tan XY, 2010, LECT NOTES COMPUT SC, V6316, P504, DOI 10.1007/978-3-642-15567-3_37
   Wang Yan, 2017, J VISUAL COMMUNICATI
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Weon Sun-Hee, 2014, INT C HUM COMP INT, P480
   Xu Zhenqi, 2016, PATTERN RECOGN
   Yang J., 2014, ARXIV14085601
   Yang JS, 2013, IEEE GLOB COMM CONF, P1, DOI 10.1109/GLOCOM.2013.6831038
   Youngshin Kim, 2009, J OPT SOC AM A
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
NR 45
TC 2
Z9 3
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
BN 978-1-4503-6889-6
PY 2019
BP 48
EP 56
DI 10.1145/3343031.3351001
PG 9
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BO2ZT
UT WOS:000509743400007
DA 2022-02-03
ER

PT J
AU Zabala-Blanco, D
   Mora, M
   Barrientos, RJ
   Hernandez-Garcia, R
   Naranjo-Torres, J
AF Zabala-Blanco, David
   Mora, Marco
   Barrientos, Ricardo J.
   Hernandez-Garcia, Ruber
   Naranjo-Torres, Jose
TI Fingerprint Classification through Standard and Weighted Extreme
   Learning Machines
SO APPLIED SCIENCES-BASEL
LA English
DT Article
DE fingerprint classification; fingerprint features; extreme learning
   machine; unbalanced dataset
ID SINGULAR POINTS
AB Fingerprint classification is a stage of biometric identification systems that aims to group fingerprints and reduce search times and computational complexity in the databases of fingerprints. The most recent works on this problem propose methods based on deep convolutional neural networks (CNNs) by adopting fingerprint images as inputs. These networks have achieved high classification performances, but with a high computational cost in the network training process, even by using high-performance computing techniques. In this paper, we introduce a novel fingerprint classification approach based on feature extractor models, and basic and modified extreme learning machines (ELMs), being the first time that this approach is adopted. The weighted ELMs naturally address the problem of unbalanced data, such as fingerprint databases. Some of the best and most recent extractors (Capelli02, Hong08, and Liu10), which are based on the most relevant visual characteristics of the fingerprint image, are considered. Considering the unbalanced classes for fingerprint identification schemes, we optimize the ELMs (standard, original weighted, and decay weighted) in terms of the geometric mean by estimating their hyper-parameters (regularization parameter, number of hidden neurons, and decay parameter). At the same time, the classic accuracy and penetration-rate metrics are computed for comparison purposes with the superior CNN-based methods reported in the literature. The experimental results show that weighted ELM with the presence of the golden-ratio in the weighted matrix (W-ELM2) overall outperforms the rest of the ELMs. The combination of the Hong08 extractor and W-ELM2 competes with CNNs in terms of the fingerprint classification efficacy, but the ELMs-based methods have been demonstrated their extremely fast training speeds in any context.
C1 [Zabala-Blanco, David; Mora, Marco; Barrientos, Ricardo J.] Univ Catolica Maule, Fac Engn Sci, Dept Comp Sci & Ind, Talca 3480112, Chile.
   [Hernandez-Garcia, Ruber; Naranjo-Torres, Jose] Univ Catolica Maule, Lab Technol Res Pattern Recognit LITRP, Talca 3480112, Chile.
C3 Universidad Catolica del Maule; Universidad Catolica del Maule
RP Zabala-Blanco, D; Mora, M (corresponding author), Univ Catolica Maule, Fac Engn Sci, Dept Comp Sci & Ind, Talca 3480112, Chile.
EM dzabala@ucm.cl; mmora@ucm.cl; rbarrientos@ucm.cl; rhernandez@ucm.cl;
   jnaranjo@ucm.cl
RI Naranjo-Torres, Jose A./E-5540-2018; Garcia, Ruber
   Hernandez/AAT-3103-2020
OI Naranjo-Torres, Jose A./0000-0002-3653-9601; Garcia, Ruber
   Hernandez/0000-0002-9311-1193; Mora, Marco/0000-0003-3619-2561;
   Barrientos, Ricardo/0000-0001-5345-7061; Zabala-Blanco,
   David/0000-0002-5692-5673
FU FONDECYT REGULAR 2020 [1200810]; Agencia Nacional de Investigacion y
   Desarrollo, Ministerio de Ciencia, Tecnologia, Conocimiento e
   Innovacion, Gobierno de Chile; Project CONICYT FONDEF/Cuarto Concurso
   IDeA en dos Etapas del Fondo de Fomento al Desarrollo Cientifico y
   Tecnologico, Programa IDeA, FONDEF/CONICYT 2017 [ID17i10254]
FX This research was funded by FONDECYT REGULAR 2020 No 1200810 Very Large
   Fingerprint Classification based on a Fast and Distributed Extreme
   Learning Machine, Agencia Nacional de Investigacion y Desarrollo,
   Ministerio de Ciencia, Tecnologia, Conocimiento e Innovacion, Gobierno
   de Chile, and Project CONICYT FONDEF/Cuarto Concurso IDeA en dos Etapas
   del Fondo de Fomento al Desarrollo Cientifico y Tecnologico, Programa
   IDeA, FONDEF/CONICYT 2017 ID17i10254.
CR Akbulut Y, 2017, SYMMETRY-BASEL, V9, DOI 10.3390/sym9080142
   Alias NA, 2016, 2016 FIFTH ICT INTERNATIONAL STUDENT PROJECT CONFERENCE (ICT-ISPC), P105, DOI 10.1109/ICT-ISPC.2016.7519247
   [Anonymous], **NON-TRADITIONAL**
   [Anonymous], **NON-TRADITIONAL**
   Bazen AM, 2002, IEEE T PATTERN ANAL, V24, P905, DOI 10.1109/TPAMI.2002.1017618
   Cao K, 2013, PATTERN RECOGN, V46, P3186, DOI 10.1016/j.patcog.2013.05.008
   Cappelli R, 2002, INT C PATT RECOG, P744, DOI 10.1109/ICPR.2002.1048096
   Cappelli R, 2002, PATTERN ANAL APPL, V5, P136, DOI 10.1007/s100440200012
   Chatfield K., 2014, ARXIV PREPRINT ARXIV, P3531
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding SF, 2015, ARTIF INTELL REV, V44, P103, DOI 10.1007/s10462-013-9405-z
   Dorasamy K, 2015, INT CONF BIOMETR, P400, DOI 10.1109/ICB.2015.7139102
   El Hamdi D, 2018, LECT NOTES COMPUT SC, V11182, P402, DOI 10.1007/978-3-030-01449-0_34
   Feng JJ, 2011, IEEE T PATTERN ANAL, V33, P209, DOI 10.1109/TPAMI.2010.77
   Ferri C, 2009, PATTERN RECOGN LETT, V30, P27, DOI 10.1016/j.patrec.2008.08.010
   Galar M, 2015, KNOWL-BASED SYST, V81, P98, DOI 10.1016/j.knosys.2015.02.015
   Galar M, 2015, KNOWL-BASED SYST, V81, P76, DOI 10.1016/j.knosys.2015.02.008
   Galar M, 2014, IEEE INT FUZZY SYST, P554, DOI 10.1109/FUZZ-IEEE.2014.6891668
   Moreno-Torres JG, 2012, IEEE T NEUR NET LEAR, V23, P1304, DOI 10.1109/TNNLS.2012.2199516
   Ge SS, 2017, PROCEEDINGS OF 2017 3RD IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P1942, DOI 10.1109/CompComm.2017.8322877
   Guo JM, 2014, EXPERT SYST APPL, V41, P752, DOI 10.1016/j.eswa.2013.07.099
   Gupta P, 2015, APPL SOFT COMPUT, V29, P411, DOI 10.1016/j.asoc.2015.01.027
   Han WH, 2019, J MED SYST, V43, DOI 10.1007/s10916-018-1154-8
   Henry E. R., 1905, CLASSIFICATION USES
   Hong JH, 2008, PATTERN RECOGN, V41, P662, DOI 10.1016/j.patcog.2007.07.004
   Huang G, 2015, NEURAL NETWORKS, V61, P32, DOI 10.1016/j.neunet.2014.10.001
   Huang G, 2014, IEEE T CYBERNETICS, V44, P2405, DOI 10.1109/TCYB.2014.2307349
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang NT, 2016, ENERGIES, V9, DOI 10.3390/en9120989
   Jain AK, 1999, IEEE T PATTERN ANAL, V21, P348, DOI 10.1109/34.761265
   Jung HW, 2015, PATTERN RECOGN, V48, P473, DOI 10.1016/j.patcog.2014.07.030
   KAWAGOE M, 1984, PATTERN RECOGN, V17, P295, DOI 10.1016/0031-3203(84)90079-7
   Khellal A, 2018, CHIN CONTR CONF, P9629, DOI 10.23919/ChiCC.2018.8482876
   Koziarski M, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107262
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lekamalage CKL, 2017, IEEE IMAGE PROC, P1297, DOI 10.1109/ICIP.2017.8296491
   Liu MH, 2010, PATTERN RECOGN, V43, P1062, DOI 10.1016/j.patcog.2009.08.011
   Lu CB, 2019, MEMET COMPUT, V11, P27, DOI 10.1007/s12293-017-0236-3
   Luo J, 2014, MATH PROBL ENG, V2014, DOI 10.1155/2014/592928
   Maimaitiyiming M, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11070740
   Maltoni D., 2009, HDB FINGERPRINT RECO, V2nd
   Michelsanti D, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 5, P202, DOI 10.5220/0006116502020209
   Nilsson K, 2003, PATTERN RECOGN LETT, V24, P2135, DOI 10.1016/S0167-8655(03)00083-7
   Pang S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3049632
   Peralta D, 2018, INT J INTELL SYST, V33, P213, DOI 10.1002/int.21948
   Peralta D, 2017, KNOWL-BASED SYST, V126, P91, DOI 10.1016/j.knosys.2017.03.014
   Rajanna U, 2010, PATTERN ANAL APPL, V13, P263, DOI 10.1007/s10044-009-0160-3
   Saeed F., 2018, P 21 SAUD COMP SOC N, P1
   Saini MK, 2013, 2013 INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND COMMUNICATION (ICSC), P267, DOI 10.1109/ICSPCom.2013.6719795
   Shen Q, 2017, MACH VISION APPL, V28, P743, DOI 10.1007/s00138-017-0828-4
   Shrein J. M., 2017, P IEEE S SER COMP IN, P1
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002
   Tang JX, 2016, IEEE T NEUR NET LEAR, V27, P809, DOI 10.1109/TNNLS.2015.2424995
   Vitello G, 2014, 2014 EIGHTH INTERNATIONAL CONFERENCE ON COMPLEX, INTELLIGENT AND SOFTWARE INTENSIVE SYSTEMS (CISIS),, P155, DOI 10.1109/CISIS.2014.23
   Wang RX, 2016, INT C PATT RECOG, P931, DOI 10.1109/ICPR.2016.7899755
   Zabala-Blanco D, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12040632
   Zabala-Blanco D, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8090921
   Zhang K, 2015, NEUROCOMPUTING, V151, P1519, DOI 10.1016/j.neucom.2014.09.022
   Zia T, 2019, IET IMAGE PROCESS, V13, P1280, DOI 10.1049/iet-ipr.2018.5466
   Zong WW, 2013, NEUROCOMPUTING, V101, P229, DOI 10.1016/j.neucom.2012.08.010
NR 60
TC 3
Z9 3
U1 2
U2 2
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2076-3417
J9 APPL SCI-BASEL
JI Appl. Sci.-Basel
PD JUN
PY 2020
VL 10
IS 12
AR 4125
DI 10.3390/app10124125
PG 22
WC Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Materials Science; Physics
GA ML2YY
UT WOS:000549339100001
OA gold
DA 2022-02-03
ER

PT J
AU Punyani, P
   Gupta, R
   Kumar, A
AF Punyani, Prachi
   Gupta, Rashmi
   Kumar, Ashwani
TI Neural networks for facial age estimation: a survey on recent advances
SO ARTIFICIAL INTELLIGENCE REVIEW
LA English
DT Article
DE Artificial neural networks; Facial age estimation; Biometrics; Soft
   biometrics; Fusion; Survey
ID FACE; RECOGNITION; IMAGES; CLASSIFICATION; FEATURES; FUSION; MODELS
AB Soft biometrics has emerged out to be a new area of interest for the researchers due to its growing real-world applications. It includes the estimation of demographic traits like age, gender, scars, ethnicity. Moreover, researchers are trying to develop models which can accurately estimate the age or the age group of a person using different biometric traits. Presently, neural networks proves out to give the best classification results for age estimation using human faces. Hence, in this paper, we have surveyed and compared all the neural network models developed and implemented for facial age estimation from 2010 to 2019. We have precisely compared all twenty-three different research works done so far to estimate age from human faces using neural networks. Most of the works are based on convolutional neural networks and a few are based on feed forward back propagation and autoencoders. Important details, issues and results of each work are thoroughly discussed for better knowledge of interested researchers. This paper also includes details on other classification techniques for facial age estimation to give an overall idea of all additional techniques adopted by the scientists till date. Details like neural network model names, datasets used, main contributions, evaluation metrics and results are adopted for a tabular and easy to understand comparison study. Finally, the paper concludes by mentioning the other relevant future research tasks that can be done in this challenging area of research.
C1 [Punyani, Prachi; Kumar, Ashwani] Indira Gandhi Delhi Tech Univ Women, Delhi, India.
   [Gupta, Rashmi] Ambedkar Inst Adv Commun Technol & Res, Shastri Pk, Delhi, India.
C3 Indira Gandhi Delhi Technical University for Women (IGDTUW); Netaji
   Subhas University of Technology
RP Punyani, P (corresponding author), Indira Gandhi Delhi Tech Univ Women, Delhi, India.
EM prachipunyani90@gmail.com
CR Akinyemi JD, 2016, 2016 IEEE S TECHN HO, P1
   Bastanfard A, 2007, INTERNATIONAL CONFERENCE ON MACHINE VISION 2007, PROCEEDINGS, P50
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Beymer D, 1996, SCIENCE, V272, P1905, DOI 10.1126/science.272.5270.1905
   Cai D., 2007, P 20 INT JOINT C ART, P708
   Cai D, 2006, IEEE T IMAGE PROCESS, V15, P3608, DOI 10.1109/TIP.2006.881945
   Chen JK, 2018, IEEE T AFFECT COMPUT, V9, P38, DOI 10.1109/TAFFC.2016.2593719
   Chen SX, 2017, PROC CVPR IEEE, P742, DOI 10.1109/CVPR.2017.86
   Choi SE, 2011, PATTERN RECOGN, V44, P1262, DOI 10.1016/j.patcog.2010.12.005
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Das A, 2013, 2013 SECOND IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR 2013), P917, DOI 10.1109/ACPR.2013.168
   Nguyen DT, 2015, SYMMETRY-BASEL, V7, P1882, DOI 10.3390/sym7041882
   Dehshibi MM, 2010, SIGNAL PROCESS, V90, P2431, DOI 10.1016/j.sigpro.2010.02.015
   Dong Y, 2016, NEUROCOMPUTING, V187, P4, DOI 10.1016/j.neucom.2015.09.115
   Duan MX, 2018, NEUROCOMPUTING, V275, P448, DOI 10.1016/j.neucom.2017.08.062
   Escalera S, 2016, IEEE COMPUT SOC CONF, P706, DOI 10.1109/CVPRW.2016.93
   Escalera S, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P243, DOI 10.1109/ICCVW.2015.40
   Farkas LG, 1994, ANTHROPOMETRY HEAD F, DOI [10.1016/0278-2391(95)90208-2, DOI 10.1016/0278-2391(95)90208-2]
   Ferreira C., 2001, Complex Systems, V13, P87
   Fisher RA, 1938, ANN EUGENIC, V8, P376, DOI 10.1111/j.1469-1809.1938.tb02189.x
   Fu Y, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1383
   Gabor D., 1946, Journal of the Institution of Electrical Engineers. III. Radio and Communication Engineering, V93, P429
   Gallagher AC, 2009, PROC CVPR IEEE, P256, DOI 10.1109/CVPRW.2009.5206828
   Gao F, 2009, LECT NOTES COMPUT SC, V5558, P132
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Geng X, 2016, IEEE T KNOWL DATA EN, V28, P1734, DOI 10.1109/TKDE.2016.2545658
   Geng X, 2013, INT CONF DAT MIN WOR, P377, DOI 10.1109/ICDMW.2013.19
   Gunay A., 2008, P 2008 23 INT S COMP, P1, DOI DOI 10.1109/ISCIS.2008.4717926
   Gunay A, 2015, INT J ADV COMPUT SC, V6, P19
   Guo G, 2018, P IEEE WORKSH APPL C, P19, DOI [10.1109/wacv.2008.4544009, DOI 10.1109/WACV.2008.4544009]
   Guo GD, 2008, IEEE T IMAGE PROCESS, V17, P1178, DOI 10.1109/TIP.2008.924280
   Guo GD, 2011, PROC CVPR IEEE, P657, DOI 10.1109/CVPR.2011.5995404
   Guo GD, 2009, PROC CVPR IEEE, P112, DOI 10.1109/CVPRW.2009.5206681
   Guodong Guo, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563041
   Han H, 2015, IEEE T PATTERN ANAL, V37, P1148, DOI 10.1109/TPAMI.2014.2362759
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hu ZZ, 2017, IEEE T IMAGE PROCESS, V26, P3087, DOI 10.1109/TIP.2016.2633868
   Huang GB, 2007, TECHNICAL REPORT, P7
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huerta I, 2015, PATTERN RECOGN LETT, V68, P239, DOI 10.1016/j.patrec.2015.06.006
   Huo ZW, 2016, IEEE COMPUT SOC CONF, P722, DOI 10.1109/CVPRW.2016.95
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kang JS, 2018, SYMMETRY-BASEL, V10, DOI 10.3390/sym10040108
   Krizhevsky A., 2012, INT C NEUR INF PROC, V25, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
   Lanitis A, 2004, IEEE T SYST MAN CY B, V34, P621, DOI 10.1109/TSMCB.2003.817091
   Laskar BZ, 2015, J KING SAUD UNIV-COM, V27, P458, DOI 10.1016/j.jksuci.2014.06.017
   Levi Gil, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P34, DOI 10.1109/CVPRW.2015.7301352
   Li K, 2017, PATTERN RECOGN, V66, P1
   Liu KH, 2014, IEEE WINT CONF APPL, P445, DOI 10.1109/WACV.2014.6836068
   Liu X, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P258, DOI 10.1109/ICCVW.2015.42
   Lu JW, 2013, IEEE T HUM-MACH SYST, V43, P249, DOI 10.1109/TSMCC.2012.2192727
   Malli RC, 2016, IEEE COMPUT SOC CONF, P714, DOI 10.1109/CVPRW.2016.94
   NIU ZX, 2016, PROC CVPR IEEE, P4920, DOI DOI 10.1109/CVPR.2016.532
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ojala T., 2001, Advances in Pattern Recognition - ICAPR 2001. Second International Conference. Proceedings (Lecture Notes in Computer Science Vol.2013), P397
   Onifade OFW, 2015, INT J IMAGE GRAPHICS, V5, P1
   Panis G, 2016, IET BIOMETRICS, V5, P37, DOI 10.1049/iet-bmt.2014.0053
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Punyani P, 2018, INT J IMAGE DATA FUS, V9, P222, DOI 10.1080/19479832.2018.1423644
   Qawaqneh Z, 2017, EXPERT SYST APPL, V85, P76, DOI 10.1016/j.eswa.2017.05.037
   Ramanathan K, 2006, WORL CON PHOTOVOLT E, P380
   Rattani A, 2018, P IEEE INT JOINT C B, P756
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Rodriguez P, 2017, PATTERN RECOGN, V72, P563, DOI 10.1016/j.patcog.2017.06.028
   Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41
   Sabharwal T, 2019, ARTIF INTELL REV, V52, P1009, DOI 10.1007/s10462-018-9660-0
   Sai PK, 2015, NEUROCOMPUTING, V149, P364, DOI 10.1016/j.neucom.2014.03.074
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/CVPR.2015.7298594
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Taheri S, 2018, IET BIOMETRICS, V8, P124
   Taheri S, 2019, NEUROCOMPUTING, V329, P300, DOI 10.1016/j.neucom.2018.10.071
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Thukral P, 2012, INT CONF ACOUST SPEE, P1529, DOI 10.1109/ICASSP.2012.6288182
   Triggs B., 2005, PROC CVPR IEEE, V1, P886, DOI DOI 10.1109/CVPR.2005.177
   Ueki K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P43
   Wan J, 2018, IEEE T CYBERNETICS, V48, P2531, DOI 10.1109/TCYB.2017.2741998
   Wang JW, 2013, 2013 IEEE 10TH INTERNATIONAL CONFERENCE ON AND 10TH INTERNATIONAL CONFERENCE ON AUTONOMIC AND TRUSTED COMPUTING (UIC/ATC) UBIQUITOUS INTELLIGENCE AND COMPUTING, P17, DOI 10.1109/UIC-ATC.2013.19
   Wang XL, 2015, IEEE WINT CONF APPL, P534, DOI 10.1109/WACV.2015.77
   Wen-Bing H., 2001, TAMKANG J SCI ENG, V4, P183
   Wu T, 2012, IEEE T INF FOREN SEC, V7, P1780, DOI 10.1109/TIFS.2012.2213812
   Yan S., 2007, P IEEE INT C COMP VI, P1
   Yan SC, 2008, INT CONF ACOUST SPEE, P737
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang M, 2011, PROC CVPR IEEE, P505, DOI 10.1109/CVPR.2011.5995481
   Yang SF, 2015, IEEE I CONF COMP VIS, P1215, DOI 10.1109/ICCV.2015.144
   Yang X, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P344, DOI 10.1109/ICCVW.2015.53
   Yang Y, 2016, IEEE IMAGE PROC, P584, DOI 10.1109/ICIP.2016.7532424
   Yoo B, 2018, IEEE SIGNAL PROC LET, V25, P808, DOI 10.1109/LSP.2018.2822241
   Zaghbani S, 2018, COMPUT ELECTR ENG, V68, P337, DOI 10.1016/j.compeleceng.2018.04.012
   Zhang Y, 2010, PROC CVPR IEEE, P2622, DOI 10.1109/CVPR.2010.5539975
   Zhuang X, 2008, P 19 INT C PATT REC, P1
NR 94
TC 8
Z9 8
U1 1
U2 8
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0269-2821
EI 1573-7462
J9 ARTIF INTELL REV
JI Artif. Intell. Rev.
PD JUN
PY 2020
VL 53
IS 5
BP 3299
EP 3347
DI 10.1007/s10462-019-09765-w
PG 49
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LP2EN
UT WOS:000534130800008
DA 2022-02-03
ER

PT J
AU Alshdadi, AA
   Mehboob, R
   Dawood, H
   Alassafi, MO
   Alghamdi, R
   Dawood, H
AF Alshdadi, Abdulrahman A.
   Mehboob, Rubab
   Dawood, Hassan
   Alassafi, Madini O.
   Alghamdi, Rayed
   Dawood, Hussain
TI Exploiting Level 1 and Level 3 features of fingerprints for liveness
   detection
SO BIOMEDICAL SIGNAL PROCESSING AND CONTROL
LA English
DT Article
DE Fingerprint recognition; Fingerprint identification; Level 1 and Level 3
   feature extraction; Live and spoof fingerprint images; Feature
   integration
ID SCALE
AB Fingerprint-based biometric systems are designed to authenticate and provide authorized access to the users in various security applications. However, such systems can be jeopardized by different kinds of presentation and spoof attacks by using different spoof materials. In this paper, an improved feature descriptor named as Quantized Fundamental Fingerprint Features (Q-FFF) is proposed for live fingerprint detection. The proposed feature descriptor integrates Level 1 and Level 3 features of fingerprints. Level 3 features of fingerprints such as ridge contours are extracted by the weighted linear combination (WLC) of magnitude of perceived stimuli and local contrast information in spatial domain. The novel magnitude of perceived stimuli with modified Weber law is proposed to compute the perceived stimuli of initial pixel intensities. The weights are explicitly determined by sigmoid function to interpolate the sparse information. Level 1 features such as ridge orientation is computed by retaining only the significant frequency components in frequency domain. Both ridge contours and ridge orientation are explicitly quantized into predefined intervals. The quantized feature sets are then integrated to populate a 2-D histogram. Performance evaluation of Q-FFF is determined on the three standard datasets of LivDet competition i.e. LivDet 2011, 2013 and 2015. Results comparison showed a reduction of Average Error Rates (AER) to 5.28, 2.0 and 4.62 on LivDet 2011, LivDet 2013 and LivDet 2015, respectively. (C) 2020 Elsevier Ltd. All rights reserved.
C1 [Dawood, Hussain] Univ Jeddah, Coll Comp Sci & Engn, Dept Comp & Network Engn, Jeddah, Saudi Arabia.
   [Mehboob, Rubab; Dawood, Hassan] Univ Engn & Technol, Dept Software Engn, Taxila 47050, Pakistan.
   [Alassafi, Madini O.; Alghamdi, Rayed] King Abdulaziz Univ, Fac Comp & IT, Dept Informat Technol, Jeddah 21589, Saudi Arabia.
   [Alshdadi, Abdulrahman A.] Univ Jeddah, Coll Comp Sci & Engn, Dept Informat Syst & Technol, Jeddah, Saudi Arabia.
C3 University of Jeddah; University of Engineering & Technology Taxila;
   King Abdulaziz University; University of Jeddah
RP Dawood, H (corresponding author), Univ Engn & Technol, Dept Software Engn, Taxila 47050, Pakistan.
EM hassan.dawood@uettaxila.edu.pk
RI Dawood, Hassan/AAZ-8114-2021; AlGhamdi, Rayed/H-8753-2012
OI AlGhamdi, Rayed/0000-0002-6277-2124; Alassafi, Madini
   O./0000-0001-9919-8368
FU Deanship of Scientific Research(DSR), King Abdulaziz University, Jeddah
   [DF-456611-1441]; DSR
FX This work was supported by the Deanship of Scientific Research(DSR),
   King Abdulaziz University, Jeddah, under grant No. (DF-456611-1441). The
   authors, therefore, gratefully acknowledge DSR technical and financial
   support.
CR Akhtar Z, 2015, IEEE SECUR PRIV, V13, P63, DOI 10.1109/MSP.2015.116
   Chen J, 2010, IEEE T PATTERN ANAL, V32, P1705, DOI 10.1109/TPAMI.2009.155
   CHEN JS, 1987, IEEE T PATTERN ANAL, V9, P584, DOI 10.1109/TPAMI.1987.4767946
   Chugh T, 2018, IEEE T INF FOREN SEC, V13, P2190, DOI 10.1109/TIFS.2018.2812193
   Coli P, 2007, 2007 IEEE WORKSHOP ON AUTOMATIC IDENTIFICATION ADVANCED TECHNOLOGIES, PROCEEDINGS, P169, DOI 10.1109/AUTOID.2007.380614
   Dawood H, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.6.063035
   Ding LJ, 2001, PATTERN RECOGN, V34, P721, DOI 10.1016/S0031-3203(00)00023-6
   Dubey RK, 2016, IEEE T INF FOREN SEC, V11, P1461, DOI 10.1109/TIFS.2016.2535899
   Ghiani L, 2017, IET BIOMETRICS, V6, P224, DOI 10.1049/iet-bmt.2016.0007
   Ghiani L, 2017, IMAGE VISION COMPUT, V58, P110, DOI 10.1016/j.imavis.2016.07.002
   Ghiani L, 2013, INT CONF BIOMETR, DOI 10.1109/ICB.2013.6613027
   Ghiani L, 2012, INT C PATT RECOG, P537
   Gragnaniello Diego, 2013, Proceedings of the 2013 IEEE Workshop on Biometric Measurements and Systems for Security and Medical Applications (BIOMS), P46, DOI 10.1109/BIOMS.2013.6656148
   Gragnaniello D, 2015, IEEE T INF FOREN SEC, V10, P849, DOI 10.1109/TIFS.2015.2404294
   Gragnaniello D, 2015, PATTERN RECOGN, V48, P1050, DOI 10.1016/j.patcog.2014.05.021
   Jain A, 2006, INT C PATT RECOG, P477
   Jia XF, 2014, INFORM SCIENCES, V268, P91, DOI 10.1016/j.ins.2013.06.041
   Jiang YJ, 2018, J ELECTR COMPUT ENG, V2018, DOI 10.1155/2018/1539298
   Jin CL, 2007, LECT NOTES COMPUT SC, V4817, P168
   Jung HY, 2018, ELECTRON LETT, V54, P564, DOI 10.1049/el.2018.0621
   Kasban H, 2016, NEUROCOMPUTING, V171, P910, DOI 10.1016/j.neucom.2015.07.030
   Kim H, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P375, DOI 10.1109/MIPR.2019.00074
   KUMAR M, 2013, SIGNAL IMAGE PROCESS, V4, P65
   Kundargi Jayshree, 2018, Proceedings of 2nd International Conference on Computer Vision & Image Processing. CVIP 2017. Advances in Intelligent Systems and Computing (703), P187, DOI 10.1007/978-981-10-7895-8_15
   Labati RD, 2018, PATTERN RECOGN LETT, V113, P58, DOI 10.1016/j.patrec.2017.04.001
   Levkine G., 2012, IMAGE PROCESS
   Marasco E, 2012, PATTERN RECOGN LETT, V33, P1148, DOI 10.1016/j.patrec.2012.01.009
   Marcel S., 2014, HDB BIOMETRIC ANTISP, V1
   Marcialis Gian Luca, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1289, DOI 10.1109/ICPR.2010.321
   Meer P, 2001, IEEE T PATTERN ANAL, V23, P1351, DOI 10.1109/34.977560
   Mehboob R, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.5.053038
   Menotti D, 2015, IEEE T INF FOREN SEC, V10, P864, DOI 10.1109/TIFS.2015.2398817
   Nikam S., 2013, CONTOURLET BASED FIN, P153
   Nikam SB, 2008, INT C WAVEL ANAL PAT, P717, DOI 10.1109/ICWAPR.2008.4635872
   Nogueira RF, 2016, IEEE T INF FOREN SEC, V11, P1206, DOI 10.1109/TIFS.2016.2520880
   Nogueira RF, 2014, 2014 IEEE WORKSHOP ON BIOMETRIC MEASUREMENTS AND SYSTEMS FOR SECURITY AND MEDICAL APPLICATIONS (BIOMS) PROCEEDINGS, P22, DOI 10.1109/BIOMS.2014.6951531
   Nosaka R., 2012, P AS C COMP VIS, P15
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ojansivu V., 2008, 2008 19 INT C PATT R, P1, DOI DOI 10.1109/ICPR.2008.4761377
   Rehman YAU, 2019, J VIS COMMUN IMAGE R, V59, P574, DOI 10.1016/j.jvcir.2019.02.014
   SCHNAPF JL, 1987, NATURE, V325, P439, DOI 10.1038/325439a0
   Shao R, 2019, IEEE T INF FOREN SEC, V14, P923, DOI 10.1109/TIFS.2018.2868230
   Sharma RP, 2019, VISUAL COMPUT, V35, P1393, DOI 10.1007/s00371-018-01618-x
   SHEPARD RN, 1987, SCIENCE, V237, P1317, DOI 10.1126/science.3629243
   Uz T, 2009, COMPUT VIS IMAGE UND, V113, P979, DOI 10.1016/j.cviu.2009.04.002
   Wayne Asera A, 2019, IEICE T INF SYST, VE102D, P1422, DOI 10.1587/transinf.2019EDL8044
   Weickert J, 2002, J VIS COMMUN IMAGE R, V13, P103, DOI 10.1006/jvci.2001.0495
   Wu M, 2020, COMPUT VIS IMAGE UND, V191, DOI 10.1016/j.cviu.2019.102866
   Xia Z., 2018, IEEE T SYST MAN CYBE
   Xia ZH, 2017, SIGNAL IMAGE VIDEO P, V11, P381, DOI 10.1007/s11760-016-0936-z
   Xu XY, 2011, PATTERN RECOGN LETT, V32, P956, DOI 10.1016/j.patrec.2011.01.021
   Xu YR, 2019, PATTERN RECOGN LETT, V125, P773, DOI 10.1016/j.patrec.2019.08.006
   Yambay D., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P208, DOI 10.1109/ICB.2012.6199810
   Yuan C., 2019, IEEE T COGNIT DEV SY
   Yuan CS, 2019, SOFT COMPUT, V23, P5157, DOI 10.1007/s00500-018-3182-1
   Yuan CS, 2019, IEEE ACCESS, V7, P26953, DOI 10.1109/ACCESS.2019.2901235
   Yuan CS, 2016, CHINA COMMUN, V13, P60, DOI 10.1109/CC.2016.7559076
   Zhao QJ, 2010, PATTERN RECOGN, V43, P2833, DOI 10.1016/j.patcog.2010.02.016
   Zhao QJ, 2010, PATTERN RECOGN, V43, P1050, DOI 10.1016/j.patcog.2009.08.004
NR 59
TC 4
Z9 4
U1 2
U2 6
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 1746-8094
EI 1746-8108
J9 BIOMED SIGNAL PROCES
JI Biomed. Signal Process. Control
PD AUG
PY 2020
VL 61
AR 102039
DI 10.1016/j.bspc.2020.102039
PG 14
WC Engineering, Biomedical
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Engineering
GA MP0TY
UT WOS:000551927300034
DA 2022-02-03
ER

PT C
AU Chen, ZJ
   Liu, HW
   Wang, Y
AF Chen, Zhangjie
   Liu, Hanwei
   Wang, Ya
BE Pakzad, S
TI Support Vector Machine-Based Face Direction Detection Using an Infrared
   Array Sensor
SO DYNAMICS OF CIVIL STRUCTURES, VOL 2
SE Conference Proceedings of the Society for Experimental Mechanics Series
LA English
DT Proceedings Paper
CT 36th International Modal Analysis Conference and Exposition (IMAC) on
   Structural Dynamics
CY FEB 12-15, 2018
CL Orlando, FL
DE Infrared array sensor; Facing direction; SVM; CNN; Human computer
   interaction
AB Facing direction detection plays a critical role in human computer interaction, such as face recognition and head pose estimation in biometric identification, spatial microphone/loudspeaker devices, virtual reality games and etc. Currently detection methods are mainly focused on extracting specific patterns of various facial features from the user's optical images, which raises concerns on privacy invasion and these detection techniques do not usually work in the dark environment. To address these concerns, this paper proposes a pervasive solution for a coarse facing direction detection using a low pixel infrared thermopile array sensor. Support vector machine (SVM) method is selected as the classifier. Two methods for feature extraction are compared. One is based on pre-defined features and the other is based on pre-trained convolutional neural network (CNN) model. The detection accuracy resulted from using pre-defined features reaches 87% for identifying five different facing directions up to 1.2 m. However, this accuracy is largely descended when the detection range is increased to 1.8 m. Interestingly, the accuracy resulted from using pre-trained CNN features, however, demonstrates a reliable and steady performance up to 1.8 m. The accuracy keeps above 95% at both detection ranges (1.2 and 1.8 m). This proposed solution leads to many advantages, such as low resolution leading to no intention on privacy invasion, and the low-cost intriguing a potentially large market for human computer interaction in smart home appliances control and computer games.
C1 [Chen, Zhangjie; Wang, Ya] SUNY Stony Brook, Dept Mech Engn, Stony Brook, NY 11794 USA.
   [Liu, Hanwei] SUNY Stony Brook, Dept Elect Engn, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook
RP Wang, Y (corresponding author), SUNY Stony Brook, Dept Mech Engn, Stony Brook, NY 11794 USA.
EM ya.s.wang@stonybrook.edu
CR Al-Rahayfeh A, 2013, IEEE SIG PROC MED
   Al-Rahayfeh A, 2013, IEEE J TRANSL ENG HE, V1, DOI 10.1109/JTEHM.2013.2289879
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Kim S., 2010, 2010 3 INT C BIOM EN
   Liu K., 2008, VEH POW PROP C VPPC
   Manogna S, 2010, INT CONF BIOINFORM
   Miyanokoshi Y., 2006, SICEICASE INT JOINT, P5429
   Murphy-Chutorian E, 2010, IEEE T INTELL TRANSP, V11, P300, DOI 10.1109/TITS.2010.2044241
   O'Regan S, 2013, J NEUROSCI METH, V218, P110, DOI 10.1016/j.jneumeth.2013.04.017
   Paone J, 2015, IEEE INT VEH SYM, P174, DOI 10.1109/IVS.2015.7225682
   Sasou A., 2009, 2 INT C ROB COMM COO
   Tyndall A, 2016, IEEE SENS J, V16, P3784, DOI 10.1109/JSEN.2016.2530824
   Ugurlu Y., 2012, 2012 21 INT C PATT R
   Xie D., 2012, 2012 9 INT C FUZZ SY
   Yim J., 2008, P 2008 C FUT PLAY RE
   Yun J, 2014, IEEE SENS J, V14, P1482, DOI 10.1109/JSEN.2013.2296601
   Zisserman A, 2016, VGG CONVOLUTIONAL NE
NR 17
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 2191-5644
EI 2191-5652
BN 978-3-319-74421-6; 978-3-319-74420-9
J9 C PROC SOC EXP MECH
PY 2019
BP 309
EP 317
DI 10.1007/978-3-319-74421-6_41
PG 9
WC Engineering, Civil; Engineering, Mechanical
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Engineering
GA BP5SZ
UT WOS:000558187900041
DA 2022-02-03
ER

PT J
AU Kim, CM
   Hong, EJ
   Chung, K
   Park, RC
AF Kim, Chang-Min
   Hong, Ellen J.
   Chung, Kyungyong
   Park, Roy C.
TI Driver Facial Expression Analysis Using LFA-CRNN-Based Feature
   Extraction for Health-Risk Decisions
SO APPLIED SCIENCES-BASEL
LA English
DT Article
DE facial expression analysis; line segment feature analysis;
   dimensionality reduction; convolutional recurrent neural network; driver
   health risk
ID NEURAL-NETWORK; RECOGNITION; SYSTEM
AB As people communicate with each other, they use gestures and facial expressions as a means to convey and understand emotional state. Non-verbal means of communication are essential to understanding, based on external clues to a person's emotional state. Recently, active studies have been conducted on the lifecare service of analyzing users' facial expressions. Yet, rather than a service necessary for everyday life, the service is currently provided only for health care centers or certain medical institutions. It is necessary to conduct studies to prevent accidents that suddenly occur in everyday life and to cope with emergencies. Thus, we propose facial expression analysis using line-segment feature analysis-convolutional recurrent neural network (LFA-CRNN) feature extraction for health-risk assessments of drivers. The purpose of such an analysis is to manage and monitor patients with chronic diseases who are rapidly increasing in number. To prevent automobile accidents and to respond to emergency situations due to acute diseases, we propose a service that monitors a driver's facial expressions to assess health risks and alert the driver to risk-related matters while driving. To identify health risks, deep learning technology is used to recognize expressions of pain and to determine if a person is in pain while driving. Since the amount of input-image data is large, analyzing facial expressions accurately is difficult for a process with limited resources while providing the service on a real-time basis. Accordingly, a line-segment feature analysis algorithm is proposed to reduce the amount of data, and the LFA-CRNN model was designed for this purpose. Through this model, the severity of a driver's pain is classified into one of nine types. The LFA-CRNN model consists of one convolution layer that is reshaped and delivered into two bidirectional gated recurrent unit layers. Finally, biometric data are classified through softmax. In addition, to evaluate the performance of LFA-CRNN, the performance was compared through the CRNN and AlexNet Models based on the University of Northern British Columbia and McMaster University (UNBC-McMaster) database.
C1 [Kim, Chang-Min] Sangji Univ, Div Comp Informat & Engn, Wonju 26339, South Korea.
   [Hong, Ellen J.] Yonsei Univ, Dept Comp & Telecommun Engn, Wonju 26493, South Korea.
   [Chung, Kyungyong] Kyonggi Univ, Div Comp Sci & Engn, Suwon 16227, South Korea.
   [Park, Roy C.] Sangji Univ, Dept Informat Commun Software Engn, Wonju 26339, South Korea.
C3 Sangji University; Yonsei University; Kyonggi University; Sangji
   University
RP Park, RC (corresponding author), Sangji Univ, Dept Informat Commun Software Engn, Wonju 26339, South Korea.
EM changingstart@gmail.com; jeonghee.hong@gmail.com; dragonhci@gmail.com;
   roypark1984@gmail.com
OI Chung, Kyungyong/0000-0002-6439-9992; Hong, Jeong
   hee/0000-0002-0948-9944
FU Korea Agency for Infrastructure Technology Advancement (KAIA) - Ministry
   of Land, Infrastructure and Transport [20CTAP-C157011-01]
FX This work is supported by the Korea Agency for Infrastructure Technology
   Advancement (KAIA) grant funded by the Ministry of Land, Infrastructure
   and Transport (Grant 20CTAP-C157011-01).
CR Agbolade O, 2019, BMC BIOINFORMATICS, V20, DOI 10.1186/s12859-019-3153-2
   Baek JW, 2020, IEEE ACCESS, V8, P18171, DOI 10.1109/ACCESS.2020.2968393
   Baek JW, 2021, MULTIMED TOOLS APPL, V80, P34499, DOI 10.1007/s11042-019-08607-9
   Chung K, 2020, PEER PEER NETW APPL, V13, P610, DOI 10.1007/s12083-019-00791-7
   Chung K, 2019, TECHNOL HEALTH CARE, V27, P473, DOI 10.3233/THC-191731
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Guo JZ, 2018, IEEE ACCESS, V6, P26391, DOI 10.1109/ACCESS.2018.2831927
   Halim Sobia Ahsan, 2018, International Journal of Biology and Biotechnology, V15, P29
   Han XB, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9080848
   Haz H., 2020, ADV COMPUT RES, V10, P34, DOI [10.19101/ijacr.2019.940117, DOI 10.19101/IJACR.2019.940117]
   Hu YY, 2018, IEEE ACCESS, V6, P10402, DOI 10.1109/ACCESS.2018.2808337
   Jabon ME, 2011, IEEE PERVAS COMPUT, V10, P84, DOI 10.1109/MPRV.2010.46
   Jeong M, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18124270
   JIE YEEM MYEONG, 2019, [JOURNAL OF EMOTIONAL & BEHAVIORAL DISORDERS, 정서Â·행동장애연구], V35, P265, DOI 10.33770/JEBD.35.2.13
   Kang Hyunwoo, 2017, [JOURNAL OF KOREA MULTIMEDIA SOCIETY, 멀티미디어학회논문지], V20, P748, DOI 10.9717/kmms.2017.20.5.748
   Kang JS, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9204284
   Kang X, 2018, IEEE-CAA J AUTOMATIC, V5, P204, DOI 10.1109/JAS.2017.7510421
   Kim J, 2019, WIRELESS PERS COMMUN, V105, P655, DOI 10.1007/s11277-018-5978-9
   Kim JC, 2019, KSII T INTERNET INF, V13, P2060, DOI 10.3837/tiis.2019.04.018
   Lenc L, 2015, COMPUT ELECTR ENG, V46, P256, DOI 10.1016/j.compeleceng.2015.01.014
   Lim Kil-Taek, 2016, [JOURNAL OF KOREA MULTIMEDIA SOCIETY, 멀티미디어학회논문지], V19, P1014, DOI 10.9717/kmms.2016.19.6.1014
   Liu L, 2019, IEEE ACCESS, V7, P36140, DOI 10.1109/ACCESS.2019.2904881
   Lucey P, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P57, DOI 10.1109/FG.2011.5771462
   Luttrell J, 2017, PROCEEDINGS 2017 INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE AND COMPUTATIONAL INTELLIGENCE (CSCI), P576, DOI 10.1109/CSCI.2017.98
   Muhammad G, 2017, IEEE ACCESS, V5, P10871, DOI 10.1109/ACCESS.2017.2712788
   Nassih B, 2019, PROCEDIA COMPUT SCI, V148, P116, DOI 10.1016/j.procs.2019.01.015
   Olderbak S, 2019, COGNITION EMOTION, V33, P579, DOI 10.1080/02699931.2018.1454403
   Pan SW, 2019, IEEE ACCESS, V7, P177997, DOI 10.1109/ACCESS.2019.2958825
   Park BH, 2017, EXPERT SYST APPL, V89, P66, DOI 10.1016/j.eswa.2017.07.018
   Peng DD, 2019, IEEE ACCESS, V7, P10278, DOI 10.1109/ACCESS.2018.2888842
   Perlovsky L, 2019, PHYS LIFE REV, V31, P257, DOI 10.1016/j.plrev.2019.10.007
   Poria S, 2019, IEEE ACCESS, V7, P100943, DOI 10.1109/ACCESS.2019.2929050
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   SeungTakRa, 2018, [Journal of IKEEE, 전기전자학회논문지], V22, P933, DOI 10.7471/ikeee.2018.22.4.933
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shin DH, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9224830
   Song XN, 2019, PATTERN RECOGN, V88, P127, DOI 10.1016/j.patcog.2018.11.008
   Sun Yi, 2015, ARXIV150200873, P1
   Sun Yi, 2014, ADV NEURAL INFORM PR
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Wang YY, 2019, INFORMATION, V10, DOI 10.3390/info10120375
   Yuan QY, 2019, IEEE ACCESS, V7, P7286, DOI 10.1109/ACCESS.2018.2890373
   Zhang SL, 2018, IEEE ACCESS, V6, P7675, DOI 10.1109/ACCESS.2017.2785763
NR 43
TC 11
Z9 11
U1 6
U2 7
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2076-3417
J9 APPL SCI-BASEL
JI Appl. Sci.-Basel
PD FEB
PY 2020
VL 10
IS 8
AR 2956
DI 10.3390/app10082956
PG 19
WC Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Materials Science; Physics
GA LO0XE
UT WOS:000533352100320
OA gold
DA 2022-02-03
ER

PT J
AU Tan, KP
   Kanniah, KD
   Cracknell, AP
AF Tan, Kian Pang
   Kanniah, Kasturi Devi
   Cracknell, Arthur Philip
TI On the upstream inputs into the MODIS primary productivity products
   using biometric data from oil palm plantations
SO INTERNATIONAL JOURNAL OF REMOTE SENSING
LA English
DT Article
ID GROSS PRIMARY PRODUCTIVITY; NET PRIMARY PRODUCTION; LEAF-AREA INDEX;
   UK-DMC 2; TROPICAL SAVANNA; RADIATION; COVER; TREES
AB This study evaluated the influence of upstream inputs into the Moderate Resolution Imaging Spectroradiometer (MODIS) primary productivity products, termed the MOD17, at tropical oil palm plantations (Elaeis guineensis Jacq.). Evaluation of MOD17 using oil palm plantations as test sites is ideal because the plantations are cultivated on large areas which are comparable with the size of MODIS pixels. It is difficult to find test sites covered by other single species in a whole pixel. The upstream inputs studied included (1) MODIS land cover, (2) the National Centers for Environmental Prediction-Department of Energy (NCEP-DOE) Reanalysis 2 meteorological data set, (3) MODIS leaf area index/fraction of photosynthetically active radiation (LAI/fPAR), and (4) MODIS maximum light-use efficiency (maximum LUE). Oil palm biometric and local meteorological data were utilized as ground data. Furthermore, scaling up oil palm LAI and fPAR from plot scale to regional scale (Peninsular Malaysia) was done empirically by correlating oil palm LAI derived from the hemispherical photography technique with radiance information from the Disaster Monitoring Constellation 2 satellite (UK-DMC 2). The upscaled LAI/fPAR developed in this study was used to evaluate the MODIS LAI/fPAR. The results showed that the MODIS land-cover product has an overall accuracy of 78.8% when compared to the Peninsular Malaysia land-use map produced by the Department of Agriculture, Malaysia. Regarding the NCEP-DOE Reanalysis 2 data set, vapour pressure deficit (VPD) and photosynthetically active radiation (PAR) contain large uncertainties in our study area. However, MODIS LAI and fPAR were correlated relatively well with the upscaled LAI (R-2 = 0.50) and the upscaled fPAR (R-2 = 0.60), respectively. The constant values of maximum LUE for croplands and evergreen broadleaf forest ecosystems are lower than the maximum LUE of oil palm. The relative predictive error assessment showed that the MOD17 net primary productivity (NPP) overestimated oil palm NPP derived from biometric methods by 142-204%. We replaced the upstream inputs of MOD17 by the local inputs for estimating oil palm GPP and NPP in Peninsular Malaysia. This was done by (1) assigning maximum LUE for oil palm plantations as a constant at 1.68 g C m(-2) day(-1), (2) utilizing meteorological data from local meteorological stations, and (3) using the upscaled fPAR of oil palm plantations. The amount of oil palm GPP and NPP for Peninsular Malaysia in 2010 were estimated to be similar to 0.09 Pg C year(-1) (or equivalent to similar to 0.33 Pg CO2 year(-1)) and similar to 0.03 Pg C year(-1) (similar to 0.11 Pg CO2 year(-1)), respectively, indicating that oil palm plantations in Peninsular Malaysia can play an important role in global carbon sequestration. In the future there is likely to be a demand for MODIS GPP and NPP products that are more accurate than those currently generated by MOD17. We recommend future developments of the MOD17 processing system to allow improvements in the upstream input parameters, in the manner described in this article, both for global processing and for the production of more accurate values for GPP and NPP at regional and local scales.
C1 [Tan, Kian Pang; Kanniah, Kasturi Devi] Univ Teknol Malaysia, Fac Geoinformat & Real Estate, Dept Geoinformat, Skudai 81310, Johor, Malaysia.
   [Cracknell, Arthur Philip] Univ Dundee, Div Elect Engn & Phys, Dundee DDI 4HN, Scotland.
C3 Universiti Teknologi Malaysia; University of Dundee
RP Kanniah, KD (corresponding author), Univ Teknol Malaysia, Fac Geoinformat & Real Estate, Dept Geoinformat, Skudai 81310, Johor, Malaysia.
EM kasturi@utm.my
RI Kanniah, Kasturi Devi/K-6777-2012
OI Kanniah, Kasturi Devi/0000-0001-6736-4819
FU Universiti Teknologi Malaysia (UTM) [Q. J130000.2527.03H23,
   Q.J130000.2527.04H96]; Ministry of Higher Education of Malaysia (MOHE);
   UTM Zamalah Scholarship
FX This study was funded by Universiti Teknologi Malaysia (UTM) research
   grants (vote no: Q. J130000.2527.03H23 and Q.J130000.2527.04H96) and the
   Ministry of Higher Education of Malaysia (MOHE). The first author was
   the recipient of the UTM Zamalah Scholarship.
CR Awal MA, 2010, PERTANIKA J SCI TECH, V18, P23
   Beringer J, 2011, B AM METEOROL SOC, V92, P1467, DOI 10.1175/2011BAMS2948.1
   CHAVEZ PS, 1988, REMOTE SENS ENVIRON, V24, P459, DOI 10.1016/0034-4257(88)90019-3
   CHEN JM, 1992, PLANT CELL ENVIRON, V15, P421, DOI 10.1111/j.1365-3040.1992.tb00992.x
   Chen XY, 2003, OECOLOGIA, V137, P405, DOI 10.1007/s00442-003-1358-5
   Clark DA, 2001, ECOL APPL, V11, P356, DOI 10.1890/1051-0761(2001)011[0356:MNPPIF]2.0.CO;2
   COHEN WB, 1991, PHOTOGRAMM ENG REM S, V57, P195
   Corley RHV, 2003, OIL PALM
   CRACKNELL AP, 2010, P REM SENS PHOT SOC
   Cracknell AP, 2013, INT J REMOTE SENS, V34, P7400, DOI 10.1080/01431161.2013.820367
   CROWLEY G, 2010, DMC DATA PRODUCT MAN
   De Kauwe MG, 2011, REMOTE SENS ENVIRON, V115, P767, DOI 10.1016/j.rse.2010.11.004
   Demarez V, 2008, AGR FOREST METEOROL, V148, P644, DOI 10.1016/j.agrformet.2007.11.015
   *FED DEP TOWN COUN, 2010, NAT PHYS PLAN 2
   Gebremichael M, 2006, REMOTE SENS ENVIRON, V100, P150, DOI 10.1016/j.rse.2005.10.009
   Gong P, 2013, INT J REMOTE SENS, V34, P2607, DOI 10.1080/01431161.2012.748992
   Heinsch F.A., 2003, USERS GUIDE GPP NPP
   Heinsch FA, 2006, IEEE T GEOSCI REMOTE, V44, P1908, DOI 10.1109/TGRS.2005.853936
   Henson I. E., 2005, Journal of Oil Palm Research, V17, P73
   Henson I. E., 2008, Planter, V84, P445
   Henson I. E., 2003, Oil Palm Bulletin, P15
   Henson IE, 2012, J OIL PALM RES, V24, P1473
   HUETE A R, 1988, Remote Sensing of Environment, V25, P295
   Huston MA, 2009, ECOL MONOGR, V79, P343, DOI 10.1890/08-0588.1
   JARVIS PG, 1983, PRODUCTIVITY TEMPERA
   Kanamitsu M, 2002, B AM METEOROL SOC, V83, P1631, DOI [10.1175/BAMS-83-11-1631(2002)083<1631:NAR>2.3.CO;2, 10.1175/Bams-83-11-1631]
   Kanniah KD, 2009, REMOTE SENS ENVIRON, V113, P1808, DOI 10.1016/j.rse.2009.04.013
   Kanniah KD, 2012, INT GEOSCI REMOTE SE, P6569, DOI 10.1109/IGARSS.2012.6352094
   Kanniah KD, 2012, PROG PHYS GEOG, V36, P209, DOI 10.1177/0309133311434244
   Kobayashi H, 2004, J GEOPHYS RES-ATMOS, V109, DOI 10.1029/2003JD003807
   *LAND PROC DISTR A, 2013, LAND COV TYP YEARL L
   LIETH H, 1975, PRIMARY PRODUCTION B, P203, DOI DOI 10.1007/978-3-642-80913-2
   *MAL PALM OIL BOAR, 2013, OIL PALM PLANT AR ST
   Mohd Roslan M. N., 2004, Oil Palm Bulletin, P11
   MONTEITH JL, 1972, J APPL ECOL, V9, P747, DOI 10.2307/2401901
   Murray FW., 1967, J APPL METEOROL, V6, P203
   Ni J, 2004, PLANT ECOL, V174, P217, DOI 10.1023/B:VEGE.0000049097.85960.10
   Ohtsuka T, 2005, AGR FOREST METEOROL, V134, P27, DOI 10.1016/j.agrformet.2005.11.005
   Propastin P, 2012, REMOTE SENS ENVIRON, V121, P252, DOI 10.1016/j.rse.2012.02.005
   Rouse J.W., 1974, NASAGSFC TYPE 3 FINA, V351, P309
   SHABANOV N, 2007, COLLECTION 5 MODIS L
   SHARMA B, 2009, MODELLING CARBON STO
   Sjostrom M, 2013, REMOTE SENS ENVIRON, V131, P275, DOI 10.1016/j.rse.2012.12.023
   SQUIRE GR, 1984, INTERNAL REPORT
   Tan B, 2005, J GEOPHYS RES-ATMOS, V110, DOI 10.1029/2004JD004860
   Tan KP, 2013, INT J REMOTE SENS, V34, P7424, DOI 10.1080/01431161.2013.822601
   Tan KP, 2012, PROG PHYS GEOG, V36, P655, DOI 10.1177/0309133312452187
   Tan KP, 2011, INT GEOSCI REMOTE SE, P756, DOI 10.1109/IGARSS.2011.6049240
   Turner DP, 2006, IEEE T GEOSCI REMOTE, V44, P1899, DOI 10.1109/TGRS.2006.876027
   Weiss M., 2010, CAN EYE V6 1 USER MA
   Zhao MS, 2010, SCIENCE, V329, P940, DOI [10.1126/science.1192666, 10.1126/science.1189590]
   Zhao MS, 2005, REMOTE SENS ENVIRON, V95, P164, DOI 10.1016/j.rse.2004.12.011
   2005, POTENTIAL OIL PALM F
NR 53
TC 16
Z9 16
U1 0
U2 23
PU TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN 0143-1161
EI 1366-5901
J9 INT J REMOTE SENS
JI Int. J. Remote Sens.
PD MAR 19
PY 2014
VL 35
IS 6
BP 2215
EP 2246
DI 10.1080/01431161.2014.889865
PG 32
WC Remote Sensing; Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Remote Sensing; Imaging Science & Photographic Technology
GA AE4YU
UT WOS:000333995200011
DA 2022-02-03
ER

PT J
AU Li, WJ
   Tan, J
   Meng, WZ
   Wang, Y
AF Li, Wenjuan
   Tan, Jiao
   Meng, Weizhi
   Wang, Yu
TI A swipe-based unlocking mechanism with supervised learning on
   smartphones: Design and evaluation
SO JOURNAL OF NETWORK AND COMPUTER APPLICATIONS
LA English
DT Article
DE User authentication; Behavioral biometric; Swipe behavior; Smartphone
   security; Touch dynamics
ID GRAPHICAL PASSWORD; AUTHENTICATION; TOUCH; SECURITY
AB With the rapid development of mobile devices, smartphones have become common in people's daily lives, i.e., retrieving community happenings and connecting with peers. Due to the convenience, users often store a large amount of private information on their phones (e.g., photos) and use the phone to process sensitive operations (e.g., financial transactions). Thus, there is a great need to protect the devices from unauthorized access in order to avoid privacy leakage and financial loss. Passwords are the most widely used authentication method, but attackers can take over the phone after it is unlocked. Instead, behavioral authentication can verify current users in a continuous way, which can complement the existing authentication mechanisms like passwords. With the increasing capability of smartphone sensors, users can perform various touch actions to interact with their devices. Motivated by this, in this work, we focus on swipe behavior and develop SwipeVlock, a supervised unlocking mechanism on smartphones, which can authenticate users based on their way of swiping the phone screen with a background image. In the evaluation, we measure several typical supervised learning algorithms and conduct two user studies with over 150 participants. As compared with similar schemes, it is found that participants could perform well with SwipeVLock, i.e., with a success rate of 98% during login and retention.
C1 [Li, Wenjuan; Meng, Weizhi; Wang, Yu] Guangzhou Univ, Inst Artificial Intelligence & Blockchain, Guangzhou, Guangdong, Peoples R China.
   [Li, Wenjuan] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Tan, Jiao] KOTO Res Ctr, Macau, Peoples R China.
   [Meng, Weizhi] Tech Univ Denmark, Dept Appl Math & Comp Sci, Odense, Denmark.
C3 Guangzhou University; Hong Kong Polytechnic University; Technical
   University of Denmark
RP Meng, WZ (corresponding author), Tech Univ Denmark, Dept Appl Math & Comp Sci, Odense, Denmark.
EM weme@dtu.dk
OI Li, Wenjuan/0000-0003-3745-5669
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China (NSFC) [61802077]
FX We would like to thank the participants for their hard work in the user
   study. This work was partially supported by National Natural Science
   Foundation of China (No. 61802077).
CR [Anonymous], 2010, P 4 USENIX C OFF TEC
   Bonneau J, 2012, P IEEE S SECUR PRIV, P538, DOI 10.1109/SP.2012.49
   Chiasson S., 2007, P 3 S US PRIV SEC, P1, DOI DOI 10.1145/1280680.1280682
   Chiasson S, 2012, IEEE T DEPEND SECURE, V9, P222, DOI 10.1109/TDSC.2011.55
   Churchill Berkeley, 2013, UNLOCK PATTERN GENER
   Davis D, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 13TH USENIX SECURITY SYMPOSIUM, P151
   Dirik Ahmet Emir, 2007, P 3 S US PRIV SEC, P20, DOI [10.1145/1280680.1280684, DOI 10.1145/1280680.1280684]
   Dunphy P, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P36
   Feng T, 2012, 2012 IEEE INTERNATIONAL CONFERENCE ON TECHNOLOGIES FOR HOMELAND SECURITY, P451, DOI 10.1109/THS.2012.6459891
   Fox S., 2010, FUTURE ONLINE PASSWO
   Frank M, 2013, IEEE T INF FOREN SEC, V8, P136, DOI 10.1109/TIFS.2012.2225048
   Gyorffy JC, 2011, INT J INF SECUR, V10, P321, DOI 10.1007/s10207-011-0147-0
   Hang Alina, 2012, P ACM CHI, P987, DOI DOI 10.1145/2207676.2208544
   Jermyn I, 1999, USENIX ASSOCIATION PROCEEDINGS OF THE EIGHTH USENIX SECURITY SYMPOSIUM (SECURITY '99), P1
   Karger D.R., 2003, INT C MACH LEARN ICM, P616, DOI DOI 10.1186/1477-3155-8-16
   Lavanya S, 2019, MOBILE NETW APPL, V24, P1152, DOI 10.1007/s11036-019-01252-4
   Lee SC, 2019, INT J HUM-COMPUT INT, V35, P1532, DOI 10.1080/10447318.2018.1554320
   Lin D., 2007, P 3 S US PRIV SEC SO, P161, DOI DOI 10.1145/1280680.1280708
   Meng B, 2015, 2015 INTERNATIONAL CONFERENCE ON APPLIED MECHANICS AND MECHATRONICS ENGINEERING (AMME 2015), P147
   Meng W, 2017, P ICISC, P327
   Meng WZ, 2019, FUTURE GENER COMP SY, V101, P1018, DOI 10.1016/j.future.2019.07.038
   Meng WZ, 2017, LECT NOTES COMPUT SC, V10343, P301, DOI 10.1007/978-3-319-59870-3_17
   Meng WZ, 2017, COMPUT SECUR, V65, P213, DOI 10.1016/j.cose.2016.11.010
   Meng WZ, 2016, LECT NOTES COMPUT SC, V9696, P629, DOI 10.1007/978-3-319-39555-5_34
   Meng WZ, 2015, IEEE COMMUN SURV TUT, V17, P1268, DOI 10.1109/COMST.2014.2386915
   Meng Y., 2012, P 2012 ACM RES APPL, P322
   Meng YX, 2013, IFIP ADV INF COMM TE, V405, P55
   NELSON DL, 1976, J EXP PSYCHOL-HUM L, V2, P523, DOI 10.1037/0278-7393.2.5.523
   Nyang D, 2018, COMPUT SECUR, V78, P1, DOI 10.1016/j.cose.2018.05.012
   Quinlan JR, 1996, J ARTIF INTELL RES, V4, P77, DOI 10.1613/jair.279
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Shahzad M, 2017, IEEE T MOBILE COMPUT, V16, P2726, DOI 10.1109/TMC.2016.2635643
   Sharma V, 2017, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON SECURITY AND PRIVACY IN WIRELESS AND MOBILE NETWORKS (WISEC 2017), P1, DOI 10.1145/3098243.3098262
   SHEPARD RN, 1967, J VERB LEARN VERB BE, V6, P156, DOI 10.1016/S0022-5371(67)80067-7
   Smith-Creasey Max, 2016, 2016 14th Annual Conference on Privacy, Security and Trust (PST), P104, DOI 10.1109/PST.2016.7906944
   Spitzer J., 2010, J COMPUT SCI COLL, V26, P7
   SplashData Inc, WORST PASSW 2018
   Sun H, 2012, P ASIACCS, P99
   Suo XY, 2005, 21ST ANNUAL COMPUTER SECURITY APPLICATIONS CONFERENCE, PROCEEDINGS, P419
   Tao H., 2008, INT J NETW SECUR, V7, P273, DOI 10.6633/IJNS.200809.7(2).18
   Thorpe TA, 2013, PLANT TISSUE CULTURE: TECHNIQUES AND EXPERIMENTS, 3RD EDITION, P1, DOI 10.1016/B978-0-12-415920-4.00001-3
   Weir M, 2010, PROCEEDINGS OF THE 17TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'10), P162, DOI 10.1145/1866307.1866327
   Weizhi Meng, 2017, Information Security. 20th International Conference, ISC 2017. Proceedings: LNCS 10599, P291, DOI 10.1007/978-3-319-69659-1_16
   Weizhi Meng, 2017, Applied Cryptography and Network Security. 15th International Conference, ACNS 2017. Proceedings: LNCS 10355, P145, DOI 10.1007/978-3-319-61204-1_8
   Wenjuan Li, 2019, Machine Learning for Cyber Security. Second International Conference, ML4CS 2019. Proceedings: Lecture Notes in Computer Science (LNCS 11806), P140, DOI 10.1007/978-3-030-30619-9_11
   Wiedenbeck S, 2005, INT J HUM-COMPUT ST, V63, P102, DOI 10.1016/j.ijhcs.2005.04.010
   Yan J, 2004, IEEE SECUR PRIV, V2, P25, DOI 10.1109/MSP.2004.81
   Yang YF, 2019, AD HOC NETW, V84, P9, DOI 10.1016/j.adhoc.2018.09.015
   Yildiz C., 2013, P 6 ACM C SEC PRIV W, P1
   Yu XJ, 2017, COMPUT SECUR, V70, P179, DOI 10.1016/j.cose.2017.05.006
   Yuxin Meng, 2013, Information Security and Cryptology. 8th International Conference, Inscrypt 2012. Revised Selected Papers, P331, DOI 10.1007/978-3-642-38519-3_21
   Yuxin Meng, 2012, Information and Communication Security. 14th International Conference (ICICS 2012). Proceedings, P349, DOI 10.1007/978-3-642-34129-8_32
   Yuxin Meng, 2012, 2012 IEEE 7th International Conference on Networking, Architecture, and Storage (NAS), P39, DOI 10.1109/NAS.2012.9
   Zhao X, 2014, IEEE T INF FOREN SEC, V9, DOI 10.1109/TIFS.2014.2350916
   Zheng N, 2014, I C NETWORK PROTOCOL, P221, DOI 10.1109/ICNP.2014.43
NR 55
TC 4
Z9 4
U1 0
U2 7
PU ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD
PI LONDON
PA 24-28 OVAL RD, LONDON NW1 7DX, ENGLAND
SN 1084-8045
EI 1095-8592
J9 J NETW COMPUT APPL
JI J. Netw. Comput. Appl.
PD SEP 1
PY 2020
VL 165
AR 102687
DI 10.1016/j.jnca.2020.102687
PG 8
WC Computer Science, Hardware & Architecture; Computer Science,
   Interdisciplinary Applications; Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MO1CR
UT WOS:000551273600008
DA 2022-02-03
ER

PT J
AU Kim, JJ
   Stafford, GR
   Beauchamp, C
   Kim, SA
AF Kim, Jeffrey J.
   Stafford, Gery R.
   Beauchamp, Carlos
   Kim, Shin Ae
TI Development of a Dental Implantable Temperature Sensor for Real-Time
   Diagnosis of Infectious Disease
SO SENSORS
LA English
DT Article
DE implantable temperature sensor; dental implants; peri-implantitis;
   flexible sensor; polyimide film; microfabrication; local and continuous
   monitoring; early diagnosis
ID MOUTHGUARD BIOSENSOR; WOUND-INFECTION; BIOCOMPATIBILITY; TELEMETRY;
   DEVICE; SALIVA
AB Implantable sensors capable of real-time measurements are powerful tools to diagnose disease and maintain health by providing continuous or regular biometric monitoring. In this paper, we present a dental implantable temperature sensor that can send early warning signals in real time before the implant fails. Using a microfabrication process on a flexible polyimide film, we successfully fabricated a multi-channel temperature sensor that can be wrapped around a dental implant abutment wing. In addition, the feasibility, durability, and implantability of the sensor were investigated. First, high linearity and repeatability between electrical resistance and temperature confirmed the feasibility of the sensor with a temperature coefficient of resistance (TCR) value of 3.33 x 10(-3)/degrees C between 20 and 100 degrees C. Second, constant TCR values and robust optical images without damage validated sufficient thermal, chemical, and mechanical durability in the sensor's performance and structures. Lastly, the elastic response of the sensor's flexible substrate film to thermal and humidity variations, simulating in the oral environment, suggested its successful long-term implantability. Based on these findings, we have successfully developed a polymer-based flexible temperature sensor for dental implant systems.
C1 [Kim, Jeffrey J.; Kim, Shin Ae] Amer Dent Assoc, Amer Dent Assoc Sci & Res Inst, Gaithersburg, MD 20899 USA.
   [Stafford, Gery R.; Beauchamp, Carlos] NIST, Mat Sci & Engn Lab, Gaithersburg, MD 20899 USA.
   [Kim, Jeffrey J.] US Navy, Naval Postgrad Dent Sch, Bethesda, MD 20899 USA.
C3 National Institute of Standards & Technology (NIST) - USA
RP Kim, SA (corresponding author), Amer Dent Assoc, Amer Dent Assoc Sci & Res Inst, Gaithersburg, MD 20899 USA.
EM kimj@ada.org; gery.stafford@nist.gov; carlos.beauchamp@nist.gov;
   kims@ada.org
OI Kim, Shinae/0000-0002-6729-8097
FU American Dental Association Science & Research Institute (ADASRI), LLC;
   ADAF; ADAAmerican Diabetes Association
FX Partial financial support was provided through the American Dental
   Association Science & Research Institute (ADASRI), LLC. Research
   performed in part at the Center for Nanoscale Science and Technology
   (CNST) at the National Institute of Standards and Technology (NIST). The
   authors would like to thank ADAF and ADA for their partial financial
   support and also thank the NIST staff and ADASRI colleagues for their
   technical support.
CR Ahn D, 2016, IEEE T BIOMED CIRC S, V10, P125, DOI 10.1109/TBCAS.2014.2370794
   Badr BM, 2017, WIREL POWER TRANSF, V4, P21, DOI 10.1017/wpt.2016.12
   Berglundh T, 2002, J CLIN PERIODONTOL, V29, P197, DOI 10.1034/j.1600-051X.29.s3.12.x
   Cao H, 2015, SENSORS, V15, P28889, DOI DOI 10.3390/S151128889
   Chaimanonart Nattapon, 2009, 15th International Conference on Solid-State Sensors, Actuators and Microsystems. Transducers 2009, P1473, DOI 10.1109/SENSOR.2009.5285822
   Chanmugam A, 2017, ADV SKIN WOUND CARE, V30, P406, DOI 10.1097/01.ASW.0000522161.13573.62
   Cheng C.-M., 2015, IN VITRO DIAGNOSTIC
   Choi Jung Eun, 2017, BDJ Open, V3, P17015, DOI 10.1038/bdjopen.2017.15
   Choi Jung Eun, 2016, BDJ Open, V2, P16008, DOI 10.1038/bdjopen.2016.8
   FEDI PF, 1992, J PERIODONTOL, V63, P24, DOI 10.1902/jop.1992.63.1.24
   Fierheller M, 2010, ADV SKIN WOUND CARE, V23, P369, DOI 10.1097/01.ASW.0000383197.28192.98
   Floro JA, 1997, J ELECTRON MATER, V26, P969, DOI 10.1007/s11664-997-0233-2
   Frieberg BR, 2016, ACS APPL MATER INTER, V8, P33240, DOI 10.1021/acsami.6b12423
   Froum SJ, 2012, INT J PERIODONT REST, V32, P533
   GRAF H, 1966, HELV ODONTOL ACTA, V10, P94
   GRAF H, 1969, ARCH ORAL BIOL, V14, P259, DOI 10.1016/0003-9969(69)90228-3
   Kang SK, 2016, NATURE, V530, P71, DOI 10.1038/nature16492
   Kim J, 2015, BIOSENS BIOELECTRON, V74, P1061, DOI 10.1016/j.bios.2015.07.039
   Kim J, 2014, ANALYST, V139, P1632, DOI 10.1039/c3an02359a
   Kireev D, 2017, IEEE T NANOTECHNOL, V16, P140, DOI 10.1109/TNANO.2016.2639028
   Koh A, 2016, ADV HEALTHC MATER, V5, P373, DOI 10.1002/adhm.201500451
   KUNG RTV, 1990, J CLIN PERIODONTOL, V17, P557, DOI 10.1111/j.1600-051X.1990.tb01105.x
   Lang NP, 2004, INT J ORAL MAX IMPL, V19, P150
   Lee DS, 2011, LAB CHIP, V11, P120, DOI [10.1039/c0lc00209g, 10.1039/c01c00209g]
   Levallois B, 1998, DENT MATER, V14, P441, DOI 10.1016/S0300-5712(99)00019-6
   Li C, 2013, ADV INTEL SYS RES, V39, P41
   Li XM, 2020, ADV MATER, V32, DOI 10.1002/adma.202000060
   Li YJ, 2015, SENSORS-BASEL, V15, P24961, DOI 10.3390/s151024961
   Lou D, 2020, BIOSENS BIOELECTRON, V162, DOI 10.1016/j.bios.2020.112275
   Mannoor MS, 2012, NAT COMMUN, V3, DOI 10.1038/ncomms1767
   Markov A, 2018, ACS APPL MATER INTER, V10, P18507, DOI 10.1021/acsami.8b02948
   Mombelli A, 2012, CLIN ORAL IMPLAN RES, V23, P67, DOI 10.1111/j.1600-0501.2012.02541.x
   Moser Y, 2007, J MICROELECTROMECH S, V16, P1349, DOI 10.1109/JMEMS.2007.908437
   Ngamchuea K, 2018, ANALYST, V143, P81, DOI 10.1039/c7an01571b
   Padial-Molina M, 2014, INT J PERIODONT REST, V34, P763
   Page KA, 2015, ACS APPL MATER INTER, V7, P17874, DOI 10.1021/acsami.5b04080
   Quadir NA, 2018, IEEE SENS J, V18, P3003, DOI 10.1109/JSEN.2018.2791426
   Ramanauskaite A., 2016, J ORAL MAXILLOFAC RE, V7
   Song JK, 2009, BIOELECTROMAGNETICS, V30, P374, DOI 10.1002/bem.20482
   Stoney G.G., TENSION METALLIC FIL
   Sun Y, 2009, J BIOMED MATER RES A, V90A, P648, DOI 10.1002/jbm.a.32125
   Taghadosi M., 2019, IOT PHYS LAYER, P105
   Tseng P, 2018, ADV MATER, V30, DOI 10.1002/adma.201703257
   Tyson J, 2017, SOLID STATE ELECTRON, V136, P113, DOI 10.1016/j.sse.2017.06.019
   Webb RC, 2013, NAT MATER, V12, P938, DOI [10.1038/NMAT3755, 10.1038/nmat3755]
NR 45
TC 0
Z9 0
U1 6
U2 12
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD JUL
PY 2020
VL 20
IS 14
AR 3953
DI 10.3390/s20143953
PG 18
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Chemistry; Engineering; Instruments & Instrumentation
GA MW7UU
UT WOS:000557237500001
PM 32708671
OA gold, Green Published
DA 2022-02-03
ER

PT J
AU Lupi, D
   Jucker, C
   Rocco, A
   Harrison, R
   Colombo, M
AF Lupi, Daniela
   Jucker, Costanza
   Rocco, Anna
   Harrison, Ruby
   Colombo, Mario
TI Notes on biometric variability in invasive species: the case of
   Psacothea hilaris hilaris
SO BULLETIN OF INSECTOLOGY
LA English
DT Article
DE phenotype; environmental induction; morphometry; colourimetry; image
   processing; ecology
ID SPOTTED LONGICORN BEETLE; PHENOTYPIC PLASTICITY; PASCOE COLEOPTERA;
   BODY-SIZE; CLUTCH SIZE; PHOTOPERIODIC RESPONSE; REPRODUCTIVE-BIOLOGY;
   GEOGRAPHIC-VARIATION; COLOR POLYMORPHISM; LARVAL DEVELOPMENT
AB Species morphometric variability is the result of the combined effect of genes and environment. This is emphasized in insects, especially in ones that rely on discrete food resources, such as xylophagous insects. Psacothea hilaris hilaris (Pascoe) (Coleoptera Cerambycidae Lamiinae), an exotic beetle already established in Italy, is used as a model species for the study. The findings presented in this research increase knowledge of morphological and colourimetric traits in P. h. hilaris and support the hypothesis that environmental cues can impact certain important morphometric features of exotic insects. Principal component analysis (PCA) and Bayesian's posterior probability applied to the dimensions of specimens collected over a four-year period showed that some morphological parameters changed significantly over the years. According to PCA the most meaningful morphometric variables were body length, elytral length, and antenna-to-body length ratio. One of the most significant results is the variability of the antenna-to-body length ratio over the period of the study. In cerambycids longer antennae allow for better detection of host tree, oviposition site, and favour mating strategies. Consequently variability in this physical trait can influence the ability of the species to adapt to a new habitat.
C1 [Lupi, Daniela; Jucker, Costanza; Rocco, Anna; Colombo, Mario] Univ Milan, Dept Food Environm & Nutr Sci DeFENS, I-20133 Milan, Italy.
   [Harrison, Ruby] Univ Wisconsin, Dept Entomol, Madison, WI 53706 USA.
C3 University of Milan; University of Wisconsin System; University of
   Wisconsin Madison
RP Lupi, D (corresponding author), Univ Milan, Dept Food Environm & Nutr Sci DeFENS, Via Celoria 2, I-20133 Milan, Italy.
EM daniela.lupi@unimi.it
RI LUPI, DANIELA/E-8281-2016; Jucker, Costanza/N-3760-2015
OI LUPI, DANIELA/0000-0002-9467-2419; Jucker, Costanza/0000-0001-5052-3705;
   Harrison, Ruby/0000-0003-1207-6734
FU project "Insects and globalization: sustainable control of exotic
   species in agro-forestry ecosystems (GEISCA, PRIN) - Italian Ministry
   for Education, University and ResearchMinistry of Education,
   Universities and Research (MIUR)
FX The research was supported by the project "Insects and globalization:
   sustainable control of exotic species in agro-forestry ecosystems
   (GEISCA, PRIN 2010-2011)", financed by Italian Ministry for Education,
   University and Research (Project coordinator: Alma Mater Studiorum
   University of Bologna). We also thank Prof. Bruno Rossaro for his
   statistical support and Francesco Legnani for his help in photographic
   tools.
CR Agrawal AA, 2001, SCIENCE, V294, P321, DOI 10.1126/science.1060701
   Alattal Y, 2014, B INSECTOL, V67, P31
   Allison JD, 2004, CHEMOECOLOGY, V14, P123, DOI 10.1007/s00049-004-0277-1
   Amarillo-Suarez AR, 2006, OECOLOGIA, V150, P247, DOI 10.1007/s00442-006-0516-y
   Andere C, 2008, ECOL MODEL, V214, P53, DOI 10.1016/j.ecolmodel.2008.01.010
   Angilletta MJ, 2004, INTEGR COMP BIOL, V44, P498, DOI 10.1093/icb/44.6.498
   Azevedo RBR, 2002, J INSECT PHYSIOL, V48, P231, DOI 10.1016/S0022-1910(01)00168-8
   Basset Yves, 1994, Mitteilungen der Schweizerischen Entomologischen Gesellschaft, V67, P347
   Chamberlain NL, 2009, SCIENCE, V326, P847, DOI 10.1126/science.1179141
   COILE T. S., 1936, ECOL MONOGR, V6, P533, DOI 10.2307/1943241
   DALY HV, 1985, ANNU REV ENTOMOL, V30, P415, DOI 10.1146/annurev.en.30.010185.002215
   Davis ALV, 2008, ECOL ENTOMOL, V33, P771, DOI 10.1111/j.1365-2311.2008.01033.x
   DMITRIEW C., 2011, PLOS ONE, DOI [10.1371/journal.pone.0017399, DOI 10.1371/JOURNAL.P0NE.0017399]
   Dray S, 2007, J STAT SOFTW, V22, P1, DOI 10.18637/jss.v022.i04
   Duffy EAJ, 1952, HDB IDENTIFICATION B, V5
   Evans JD, 2001, BIOESSAYS, V23, P62, DOI 10.1002/1521-1878(200101)23:1&lt;62::AID-BIES1008&gt;3.0.CO;2-7
   Fox CW, 2000, ECOLOGY, V81, P3, DOI 10.2307/177128
   Fukaya M, 2004, APPL ENTOMOL ZOOL, V39, P603, DOI 10.1303/aez.2004.603
   Fukaya M, 1996, APPL ENTOMOL ZOOL, V31, P51, DOI 10.1303/aez.31.51
   FUKAYA M, 1992, APPL ENTOMOL ZOOL, V27, P89, DOI 10.1303/aez.27.89
   Gomez-Mestre I, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2013.1869
   Hanks LM, 2005, ENTOMOL EXP APPL, V114, P25, DOI 10.1111/j.0013-8703.2005.00225.x
   HARDY ICW, 1992, J ANIM ECOL, V61, P121, DOI 10.2307/5515
   Hazel WN, 2002, EVOLUTION, V56, P342, DOI 10.1111/j.0014-3820.2002.tb01344.x
   Hochkirch A, 2008, EVOL DEV, V10, P350, DOI 10.1111/j.1525-142X.2008.00243.x
   Hunt J, 2000, EVOLUTION, V54, P936, DOI 10.1111/j.0014-3820.2000.tb00093.x
   IBA M, 1976, J SERIC SCI JPN, V45, P443
   JERMY T, 1984, AM NAT, V124, P609, DOI 10.1086/284302
   Jirotkul M, 1999, ANIM BEHAV, V58, P1169, DOI 10.1006/anbe.1999.1248
   Jucker C., 2006, Bollettino di Zoologia Agraria e di Bachicoltura, V38, P187
   Kim KG, 2009, MOL CELLS, V27, P429, DOI 10.1007/s10059-009-0064-5
   Lambers H., 2008, PLANT PHYSL ECOLOGY
   LINGAFELTER S. W., 2002, REV GENUS ANOPLOPHOR
   LINSLEY E. GORTON, 1961, UNIV CALIFORNIA PUBL ENT, V18, P1
   Lupi D., 2013, Bulletin OEPP, V43, P316, DOI 10.1111/epp.12045
   Mackauer M, 2001, FUNCT ECOL, V15, P335, DOI 10.1046/j.1365-2435.2001.00532.x
   MCLAIN DK, 1982, EVOLUTION, V36, P1227, DOI 10.1111/j.1558-5646.1982.tb05491.x
   Michalcewicz Jakub, 2012, Polish Journal of Entomology, V81, P311, DOI 10.2478/v10200-012-0011-1
   Mondor EB, 2005, OECOLOGIA, V142, P104, DOI 10.1007/s00442-004-1710-4
   MOUSSEAU TA, 1991, ANNU REV ENTOMOL, V36, P511, DOI 10.1146/annurev.en.36.010191.002455
   Mousseau TA, 1998, MATERNAL EFFECTS ADA
   Musolin DL, 2003, PHYSIOL ENTOMOL, V28, P65, DOI 10.1046/j.1365-3032.2003.00307.x
   Nijhout HF, 1999, BIOSCIENCE, V49, P181, DOI 10.2307/1313508
   Olendorf R, 2006, NATURE, V441, P633, DOI 10.1038/nature04646
   OSAWA N, 1992, HEREDITY, V69, P297, DOI 10.1038/hdy.1992.129
   Pascoe FP, 1857, T ROY ENT SOC LONDON, V4, P89, DOI [DOI 10.1111/J.1365-2311.1857.TB01817.X, 10.1111/j.1365-2311.1857.tb01817.x]
   Pener MP, 1998, J INSECT PHYSIOL, V44, P365, DOI 10.1016/S0022-1910(97)00169-8
   Pinna M., 1978, ATMOSFERA CLIMA
   Price TD, 2003, P ROY SOC B-BIOL SCI, V270, P1433, DOI 10.1098/rspb.2003.2372
   REAGEL P. F., 2012, CAN ENTOMOL, V144, P1
   Ripley B.D., 1996, PATTERN RECOGN
   SAKAKIBARA M, 1995, JPN J APPL ENTOMOL Z, V39, P59, DOI 10.1303/jjaez.39.59
   Shintani Y, 1997, APPL ENTOMOL ZOOL, V32, P347, DOI 10.1303/aez.32.347
   Shintani Y, 2003, J INSECT PHYSIOL, V49, P975, DOI 10.1016/S0022-1910(03)00167-7
   Shintani Y, 1996, APPL ENTOMOL ZOOL, V31, P495, DOI 10.1303/aez.31.495
   Shoda E, 2003, APPL ENTOMOL ZOOL, V38, P369, DOI 10.1303/aez.2003.369
   Sokal R., 1995, BIOMETRY, V3rd
   STARZYK J R, 1985, Polskie Pismo Entomologiczne, V55, P491
   STEWART AJA, 1986, BIOL J LINN SOC, V27, P79, DOI 10.1111/j.1095-8312.1986.tb01727.x
   Trematerra P, 2013, ARTHROPOD-PLANT INTE, V7, P287, DOI 10.1007/s11829-012-9243-y
   Tsai ML, 2001, OIKOS, V92, P13, DOI 10.1034/j.1600-0706.2001.920102.x
   Vellend M, 2007, TRENDS ECOL EVOL, V22, P481, DOI 10.1016/j.tree.2007.02.017
   Venables W.N., 2002, MODERN APPL STAT S 4
   VON BREUNING S., 1943, NOVITATES ENTOMOLOGI, V13, P137
   Wenninger EJ, 2008, ANN ENTOMOL SOC AM, V101, P585, DOI 10.1603/0013-8746(2008)101[585:DASPIA]2.0.CO;2
   Willmore KE, 2007, EVOL BIOL, V34, P99, DOI 10.1007/s11692-007-9008-1
   Wund MA, 2012, INTEGR COMP BIOL, V52, P5, DOI 10.1093/icb/ics050
   YOKOI N, 1990, APPL ENTOMOL ZOOL, V25, P383, DOI 10.1303/aez.25.383
NR 68
TC 9
Z9 10
U1 1
U2 13
PU ALMA MATER STUDIORUM, UNIV BOLOGNA
PI BOLOGNA
PA DEPT AGROENVIRONMENTAL SCIENCES & TECHNOLOGY, VIALE G FANIN, 42,
   BOLOGNA, 40127, ITALY
SN 1721-8861
J9 B INSECTOL
JI Bull. Insectology
PD JUN
PY 2015
VL 68
IS 1
BP 135
EP 145
PG 11
WC Entomology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Entomology
GA CK1JN
UT WOS:000355963700019
DA 2022-02-03
ER

PT C
AU Panahiazar, M
   Taslimitehrani, V
   Jadhav, A
   Pathak, J
AF Panahiazar, Maryam
   Taslimitehrani, Vahid
   Jadhav, Ashutosh
   Pathak, Jyotishman
GP IEEE
BE Lin, J
   Hu, XH
   Chang, W
   Nambiar, R
   Aggarwal, C
   Cercone, N
   Honavar, V
   Huan, J
   Mobasher, B
   Pyne, S
TI Empowering Personalized Medicine with Big Data and Semantic Web
   Technology: Promises, Challenges, and Use Cases
SO 2014 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
SE IEEE International Conference on Big Data
LA English
DT Proceedings Paper
CT IEEE International Conference on Big Data
CY OCT 27-30, 2014
CL Washington, DC
DE Personalized Medicine; Big Data; Semantic Web; Smart Data; Health Care
AB In healthcare, big data tools and technologies have the potential to create significant value by improving outcomes while lowering costs for each individual patient. Diagnostic images, genetic test results and biometric information are increasingly generated and stored in electronic health records presenting us with challenges in data that is by nature high volume, variety and velocity, thereby necessitating novel ways to store, manage and process big data. This presents an urgent need to develop new, scalable and expandable big data infrastructure and analytical methods that can enable healthcare providers access knowledge for the individual patient, yielding better decisions and outcomes.
   In this paper, we briefly discuss the nature of big data and the role of semantic web and data analysis for generating "smart data" which offer actionable information that supports better decision for personalized medicine. In our view, the biggest challenge is to create a system that makes big data robust and smart for healthcare providers and patients that can lead to more effective clinical decision-making, improved health outcomes, and ultimately, managing the healthcare costs. We highlight some of the challenges in using big data and propose the need for a semantic data-driven environment to address them. We illustrate our vision with practical use cases, and discuss a path for empowering personalized medicine using big data and semantic web technology.
C1 [Panahiazar, Maryam; Taslimitehrani, Vahid; Jadhav, Ashutosh; Pathak, Jyotishman] Mayo Clin, Ctr Sci & Healthcare Delivery, Rochester, MN 55902 USA.
   [Panahiazar, Maryam; Taslimitehrani, Vahid; Jadhav, Ashutosh] Wright State Univ, Coll Comp Sci & Engn, Ohio Ctr Excellence Knowledge Enabled Comp Kno E, Dayton, OH 45435 USA.
C3 Mayo Clinic; Wright State University Dayton
RP Panahiazar, M (corresponding author), Mayo Clin, Ctr Sci & Healthcare Delivery, Rochester, MN 55902 USA.
FU NIGMS NIH HHSUnited States Department of Health & Human ServicesNational
   Institutes of Health (NIH) - USANIH National Institute of General
   Medical Sciences (NIGMS) [R01 GM105688] Funding Source: Medline;
   NATIONAL INSTITUTE OF GENERAL MEDICAL SCIENCESUnited States Department
   of Health & Human ServicesNational Institutes of Health (NIH) - USANIH
   National Institute of General Medical Sciences (NIGMS) [R01GM105688]
   Funding Source: NIH RePORTER
CR Aronson AR, 2001, J AM MED INFORM ASSN, P17
   Casey O., 2013, PERSONOLIZED MED CHA, V10, P453
   Datta M., 2013, TECH REP
   Dean J., 2004, TECH REP
   Hortonwork, 2014, TECH REP
   Jadhav A. S., 2014, ONLINE INFORM SEARCH
   Jain KK, 2009, TXB PERSONALIZED MED
   Krishnaprasad T., SEMANTICS BIG DATA
   Panahiazar M., 2012, IEEE INT C BIOINF BI
   Panahiazar M, 2012, AMIA ANN S P TBI IEV, P175
   Panahiazar M., 2014, 2014 TBI 2014 JOINT
   Panahiazar M., 2012, IEEE INT C BIOINF BI, P106
   Panahiazar M, 2013, BMC MED GENOMICS, V6, DOI 10.1186/1755-8794-6-S3-S5
   Pathak J, 2012, J BIOMED SEMANT, V3, DOI 10.1186/2041-1480-3-10
   Ranabahu A, 2011, IEEE INT C SEMANT CO, P205, DOI 10.1109/ICSC.2011.79
   Roski J, 2014, HEALTH AFFAIR, V33, P1115, DOI 10.1377/hlthaff.2014.0147
   Teli N., 2014, BIG DATA CATALYST PE
   White Tom, 2012, HADOOP DEFINITIVE GU
NR 18
TC 39
Z9 40
U1 1
U2 14
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2639-1589
BN 978-1-4799-5666-1
J9 IEEE INT CONF BIG DA
PY 2014
BP 790
EP 795
PG 6
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BF2FK
UT WOS:000380462900105
PM 25705726
OA Green Accepted
DA 2022-02-03
ER

PT C
AU Zaki, Z
   Kchouk, ML
   Douik, A
   Ben Salem, A
   Ghorbel, A
   Annabi, M
AF Zaki, Z
   Kchouk, ML
   Douik, A
   Ben Salem, A
   Ghorbel, A
   Annabi, M
BE Kuden, AB
   Kuden, A
TI Electronic imagery: A new method for grape identification
SO FIFTH INTERNATIONAL SYMPOSIUM ON TEMPERATE ZONE FRUITS IN THE TROPICS
   AND SUBTROPICS - PROCEEDINGS
SE ACTA HORTICULTURAE
LA English
DT Proceedings Paper
CT 5th International Symposium on Temperate Zone Fruits in the Tropics and
   Subtropics
CY MAY 29-JUN 01, 1996
CL ADANA, TURKEY
AB The grape variety Beldi (Vitis vinifera L.) was extensively cultivated before 1900 and gave an excellent white wine (Berger-Levrault et nl., 1900) but has been progressively abandoned and replaced by European cultivars. However, it remains agroeconomically important in some traditional orchards, as do many other indigenous grapes. Conservation of fruit genetic resources requires an accurate identification of varieties, using various descriptors. In grapes, ampelographic methods are important in grapevine taxonomy (Galet, 1988), as well as in varietal identification. Local and indigenous varieties are threatened with extinction and difficult to find, since only a few individuals remain. Moreover, classical ampelography often gives erroneous conclusions about varietal names, homonyms, and synonyms, i.e. different varieties have the same name or the same cultivar has different names. Standard techniques of varietal identification are based on the measurement of biometric (ampelometric) characters of leaves and/or bunches, and are therefore extremely subjective and time-consuming. Computer-aided survey systems were developed to overcome problems of synonymy and homonymy (Costacurta er nl., 1992). However, these procedures are semi-automatic and still require the involvement of the manipulator, since they use the mouse click to point out the grape leaf characteristics on the computer screen. A completely automatic methodology is reported herein for the identification of grapes using a Digital Image Processing (DIP) software applied to leaves. The Tunisian variety Beldi was chosen as a model for indigenous grapes.
C1 INRST, Hammam Lif 2050, Tunisia.
C3 Institut National de Recherche Scientifique et Technique
RP Zaki, Z (corresponding author), INRST, BP 95, Hammam Lif 2050, Tunisia.
RI Kchouk, Mohamed Elyes/E-6612-2016
OI Kchouk, Mohamed Elyes/0000-0003-2183-1893; DOUIK,
   Ali/0000-0002-0178-501X
NR 0
TC 1
Z9 1
U1 0
U2 1
PU INTERNATIONAL SOCIETY HORTICULTURAL SCIENCE
PI LEUVEN 1
PA PO BOX 500, 3001 LEUVEN 1, BELGIUM
SN 0567-7572
BN 90-6605-948-6
J9 ACTA HORTIC
PY 1997
IS 441
BP 317
EP 324
DI 10.17660/ActaHortic.1997.441.44
PG 8
WC Agronomy; Food Science & Technology; Horticulture
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Agriculture; Food Science & Technology
GA BK86M
UT WOS:000073700400044
DA 2022-02-03
ER

PT C
AU Li, NX
   Busso, C
AF Li, Nanxiang
   Busso, Carlos
GP ACM
TI Evaluating the Robustness of an Appearance-based Gaze Estimation Method
   for Multimodal Interfaces
SO ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON
   MULTIMODAL INTERACTION
LA English
DT Proceedings Paper
CT 15th ACM International Conference on Multimodal Interaction (ICMI)
CY DEC 09-13, 2013
CL Sydney, AUSTRALIA
DE Gaze estimation; eigenspace analysis; computer user interface;
   multimodal interfaces
AB Given the crucial role of eye movements on visual attention, tracking gaze behaviors is an important research problem in various applications including biometric identification, attention modeling and human-computer interaction. Most of the existing gaze tracking methods require a repetitive system calibration process and are sensitive to the user's head movements. Therefore, they cannot be easily implemented in current multimodal interfaces. This paper investigates an appearance-based approach for gaze estimation that requires minimum calibration and is robust against head motion. The approach consists in building an orthonormal basis, or eigenspace, of the eye appearance with principal component analysis (PCA). Unlike previous studies, we build the eigenspace using image patches displaying both eyes. The projections into the basis are used to train regression models which predict the gaze location. The approach is trained and tested with a new multimodal corpus introduced in this paper. We consider several variables such as the distance between user and the computer monitor, and head movement. The evaluation includes the performance of the proposed gaze estimation system with and without head movement. It also evaluates the results in subject-dependent versus subject-independent conditions under different distances. We report promising results which suggest that the proposed gaze estimation approach is a feasible and flexible scheme to facilitate gaze-based multimodal interfaces.
C1 [Li, Nanxiang; Busso, Carlos] Univ Texas Dallas, MSP Lab, 800 W Campbell Rd, Richardson, TX 75080 USA.
C3 University of Texas System; University of Texas Dallas
RP Li, NX (corresponding author), Univ Texas Dallas, MSP Lab, 800 W Campbell Rd, Richardson, TX 75080 USA.
EM nxl056000@utdallas.edu; busso@utdallas.edu
OI Busso, Carlos/0000-0002-4075-4072
CR Anders Geerd, 2001, INT S AV PSYCH ISAP
   Baluja S., 1994, CMUCS94102
   Castrillon M, 2007, J VIS COMMUN IMAGE R, V18, P130, DOI 10.1016/j.jvcir.2006.11.004
   FREY LA, 1990, IEEE T SYST MAN CYB, V20, P944, DOI 10.1109/21.105094
   Ghaoui C., 2005, ENCY HUMAN COMPUTER
   Guestrin ED, 2006, IEEE T BIO-MED ENG, V53, P1124, DOI 10.1109/TBME.2005.863952
   Hansen DW, 2010, IEEE T PATTERN ANAL, V32, P478, DOI 10.1109/TPAMI.2009.30
   Hillman PM, 2003, ISPA 2003: PROCEEDINGS OF THE 3RD INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, PTS 1 AND 2, P359
   Huang WM, 2000, INT C PATT RECOG, P722, DOI 10.1109/ICPR.2000.903019
   KIRBY M, 1990, IEEE T PATTERN ANAL, V12, P103, DOI 10.1109/34.41390
   Li NX, 2013, IEEE INT CON MULTI
   Matsumoto Y, 2001, ROBOT AND HUMAN COMMUNICATION, PROCEEDINGS, P262, DOI 10.1109/ROMAN.2001.981912
   MCCONKIE GW, 1975, PERCEPT PSYCHOPHYS, V17, P578, DOI 10.3758/BF03203972
   MERCHANT J, 1974, IEEE T BIO-MED ENG, VBM21, P309, DOI 10.1109/TBME.1974.324318
   Ohno T., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P125
   Ono Y, 2006, LECT NOTES COMPUT SC, V4319, P178
   Proscevicius T, 2010, ELEKTRON ELEKTROTECH, P67
   Rayner K, 2001, J EXP PSYCHOL-APPL, V7, P219, DOI 10.1037/1076-898X.7.3.219
   Salvucci D. D., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P273
   Scott D., 1991, VISUAL SEARCH EYE MO
   Skovsgaard H, 2011, BEHAV INFORM TECHNOL, V30, P821, DOI 10.1080/0144929X.2011.563801
   Stiefelhagen R., 1997, P WORKSH PERC US INT, P98
   Tan KH, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P191, DOI 10.1109/ACV.2002.1182180
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Williams O., 2006, P 2006 IEEE COMP SOC, P230, DOI DOI 10.1109/CVPR.2006.285
NR 26
TC 4
Z9 4
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
BN 978-1-4503-2129-7
PY 2013
BP 91
EP 98
DI 10.1145/2522848.2522876
PG 8
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BF1AZ
UT WOS:000380272900017
DA 2022-02-03
ER

PT J
AU Adamovich, TA
   Ashikhmina, TY
   Kantor, GY
AF Adamovich, T. A.
   Ashikhmina, T. Ya.
   Kantor, G. Ya.
TI Use of various combinations of spectral channels of satellite images
   from the Landsat 8 satellite for an assessment of natural environments
   and objects (review)
SO THEORETICAL AND APPLIED ECOLOGY
LA Russian
DT Review
DE Landsat 8; combinations of channels; multispectral satellite images;
   natural objects
AB The Landsat program of remote sensing of Earth from space has been started in July, 1972 with launch of the first Landsat satellite in USA. Now two space vehicles of this series - Landsat 7 and Landsat 8 work at their orbits. Landsat 7 is close to exhaustion of a technical resource therefore the main source of urgent imagery is the Landsat 8 satellite. The satellite images received from the Landsat 8 are widely used for the solution of the scientific and practical tasks related to operational control of natural resources, a research of dynamics of natural processes and the phenomena, the analysis of the reasons, forecasting of possible consequences and the choice of ways of the prevention of emergency situations. In comparison with the previous devices of a series the spectral range of the Landsat 8 equipment is expanded at the expense of two new channels which allows to carry out studying of water objects and aerosols.
   Various combinations of channels of visible and near infrared radiation are used for the solution of a large number of thematic tasks: classification and analysis of a condition of a vegetable cover; studying of farmlands, wetlands; analysis of a condition of water objects; classification of changes in forests; mapping of economic and biometric characteristics of forest plantings; definition of stocks of tree species; mapping of soils; studying of dynamics of the fires and post-fire analysis of the territory. Combination of various channels of the Landsat 8 satellite for research depends on conditions of a concrete scene (the area, a shooting season etc.).
   The possibilities of use of various combinations of spectral channels of satellite imagery from the Landsat 8 satellite are considered in this paper on the example of two sites of the territory of the Kirov region differing on degree of anthropogenic load - the State Nature Reserve "Nurgush" and the administrative center of the Kirov region (city of Kirov). Use of these tools of satellite imagery allows revealing various characteristics of natural ecosystems in large territories without carrying out extra field researches.
C1 [Adamovich, T. A.; Ashikhmina, T. Ya.; Kantor, G. Ya.] Vyatka State Univ, 36 Moskovskaya St, Kirov 610000, Russia.
   [Ashikhmina, T. Ya.; Kantor, G. Ya.] RAS, Ural Branch, Komi Sci Ctr, Inst Biol, 28 Kommunisticheskaya St, Syktyvkar 167982, Russia.
C3 Vyatka State University; Russian Academy of Sciences; Institute of
   Biology, Komi Scientific Centre, Ural Branch RAS; Komi Science Centre of
   the Ural Branch of the Russian Academy of Sciences
RP Adamovich, TA (corresponding author), Vyatka State Univ, 36 Moskovskaya St, Kirov 610000, Russia.
EM ttjnadamvich@rambler.ru
RI Adamovich, Tatyana/N-3967-2016; Надежда, Сырчина/ABF-2311-2020
OI Adamovich, Tatyana/0000-0002-8684-927X; 
CR Adamovich T. A., 2017, 12 ALL RUSS SCI PRAC, P19
   [Anonymous], 2014, REMOTE SENS, P7952
   [Anonymous], 2015, LANDS 8 L8 DAT US HD
   [Аншаков Геннадий Петрович Anshakov Gennady Petrovich], 2013, [Вестник Самарского государственного аэрокосмического университета им. академика С.П. Королёва (национального исследовательского университета), Vestnik Samarskogo gosudarstvennogo aerokosmicheskogo universiteta im. akademika S.P. Koroleva (natsional'nogo issledovatel'skogo universiteta)], P38
   [Барталев С.А. Bartalev S.A.], 2014, [Современные проблемы дистанционного зондирования Земли из космоса, Current problems in remote sensing of the Earth from space, Sovremennye problemy distantsionnogo zondirovaniya Zemli iz kosmosa], V11, P9
   Bartalev S.A., 2005, MODERN PROBLEMS REMO, V2, P343
   Belova E. I., 2010, 8 OP ALL RUSS C MOD, P33
   Belyayev B., 2016, ZEMLYA BELARUSI, P42
   Chandra A. M., 2008, REMOTE SENSING GEOGR
   Cherepanov A.S, 2011, GEOMATIKA, V2, P98
   Evdokimov S. I., 2015, VESTNIK PSKOVSK EFMN, P21
   Feyisa GL, 2014, REMOTE SENS ENVIRON, V140, P23, DOI 10.1016/j.rse.2013.08.029
   Garbuk S. V., 1997, SATELLITE REMOTE SEN
   Garkusha I. N., 2013, ZB NAUK PRATS NGU, P114
   Golitsyn G. S, 2006, SOVREMENNYYE PROBLEM, V3, P263
   Gornyy V. I., 2004, SOVREMENNYYE PROBLEM, V2, P10
   Isayev A. S, 2009, ISSLEDOVANIYE ZEMLI, P1
   Kashkin V. B., 2001, DIGITAL IMAGE PROCES
   Klimanova O. A., 2016, IZVESTIYA VYSSHIKH U, V60, P13
   Knizhnikov Yu.F., 2004, AEROSPACE METHODS GE
   Kondratenkov G. S., 2005, TXB HIGH SCH
   Kornienko S.G, 2009, ISSLEDOVANIYE ZEMLI, P78
   Koshko A. A., INT SCI TECHN INT C
   Kravtsova V. I, 2005, SPACE METHODS SOIL I
   Kurganovich K. A, 2015, VESTNIK ZABGU, V6, P16
   [Куулар Х.Б. Kuular Kh.B.], 2013, [Современные проблемы дистанционного зондирования Земли из космоса, Current problems in remote sensing of the Earth from space, Sovremennye problemy distantsionnogo zondirovaniya Zemli iz kosmosa], V10, P239
   [Лаверов Н.П. Laverov N.P.], 2015, [Современные проблемы дистанционного зондирования Земли из космоса, Current problems in remote sensing of the Earth from space, Sovremennye problemy distantsionnogo zondirovaniya Zemli iz kosmosa], V12, P145
   Li S., 2014, INT ARCH PHOTOGRAMM, V40, P139, DOI [DOI 10.5194/isprsarchives-XL-4-139-2014, 10.5194/isprsarchives-XL-4-139-2014, DOI 10.5194/ISPRSARCHIVES-XL-4-139-2014]
   Loupian E. A., 2006, Mitigation and Adaptation Strategies for Global Change, V11, P113, DOI 10.1007/s11027-006-1013-7
   Lupyan E. A., 2012, SOVREMENNYYE PROBLEM, V9, P307
   Martynyuk V.A, 2013, 2 MEZHD NAUCH PRAKT, P118
   MEYER P, 1993, ISPRS J PHOTOGRAMM, V48, P17, DOI 10.1016/0924-2716(93)90028-L
   [Миклашевич Т.С. Miklashevich T.S.], 2016, [Современные проблемы дистанционного зондирования Земли из космоса, Current problems in remote sensing of the Earth from space, Sovremennye problemy distantsionnogo zondirovaniya Zemli iz kosmosa], V13, P9
   Olsson H, 2009, INT J REMOTE SENS, V30, P5117, DOI 10.1080/01431160903022993
   [Повх B.И. Povkh V.I.], 2006, [Исследование Земли из космоса, Issledovanie Zemli iz kosmosa], P89
   [Прошин А.А. Proshin A.A.], 2016, [Современные проблемы дистанционного зондирования Земли из космоса, Current problems in remote sensing of the Earth from space, Sovremennye problemy distantsionnogo zondirovaniya Zemli iz kosmosa], V13, P9
   Quinn W. J, BAND COMBINATION
   Roy DP, 2014, REMOTE SENS ENVIRON, V145, P154, DOI 10.1016/j.rse.2014.02.001
   [Савин И.Ю. Savin I.], 2012, [Современные проблемы дистанционного зондирования Земли из космоса, Current problems in remote sensing of the Earth from space, Sovremennye problemy distantsionnogo zondirovaniya Zemli iz kosmosa], V9, P104
   Savorskiy V. P, 2006, SOVREMENNYYE PROBLEM, V3, P198
   Sborishuk Yu. N, 1992, REMOTE METHODS INV 1
   Shovengerdt R.A., 2010, REMOTE SENSING MODEL
   Sidko AF, 1997, DOKL AKAD NAUK+, V354, P120
   [Сидоренков Виктор Михайлович Sidorenkov Viktor Mikhailovich], 2015, [Лесотехнический журнал, Lesotekhnicheskii zhurnal], V5, P97, DOI 10.12737/11267
   [Силкин К.Ю. Silkin K. Yu.], 2012, [Вестник Воронежского государственного университета. Серия: Геология, Proceedings of Voronezh State University. Series: Geology, Vestnik Voronezhskogo gosudarstvennogo universiteta. Seriya: Geologiya], P220
   SMITH JA, 1980, PHOTOGRAMM ENG REM S, V46, P1183
   Smyshlyakov S. G., 2013, GEOMATIKA, P53
   [Сочилова Е.Н. Sochilova E.N.], 2012, [Современные проблемы дистанционного зондирования Земли из космоса, Current problems in remote sensing of the Earth from space, Sovremennye problemy distantsionnogo zondirovaniya Zemli iz kosmosa], V9, P277
   Storey J, 2014, REMOTE SENS-BASEL, V6, P11153, DOI 10.3390/rs61111153
   [Терехин Э.А. Terekhin E.A.], 2012, [Современные проблемы дистанционного зондирования Земли из космоса, Current problems in remote sensing of the Earth from space, Sovremennye problemy distantsionnogo zondirovaniya Zemli iz kosmosa], V9, P122
   [Терехов А.Г. Terekhov A.G.], 2015, [Современные проблемы дистанционного зондирования Земли из космоса, Current problems in remote sensing of the Earth from space, Sovremennye problemy distantsionnogo zondirovaniya Zemli iz kosmosa], V12, P174
   Tikhomirov O.A., 2016, VESTNIK TVERSKOGO K, P230
   Tsydypov B. Z., 2012, VESTNIK IRGTU, P67
   Ulanova S. S., 2012, IZVESTIYA SARATO KBE, V2, P94
   [Воробьев О.Н. Vorobev O.N.], 2016, [Современные проблемы дистанционного зондирования Земли из космоса, Current problems in remote sensing of the Earth from space, Sovremennye problemy distantsionnogo zondirovaniya Zemli iz kosmosa], V13, P124, DOI 10.21046/2070-7401-2016-13-3-124-134
   Zhilenev M. Y., 2009, GEOMATIKA, P56
NR 56
TC 5
Z9 5
U1 0
U2 1
PU LLC PUBLISHING HOUSE, KAMERTON
PI MOSCOW
PA 9, STROMYNKA ST, MOSCOW, 107014, RUSSIA
SN 1995-4301
EI 2618-8406
J9 THEOR APPL ECOL
JI Theor. Appl. Ecol.
PY 2017
IS 2
BP 9
EP 18
PG 12
WC Ecology
WE Emerging Sources Citation Index (ESCI)
SC Environmental Sciences & Ecology
GA VI2YY
UT WOS:000468547000002
DA 2022-02-03
ER

PT J
AU Wu, C
   Yang, J
   Zhu, JJ
   Cong, WJ
   Ai, DN
   Song, H
   Liang, XH
   Wang, YT
AF Wu, Chan
   Yang, Jian
   Zhu, Jianjun
   Cong, Weijian
   Ai, Danni
   Song, Hong
   Liang, Xiaohui
   Wang, Yongtian
TI Hybrid constraint optimization for 3D subcutaneous vein reconstruction
   by near-infrared images
SO COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
LA English
DT Article
DE Near-infrared; Subcutaneous vein; Matching; Stereo reconstruction
ID 3-D RECONSTRUCTION; CORONARY-ARTERY; RECOGNITION; FEATURES; MODEL
AB Background and objective: The development of biometric identification technology and intelligent medication has enabled researchers to analyze subcutaneous veins from near-infrared images. However, the stereo reconstruction of subcutaneous veins has not been well studied, and the results are difficult to utilize in clinical practice.
   Methods: We present a hybrid constraint optimization (HCO) matching algorithm for vein reconstruction to solve the matching failure problems caused by the incomplete segmentation of vein structures captured from different views. This algorithm initially introduces the existence of the epipolar and homography constraints in the subcutaneous vein matching. Then, the HCO matching algorithm of the vascular centerline is established by homography point-to-point matching, homography matrix optimization, and vascular section matching. Finally, the 3D subcutaneous vein is reconstructed on the basis of the principle of triangulation and system calibration parameters.
   Results: To validate the performance of the proposed matching method, we designed a series of experiments to evaluate the effectiveness of the hybrid constraint optimization method. The experiments were performed on simulated and real datasets. 42 real vascular images were analyzed on the basis of different matching strategies. Experimental result shows that the matching accuracy increased significantly with the proposed optimization matching method. To calculate the reconstruction accuracy, we reconstructed seven simulated cardboards and measured 10 vascular distances in each simulated cardboard. The average vascular distance error of each simulated image was within 1.0 mm, and the distance errors of 75% feature points were less than 1.5 mm. Also, we printed a 3D simulated vein model to improve the illustration of this system. The reconstruction error extends from -3.58 mm to 1.94 mm with a standard deviation of 0.68 mm and a mean of 0.07 mm.
   Conclusions: The algorithm is validated in terms of homography optimization, matching efficiency, and simulated vascular reconstruction error. The experimental results demonstrate that the veins captured from the left and right views can be accurately matched through the proposed algorithm. (C) 2018 Elsevier B.V. All rights reserved.
   Results: To validate the performance of the proposed matching method, we designed a series of experiments to evaluate the effectiveness of the hybrid constraint optimization method. The experiments were performed on simulated and real datasets. 42 real vascular images were analyzed on the basis of different matching strategies. Experimental result shows that the matching accuracy increased significantly with the proposed optimization matching method. To calculate the reconstruction accuracy, we reconstructed seven simulated cardboards and measured 10 vascular distances in each simulated cardboard. The average vascular distance error of each simulated image was within 1.0 mm, and the distance errors of 75% feature points were less than 1.5 mm. Also, we printed a 3D simulated vein model to improve the illustration of this system. The reconstruction error extends from -3.58 mm to 1.94 mm with a standard deviation of 0.68 mm and a mean of 0.07 mm.
C1 [Wu, Chan; Yang, Jian; Zhu, Jianjun; Ai, Danni; Wang, Yongtian] Beijing Inst Technol, Sch Opt & Elect, Beijing Engn Res Ctr Mixed Real & Adv Display, Beijing 100081, Peoples R China.
   [Cong, Weijian; Liang, Xiaohui] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100083, Peoples R China.
   [Song, Hong] Beijing Inst Technol, Sch Software, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology; Beihang University; Beijing Institute
   of Technology
RP Cong, WJ (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100083, Peoples R China.
EM congwj@buaa.edu.cn
OI Yang, Jian/0000-0003-1250-6319
FU National Key Research and Development Program of China [2017YFC0107900];
   National Science Foundation Program of China [61672099, 81627803,
   61501030, 61527827]
FX This work was supported by the National Key Research and Development
   Program of China (2017YFC0107900), the National Science Foundation
   Program of China(61672099, 81627803, 61501030, and 61527827).
CR Ai DN, 2016, BIOMED OPT EXPRESS, V7, P2565, DOI 10.1364/BOE.7.002565
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Brewer RD, 2010, IEEE INT CONF ROBOT, P4597, DOI 10.1109/ROBOT.2010.5509575
   Canero C, 2002, IEEE T MED IMAGING, V21, P1188, DOI 10.1109/TMI.2002.804421
   Cong WJ, 2015, IEEE T BIO-MED ENG, V62, P2079, DOI 10.1109/TBME.2015.2408633
   Dorsaz PA, 2000, IEEE T MED IMAGING, V19, P759, DOI 10.1109/42.875203
   Fan JF, 2018, IEEE ACCESS, V6, P11265, DOI 10.1109/ACCESS.2018.2799423
   Fernandez R, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17040897
   Gonzalez RP, 2011, AM J SURG, V201, P344, DOI 10.1016/j.amjsurg.2010.09.021
   Hu YP, 2014, INFRARED PHYS TECHN, V62, P110, DOI 10.1016/j.infrared.2013.10.004
   Huang D, 2017, IMAGE VISION COMPUT, V58, P266, DOI 10.1016/j.imavis.2016.07.001
   Huang D, 2015, IEEE T CYBERNETICS, V45, P1823, DOI 10.1109/TCYB.2014.2360894
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Juric S, 2014, SCI WORLD J, DOI 10.1155/2014/365902
   Kandani H, 2008, P SOC PHOTO-OPT INS, P7055
   Liao R, 2010, INT J CARDIOVAS IMAG, V26, P733, DOI 10.1007/s10554-009-9528-0
   Meriaudeau F, 2009, 2009 16TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOLS 1-6, P2857, DOI 10.1109/ICIP.2009.5414511
   Molina C, 1998, P SOC PHOTO-OPT INS, V3338, P504, DOI 10.1117/12.310929
   Nakamachi E, 2011, INT J OPT, V2012, P1
   Sun Z, 2010, COMPUT MED IMAG GRAP, V34, P333, DOI 10.1016/j.compmedimag.2009.12.004
   Yang J, 2014, PHYS MED BIOL, V59, P975, DOI 10.1088/0031-9155/59/4/975
   Yang J, 2009, IEEE T IMAGE PROCESS, V18, P1563, DOI 10.1109/TIP.2009.2017363
   Zhang Q., 2013, P IEEE 6 INT C BIOM, P1, DOI DOI 10.1080/00207543.2013
   Zharov VP, 2004, LASER SURG MED, V34, P56, DOI 10.1002/lsm.10248
   Zifan A, 2008, INT J BIOL MED SCI, V2, P105
NR 26
TC 0
Z9 1
U1 1
U2 19
PU ELSEVIER IRELAND LTD
PI CLARE
PA ELSEVIER HOUSE, BROOKVALE PLAZA, EAST PARK SHANNON, CO, CLARE, 00000,
   IRELAND
SN 0169-2607
EI 1872-7565
J9 COMPUT METH PROG BIO
JI Comput. Meth. Programs Biomed.
PD SEP
PY 2018
VL 163
BP 123
EP 133
DI 10.1016/j.cmpb.2018.06.008
PG 11
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Theory & Methods; Engineering, Biomedical; Medical Informatics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Medical Informatics
GA GQ2TI
UT WOS:000441510100013
PM 30119847
DA 2022-02-03
ER

PT J
AU Kahler, C
   Schramm, T
   Bald, R
   Gembruch, U
   Merz, E
   Eichhorn, KH
AF Kaehler, Christiane
   Schramm, Thomas
   Bald, Rainer
   Gembruch, Ulrich
   Merz, Eberhard
   Eichhorn, Karl-Heinz
TI Updated DEGUM Quality Requirements for the Basic Prenatal Screening
   Ultrasound Examination (DEGUM Level I) between 18+0 and 21+6 weeks of
   gestation
SO ULTRASCHALL IN DER MEDIZIN
LA English
DT Article
DE prenatal diagnosis; second trimester; level I examination; quality
   requirements
ID DIAGNOSIS
AB A precondition for the early detection of fetal abnormalities is the high quality of prenatal basic ultrasound (screening examination). The objective of ultrasound screening is the recognition of abnormal fetal growth and fetal anatomical anomalies. The prenatal detection of fetal abnormalities enables detailed prenatal counselling of parents, improved care at birth and potentially a reduction in morbidity and mortality. In the guidelines for maternity care in Germany ("Mutterschaftsrichtlinien"), the performance of basic ultrasound in pregnancy is not clearly defined. The required image documentation includes a few biometric measurements only. Therefore, adherence to a standard technique and the possibility of audit are limited, thus not necessarily resulting in high screening quality. In this update of the DEGUM quality requirements for level I screening ultrasound examination between 18 + 0 and 21 + 6 weeks of gestation, the required parameters, standard planes and required documentation are described in detail. The greater experience of gynecologists in the field of sonographic screening examinations and the use of a modern ultrasound technique allow improvement of the screening quality. This will improve the standard of basic ultrasound screening. Due to the enhanced standard of the DEGUM I examination, more pregnant women may benefit from a detailed ultrasound examination and specialized therapy in DEGUM level II and III centers. The required fetal structures are described in detail. This update of the requirements for level I DEGUM basic ultrasound examination between 18 + 0 and 21 + 6 weeks of gestation goes far beyond the guidelines for maternity care in Germany (the "Mutterschaftsrichtlinien") thereby elevating standards.
C1 [Kaehler, Christiane] Practice Prenatal Med Erfurt, Obst Gyn, Anger 81, D-99084 Erfurt, Germany.
   [Schramm, Thomas] Prenatal Med Munchen, Ultrasound, Munich, Germany.
   [Bald, Rainer] Klinikum Leverkusen gGmbH, Prenatal Med, Leverkusen, Germany.
   [Gembruch, Ulrich] Univ Hosp, Obstet & Prenatal Med, Bonn, Germany.
   [Merz, Eberhard] Ctr Ultrasound & Prenatal Med, Frankfurt, Germany.
   [Eichhorn, Karl-Heinz] Practice Prenatal Med Weimar, Gynecol & Obstet, Weimar, Germany.
C3 University of Bonn
RP Kahler, C (corresponding author), Practice Prenatal Med Erfurt, Obst Gyn, Anger 81, D-99084 Erfurt, Germany.
EM kaehler@praenatalmedizin-erfurt.de
OI Gembruch, Ulrich/0000-0001-8284-4669
CR Berg C, 2007, ULTRASCHALL MED, V28, P133, DOI 10.1055/s-2007-963085
   Berg C, 2009, ULTRASCHALL MED, V30, P128, DOI 10.1055/s-0028-1109178
   Dolk H, 2010, ADV EXP MED BIOL, V686, P349, DOI 10.1007/978-90-481-9485-8_20
   Eichhorn KH, 2006, ULTRASCHALL MED, V27, P185, DOI 10.1055/s-2006-926622
   Hansmann M., 1981, ULTRASCHALL MED, V2, P206, DOI 10.1055/s-2007-1010074
   Hindryckx A, 2011, FACTS VIEWS VIS OBGY, V3, P165
   Holland BJ, 2015, ULTRASOUND OBST GYN, V45, P631, DOI 10.1002/uog.14882
   Institut fur Qualitat und Wirtschaftlichkeit im Gesundheitswesen, 2008, S0503 IQWIG
   Merz E, 2012, ULTRASCHALL MED, V33, P593, DOI 10.1055/s-0032-1325500
   Pandipati S, 2015, AM J OBSTET GYNECOL, V213, DOI 10.1016/j.ajog.2015.04.022
   Rempen A, 2016, ULTRASCHALL MED, V37, P579, DOI 10.1055/s-0042-115581
   van Velzen CL, 2015, ULTRASOUND OBST GYN, V45, P320, DOI 10.1002/uog.14689
   von Kaisenberg C, 2016, ULTRASCHALL MED, V37, P297, DOI 10.1055/s-0042-105514
NR 13
TC 2
Z9 2
U1 0
U2 0
PU GEORG THIEME VERLAG KG
PI STUTTGART
PA RUDIGERSTR 14, D-70469 STUTTGART, GERMANY
SN 0172-4614
EI 1438-8782
J9 ULTRASCHALL MED
JI Ultraschall Med.
PD OCT
PY 2020
VL 41
IS 05
BP 499
EP 503
DI 10.1055/a-1018-1752
PG 5
WC Acoustics; Radiology, Nuclear Medicine & Medical Imaging
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Acoustics; Radiology, Nuclear Medicine & Medical Imaging
GA OH5GO
UT WOS:000582609700007
OA Bronze
DA 2022-02-03
ER

PT C
AU Ozbulak, G
   Aytar, Y
   Ekenel, HK
AF Ozbulak, Gokhan
   Aytar, Yusuf
   Ekenel, Hazim Kemal
BE Bromme, A
   Busch, C
   Rathgeb, C
   Uhl, A
TI How Transferable are CNN-based Features for Age and Gender
   Classification?
SO PROCEEDINGS OF THE 15TH INTERNATIONAL CONFERENCE OF THE BIOMETRICS
   SPECIAL INTEREST GROUP (BIOSIG 2016)
SE Lecture Notes in Informatics-Proceedings
LA English
DT Proceedings Paper
CT 15th International Conference of the Biometrics Special Interest Group
   (BIOSIG)
CY SEP 21-23, 2016
CL Darmstadt, GERMANY
DE soft biometrics; age classification; gender classification;
   convolutional neural networks; deep learning; transfer learning
AB Age and gender are complementary soft biometric traits for face recognition. Successful estimation of age and gender from facial images taken under real-world conditions can contribute improving the identification results in the wild. In this study, in order to achieve robust age and gender classification in the wild, we have benefited from Deep Convolutional Neural Networks based representation. We have explored transferability of existing deep convolutional neural network (CNN) models for age and gender classification. The generic AlexNet-like architecture and domain specific VGG-Face CNN model are employed and fine-tuned with the Adience dataset prepared for age and gender classification in uncontrolled environments. In addition, task specific GilNet CNN model has also been utilized and used as a baseline method in order to compare with transferred models. Experimental results show that both transferred deep CNN models outperform the GilNet CNN model, which is the state-of-the-art age and gender classification approach on the Adience dataset, by an absolute increase of 7% and 4.5% in accuracy, respectively. This outcome indicates that transferring a deep CNN model can provide better classification performance than a task specific CNN model, which has a limited number of layers and trained from scratch using a limited amount of data as in the case of GilNet. Domain specific VGG-Face CNN model has been found to be more useful and provided better performance for both age and gender classification tasks, when compared with generic AlexNet-like model, which shows that transfering from a closer domain is more useful.
C1 [Ozbulak, Gokhan; Ekenel, Hazim Kemal] Istanbul Tech Univ, Dept Comp Engn, Maslak, Sariyer Istanbu, Turkey.
   [Aytar, Yusuf] MIT, CSAIL, Cambridge, MA 02139 USA.
C3 Istanbul Technical University; Massachusetts Institute of Technology
   (MIT)
RP Ozbulak, G (corresponding author), Istanbul Tech Univ, Dept Comp Engn, Maslak, Sariyer Istanbu, Turkey.
EM gokhan.ozbulak@itu.edu.tr; yusuf@csail.mit.edu; ekenel@itu.edu.tr
RI EKENEL, HAZIM KEMAL/A-5293-2016
OI EKENEL, HAZIM KEMAL/0000-0003-3697-8548
CR Antipov G, 2016, PATTERN RECOGN LETT, V70, P59, DOI 10.1016/j.patrec.2015.11.011
   Baluja S, 2007, INT J COMPUT VISION, V71, P111, DOI 10.1007/s11263-006-8910-9
   Choi SE, 2011, PATTERN RECOGN, V44, P1262, DOI 10.1016/j.patcog.2010.12.005
   Danisman T, 2014, INT C PATT RECOG, P3144, DOI 10.1109/ICPR.2014.542
   Dantcheva A., 2015, T INFORM FORENSICS S, P441
   Demirkus M., 2012, CVPR WORKSH, P130
   Eidinger E, 2014, IEEE T INF FOREN SEC, V9, P2170, DOI 10.1109/TIFS.2014.2359646
   Escalera S., 2015, P ICCV WORKSH
   Fu Y, 2008, IEEE T MULTIMEDIA, V10, P578, DOI 10.1109/TMM.2008.921847
   Gallagher AC, 2009, PROC CVPR IEEE, P256, DOI 10.1109/CVPRW.2009.5206828
   Gao F, 2009, LECT NOTES COMPUT SC, V5558, P132
   Girshick R., P IEEE C COMP VIS PA
   Guo G., 2008, P IEEE WORKSH APPL C, P721
   Huang, 2007, LABELED FACES WILD D, P7
   Huang F.J., 2006, C COMP VIS PATT REC, V1, P284, DOI DOI 10.1109/CVPR.2006.164
   Jain AK, 2004, PROC SPIE, V5404, P561, DOI 10.1117/12.542890
   Jain AK, 2004, LECT NOTES COMPUT SC, V3072, P731
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Krizhevsky A., 2012, INT C NEUR INF PROC, V25, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
   Kuang Z., 2015, ICCV WORKSH
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   KWON YH, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P762, DOI 10.1109/CVPR.1994.323894
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   Lanitis A., 2015, FG NET AGING DATABAS
   Levi Gil, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P34, DOI 10.1109/CVPRW.2015.7301352
   Mansanet J, 2016, PATTERN RECOGN LETT, V70, P80, DOI 10.1016/j.patrec.2015.11.015
   Moghaddam B, 2002, IEEE T PATTERN ANAL, V24, P707, DOI 10.1109/34.1000244
   Niinuma K, 2010, IEEE T INF FOREN SEC, V5, P771, DOI 10.1109/TIFS.2010.2075927
   Oquab M, 2014, CVPR
   Parkhi O. M., 2015, P BRIT MACH VIS
   Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X
   Pontes J. K., 2015, PATTERN RECOGNITION
   Ranjan R., 2015, P IEEE INT C COMP VI, P109
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41
   Russakovsky O., 2015, IJCV
   Simonyan K., 2015, PROC INT C LEARN REP
   Yosinski J, 2014, ADV NEUR IN, V27
NR 39
TC 2
Z9 2
U1 1
U2 3
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1617-5468
BN 978-3-8857-9654-1
J9 LECT NOTE INFORM
PY 2016
VL P-260
PG 6
WC Computer Science, Artificial Intelligence
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BG7LL
UT WOS:000391421200026
DA 2022-02-03
ER

PT J
AU Jain, AK
   Feng, JJ
AF Jain, Anil K.
   Feng, Jianjiang
TI Latent Palmprint Matching
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Palmprint; forensics; latents; minutiae; MinutiaCode; matching; region
   growing
ID FINGERPRINT; ALGORITHM
AB The evidential value of palmprints in forensic applications is clear as about 30 percent of the latents recovered from crime scenes are from palms. While biometric systems for palmprint-based personal authentication in access control type of applications have been developed, they mostly deal with low-resolution (about 100 ppi) palmprints and only perform full-to-full palmprint matching. We propose a latent-to-full palmprint matching system that is needed in forensic applications. Our system deals with palmprints captured at 500 ppi (the current standard in forensic applications) or higher resolution and uses minutiae as features to be compatible with the methodology used by latent experts. Latent palmprint matching is a challenging problem because latent prints lifted at crime scenes are of poor image quality, cover only a small area of the palm, and have a complex background. Other difficulties include a large number of minutiae in full prints (about 10 times as many as fingerprints), and the presence of many creases in latents and full prints. A robust algorithm to reliably estimate the local ridge direction and frequency in palmprints is developed. This facilitates the extraction of ridge and minutiae features even in poor quality palmprints. A fixed-length minutia descriptor, MinutiaCode, is utilized to capture distinctive information around each minutia and an alignment-based minutiae matching algorithm is used to match two palmprints. Two sets of partial palmprints (150 live-scan partial palmprints and 100 latent palmprints) are matched to a background database of 10,200 full palmprints to test the proposed system. Despite the inherent difficulty of latent-to-full palmprint matching, rank-1 recognition rates of 78.7 and 69 percent, respectively, were achieved in searching live-scan partial palmprints and latent palmprints against the background database.
C1 [Jain, Anil K.; Feng, Jianjiang] Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA.
C3 Michigan State University
RP Jain, AK (corresponding author), Michigan State Univ, Dept Comp Sci & Engn, 3115 Engn Bldg, E Lansing, MI 48824 USA.
EM jain@cse.msu.edu; jfeng@cse.msu.edu
RI Feng, Jianjiang/I-3386-2012
FU US Army Research Office [W911NF-061-0418]; National Institute of Justice
   [2007-RG-CXK183]; US National Science Foundation IUC on Identification
   Technology Research (CITeR)National Science Foundation (NSF)
FX The authors would like to thank Karthik Nandakumar, Abhishek Nagar, and
   the anonymous reviewers for their valuable comments. The authors would
   also like to thank Meltem Demirkus for collecting live- scan palmprints.
   The latent and full palmprint databases used in our experiments were
   provided by Lt. Gregoire Michaud and Sgt. Scott Hrcka of the Forensic
   Science Division of the Michigan State Police and Austin Hicklin of
   Noblis. This work was supported by US Army Research Office grant
   W911NF-061-0418, US National Institute of Justice grant 2007-RG-CXK183,
   and a grant from the US National Science Foundation IUC on
   Identification Technology Research (CITeR).
CR ASHBAUGH DR, 1999, CRC SER PR CRIM, P1
   BAZEN AM, 2000, P WORKSH CIRC SYST S, P205
   Chen XJ, 2006, IEEE T IMAGE PROCESS, V15, P767, DOI 10.1109/TIP.2005.860597
   Cummins H., 1961, FINGER PRINTS PALMS
   DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160
   DEWAN SK, 2003, NY TIMES         NOV
   Feng JJ, 2008, PATTERN RECOGN, V41, P342, DOI 10.1016/j.patcog.2007.04.016
   Funada J, 1998, INT C PATT RECOG, P1849, DOI 10.1109/ICPR.1998.712091
   Galton F., 1965, FINGERPRINTS
   Germain RS, 1997, IEEE COMPUT SCI ENG, V4, P42, DOI 10.1109/99.641608
   Hong L, 1998, IEEE T PATTERN ANAL, V20, P777, DOI 10.1109/34.709565
   Jain A, 2002, INT CONF ACOUST SPEE, P4064
   Jain A, 1997, IEEE T PATTERN ANAL, V19, P302, DOI 10.1109/34.587996
   Jain AK, 2008, LATENT PALMPRINT MAT
   JAIN AK, 2008, P CVPR WORKSH BIOM J
   Jiang XD, 2000, INT C PATT RECOG, P1038, DOI 10.1109/ICPR.2000.906252
   KOMARINSKI P, 2004, AUTOMATED FINGERPRIN
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MALTONI D, 2003, HDB FINGERPRINT RECO
   Neurotechnology Inc, 2009, VERIFINGER
   *NGI, 2008, FBIS NEXT GEN ID
   *NIST, 2009, NIST SPEC DAT, V27
   *NIST, NIST MIN INT EXCH TE
   *NSTC SUBCOMM BIOM, 2009, PALM PRINT REC
   Rowe RK, 2007, 2007 BIOMETRICS SYMPOSIUM, P18
   *RSMITH ASS INC, 2009, DEM PALM PRINTS
   Sun ZN, 2005, PROC CVPR IEEE, P279
   Tico M, 2003, IEEE T PATTERN ANAL, V25, P1009, DOI 10.1109/TPAMI.2003.1217604
   Wang Y, 2007, IEEE T PATTERN ANAL, V29, P573, DOI 10.1109/TPAMI.2007.1003
   Wilson C., 2004, 7123 NISTIR
   Zhang D, 2003, IEEE T PATTERN ANAL, V25, P1041, DOI 10.1109/TPAMI.2003.1227981
   Zhou J, 2004, IEEE T IMAGE PROCESS, V13, P821, DOI 10.1109/TIP.2003.822608
   2007, EVALUATION LATENT FI
   2009, FVC2006 4 INT FING V
NR 34
TC 174
Z9 185
U1 1
U2 34
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD JUN
PY 2009
VL 31
IS 6
BP 1032
EP 1047
DI 10.1109/TPAMI.2008.242
PG 16
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA 431YF
UT WOS:000265100000006
PM 19372608
OA Green Submitted
DA 2022-02-03
ER

PT C
AU Alsedais, R
   Guest, R
AF Alsedais, Rawabi
   Guest, Richard
BE Morales, A
   Vera-Rodriguez, R
   Lazzeretti, R
   Fierrez, J
   OrtegaGarcia, J
TI Person Re-Identification From CCTV Silhouettes Using Generic Fourier
   Descriptor
SO 2017 INTERNATIONAL CARNAHAN CONFERENCE ON SECURITY TECHNOLOGY (ICCST)
SE International Carnahan Conference on Security Technology Proceedings
LA English
DT Proceedings Paper
CT International Carnahan Conference on Security Technology (ICCST)
CY OCT 23-26, 2017
CL Madrid, SPAIN
DE Computer Vision; Generic Fourier Descriptor; Biometric
   re-identification; Silhouette
ID RECOGNITION
AB Person re-identification in public areas (such as airports, train stations and shopping malls) has recently received increased attention within computer vision research due, in part, to the demand for enhanced levels of security. Re-identifying subjects within non-overlapped camera networks can be considered as a challenging task. Illumination changes in different scenes, variations in camera resolutions, field of view and human natural motion are the key obstacles to accurate implementation. This study assesses the use of Generic Fourier Shape Descriptors (GFD) on person silhouettes for re-identification and furthermore identifies which sections of a subjects' silhouette is able to deliver optimum performance. Human silhouettes of 90 subjects from the CASIA dataset walking 0 degrees and 90 degrees to a fixed CCTV camera were used for the purpose of re-identification. Each subject's video sequence comprised between 10 and 50 frames. For both views, silhouettes were segmented into eight algorithmically-defined areas: head and neck, shoulders, upper 50%, lower 50%, upper 15%, middle 35%, lower 40% and whole body. A GFD was used independently on each segment at each angle. After extracting the GFD feature for each frame, a linear discriminant analysis (LDA) classifier was used to investigate re-identification accuracy rate, where 50% of each subject's frames were used for training and the other 50% were used for testing. The results show that 97% identification accuracy rate at the 10th rank is achieved by using GFD on the upper 50% segment of the human silhouette front (0 degrees) side. For 90 degrees images, using GFD on the upper 15% silhouette segment resulted in almost 98% accuracy rate at the 10th rank.
C1 [Alsedais, Rawabi; Guest, Richard] Univ Kent, Engn & Digital Arts, Canterbury, Kent, England.
C3 University of Kent
RP Alsedais, R (corresponding author), Univ Kent, Engn & Digital Arts, Canterbury, Kent, England.
EM raaa3@kent.ac.uk; R.M.Guest@kent.ac.uk
FU Ministry of Education in Saudi Arabia, King Faisal University
FX This work has been co-funded by the Ministry of Education in Saudi
   Arabia, King Faisal University.
CR Collins Matthew, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1235, DOI 10.1109/ICCVW.2009.5457467
   Collins RT, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P366, DOI 10.1109/AFGR.2002.1004181
   Dantcheva A., 2015, IEEE T INF FOREN SEC, P1
   Denman S, 2009, 2009 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA 2009), P196, DOI 10.1109/DICTA.2009.38
   Ge  Y., 2013, 2013 IEEE INT C MULT, P1
   Gonzalez R., 1990, COMPUT VIS GRAPH IMA, V49, P122
   Gonzalez-Sosa E., 2017, P INT C PATT, P3061
   Jaha ES, 2016, IEEE T INF FOREN SEC, V11, P2377, DOI 10.1109/TIFS.2016.2584001
   MOKHTARIAN F, 1986, IEEE T PATTERN ANAL, V8, P34, DOI 10.1109/TPAMI.1986.4767750
   Nixon MS, 2015, PATTERN RECOGN LETT, V68, P218, DOI 10.1016/j.patrec.2015.08.006
   Ramstrand N, 2011, FORENSIC SCI INT, V212, P27, DOI 10.1016/j.forsciint.2011.05.002
   TEH CH, 1988, IEEE T PATTERN ANAL, V10, P496, DOI 10.1109/34.3913
   Tieng QM, 1997, IEEE T PATTERN ANAL, V19, P910, DOI 10.1109/34.608294
   Venkat I, 2011, INT J COMPUT VISION, V91, P7, DOI 10.1007/s11263-010-0362-6
   Yu SQ, 2006, INT C PATT RECOG, P441
   Zhang D., 2017, IEEE T CIRCUITS SYST, VXX, P1
   Zhang DS, 2002, SIGNAL PROCESS-IMAGE, V17, P825, DOI 10.1016/S0923-5965(02)00084-X
NR 17
TC 1
Z9 1
U1 0
U2 0
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 1071-6572
BN 978-1-5386-1585-0
J9 INT CARN CONF SECU
PY 2017
PG 6
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Engineering
GA BJ4BL
UT WOS:000424779200051
DA 2022-02-03
ER

PT C
AU Alhadethy, AH
   Darwish, S
AF Alhadethy, Ahmed H.
   Darwish, Saad
GP IEEE
TI A Modified Finger Vein Identification Approach based on Niching Genetic
   Algorithm
SO 2019 15TH INTERNATIONAL COMPUTER ENGINEERING CONFERENCE (ICENCO 2019)
SE International Computer Engineering Conference
LA English
DT Proceedings Paper
CT 15th International Computer Engineering Conference (ICENCO)
CY DEC 29-30, 2019
CL Cairo Univ, Fac Engn, Comp Engn Dept, Giza, EGYPT
HO Cairo Univ, Fac Engn, Comp Engn Dept
DE Finger Vein; Context Based Clearing (NGA); enhanced feature selection;
   Gabor Transformation
ID PATTERNS
AB Finger vein engineering is the most modern biometric software using vein recognition patterns. Since these patterns are veiled under the skin surface, they offer great privateness, and therefore provide tremendous protection and are therefore extremely difficult to shape. The detection of the finger vein has received more consideration because previous methods have Highly vulnerable such as imbalanced finger vein dataset, and the extraction of salient feature within the low-quality images. Such defect has led the optimization algorithm not to be converged or its output to be reduced due to the limitations of the static means of finger vein identification, the need for intelligent approaches to finger vein identification is thus imperative. One of such intelligent approaches is machine learning which can be seen as the acquisition of structure description from examples. The kind of descriptions found can be used for identification, explanation, and understanding. The main contribution of the work presented in this thesis is to investigate the effect of genetic algorithm in selecting optimal finger vein features vector by adding niching concept in form of Context Based Clearing (CBC) procedure to increase the heterogeneity of items within the feature vector with the aim of removing the correlation between items of features vector. The enhanced feature selection approach produces an optimal vector that able to deal efficiently with the massive intra-class variations and the diminutive inter-class similarity. Besides, it yields the concept of feature set reduction to remove redundancy without degradation of the accuracy. The performance analysis of suggested model is carried through several experiments and the results show an improvement on an average by 6% in terms of accuracy compared with some Up-to-date distinguishing finger vein mechanisms available in the literature.
C1 [Alhadethy, Ahmed H.] AL Maaref Univ Coll, Dept Comp Sci, Alanbar, Iraq.
   [Darwish, Saad] Alexandria Univ, Dept Informat Technol, Inst Grad Studies & Res, Alexandria, Egypt.
C3 Egyptian Knowledge Bank (EKB); Alexandria University
RP Alhadethy, AH (corresponding author), AL Maaref Univ Coll, Dept Comp Sci, Alanbar, Iraq.
EM Ahmadhusham1985@gmail.com; darwish@alex-igsr.edu.eg
RI Darwish, Saad Mohamed/I-9961-2019
OI Darwish, Saad Mohamed/0000-0003-2723-1549
CR Adeoye, 2010, INT J COMPUT APPL, V9, P0975
   Banerjee A, 2018, MULTIMED TOOLS APPL, V77, P5857, DOI 10.1007/s11042-017-4501-8
   Beining Huang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1269, DOI 10.1109/ICPR.2010.316
   Bhargava N., 2013, INT J COMPUTER TREND, V4, P515
   Ezhilmaran D, 2017, ADV COMPU INTELL ROB, P1, DOI 10.4018/978-1-5225-2053-5.ch001
   Fayek MB, 2010, J ADV RES, V1, P301, DOI 10.1016/j.jare.2010.09.001
   Garg P., 2012, GLOB J COMPUT SCI TE, V12, P1
   Haghighat M, 2015, EXPERT SYST APPL, V42, P7905, DOI 10.1016/j.eswa.2015.06.025
   He CL, 2017, 2017 IEEE 16TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS & COGNITIVE COMPUTING (ICCI*CC), P456, DOI 10.1109/ICCI-CC.2017.8109788
   Van HT, 2015, 2015 SEVENTH INTERNATIONAL CONFERENCE ON KNOWLEDGE AND SYSTEMS ENGINEERING (KSE), P348, DOI 10.1109/KSE.2015.12
   Iqbal K, 2012, J COMPUT SYST SCI, V78, P1258, DOI 10.1016/j.jcss.2011.10.013
   Jaiswal S., 2011, J GLOBAL RES COMPUTE, V2, P19
   Kaur S., 2017, INT J SCI ENG RES, V8, P936
   Kono M, 2002, APPL OPTICS, V41, P7429, DOI 10.1364/AO.41.007429
   Liu Z, 2010, J NETW COMPUT APPL, V33, P275, DOI 10.1016/j.jnca.2009.12.006
   Miura N, 2004, MACH VISION APPL, V15, P194, DOI 10.1007/s00138-004-0149-2
   Miura N., 2005, P IAPR C MACH VIS AP, P347
   Moganeshwaran R, 2012, 2012 IEEE INTERNATIONAL CONFERENCE ON CIRCUITS AND SYSTEMS (ICCAS), P237, DOI 10.1109/ICCircuitsAndSystems.2012.6408317
   Parthiban K, 2014, 2014 FIFTH INTERNATIONAL CONFERENCE ON THE APPLICATIONS OF DIGITAL INFORMATION AND WEB TECHNOLOGIES (ICADIWT), P143, DOI 10.1109/ICADIWT.2014.6814681
   Ragan R, 2014, P IEEE INT C EM RES, P1
   Ratha NK, 2007, IEEE T PATTERN ANAL, V29, P561, DOI 10.1109/TPAMI.2007.1004
   Saad, 2016, IND J SCI TECHNOL, V9, P1
   Shaheed K, 2018, INFORMATION, V9, DOI 10.3390/info9090213
   Shazia S., 2016, INDIAN J SCI TECHNOL, V9, P1
   Sheikh Rahila H, 2008, 2008 1st International Conference on Emerging Trends in Engineering and Technology (ICETET), P314, DOI 10.1109/ICETET.2008.48
   Shinde SR., 2017, INT RES J ENG TECHNO, V4, P1517
   Su J, 2017, J CRYST GROWTH, V468, P914, DOI 10.1016/j.jcrysgro.2016.10.061
   Unnikrishnan P., 2014, THESIS
   Veluchamy S, 2017, IET BIOMETRICS, V6, P232, DOI 10.1049/iet-bmt.2016.0112
   Vishi K, 2013, 2013 NINTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING (IIH-MSP 2013), P334, DOI 10.1109/IIH-MSP.2013.91
   Wang K., 2011, BIOMETRICS
   Wu JD, 2011, EXPERT SYST APPL, V38, P5423, DOI 10.1016/j.eswa.2010.10.013
   Yang JF, 2012, PATTERN RECOGN LETT, V33, P623, DOI 10.1016/j.patrec.2011.11.002
   Yang JF, 2011, COMPUT HUM BEHAV, V27, P1565, DOI 10.1016/j.chb.2010.10.029
   Yang L, 2014, LECT NOTES COMPUT SC, V8833, P234, DOI 10.1007/978-3-319-12484-1_26
   Yea F., 2011, PROCEDIA ENG, V16, P383
NR 36
TC 1
Z9 1
U1 0
U2 1
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2475-2312
EI 2475-2320
BN 978-1-7281-5146-5
J9 INT COMPUT ENG CONF
PY 2019
BP 100
EP 109
PG 10
WC Computer Science, Theory & Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BQ0NP
UT WOS:000573255800017
DA 2022-02-03
ER

PT C
AU Mohamed, M
   Saxena, N
AF Mohamed, Manar
   Saxena, Nitesh
GP Assoc Comp Machinery
TI Gametrics: Towards Attack-Resilient Behavioral Authentication with
   Simple Cognitive Games
SO 32ND ANNUAL COMPUTER SECURITY APPLICATIONS CONFERENCE (ACSAC 2016)
SE Annual Computer Security Applications Conference - Proceedings
LA English
DT Proceedings Paper
CT 32nd Annual Computer Security Applications Conference (ACSAC)
CY DEC 05-09, 2016
CL Los Angeles, CA
AB Authenticating a user based on her unique behavioral biometric traits has been extensively researched over the past few years. The most researched behavioral biometrics techniques are based on keystroke and mouse dynamics. These schemes, however, have been shown to be vulnerable to human-based and robotic attacks that attempt to mimic the user's behavioral pattern to impersonate the user.
   In this paper, we aim to verify the user's identity through the use of active, cognition-based user interaction in the authentication process. Such interaction boasts to provide two key advantages. First, it may enhance the security of the authentication process as multiple rounds of active interaction would serve as a mechanism to prevent against several types of attacks, including zero effort attack, expert trained attackers, and automated attacks. Second, it may enhance the usability of the authentication process by actively engaging the user in the process.
   We explore the cognitive authentication paradigm through very simplistic interactive challenges, called Dynamic Cognitive Comes, which involve objects floating around within the images, where the user's task is to match the objects with their respective target(s) and drag/drop them to the target location(s). Specifically, we introduce, build and study Gametrics ("Game-based biometrics"), an authentication mechanism based on the unique way the user solves such simple challenges captured by multiple features related to her cognitive abilities and mouse dynamics. Based on a comprehensive data set collected in both online and lab settings, we show that Gametrics can identify the users with a high accuracy (false negative rates, FAR, as low as 0.02) while rejecting zero-effort attackers (false positive rates, EPR, as low as 0.02). Moreover, Gametrics shows promising results in defending against expert attackers that try to learn and later mimic the user's pattern of solving the challenges (FPR for expert human attacker as low as 0.03). Furthermore, we argue that the proposed biometrics is hard to be replayed or spoofed by automated means, such as robots or malware attacks.
C1 [Mohamed, Manar; Saxena, Nitesh] Univ Alabama Birmingham, Birmingham, AL 35294 USA.
C3 University of Alabama System; University of Alabama Birmingham
RP Mohamed, M (corresponding author), Univ Alabama Birmingham, Birmingham, AL 35294 USA.
EM manar@uab.edu; saxena@uab.edu
CR Al Galib A, 2015, LECT NOTES COMPUT SC, V8975, P254, DOI 10.1007/978-3-662-47854-7_16
   Bergadano F., 2002, ACM Transactions on Information and Systems Security, V5, P367, DOI 10.1145/581271.581272
   Biddle R., 2009, TECHNICAL REPORT T 0
   Brooke J., 1996, USABILITY EVALUATION
   Chen K-T, 2007, P 6 ACM SIGCOMM WORK, P7, DOI DOI 10.1145/1326257.1326259
   Chen Kuan-Ta, 2007, TI WORKSHOP NETWORK
   De Luca Alexander, 2012, CHI 12, P987, DOI [10.1145/2207676.2208544, DOI 10.1145/2207676.2208544]
   Dunphy P, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P36
   Epp C, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P715
   Frank M, 2013, IEEE T INF FOREN SEC, V8, P136, DOI 10.1109/TIFS.2012.2225048
   Goldwasser Gabriel, 2010, FASTER TIMES
   Hern A., GOOGLE AIMS KILL PAS
   Jernwn I., 1999, SSYM M 99 P 8TH C US
   Kaminsky R., 2008, IDENTIFYING GAME PLA
   Lin C. -C., 2014, AIDSS
   Maxion RA, 2010, I C DEPEND SYS NETWO, P201, DOI 10.1109/DSN.2010.5544311
   Messerman A, 2011, P IEEE INT JOINT C B, P1, DOI DOI 10.1109/IJCB.2011.6117552
   Mohamed M, 2016, CODASPY'16: PROCEEDINGS OF THE SIXTH ACM CONFERENCE ON DATA AND APPLICATION SECURITY AND PRIVACY, P152, DOI 10.1145/2857705.2857749
   Mohamed Manar, 2014, 9 ACMSYMPOSIUM INFOR, P195, DOI DOI 10.1145/2590296.2590298
   Monrose F, 1997, P 4 ACM C COMP COMM, P48, DOI 10.1145/266420.266434
   Owusu F., 2012, P TWELFTH WORKSHOP M, P9
   Perrig Adrian, 1999, CRYPTEC
   Pusara Maja, 2004, WORKSHOP VISUALIZATI
   Rabkin A., 2008, SOUPS 08 P 4TH SY7NP
   Schechter S. E., 2009, P 5TH S USABLE PRIVA
   Schechter S, 2009, P IEEE S SECUR PRIV, P375, DOI 10.1109/SP.2009.11
   Schonfeld Erick, 2010, TECH CRUNCH
   Serwadda Abdul, 2013, P 2013 ACM SIGSAC C, P599, DOI 10.1145/2508859.2516659
   Sim N., 2005, ACSAC
   Tey C. M., 2013, 20TH ANN NETWORK DIS
   Valve Corporation, 2010, STEAM CAINE PLAYER S
   Wiedenbeck S., 2005, INT J HAMAN COMPUTER
   Wiedenbeck S., 2006, P WORKING C ADV VISU
   Zheng N, 2011, PROCEEDINGS OF THE 18TH ACM CONFERENCE ON COMPUTER & COMMUNICATIONS SECURITY (CCS 11), P139
   Zhong Y., 2012, 2012 IEEE COMP VIS P, P117, DOI [DOI 10.1109/CVPRW.2012.6239225, 10.1109/CVPRW.2012.6239225]
NR 35
TC 2
Z9 2
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1515 BROADWAY, NEW YORK, NY 10036-9998 USA
SN 1063-9527
BN 978-1-4503-4771-6
J9 ANN COMPUT SECURITY
PY 2016
BP 277
EP 288
DI 10.1145/2991079.2991096
PG 12
WC Computer Science, Information Systems; Computer Science, Theory &
   Methods
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA BR0QM
UT WOS:000630180100023
DA 2022-02-03
ER

PT J
AU Chibani, R
   Tlili, A
   Ben Salem, F
   Louhaichi, M
   Belgacem, AO
   Neffati, M
AF Chibani, Roukaya
   Tlili, Abderrazak
   Ben Salem, Farah
   Louhaichi, Mounir
   Belgacem, Azaiez Ouled
   Neffati, Mohamed
TI Assessment of long-term protection on the aboveground biomass and
   organic carbon content using two non-destructive techniques: case of the
   Sidi Toui National Park in southern Tunisia
SO AFRICAN JOURNAL OF RANGE & FORAGE SCIENCE
LA English
DT Article; Early Access
DE arid rangelands; non-destructive methods; photographic monitoring
   technique; VegMeasure (R); rangeland assessment
ID SPECIES-DIVERSITY; GRAZING EXCLUSION; INNER-MONGOLIA; COVER;
   SEQUESTRATION; VEGETATION; RANGELAND; RESPONSES; DESERT
AB Long-term protection of arid ecosystems changes the vegetation and soil structures. The quantification of aboveground biomass and carbon content are among the principal indicators to evaluate these changes. Most methods used to quantify these parameters are costly, time consuming and destructive. In this paper, two non-destructive methods were compared: digital image processing and biometric measurements. Aboveground biomass and carbon content of five perennial shrubs were studied inside and outside Sidi Toui National Park, southern Tunisia. The effect of long-term protection on aboveground biomass and carbon content was assessed. The main results indicated that both methods provided similar aboveground biomass and carbon values. Aboveground biomass and carbon content were strongly correlated with canopy cover and biovolume (R-2 > 0.7). Positive linear relationships were found for all the studied species. Apart from the large canopy cover of Haloxylon schmittianum and Haloxylon scoparium, the obtained results showed that long-term protection had no significant effect on aboveground biomass and carbon content for the other species. These results suggest that digital charting technique is an accurate method for assessing rangeland productivity in a timely and cost-efficient manner and that long-term protection is not always suitable for enhancing carbon content and therefore ecosystem resilience under climate change conditions.
C1 [Chibani, Roukaya; Tlili, Abderrazak; Ben Salem, Farah; Neffati, Mohamed] Univ Gabes, Inst Reg Arides, Lab Ecosyst Pastoraux & Valorisat Plantes Spontan, Medenine, Tunisia.
   [Chibani, Roukaya] Inst Natl Agron Tunisie, Tunis, Tunisia.
   [Louhaichi, Mounir] Int Ctr Agr Res Dry Areas ICARDA, Tunis, Tunisia.
   [Belgacem, Azaiez Ouled] Food & Agr Org United Nations FAO, Riadh, Saudi Arabia.
C3 Institut des Regions Arides; Universite de Gabes; Universite de
   Carthage; CGIAR; International Center for Agricultural Research in the
   Dry Areas (ICARDA)
RP Tlili, A (corresponding author), Univ Gabes, Inst Reg Arides, Lab Ecosyst Pastoraux & Valorisat Plantes Spontan, Medenine, Tunisia.
EM tlili_abderrazak@yahoo.fr
OI TLILI, ABDERRAZAK/0000-0001-5690-703X
FU Institut des Regions Arides, Medenine (IRA, Tunisia); International
   Center for Agricultural Research in the Dry Areas (ICARDA)CGIAR; CGIAR
   Research Program on Livestock (CRP Livestock)CGIAR
FX This work was supported by the Institut des Regions Arides, Medenine
   (IRA, Tunisia), the International Center for Agricultural Research in
   the Dry Areas (ICARDA), and the CGIAR Research Program on Livestock (CRP
   Livestock). We thank Dr Sawsan Hassan and Dr Hedi Mighri for their
   revisions.
CR Abdallah F., 2014, American Journal of Plant Sciences, V5, P899
   ALLEN S E, 1986, P285
   BASKERVILLE G L, 1972, Canadian Journal of Forest Research, V2, P49, DOI 10.1139/x72-009
   Belgacem AO, 2013, LAND DEGRAD DEV, V24, P57, DOI 10.1002/ldr.1103
   Belgacem AO, 2013, RANGE MANAG AGROFOR, V34, P88
   Belgacem AO, 2013, CLIMATIC CHANGE, V119, P451, DOI 10.1007/s10584-013-0701-z
   Bisigato AJ, 2008, J ARID ENVIRON, V72, P1464, DOI 10.1016/j.jaridenv.2008.02.016
   Boyda Eric D., 2015, Prairie Naturalist, V47, P73
   Castro AJ, 2015, APPL GEOGR, V60, P1, DOI 10.1016/j.apgeog.2015.02.012
   Chavan BL., 2011, GLOBAL J RES ENG, V11, P14
   Chen SP, 2009, GLOBAL CHANGE BIOL, V15, P2450, DOI 10.1111/j.1365-2486.2009.01879.x
   Chen YP, 2012, ENVIRON MANAGE, V50, P622, DOI 10.1007/s00267-012-9919-1
   Daryanto S, 2013, AGR ECOSYST ENVIRON, V169, P1, DOI 10.1016/j.agee.2013.02.001
   DELROSARIO I, 2012, FOREST ECOL MANAG, V265, P218, DOI DOI 10.1016/J.FORECO.2011.10.036
   Derner JD, 2007, J SOIL WATER CONSERV, V62, P77
   DGF (Direction Generale des Foruts), 2010, INV FOR TEL RES DEUX
   Djomo AN, 2010, FOREST ECOL MANAG, V260, P1873, DOI 10.1016/j.foreco.2010.08.034
   FAO (Food and Agriculture Organization), 2002, STAT FOOD AGR 2002
   Flombaum P, 2007, J ARID ENVIRON, V69, P352, DOI 10.1016/j.jaridenv.2006.09.008
   Fusco EJ, 2019, ECOSPHERE, V10, DOI 10.1002/ecs2.2821
   Hamada Y, 2011, REMOTE SENS ENVIRON, V115, P3056, DOI 10.1016/j.rse.2011.06.008
   Homann S, 2008, HUM ECOL, V36, P503, DOI 10.1007/s10745-008-9180-7
   HUGHES HG, 1987, J RANGE MANAGE, V40, P367, DOI 10.2307/3898739
   Idi A, 2010, AFR J ECOL, V48, P51, DOI 10.1111/j.1365-2028.2009.01081.x
   Intergovernmental Panel on Climate Change, 2007, AR4 CLIM CHANG 2007
   Ionesco T., 1973, FAOTUN71525
   Issa S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12122008
   Keller M, 2001, FOREST ECOL MANAG, V154, P371, DOI 10.1016/S0378-1127(01)00509-6
   Ketterings QM, 2001, FOREST ECOL MANAG, V146, P199, DOI 10.1016/S0378-1127(00)00460-6
   Lal R, 2004, ENVIRON MANAGE, V33, P528, DOI 10.1007/s00267-003-9110-9
   Laliberte AS, 2007, J ARID ENVIRON, V69, P1, DOI 10.1016/j.jaridenv.2006.08.016
   Li QX, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-30595-3
   Li XR, 2003, PLANT SOIL, V251, P237, DOI 10.1023/A:1023023702248
   Louhaichi M, 2012, J ARID ENVIRON, V79, P101, DOI 10.1016/j.jaridenv.2011.11.024
   Louhaichi M., 2018, VEGMEASURE, V2
   Louhaichi M, 2018, AGROFOREST SYST, V92, P1341, DOI 10.1007/s10457-017-0079-4
   Louhaichi M, 2010, INT J AGRIC BIOL, V12, P406
   Lysenko I., 2008, CARBON STORAGE PROTE
   Ma SH, 2018, BIOGEOSCIENCES, V15, P693, DOI 10.5194/bg-15-693-2018
   Mandal G, 2015, J FORESTRY RES, V26, P291, DOI 10.1007/s11676-015-0038-8
   Neffati M., 2002, SECHERESSE, V13, P195
   Ohmann L. F., 1981, General Technical Report, North Central Forest Experiment Station, USDA Forest Service
   PECHANEC J. F., 1937, Journal of the American Society of Agronomy, V29, P894
   RUIJUN L, 2010, SUSTAINABLE USE RANG, P127
   Sampaio Everardo V.S.B., 2005, Acta Bot. Bras., V19, P935, DOI 10.1590/S0102-33062005000400028
   Tarhouni M., 2014, INT J BIODIVERSITY, V2014, P1
   Tarhouni Mohamed, 2007, Secheresse (Montrouge), V18, P240, DOI 10.1684/sec.20070108
   Tarhouni M, 2016, BOT LETT, V163, P281, DOI 10.1080/23818107.2016.1196147
   Tessema ZK, 2011, J ARID ENVIRON, V75, P662, DOI 10.1016/j.jaridenv.2011.02.004
   Touhami I., 2016, Options Mediterraneennes. Serie A, Seminaires Mediterraneens, P365
   UNEP (United Nations Environmental Programme), 2008, ANN REPORT
   Vashum K.T, 2012, J ECOSYSTEM ECOGRAPH, V2, P1, DOI 10.4172/2157-7625.1000116
   WOODROFFE K., 1941, JOUR AUSTRALIAN INST AGRIC SCI, V7, P117
   Wu GL, 2009, PLANT SOIL, V319, P115, DOI 10.1007/s11104-008-9854-3
   Xiong DP, 2016, ECOL ENG, V94, P647, DOI 10.1016/j.ecoleng.2016.06.124
   Yang HT, 2017, ARID LAND RES MANAG, V31, P283, DOI 10.1080/15324982.2017.1301595
   Yayneshet T, 2009, J ARID ENVIRON, V73, P542, DOI 10.1016/j.jaridenv.2008.12.002
   Zhang W, 1998, BIODIVERS CONSERV, V7, P1365, DOI 10.1023/A:1008852017493
   Zribi L, 2016, FOREST SYST, V25, DOI 10.5424/fs/2016252-08062
NR 59
TC 0
Z9 0
U1 1
U2 2
PU TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN 1022-0119
EI 1727-9380
J9 AFR J RANGE FOR SCI
JI Afr. J. Range Forage Sci.
DI 10.2989/10220119.2021.1928752
EA JUN 2021
PG 11
WC Ecology; Environmental Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Environmental Sciences & Ecology
GA SL9MC
UT WOS:000657237400001
OA hybrid
DA 2022-02-03
ER

PT J
AU Hui, XN
   Conroy, TB
   Kan, EC
AF Hui, Xiaonan
   Conroy, Thomas B.
   Kan, Edwin C.
TI Multi-Point Near-Field RF Sensing of Blood Pressures and Heartbeat
   Dynamics
SO IEEE ACCESS
LA English
DT Article
DE Biomedical signal processing; cardiography; microwave sensors; MIMO;
   stethoscope; wearable sensors
ID EMPIRICAL-MODE DECOMPOSITION; UNCERTAINTY PRINCIPLES;
   PHOTOPLETHYSMOGRAPHY; SENSORS; RADIO
AB Systolic and diastolic blood pressure estimation using arm-cuff monitors is one of the most common cardiovascular evaluation criteria in healthcare today, however these measures lack critical heartbeat and pressure dynamics. Pulse-transit time can be used as an alternative for arm-cuff monitors, but gives only one parameter for two pressure quantities. Ultrasound, computed tomography scan, and magnetic resonance imaging can retrieve geometrical features of the heart, but cannot directly estimate vascular pressures. Here we show a novel radio-frequency heartbeat sensor based on multi-point near-field observation. By comparing to synchronized electrocardiogram and auscultation, the multi-point sensor can assess motion and pressure in different parts of the heart following the Wiggers diagram. By applying the Hilbert-Huang frequency-time transform, the central blood pressure can be derived from the vascular vibration characteristics as continuous transients, including during the pulmonary cycle which is previously inaccessible from branchial measurements. Our scheme can be further extended to a full multiple-input-multiple-output (MIMO) channel arrangement, producing rich content for diagnostic and biometric applications. We employ dynamic time warping analyses to illustrate the sensor & x2019;s wealth of diversified information across different channels and persons. This new multi-point sensor, which can be worn conveniently over clothing, enables unprecedented monitoring capabilities of detailed central blood pressure transients and heartbeat dynamics under the given measurement instructions for at-home and clinical cardiovascular diagnostics.
C1 [Hui, Xiaonan; Conroy, Thomas B.; Kan, Edwin C.] Cornell Univ, Sch Elect & Comp Engn, Ithaca, NY 14853 USA.
C3 Cornell University
RP Hui, XN (corresponding author), Cornell Univ, Sch Elect & Comp Engn, Ithaca, NY 14853 USA.
EM xh273@cornell.edu
FU Department of Energy (DoE) of United States through the Advanced
   Research Projects Agency-Energy (ARPA-E)United States Department of
   Energy (DOE) [DE-AR0000946]; Department of Defense of United States
   through the Office of the Congressionally Directed Medical Research
   Programs (CDMRP) [PR-182496]
FX This work was supported in part by the Department of Energy (DoE) of
   United States through the Advanced Research Projects Agency-Energy
   (ARPA-E) under Project DE-AR0000946, and in part by the Department of
   Defense of United States through the Of~ce of the Congressionally
   Directed Medical Research Programs (CDMRP) Discovery under Award
   PR-182496.
CR Allen J, 2007, PHYSIOL MEAS, V28, pR1, DOI 10.1088/0967-3334/28/3/R01
   Avolio A, 2008, HYPERTENSION, V51, P1470, DOI 10.1161/HYPERTENSIONAHA.107.108910
   Balanis C. A., 2016, ANTENNA THEORY ANAL
   Batchelor C.K., 2000, INTRO FLUID DYNAMICS
   Cao YQ, 2016, J NEUROSCI METH, V261, P97, DOI 10.1016/j.jneumeth.2015.12.006
   Chan PK, 2019, IEEE MTT S INT MICR, P361
   Chang NF, 2011, BIOMED CIRC SYST C, P420, DOI 10.1109/BioCAS.2011.6107817
   Chen D, 2010, IEEE T INF TECHNOL B, V14, P1417, DOI 10.1109/TITB.2010.2072963
   Cohen L, 2001, APPL NUM HARM ANAL, P217
   COHEN L, 1995, P SOC PHOTO-OPT INS, V2563, P80, DOI 10.1117/12.211427
   DONOHO DL, 1989, SIAM J APPL MATH, V49, P906, DOI 10.1137/0149053
   Ebrahimi A, 2018, IEEE SENS J, V18, P5786, DOI 10.1109/JSEN.2018.2840691
   Folland GB, 1997, J FOURIER ANAL APPL, V3, P207, DOI 10.1007/BF02649110
   Grossman W, 2006, GROSSMANS CARDIAC CA
   Grubler MR, 2017, SWISS MED WEEKLY, V147, P14491
   Holz Christian, 2017, P ACM INT MOB WEAR U, V1, P3
   Huang NE, 2003, P ROY SOC A-MATH PHY, V459, P2317, DOI 10.1098/rspa.2003.1123
   Huang NE., 2014, HILBERT HUANG TRANSF
   Huang W, 1999, P NATL ACAD SCI USA, V96, P1834, DOI 10.1073/pnas.96.5.1834
   Hui X., 2018, P IEEE INT C COMM IC, P1
   Hui XN, 2019, 2019 IEEE MTT-S INTERNATIONAL MICROWAVE BIOMEDICAL CONFERENCE (IMBIOC 2019)
   Hui XN, 2019, IEEE MTT S INT MICR, P365
   Hui XN, 2019, NAT ELECTRON, V2, P125, DOI 10.1038/s41928-019-0219-0
   Hui XN, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aau0169
   Hui XN, 2018, NAT ELECTRON, V1, P74, DOI 10.1038/s41928-017-0001-0
   Hui XN, 2018, INT CONF WEARAB IMPL, P156, DOI 10.1109/BSN.2018.8329682
   Hui XA, 2017, IEEE J RADIO FREQ ID, V1, P51, DOI 10.1109/JRFID.2017.2745898
   Kovacs G., 2014, AM J RESP CRIT CARE
   Li CZ, 2017, IEEE T MICROW THEORY, V65, P1692, DOI 10.1109/TMTT.2017.2650911
   Li CZ, 2013, IEEE T MICROW THEORY, V61, P2046, DOI 10.1109/TMTT.2013.2256924
   Lin J. C., 1993, REV RADIO SCI, P771
   LIN JC, 1975, P IEEE, V63, P1530, DOI 10.1109/PROC.1975.9992
   Michard F, 2000, AM J RESP CRIT CARE, V162, P134, DOI 10.1164/ajrccm.162.1.9903035
   Mukkamala R, 2015, IEEE T BIO-MED ENG, V62, P1879, DOI 10.1109/TBME.2015.2441951
   Naqui J, 2015, J SENSORS, V2015, DOI 10.1155/2015/741853
   National Research Council, 2005, ASS POT HLTH EFF EXP
   Rudski LG, 2010, J AM SOC ECHOCARDIOG, V23, P685, DOI 10.1016/j.echo.2010.05.010
   SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055
   Tsang L., 2004, SCATTERING ELECTROMA
   Wang CH, 2018, NAT BIOMED ENG, V2, P687, DOI 10.1038/s41551-018-0287-x
   Wiggers C., 1923, MODERN ASPECTS CIRCU
   Williams B, 2006, CIRCULATION, V113, P1213, DOI 10.1161/CIRCULATIONAHA.105.595496
   Xing XM, 2016, BIOMED OPT EXPRESS, V7, P3007, DOI 10.1364/BOE.7.003007
   YEAGER DJ, 2008, RFID HDB APPL TECHNO, P00261
   YOCK PG, 1984, CIRCULATION, V70, P657, DOI 10.1161/01.CIR.70.4.657
   Zipes, 2007, BRAUNWALDS HEART DIS
NR 46
TC 4
Z9 4
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2020
VL 8
BP 89935
EP 89945
DI 10.1109/ACCESS.2020.2993994
PG 11
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Telecommunications
GA LV8XZ
UT WOS:000538727700087
OA gold
DA 2022-02-03
ER

PT C
AU Darmawahyuni, A
   Nurmaini, S
   Rachmatullah, MN
AF Darmawahyuni, A.
   Nurmaini, S.
   Rachmatullah, M. Naufal
GP IOP Publishing
TI Analysis of Classifier Performance on ECG Interpretation for Precision
   Medicine: Which performance metrics should we use?
SO 3RD FORUM IN RESEARCH, SCIENCE, AND TECHNOLOGY (FIRST 2019)
   INTERNATIONAL CONFERENCE
SE Journal of Physics Conference Series
LA English
DT Proceedings Paper
CT 3rd International Conference on Forum in Research, Science, and
   Technology (FIRST)
CY OCT 09-10, 2019
CL INDONESIA
AB In Indonesia, the prevalence of cardiac disease diagnosed by doctors and symptoms higher in rural areas and in the lowest ownership index quintiles. Software engineering based on deep learning techniques makes technology-based updates dynamically evolve for health care, especially in interpreting cardiac abnormalities. Electrocardiogram (ECG) is still one of the reliable cardiac examination tools. ECG is one source of medical big data collected, in addition to electronic health records (EHR), biomarker data, medical imaging, biometric data, etc. Medical big data will be processed for decision making to assign a diagnosis among several possible patients diagnose. The quality of a decision support system to interpret actual patient conditions is determined by how accurately the system is able to represent the diagnosis by experts. Some analytical targets of medical big data are prediction (classification). In classification, the performance classifiers can be evaluated by various performance metrics tested in a set of tests or independent validation sets. Performance metrics are used for helping the researchers interested in biomedical engineering to evaluate the performance of classifiers. Multivariate and imbalanced data are major problems in evaluating the performance. Therefore, in this paper discuss the performance metrics that used to minimize the bias of the classifier performance in ECG for enhancing the quality of diagnoses of cardiac abnormality in rural health care. This paper uses Deep Learning technique to show the results of performance metrics for ECG interpretation in classifying cardiac abnormalities.
C1 [Darmawahyuni, A.; Nurmaini, S.; Rachmatullah, M. Naufal] Univ Sriwijaya, Fac Comp Sci, Intelligent Syst Res Grp, Palembang, Indonesia.
C3 Universitas Sriwijaya
RP Nurmaini, S (corresponding author), Univ Sriwijaya, Fac Comp Sci, Intelligent Syst Res Grp, Palembang, Indonesia.
EM siti_nurmaini@unsri.ac.id
RI Darmawahyuni, Annisa/AAE-9462-2019; Nurmaini, Siti/L-6311-2013
OI Darmawahyuni, Annisa/0000-0002-0229-5717; Nurmaini,
   Siti/0000-0002-8024-2952; Rachmatullah, Muhammad
   Naufal/0000-0003-3553-3475
FU Kemenristek Dikti Indonesia under the Basic Research Fund
   [096/SP2H/LT/DRPM/2019]; Universitas Sriwijaya. Indonesia under Hibah
   Unggulan Profesi Fund 2019
FX This research is supported by The Kemenristek Dikti Indonesia under the
   Basic Research Fund Number. 096/SP2H/LT/DRPM/2019 and Universitas
   Sriwijaya. Indonesia under Hibah Unggulan Profesi Fund 2019.
CR Akosa J, 2017, P SAS GLOB FOR
   Badan Penelitian dan Pengembangan Kesehatan, 2013, LAP NAS 2013, V2013
   Ben-David A, 2008, EXPERT SYST APPL, V34, P825, DOI 10.1016/j.eswa.2006.10.022
   Bonnet F, 2005, ATHEROSCLEROSIS, V178, P339, DOI 10.1016/j.atherosclerosis.2004.08.035
   Boughorbel S, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0177678
   CAREL RS, 1978, MED PROG TECHNOL, V6, P35
   Congedo M, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0121423
   Darmawahyuni A., 2019, COMPUT ENG APPL J, V8
   Darmawahyuni A, 2019, ALGORITHMS, V12, DOI 10.3390/a12060118
   Ferri C, 2009, PATTERN RECOGN LETT, V30, P27, DOI 10.1016/j.patrec.2008.08.010
   Goldberger AL, 2000, CIRCULATION, V101, pE215, DOI 10.1161/01.CIR.101.23.e215
   Gu Q, 2009, COMM COM INF SC, V51, P461, DOI 10.1007/978-3-642-04962-0_53
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   Jiao Yasen, 2016, [Quantitative Biology, 定量生物学], V4, P320
   Mathews SM, 2018, COMPUT BIOL MED, V99, P53, DOI 10.1016/j.compbiomed.2018.05.013
   Meng HH, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE AND ENGINEERING (CSE), P7, DOI 10.1109/CSE.2014.36
   Mozaffarian D, 2016, CIRCULATION, V133, pE38, DOI 10.1161/CIR.0000000000000350
   Nurmaini A.G.S., 2018, INT J ADV SOFT COMPU, V10, P14
   Powers D. M., 2011, EVALUATION PRECISION
   Rajpurkar P., 2017, ARXIV170701836
   Schlapfer J, 2017, J AM COLL CARDIOL, V70, P1183, DOI 10.1016/j.jacc.2017.07.723
   Seliya N, 2009, PROC INT C TOOLS ART, P59, DOI 10.1109/ICTAI.2009.25
   Vincent E, 2008, INT CONF ACOUST SPEE, P109, DOI 10.1109/ICASSP.2008.4517558
NR 23
TC 0
Z9 0
U1 0
U2 0
PU IOP PUBLISHING LTD
PI BRISTOL
PA DIRAC HOUSE, TEMPLE BACK, BRISTOL BS1 6BE, ENGLAND
SN 1742-6588
EI 1742-6596
J9 J PHYS CONF SER
PY 2020
VL 1500
AR 012134
DI 10.1088/1742-6596/1500/1/012134
PG 7
WC Multidisciplinary Sciences; Physics, Multidisciplinary
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Science & Technology - Other Topics; Physics
GA BQ4TD
UT WOS:000595251900134
OA gold
DA 2022-02-03
ER

PT J
AU Percoco, G
AF Percoco, Gianluca
TI Digital close range photogrammetry for 3D body scanning for custom-made
   garments
SO PHOTOGRAMMETRIC RECORD
LA English
DT Article
DE 3D body scanning; custom-made clothing; digital close range
   photogrammetry; textile industry
ID SURFACE MEASUREMENT; PATTERN GENERATION; SYSTEM
AB Among several biometric applications, one of those currently attracting great interest is the possibility of carrying out 3D digitisation of human individuals to analyse their physical characteristics. These characteristics can be used for several purposes, such as security, medicine and tailoring for custom-made clothing. In recent years, although the development of online 3D scanning systems has been accelerating fast, little work has been devoted to offline systems, which would be particularly suitable for the textile and clothing industries. In the present research the author presents a specially designed low-cost offline 3D body digitiser, based on digital close range photogrammetry. A specially designed photogrammetric 3D scanner of the human body is presented, featuring automatic image processing procedures. The scanning system consists of eight cameras with a resolution of 5 megapixels, equipped with 16 mm wide-angle lenses; there are four white-light illuminators, of 100 W each. Tests on a tailor's dummy and on whole human bodies are reported, demonstrating the usefulness of the technique for textile applications. The digitisations performed on human bodies generally yield worse results than the corresponding ones on the dummy, and full body digitisations are worse than corset digitisations owing to the lower point density and to target distortion. Nevertheless, the results are satisfactory for tailoring applications that do not require high accuracies.
C1 Politecn Bari, Dipartimento Ingn Meccan & Gest, I-70126 Bari, Italy.
C3 Politecnico di Bari
RP Percoco, G (corresponding author), Politecn Bari, Dipartimento Ingn Meccan & Gest, Viale Japigia 182, I-70126 Bari, Italy.
EM g.percoco@poliba.it
OI Percoco, Gianluca/0000-0003-3298-5839; Percoco,
   Gianluca/0000-0002-4438-0450
FU Apulia Regional Government [PE 014]; Easy-d-Rom srl; Giovanna Sbiroli
   srl
FX The present work has been funded by the Apulia Regional Government,
   Research Project PE 014, 2006: "Development of a Prototype of an
   Anthropometric Chamber for 3D Scanning of the Human Body using Digital
   Close Range Photogrammetry for Custom Made Garments". The author wishes
   to thank Professor Luigi Galantucci, Dr Antonio Chiarella and Dr Michael
   De Robertis, from Politecnico di Bari, for their helpful suggestions and
   work in setting up the system, as well as Easy-d-Rom srl and Giovanna
   Sbiroli srl for supporting this research.
CR Ang KS, 2010, PHOTOGRAMM REC, V25, P105, DOI 10.1111/j.1477-9730.2010.00581.x
   Au CK, 1999, COMPUT AIDED DESIGN, V31, P751, DOI 10.1016/S0010-4485(99)00068-8
   Bottino A, 2001, COMPUT VIS IMAGE UND, V83, P79, DOI 10.1006/cviu.2001.0918
   Chandler JH, 2005, PHOTOGRAMM REC, V20, P12, DOI 10.1111/j.1477-9730.2005.00302.x
   Chittaro L., 2003, P 8 INT C 3D WEB TEC, P73
   Cho CS, 2010, COMPUT IND, V61, P550, DOI 10.1016/j.compind.2010.03.005
   Chong AK, 2009, PHOTOGRAMM REC, V24, P264, DOI 10.1111/j.1477-9730.2009.00540.x
   Chong AK, 2009, PHOTOGRAMM REC, V24, P51, DOI 10.1111/j.1477-9730.2009.00517.x
   CORDIER F, 2001, 2D PHOTOS YOURSELF V
   D'Apuzzo N, 2002, ISPRS J PHOTOGRAMM, V56, P360, DOI 10.1016/S0924-2716(02)00069-2
   Daanen HAM, 1997, INTERNATIONAL CONFERENCE ON RECENT ADVANCES IN 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P262, DOI 10.1109/IM.1997.603874
   Deli R, 2010, J CRANIOFAC SURG, V21, P87, DOI 10.1097/SCS.0b013e3181c3ba74
   Fraser CS, 1997, ISPRS J PHOTOGRAMM, V52, P149, DOI 10.1016/S0924-2716(97)00005-1
   Kim, 2000, INT J CLOTH SCI TECH, V12, P240
   Kim J, 2008, J INTERACT MARK, V22, P45, DOI 10.1002/dir.20113
   Kim SM, 2003, COMPUT AIDED DESIGN, V35, P611, DOI 10.1016/S0010-4485(02)00081-7
   Kim S, 2007, INT J CLOTH SCI TECH, V19, P7, DOI 10.1108/09556220710717017
   Kovacs L, 2006, J PLAST RECONSTR AES, V59, P1193, DOI 10.1016/j.bjps.2005.10.025
   Larsen PK, 2008, J FORENSIC SCI, V53, P1393, DOI 10.1111/j.1556-4029.2008.00874.x
   LIC Z, 2008, MEASUREMENT, V41, P842
   Lu JM, 2008, EXPERT SYST APPL, V35, P407, DOI 10.1016/j.eswa.2007.07.008
   Lu JM, 2010, APPL ERGON, V41, P236, DOI 10.1016/j.apergo.2009.07.002
   Petrova A, 2008, CLOTH TEXT RES J, V26, P227, DOI 10.1177/0887302X07309479
   Pribanic T, 2010, IMAGE VISION COMPUT, V28, P1255, DOI 10.1016/j.imavis.2010.01.003
   Simmons K.P., 2001, BODY MEASUREMENT TEC
   Tasdemir S, 2008, COMPSYSTECH 08
   Yang RQ, 2008, OPT LASER ENG, V46, P373, DOI 10.1016/j.optlaseng.2007.12.008
   Yu WR, 2010, IMAGE VISION COMPUT, V28, P605, DOI 10.1016/j.imavis.2009.09.015
NR 28
TC 9
Z9 9
U1 2
U2 25
PU WILEY-BLACKWELL
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0031-868X
EI 1477-9730
J9 PHOTOGRAMM REC
JI Photogramm. Rec.
PD MAR
PY 2011
VL 26
IS 133
BP 73
EP 90
DI 10.1111/j.1477-9730.2010.00605.x
PG 18
WC Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science & Photographic Technology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Physical Geography; Geology; Remote Sensing; Imaging Science &
   Photographic Technology
GA 737CH
UT WOS:000288545600006
DA 2022-02-03
ER

PT J
AU Aptel, F
   Denis, P
AF Aptel, Florent
   Denis, Philippe
TI Optical Coherence Tomography Quantitative Analysis of Iris Volume
   Changes after Pharmacologic Mydriasis
SO OPHTHALMOLOGY
LA English
DT Article
ID ANGLE-CLOSURE GLAUCOMA; ANTERIOR SEGMENT MORPHOLOGY; ULTRASOUND
   BIOMICROSCOPY; LASER IRIDOTOMY; FELLOW EYES; POPULATION; PREVALENCE;
   ETIOLOGY
AB Purpose: To describe a method using anterior segment optical coherence tomography (AS OCT) for estimating iris volume. To quantify changes in iris volume induced by pharmacologic mydriasis in narrow-angle eyes predisposed to angle-closure compared with normal open-angle eyes.
   Design: Cross-sectional study.
   Participants: Thirty fellow eyes of 30 patients who had an episode of primary acute angle-closure and 30 eyes of 30 normal age-and gender-matched subjects with open angles. All fellow eyes had a patent laser peripheral iridotomy.
   Methods: Iris volume and all biometric measurements were performed before and after instillation of 1% tropicamide and at least 1 week later 10% phenylephrine. Iris volume was estimated using AS OCT radial sections of the iris and a customized image-processing software.
   Main Outcome Measures: Iris volume, pupil diameter, angle configuration including angle opening distance at 500 mu m (AOD 500) and trabecular-iris space at 500 mu m (TISA 500), AS OCT anterior chamber depth, and A-scan ultrasonography axial length before and after pupil dilation.
   Results: Iris volumes measured before dilation of the pupil were 44.94 +/- 2.1 mm(3) and 44.29 +/- 3.9 mm(3) in the fellow eyes and open-angle eyes, respectively, which was not significantly different (P>0.1). Thirty minutes after instillation of 1% tropicamide, mean iris volume increased significantly in the fellow eyes (from 44.94 +/- 2.1 mm(3) to 49.92 +/- 2.9 mm(3); P<0.01), whereas it decreased significantly in the open-angle eyes (from 44.29 +/- 3.9 mm(3) to 37.88 +/- 2.2 mm(3); P<0.01). Similar changes were observed after instillation of 10% phenylephrine. Based on multivariate analysis, significant predictors of increase in iris volume after pupil dilation were eyes predisposed to angle-closure compared with open-angle eyes (P = 0.008), larger pupil diameter (P = 0.02), and brown eyes (P = 0.01). Relative iris volume increase was correlated significantly with AOD 500 and TISA 500 relative decrease in the narrow-angle group (P<0.05).
   Conclusions: The iris volume may be estimated with AS OCT. The iris volume increases after pupil dilation in narrow-angle eyes predisposed to acute angle closure. In those patients, this biometric change is associated with a narrowing of the angle despite a patent laser peripheral iridotomy.
C1 [Aptel, Florent; Denis, Philippe] Univ Lyon 1, Edouard Herriot Hosp, Dept Ophthalmol, F-69437 Lyon 03, France.
C3 CHU Lyon; Universite Claude Bernard Lyon 1
RP Aptel, F (corresponding author), Univ Lyon 1, Edouard Herriot Hosp, Dept Ophthalmol, 5 Pl Arsonval, F-69437 Lyon 03, France.
EM aptel_florent@hotmail.com
RI Aptel, Florent/M-6936-2014
OI Aptel, Florent/0000-0003-1537-9992
CR Ang LPK, 2000, OPHTHALMOLOGY, V107, P2092, DOI 10.1016/S0161-6420(00)00360-2
   Bonomi L, 1998, OPHTHALMOLOGY, V105, P209, DOI 10.1016/S0161-6420(98)92665-3
   Bonomi L, 2000, OPHTHALMOLOGY, V107, P998, DOI 10.1016/S0161-6420(00)00022-1
   Dandona L, 2000, OPHTHALMOLOGY, V107, P1710, DOI 10.1016/S0161-6420(00)00274-8
   Friedman DS, 2008, SURV OPHTHALMOL, V53, P250, DOI 10.1016/j.survophthal.2007.10.012
   Friedman DS, 2006, OPHTHALMOLOGY, V113, P1087, DOI 10.1016/j.ophtha.2006.02.016
   Friedman DS, 2003, ARCH OPHTHALMOL-CHIC, V121, P633, DOI 10.1001/archopht.121.5.633
   Gazzard G, 2003, OPHTHALMOLOGY, V110, P630, DOI 10.1016/S0161-6420(02)01893-6
   HARRIS JW, 1998, HDB MATH COMPUTATION, P96
   Huang EC, 2004, ANN BIOMED ENG, V32, P1276, DOI 10.1114/B:ABME.0000039361.17029.da
   Lowe R F, 1962, Br J Ophthalmol, V46, P641, DOI 10.1136/bjo.46.11.641
   LOWE RF, 1966, BRIT J OPHTHALMOL, V50, P385, DOI 10.1136/bjo.50.7.385
   LOWE RF, 1970, BRIT J OPHTHALMOL, V54, P161, DOI 10.1136/bjo.54.3.161
   PAVLIN CJ, 1992, AM J OPHTHALMOL, V113, P381, DOI 10.1016/S0002-9394(14)76159-8
   PHILLIPS CI, 1972, BRIT J OPHTHALMOL, V56, P248, DOI 10.1136/bjo.56.3.248
   Quigley HA, 2003, J GLAUCOMA, V12, P167, DOI 10.1097/00061198-200304000-00013
   Quigley HA, 1996, BRIT J OPHTHALMOL, V80, P389, DOI 10.1136/bjo.80.5.389
   Quigley HA, 2009, J GLAUCOMA, V18, P173, DOI 10.1097/IJG.0b013e31818624ce
   RILEY KF, 2003, MATH METHODS PHYS EN, P195
   Sawada A, 1997, J GLAUCOMA, V6, P288
   See JLS, 2007, BRIT J OPHTHALMOL, V91, P1485, DOI 10.1136/bjo.2006.113654
   Sihota R, 2005, J GLAUCOMA, V14, P387, DOI 10.1097/01.ijg.0000176934.14229.32
   Silver DM, 2005, J GLAUCOMA, V14, P89
   Tarongoy P, 2009, SURV OPHTHALMOL, V54, P211, DOI 10.1016/j.survophthal.2008.12.002
   Woo EK, 1999, AM J OPHTHALMOL, V127, P43, DOI 10.1016/S0002-9394(98)00283-9
   Yao BQ, 2009, OPHTHALMOLOGY, V116, P444, DOI 10.1016/j.ophtha.2008.10.019
NR 26
TC 120
Z9 126
U1 0
U2 2
PU ELSEVIER SCIENCE INC
PI NEW YORK
PA STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA
SN 0161-6420
EI 1549-4713
J9 OPHTHALMOLOGY
JI Ophthalmology
PD JAN
PY 2010
VL 117
IS 1
BP c
EP 10
DI 10.1016/j.ophtha.2009.10.030
PG 8
WC Ophthalmology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Ophthalmology
GA 555QW
UT WOS:000274529700002
PM 19923002
DA 2022-02-03
ER

PT J
AU Guaman-Guevara, F
   Austin, H
   Hicks, N
   Streeter, R
   Austin, WEN
AF Guaman-Guevara, Fabricio
   Austin, Heather
   Hicks, Natalie
   Streeter, Richard
   Austin, William E. N.
TI Impacts of ocean acidification on intertidal benthic foraminiferal
   growth and calcification
SO PLOS ONE
LA English
DT Article
ID CO2; CARBON; CALCITE; SYSTEM; RATES
AB Foraminifera are expected to be particularly susceptible to future changes in ocean carbonate chemistry as a function of increased atmospheric CO2. Studies in an experimental recirculating seawater system were performed with a dominant benthic foraminiferal species collected from intertidal mudflats. We investigated the experimental impacts of ocean acidification on survival, growth/calcification, morphology and the biometric features of a calcareous species Elphidium williamsoni. Foraminifera were exposed for 6 weeks to four different pH treatments that replicated future scenarios of a high CO2 atmosphere resulting in lower seawater pH. Results revealed that declining seawater pH caused a decline in foraminiferal survival rate and growth/calcification (mainly through test weight reduction). Scanning electron microscopy image analysis of live specimens at the end of the experimental period show changes in foraminiferal morphology with clear signs of corrosion and cracking on the test surface, septal bridges, sutures and feeding structures of specimens exposed to the lowest pH conditions. These findings suggest that the morphological changes observed in shell feeding structures may serve to alter: (1) foraminiferal feeding efficiency and their long-term ecological competitiveness, (2) the energy transferred within the benthic food web with a subsequent shift in benthic community structures and (3) carbon cycling and total CaCO3 production, both highly significant processes in coastal waters. These experimental results open-up the possibility of modelling future impacts of ocean acidification on both calcification and dissolution in benthic foraminifera within mid-latitude intertidal environments, with potential implications for understanding the changing marine carbon cycle.
C1 [Guaman-Guevara, Fabricio; Austin, Heather; Streeter, Richard; Austin, William E. N.] Univ St Andrews, Sch Geog & Sustainable Dev, St Andrews, Fife, Scotland.
   [Hicks, Natalie] Univ Essex, Sch Biol Sci, Colchester, Essex, England.
   [Austin, William E. N.] SAMS, Oban, Argyll, Scotland.
C3 University of St Andrews; University of Essex
RP Austin, WEN (corresponding author), Univ St Andrews, Sch Geog & Sustainable Dev, St Andrews, Fife, Scotland.; Austin, WEN (corresponding author), SAMS, Oban, Argyll, Scotland.
EM wena@st-andrews.ac.uk
OI Streeter, Richard/0000-0003-2261-4540; Hicks,
   Natalie/0000-0003-0843-9731; GUAMAN, FABRICIO/0000-0002-1008-1452
FU Secretaria de Educacion Superior, Ciencia, Tecnologia e Innovacion de la
   Republica del Ecuador (SENESCYT)
FX This work is a contribution to FGG's PhD study programme at the
   University of St Andrews, UK, and is funded by Secretaria de Educacion
   Superior, Ciencia, Tecnologia e Innovacion de la Republica del Ecuador
   (SENESCYT). The funder had no role in study design, data collection, and
   analysis, decision to publish, or preparation of the manuscript.
CR ALEXANDER SP, 1984, J FORAMIN RES, V14, P159, DOI 10.2113/gsjfr.14.3.159
   Allison N, 2011, CHEM GEOL, V289, P171, DOI 10.1016/j.chemgeo.2011.08.001
   ALLISON N, CULTURE STUDIES, pCO32, DOI DOI 10.1666/13051
   Allison N, 2008, CHEM GEOL, V253, P83, DOI 10.1016/j.chemgeo.2008.04.010
   Andersson AJ, 2008, MAR ECOL PROG SER, V373, P265, DOI 10.3354/meps07639
   Austin HA, 2005, MAR MICROPALEONTOL, V57, P68, DOI 10.1016/j.marmicro.2005.07.002
   Beaufort L, 2011, NATURE, V476, P80, DOI 10.1038/nature10295
   Bentov S, 2006, GEOCHEM GEOPHY GEOSY, V7, DOI 10.1029/2005GC001015
   Bernhard JM, 2004, J FORAMIN RES, V34, P96, DOI 10.2113/0340096
   Blackford JC, 2010, J MARINE SYST, V81, P12, DOI 10.1016/j.jmarsys.2009.12.016
   BOUTROUMAZEILLE.V, SEDIMENT
   Briguglio A, 2014, PALEOBIOLOGY, V40, P494, DOI 10.1666/13051
   Briguglio A, 2011, TURK J EARTH SCI, V20, P683, DOI 10.3906/yer-0910-44
   Caldeira K, 2003, NATURE, V425, P365, DOI 10.1038/425365a
   Cornwall CE, 2016, ICES J MAR SCI, V73, P572, DOI 10.1093/icesjms/fsv118
   CORTES CL, 2017, NAT COMMUN, V8, DOI DOI 10.1007/S00227-012-1921-X
   Currie AR, 2017, FRONT MICROBIOL, V8, DOI 10.3389/fmicb.2017.01599
   DEVELOPMENT C, 2014, R A LANGUAGE AND ENV, DOI DOI 10.4319/LO.2011.56.4.1200
   Dias BB, 2010, J GEOL SOC LONDON, V167, P843, DOI 10.1144/0016-76492010-050
   Dickson AG, 2003, MAR CHEM, V80, P185, DOI 10.1016/S0304-4203(02)00133-0
   DICKSON AG, 2007, PICES SPECIAL PUBLIC, pCO2, DOI DOI 10.1144/0016-76492010-050
   Dissard D, 2009, GEOCHEM GEOPHY GEOSY, V10, DOI 10.1029/2009GC002417
   Doo SS, 2014, BIOL BULL-US, V226, P169, DOI 10.1086/BBLv226n3p169
   Duarte CM, 2008, ESTUAR COAST, V31, P233, DOI 10.1007/s12237-008-9038-7
   Dupont S, 2013, MAR BIOL, V160, P1835, DOI 10.1007/s00227-012-1921-x
   Edmunds PJ, 2016, BIOSCIENCE, V66, P350, DOI 10.1093/biosci/biw023
   Engel BE, 2015, J FORAMIN RES, V45, P190, DOI 10.2113/gsjfr.45.2.190
   Fabricius KE, 2011, NAT CLIM CHANGE, V1, P165, DOI 10.1038/NCLIMATE1122
   Fabry VJ, 2008, ICES J MAR SCI, V65, P414, DOI 10.1093/icesjms/fsn048
   Feely RA, 2004, SCIENCE, V305, P362, DOI 10.1126/science.1097329
   FOX J, 2011, COMPANION
   Fujita K, 2011, BIOGEOSCIENCES, V8, P2089, DOI 10.5194/bg-8-2089-2011
   Gaylord B, 2011, J EXP BIOL, V214, P2586, DOI 10.1242/jeb.055939
   GLAS MS, GLAS, pO2, DOI DOI 10.1371/JOURNAL.PONE.0050010
   GOODAY AJ, 1992, NATO ADV SCI I C-MAT, V360, P63
   Haynert K, 2014, BIOGEOSCIENCES, V11, P1581, DOI 10.5194/bg-11-1581-2014
   Haynert K, 2011, MAR ECOL PROG SER, V432, P53, DOI 10.3354/meps09138
   Heinz P, 2002, DEEP-SEA RES PT I, V49, P517, DOI 10.1016/S0967-0637(01)00070-X
   Henehan MJ, 2017, BIOGEOSCIENCES, V14, P3287, DOI 10.5194/bg-14-3287-2017
   Hikami M, 2011, GEOPHYS RES LETT, V38, DOI 10.1029/2011GL048501
   Hintz CJ, 2004, LIMNOL OCEANOGR-METH, V2, P160, DOI 10.4319/lom.2004.2.160
   Ingels J, 2018, J EXP MAR BIOL ECOL, V502, P211, DOI 10.1016/j.jembe.2017.07.012
   IPCC, 2018, IPCC SPECIAL REPORT
   Keul N, 2013, BIOGEOSCIENCES, V10, P6185, DOI 10.5194/bg-10-6185-2013
   Khanna N, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0083118
   Kroeker KJ, 2013, GLOBAL CHANGE BIOL, V19, P1884, DOI 10.1111/gcb.12179
   Kuroyanagi A, 2009, MAR MICROPALEONTOL, V73, P190, DOI 10.1016/j.marmicro.2009.09.003
   LABELLING E, MAR ECOL PROG SER, DOI DOI 10.1038/NATURE10295
   LABELLING E, MAR ECOL PROG SER, DOI DOI 10.1007/978-4-431-54388-6_6
   LABELLING E, 2012, MAR ECOL PROG SER, V7
   LABELLING E, 2010, MAR ECOL PROG SER, V274, P87
   LABELLING E, MAR ECOL PROG SER, DOI DOI 10.1016/J.MARMICRO.2014.09.003
   LABELLING E, MAR ECOL PROG SER, DOI DOI 10.1038/NCOMMS14144
   LABELLING E, MAR ECOL PROG SER, DOI DOI 10.1093/BIOSCI/BIW023
   LABELLING E, 2014, MAR ECOL PROG SER, DOI DOI 10.1016/J.MARMICRO.2008.09.004
   LABELLING E, 2009, MAR ECOL PROG SER, V52, P75
   LABELLING E, 2000, MAR ECOL PROG SER, V202, P289
   LABELLING E, MAR ECOL PROG SER, DOI DOI 10.1371/JOURNAL.PONE.0050010
   LABELLING E, MAR ECOL PROG SER, DOI DOI 10.5194/BG-11-1581-2014
   LABELLING E, MAR ECOL PROG SER
   Langer MR, 2008, J EUKARYOT MICROBIOL, V55, P163, DOI 10.1111/j.1550-7408.2008.00321.x
   Lecroq B, 2014, ENVIRON SCI ENG, P91, DOI 10.1007/978-4-431-54388-6_6
   Lemasson AJ, 2017, J EXP MAR BIOL ECOL, V492, P49, DOI 10.1016/j.jembe.2017.01.019
   LINKE P, 1993, MAR MICROPALEONTOL, V20, P215, DOI 10.1016/0377-8398(93)90034-U
   Lombard F, 2009, MAR MICROPALEONTOL, V70, P1, DOI 10.1016/j.marmicro.2008.09.004
   Maier C, 2012, P ROY SOC B-BIOL SCI, V279, P1716, DOI 10.1098/rspb.2011.1763
   Marshall BJ, 2013, PALEOCEANOGRAPHY, V28, P363, DOI 10.1002/palo.20034
   McIntyre-Wressnig A, 2013, MAR ECOL PROG SER, V472, P45, DOI 10.3354/meps09918
   MEWES A, MICROPALEONTOLOGY, DOI DOI 10.1016/J.CHEMGEO.2008.04.010
   MITHTHAPALA S, 2013, TIDAL FLATS COASTAL, DOI DOI 10.1016/J.JEMBE.2017.07.012
   MOODLEY L, PM J
   MURRAY JW, 2006, PRESS, DOI DOI 10.1111/J.1550-7408.2008.00321.X
   MUSPACING D, 2016, J FORAMIN RES, V46, P61
   Orr JC, 2005, NATURE, V437, P681, DOI 10.1038/nature04095
   Pettit LR, 2015, ECOL EVOL, V5, P1784, DOI 10.1002/ece3.1475
   Pierrot D, 2006, ORNL CDIAC 105A, DOI [10.3334/CDIAClag.0O2SYS XIS CDIAC105a, DOI 10.3354/MEPS09918]
   Prazeres M, 2015, P ROY SOC B-BIOL SCI, V282, DOI 10.1098/rspb.2014.2782
   RAVEN, 2005, R SOC, P1, DOI DOI 10.1098/RSPB.2011.1763
   Ries J, 2011, NAT CLIM CHANGE, V1, P294, DOI 10.1038/nclimate1204
   ROGGATZ C, 2016, OCEAN ACIDIFICATION, P3914, DOI DOI 10.1002/PALO.20034
   Sabine CL, 2004, SCIENCE, V305, P367, DOI 10.1126/science.1097403
   Saraswat R, 2015, ESTUAR COAST SHELF S, V153, P96, DOI 10.1016/j.ecss.2014.12.005
   Sinutok S, 2011, LIMNOL OCEANOGR, V56, P1200, DOI 10.4319/lo.2011.56.4.1200
   UTHICKE S, 2013, SCI REP-UK, V3, DOI DOI 10.1098/RSPB.2014.2782
   VENABLES WN, 2002, SPRINGER
   Vogel N, 2012, J EXP MAR BIOL ECOL, V424, P15, DOI 10.1016/j.jembe.2012.05.008
   Waldbusser GG, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0128376
   Widdicombe S, 2008, J EXP MAR BIOL ECOL, V366, P187, DOI 10.1016/j.jembe.2008.07.024
   Wyness AJ, 2019, SCI TOTAL ENVIRON, V661, P155, DOI 10.1016/j.scitotenv.2019.01.061
   Zeebe RE, 2008, SCIENCE, V321, P51, DOI 10.1126/science.1159124
NR 90
TC 3
Z9 3
U1 3
U2 17
PU PUBLIC LIBRARY SCIENCE
PI SAN FRANCISCO
PA 1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA
SN 1932-6203
J9 PLOS ONE
JI PLoS One
PD AUG 21
PY 2019
VL 14
IS 8
AR e0220046
DI 10.1371/journal.pone.0220046
PG 21
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA IW5QN
UT WOS:000485033100008
PM 31433797
OA gold, Green Published, Green Submitted, Green Accepted
DA 2022-02-03
ER

PT J
AU Ragazzi, A
   Broggio, M
   Soldi, R
   Moricca, S
   Dellavalle, I
   Sabatini, S
AF Ragazzi, A
   Broggio, M
   Soldi, R
   Moricca, S
   Dellavalle, I
   Sabatini, S
TI Biometry and pathogenicity of Fusarium oxysporum f sp vasinfectum on a
   Na2SO4 enriched medium
SO ZEITSCHRIFT FUR PFLANZENKRANKHEITEN UND PFLANZENSCHUTZ-JOURNAL OF PLANT
   DISEASES AND PROTECTION
LA English
DT Article
DE Fusarium oxysporum f sp vasinfectum; biometry; pathogenicity; saline
   medium
ID SALINITY; PHYTOPHTHORA; COTTON
AB To test the effect of Na2SO4 on the biometry of Fusarium oxysporum f. sp. vasinfectum (FOV), the Bie Angolan isolate was cultured in a range of media enriched with 4.87, 8.82, 13.95, 20.92 and 48.32 g/l, equivalent to EC = 6, 10, 15, 20 and 40 dS/m, as well as a non-saline medium (EC = 0.4 dS/m). Macroconidial area, perimeter, length, width and shape coefficient were determined with a Leica Quantimet 520 image analyser connected to a Nikon Fotolab (x 20). To rest the effect of Na2SO4 on FOV aggressiveness against cotton, the Bie isolate cultured at EC = 10 dS/m was inoculated on calli of cotton cultivar 'Acala SJC-1', using suspensions of 10(6) and 10(4)/ml propagules, The variables measured were: colony growth rate, presence of hyphal branching, hyphal length, number of necrotic cells, and time of onset of necrosis. All saline concentrations increased the biometric parameters; the concentration with the greatest effect was the second lowest: EC = 10 dS/m. Results were confirmed by processing with analysis of variance and Duncan's multiple range test. FOV aggressiveness against cotton was also enhanced by Na2SO4: the number of necrotic cells went from 16 and 17 (after inoculation with 10(6) and 10(4)/ml propagules, respectively, from the non-saline medium) to 72 and 70 cells with the suspensions from the medium enriched to EC = 10 dS/m (our of 100 cells).
C1 IST AGRON OLTREMARE,I-50100 FLORENCE,ITALY.
   CNR,CTR STUDIO PATOL SPECIE LEGNOSE MONTANE,I-50144 FLORENCE,ITALY.
C3 Consiglio Nazionale delle Ricerche (CNR)
RP Ragazzi, A (corresponding author), UNIV FLORENCE,IST PATOL & ZOOL FORESTALE & AGR,PIAZZALE CASCINE 28,I-50144 FLORENCE,ITALY.
OI moricca, salvatore/0000-0003-3097-559X
CR AJAYI F, 1990, AFR FMR, V4, P22
   ASSIGBETSE KB, 1994, PHYTOPATHOLOGY, V84, P622, DOI 10.1094/Phyto-84-622
   BLAKER NS, 1985, PHYTOPATHOLOGY, V75, P270, DOI 10.1094/Phyto-75-270
   BLAKER NS, 1986, PHYTOPATHOLOGY, V76, P970, DOI 10.1094/Phyto-76-970
   BOOTH C, 1964, FUSARIUM OXYSPORUM F, P28
   FRANCOIS LE, 1982, IRRIGATION SCI, V3, P149, DOI 10.1007/BF00446003
   Hillocks RJ, 1992, COTTON DIS
   LEWIS JA, 1977, PHYTOPATHOLOGY, V67, P925, DOI 10.1094/Phyto-67-925
   LOCASCIO B, 1993, SALINITA ACQUE IRRIG
   NACHMIAS A, 1993, PHYTOPARASITICA, V21, P245, DOI 10.1007/BF02980946
   PAPAVIZAS GC, 1968, PHYTOPATHOLOGY, V58, P414
   RAGAZZI A, 1992, Z PFLANZENK PFLANZEN, V99, P499
   RAGAZZI A, 1994, Z PFLANZENK PFLANZEN, V101, P263
   Ragazzi A., 1992, Phytopathologia Mediterranea, V31, P85
   RASMUSSEN SL, 1988, PHYTOPATHOLOGY, V78, P1495, DOI 10.1094/Phyto-78-1495
   SERAFIM FJD, 1968, REV APPL MYCOL, V47, P3400
   TRESNER HD, 1971, APPL MICROBIOL, V22, P210, DOI 10.1128/AEM.22.2.210-213.1971
   ZAKARIA MA, 1980, PHYTOPATHOLOGY, V70, P495, DOI 10.1094/Phyto-70-495
NR 18
TC 0
Z9 0
U1 0
U2 0
PU EUGEN ULMER GMBH CO
PI STUTTGART 70
PA POSTFACH 700561 WOLLGRASWEG 41, W-7000 STUTTGART 70, GERMANY
SN 0340-8159
J9 Z PFLANZENK PFLANZEN
JI Z. Pflanzenk. Pflanzens.-J. Plant Dis. Prot.
PD JUN
PY 1996
VL 103
IS 3
BP 272
EP 278
PG 7
WC Plant Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Plant Sciences
GA UY083
UT WOS:A1996UY08300007
DA 2022-02-03
ER

PT J
AU Raju, K
   Chinnadurai, M
AF Raju, K.
   Chinnadurai, M.
TI An Identity-Based Secure and Optimal Authentication Scheme for the Cloud
   Computing Environment
SO CMC-COMPUTERS MATERIALS & CONTINUA
LA English
DT Article
DE Data security; authentication; identity-based authentication; optimal
   key generation; biometric
ID ACCESS-CONTROL; PRIVACY
AB Security is a critical issue in cloud computing (CC) because attackers can fabricate data by creating, copying, or deleting data with no user authorization. Most of the existing techniques make use of password-based authentication for encrypting data. Password-based schemes suffer from several issues and can be easily compromised. This paper presents a new concept of hybrid metaheuristic optimization as an identity-based secure and optimal authentication (HMO-ISOA) scheme for CC environments. The HMOISOA technique makes use of iris and fingerprint biometrics. Initially, the HMO-ISOA technique involves a directional local ternary quantized extrema pattern-based feature extraction process to extract features from the iris and fingerprint. Next, the features are fed into the hybrid social spider using the dragon fly algorithm to determine the optimal solution. This optimal solution acts as a key for an advanced encryption standard to encrypt and decrypt the data. A central benefit of determining the optimal value in this way is that the intruder cannot determine this value. The attacker also cannot work out which specific part of the fingerprint and iris feature values are acted upon as a key for the AES technique. Finally, the encrypted data can be saved in the cloud using a cloud simulator. Experimental analysis was performed on five fingerprint and iris images for a man-in-the-middle attack. The simulation outcome validated that the presented HMO-ISOA model achieved better results compared with other existing methods.
C1 [Raju, K.; Chinnadurai, M.] EGS Pillay Engn Coll, Dept Informat Technol, Nagapattinam, Tamil Nadu, India.
RP Raju, K (corresponding author), EGS Pillay Engn Coll, Dept Informat Technol, Nagapattinam, Tamil Nadu, India.
EM profgkr@gmail.com
CR Ambika, 2019, INT J COMPUTERS APPL, P1
   Balakrishnan S., 2018, PAK J BIOTECHNOL, V15, P293
   Deep G, 2016, ENG SCI TECHNOL, V19, P1895, DOI 10.1016/j.jestch.2016.05.006
   Dey S, 2016, J CLOUD COMPUT-ADV S, V5, DOI 10.1186/s13677-016-0068-6
   Guo C, 2018, FUTURE GENER COMP SY, V84, P190, DOI 10.1016/j.future.2017.07.038
   Irshad A, 2020, IEEE T IND APPL, V56, P4425, DOI 10.1109/TIA.2020.2966160
   Jain S., 2020, IEEE T ANTENN PROPAG, P1, DOI DOI 10.1109/TAP.2019.2938798
   Janet J., 2016, P 8 INT C GAM VIRT W, P1, DOI [10.1109/ICICES.2016.7518901, DOI 10.1109/ICICES.2016.7518901]
   Kumar V, 2020, VEH COMMUN, V22, DOI 10.1016/j.vehcom.2019.100213
   Li X, 2013, J NETW COMPUT APPL, V36, P1365, DOI 10.1016/j.jnca.2013.02.034
   Liu H, 2015, IEEE T PARALL DISTR, V26, P241, DOI 10.1109/TPDS.2014.2308218
   Liu JK, 2016, IEEE T COMPUT, V65, P1992, DOI 10.1109/TC.2015.2462840
   Liu ZS, 2015, FUTURE GENER COMP SY, V52, P61, DOI 10.1016/j.future.2014.12.001
   Luque-Chang A, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/6843923
   Mirjalili S, 2016, NEURAL COMPUT APPL, V27, P1053, DOI 10.1007/s00521-015-1920-1
   Rawal B.S., 2018, WIRELESS PERS COMMUN, P1
   Selvarani P, 2019, CLUSTER COMPUT, V22, pS4007, DOI 10.1007/s10586-018-2609-x
   Sharma Geeta, 2016, PEER PEER NETW APPL, P1
   Tameem E, 2014, ARAB J SCI ENG, V39, P7877, DOI 10.1007/s13369-014-1388-9
   Wang FF, 2020, WIREL COMMUN MOB COM, V2020, DOI 10.1155/2020/3805058
   Xiong LS, 2021, IEEE J EM SEL TOP P, V9, P4500, DOI 10.1109/JESTPE.2020.3032063
   Xiong LZ, 2015, CONCURR COMP-PRACT E, V27, P4573, DOI 10.1002/cpe.3423
   Yuen TH, 2015, IEEE T COMPUT, V64, P2595, DOI 10.1109/TC.2014.2366741
   Zhou J, 2015, IEEE T PARALL DISTR, V26, P1693, DOI 10.1109/TPDS.2014.2314119
   Zhu B, 2014, CRYPTOGR COMMUN, V6, P313, DOI 10.1007/s12095-014-0102-9
NR 25
TC 0
Z9 0
U1 2
U2 3
PU TECH SCIENCE PRESS
PI HENDERSON
PA 871 CORONADO CENTER DR, SUTE 200, HENDERSON, NV 89052 USA
SN 1546-2218
EI 1546-2226
J9 CMC-COMPUT MATER CON
JI CMC-Comput. Mat. Contin.
PY 2021
VL 69
IS 1
BP 1057
EP 1072
DI 10.32604/cmc.2021.016068
PG 16
WC Computer Science, Information Systems; Materials Science,
   Multidisciplinary
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Materials Science
GA SO5YN
UT WOS:000659047600012
OA gold
DA 2022-02-03
ER

PT J
AU Oliveira, DAB
   Pereira, LGR
   Bresolin, T
   Ferreira, REP
   Dorea, JRR
AF Oliveira, Dario Augusto Borges
   Pereira, Luiz Gustavo Ribeiro
   Bresolin, Tiago
   Ferreira, Rafael Ehrich Pontes
   Dorea, Joao Ricardo Reboucas
TI A review of deep learning algorithms for computer vision systems in
   livestock
SO LIVESTOCK SCIENCE
LA English
DT Review
DE Artificial intelligence; Cattle; Machine learning; Precision farming;
   Swine
ID OBJECT DETECTION; MACHINE VISION; CATTLE; IMAGES; CLASSIFICATION;
   RECOGNITION; NETWORK; DETECT
AB In livestock operations, systematically monitoring animal body weight, biometric body measurements, animal behavior, feed bunk, and other difficult-to-measure phenotypes is manually unfeasible due to labor, costs, and animal stress. Applications of computer vision are growing in importance in livestock systems due to their ability to generate real-time, non-invasive, and accurate animal-level information. However, the development of a computer vision system requires sophisticated statistical and computational approaches for efficient data management and appropriate data mining, as it involves massive datasets. This article aims to provide an overview of how deep learning has been implemented in computer vision systems used in livestock, and how such implementation can be an effective tool to predict animal phenotypes and to accelerate the development of predictive modeling for precise management decisions. First, we reviewed the most recent milestones achieved with computer vision systems and the respective deep learning algorithms implemented in Animal Science studies. Then, we reviewed the published research studies in Animal Science which used deep learning algorithms as the primary analytical strategy for image classification, object detection, object segmentation, and feature extraction. The great number of reviewed articles published in the last few years demonstrates the high interest and rapid development of deep learning algorithms in computer vision systems across livestock species. Deep learning algorithms for computer vision systems, such as Mask R-CNN, Faster R-CNN, YOLO (v3 and v4), DeepLab v3, U-Net and others have been used in Animal Science research studies. Additionally, network architectures such as ResNet, Inception, Xception, and VGG16 have been implemented in several studies across livestock species. The great performance of these deep learning algorithms suggests an improved predictive ability in livestock applications and a faster inference. However, only a few articles fully described the deep learning algorithms and their implementation. Thus, information regarding hyperparameter tuning, pre-trained weights, deep learning backbone, and hierarchical data structure were missing. We summarized peer-reviewed articles by computer vision tasks (image classification, object detection, and object segmentation), deep learning algorithms, animal species, and phenotypes including animal identification and behavior, feed intake, animal body weight, and many others. Understanding the principles of computer vision and the algorithms used for each application is crucial to develop efficient systems in livestock operations. Such development will potentially have a major impact on the livestock industry by predicting real-time and accurate phenotypes, which could be used in the future to improve farm management decisions, breeding programs through high-throughput phenotyping, and optimized data-driven interventions.
C1 [Oliveira, Dario Augusto Borges; Pereira, Luiz Gustavo Ribeiro; Bresolin, Tiago; Ferreira, Rafael Ehrich Pontes; Dorea, Joao Ricardo Reboucas] Dept Anim & Dairy Sci, 1675 Observ Dr,266 Anim Sci Bldg, Madison, WI 53706 USA.
   [Pereira, Luiz Gustavo Ribeiro] Embrapa Dairy Cattle, Av Eugenio Nascimento 610, BR-36038330 Aeroporto Juiz De Fora, Brazil.
C3 Empresa Brasileira de Pesquisa Agropecuaria (EMBRAPA)
RP Dorea, JRR (corresponding author), Dept Anim & Dairy Sci, 1675 Observ Dr,266 Anim Sci Bldg, Madison, WI 53706 USA.
EM joao.dorea@wisc.edu
CR Alameer A., 2020, AUTOMATED RECOGNITIO, DOI [10.25405/data.ncl.13042619.v1, DOI 10.25405/DATA.NCL.13042619.V1]
   Andrew W., 2020, OPENCOWS2020, DOI [10.5523/ bris.10m32xl88x2b61zlkkgz3fml17, DOI 10.5523/BRIS.10M32XL88X2B61ZLKKGZ3FML17]
   Andrew W, 2019, IEEE INT C INT ROBOT, P237, DOI 10.1109/IROS40897.2019.8968555
   Andrew W, 2017, IEEE INT CONF COMP V, P2850, DOI 10.1109/ICCVW.2017.336
   Andrew W, 2016, IEEE IMAGE PROC, P484, DOI 10.1109/ICIP.2016.7532404
   Barbedo JGA, 2017, VET PARASITOL, V235, P106, DOI 10.1016/j.vetpar.2017.01.020
   Atkinson GA, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-74511-0
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Benitez Pereira L. S., 2020, Data set of labeled scenes in a barn in front of automatic milking system, DOI 10.5281/zenodo.3981400
   Bezen R, 2020, COMPUT ELECTRON AGR, V172, DOI 10.1016/j.compag.2020.105345
   Oliveira DAB, 2020, I S BIOMED IMAGING, P1798, DOI 10.1109/ISBI45749.2020.9098676
   Bresolin T, 2020, FRONT GENET, V11, DOI 10.3389/fgene.2020.00923
   Brunger J, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20133710
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cang Y, 2019, IEEE ACCESS, V7, P164867, DOI 10.1109/ACCESS.2019.2953099
   Cernek P, 2020, J DAIRY SCI, V103, P9110, DOI 10.3168/jds.2019-17478
   Chen C, 2020, COMPUT ELECTRON AGR, V176, DOI 10.1016/j.compag.2020.105642
   Chen C, 2020, COMPUT ELECTRON AGR, V175, DOI 10.1016/j.compag.2020.105580
   Chen C, 2020, BIOSYST ENG, V196, P1, DOI 10.1016/j.biosystemseng.2020.05.010
   Chen C, 2020, COMPUT ELECTRON AGR, V169, DOI 10.1016/j.compag.2019.105166
   Chen L. C., 2014, 14127062 ARXIV
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chollet F., P IEEE C COMP VIS PA, P1251
   Cowton J, 2019, IEEE ACCESS, V7, P108049, DOI 10.1109/ACCESS.2019.2933060
   de Freitas DS, 2019, REV BRAS COMPUT APL, V11, P133, DOI 10.5335/rbca.v11i3.9210
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fang C, 2020, BIOSYST ENG, V190, P176, DOI 10.1016/j.biosystemseng.2019.12.002
   Fellbaum C., 1998, WORDNET ELECT LEXICA
   Fernandes AFA, 2020, J ANIM SCI, V98, DOI 10.1093/jas/skaa250
   Fernandes AFA, 2020, FRONT VET SCI, V7, DOI 10.3389/fvets.2020.551269
   Gao J., 2021, ARXIV210501938
   Geffen O, 2020, ANIMAL, V14, P2628, DOI 10.1017/S1751731120001676
   Geng L, 2019, IEEE ACCESS, V7, P92378, DOI 10.1109/ACCESS.2019.2925508
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guzhva O, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00107
   Hansen ME, 2018, COMPUT IND, V98, P145, DOI 10.1016/j.compind.2018.02.016
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   hole A., 2021, HOLSTEIN CATTLE RECO, DOI [10.34894/O1ZBSA, DOI 10.34894/O1ZBSA]
   Howard A. G., 2017, ARXIV170404861CSCV
   Huang XP, 2019, ANIMALS-BASEL, V9, DOI 10.3390/ani9070470
   Jiang B, 2019, COMPUT ELECTRON AGR, V166, DOI 10.1016/j.compag.2019.104982
   Jiang M, 2020, COMPUT ELECTRON AGR, V177, DOI 10.1016/j.compag.2020.105706
   Kang X, 2020, J DAIRY SCI, V103, P10628, DOI 10.3168/jds.2020-18288
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A., 2012, INT C NEUR INF PROC, V25, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
   Kumar S, 2018, MEASUREMENT, V116, P1, DOI 10.1016/j.measurement.2017.10.064
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee S, 2019, IEEE ACCESS, V7, P173796, DOI 10.1109/ACCESS.2019.2955761
   Li K, 2020, ISPRS J PHOTOGRAMM, V159, P296, DOI 10.1016/j.isprsjprs.2019.11.023
   Li XY, 2019, COMPUT ELECTRON AGR, V164, DOI 10.1016/j.compag.2019.104885
   Lin GS, 2016, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2016.348
   Lin M, 2013, ARXIV PREPRINT ARXIV
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XB, 2016, IEEE IMAGE PROC, P1289, DOI 10.1109/ICIP.2016.7532566
   Liu ZW, 2015, IEEE I CONF COMP VIS, P1377, DOI 10.1109/ICCV.2015.162
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luc Pauline, 2016, ARXIV161108408, DOI DOI 1398049/DOCUMENT
   Makhzani Alireza, 2015, ARXIV151105644
   Marsot M, 2020, COMPUT ELECTRON AGR, V173, DOI 10.1016/j.compag.2020.105386
   McKenna S, 2020, IEEE T AUTOM SCI ENG, V17, P1005, DOI 10.1109/TASE.2019.2960106
   Moher D, 2009, J CLIN EPIDEMIOL, V62, P1006, DOI 10.1016/j.jclinepi.2009.06.005
   Nasirahmadi A, 2017, LIVEST SCI, V202, P25, DOI 10.1016/j.livsci.2017.05.014
   Neethirajan Suresh, 2017, Sensing and Bio-Sensing Research, V12, P15, DOI 10.1016/j.sbsr.2016.11.004
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Nye J, 2020, FRONT GENET, V11, DOI 10.3389/fgene.2020.00513
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Pham H, 2018, PR MACH LEARN RES, V80
   Psota ET, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19040852
   Qiao YL, 2019, IFAC PAPERSONLINE, V52, P318, DOI 10.1016/j.ifacol.2019.12.558
   Qiao YL, 2019, COMPUT ELECTRON AGR, V165, DOI 10.1016/j.compag.2019.104958
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28
   Riekert M, 2020, COMPUT ELECTRON AGR, V174, DOI 10.1016/j.compag.2020.105391
   Rodenburg J, 2017, J DAIRY SCI, V100, P7729, DOI 10.3168/jds.2016-11715
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rutten CJ, 2013, J DAIRY SCI, V96, P1928, DOI 10.3168/jds.2012-6107
   Schwing A. G., 2015, ARXIV PREPRINT ARXIV
   Seo J, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10082878
   Sermanet P., 2014, OVERFEAT INTEGRATED
   Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/CVPR.2015.7298594
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Szegedy C, 2017, THIRTY-FIRST AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tian MX, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.05.049
   Tsai YC, 2020, BIOSYST ENG, V199, P97, DOI 10.1016/j.biosystemseng.2020.03.013
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Vougioukas K, 2020, INT J COMPUT VISION, V128, P1398, DOI 10.1007/s11263-019-01251-8
   Wang JZ, 2018, LECT NOTES COMPUT SC, V10996, P620, DOI 10.1007/978-3-319-97909-0_66
   Wu DH, 2020, BIOSYST ENG, V192, P72, DOI 10.1016/j.biosystemseng.2020.01.012
   Wurtz K, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0226669
   Yang A, 2018, BIOSYST ENG, V175, P133, DOI 10.1016/j.biosystemseng.2018.09.011
   Ye CW, 2020, POULTRY SCI, V99, P637, DOI 10.3382/ps/pez564
   Zhang YQ, 2019, COMPUT ELECTRON AGR, V163, DOI 10.1016/j.compag.2019.104884
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao KaiXuan, 2015, Transactions of the Chinese Society of Agricultural Engineering, V31, P181
   Zheng C, 2018, COMPUT ELECTRON AGR, V147, P51, DOI 10.1016/j.compag.2018.01.023
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhuang XL, 2019, BIOSYST ENG, V179, P106, DOI 10.1016/j.biosystemseng.2019.01.003
NR 111
TC 0
Z9 0
U1 23
U2 23
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1871-1413
EI 1878-0490
J9 LIVEST SCI
JI Livest. Sci.
PD NOV
PY 2021
VL 253
AR 104700
DI 10.1016/j.livsci.2021.104700
PG 15
WC Agriculture, Dairy & Animal Science
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Agriculture
GA WK8GD
UT WOS:000709958800019
DA 2022-02-03
ER

PT C
AU Smith, KZ
   Gadde, A
   Kadiyala, A
   Dawson, JM
AF Smith, Kyle Z.
   Gadde, Akshitha
   Kadiyala, Anand
   Dawson, Jeremy M.
BE Adibi, A
   Lin, SY
   Scherer, A
TI Fabrication of Two-Dimensional Visible Wavelength Nanoscale Plasmonic
   Structures Using Hydrogen Silsesquioxane Based Resist
SO PHOTONIC AND PHONONIC PROPERTIES OF ENGINEERED NANOSTRUCTURES VI
SE Proceedings of SPIE
LA English
DT Proceedings Paper
CT Conference on Photonic and Phononic Properties of Engineered
   Nanostructures VI
CY FEB 15-18, 2016
CL San Francisco, CA
DE plasmonics; hydrogen silsesquioxane; biosensors; nanostructures; e-beam
   lithography; FDTD; two-dimensional plasmonics; visible wavelength
   plasmonics
ID METAL-ENHANCED FLUORESCENCE; OPTICAL BIOSENSORS; RECENT PROGRESS;
   RESONANCE
AB In recent years, the global market for biosensors has continued to increase in combination with their expanding use in areas such as biodefense/detection, home diagnostics, biometric identification, etc. A constant necessity for inexpensive, portable bio-sensing methods, while still remaining simple to understand and operate, is the motivation behind novel concepts and designs. Labeled visible spectrum bio-sensing systems provide instant feedback that is both simple and easy to work with, but are limited by the light intensity thresholds required by the imaging systems. In comparison, label-free bio-sensing systems and other detection modalities like electrochemical, frequency resonance, thermal change, etc., can require additional technical processing steps to convey the final result, increasing the system's complexity and possibly the time required for analysis. Further decrease in the detection limit can be achieved through the addition of plasmonic structures into labeled bio-sensing systems. Nano-structures that operate in the visible spectrum have feature sizes typically in the order of the operating wavelength, calling for high aspect ratio nanoscale fabrication capabilities. In order to achieve these dimensions, electron beam lithography (EBL) is used due to its accurate feature production. Hydrogen silsesquioxane (HSQ) based electron beam resist is chosen for one of its benefits, which is after exposure to oxygen plasma, the patterned resist cures into silicon dioxide (SiO2). These cured features in conjunction with nanoscale gold particles help in producing a high electric field through dipole generation. In this work, a detailed process flow of the fabrication of square lattice of plasmonic structures comprising of gold coated silicon dioxide pillars designed to operate at 560 nm wavelength and produce an intensity increase of roughly 100 percent will be presented.
C1 [Smith, Kyle Z.; Gadde, Akshitha; Kadiyala, Anand; Dawson, Jeremy M.] West Virginia Univ, Lane Dept Comp Sci & Elect Engn, Morgantown, WV 26506 USA.
C3 West Virginia University
RP Smith, KZ (corresponding author), West Virginia Univ, Lane Dept Comp Sci & Elect Engn, Morgantown, WV 26506 USA.
RI Kadiyala, Anand/S-1998-2017
OI Kadiyala, Anand/0000-0002-6649-4192
CR Anker JN, 2008, NAT MATER, V7, P442, DOI 10.1038/nmat2162
   Aslan K, 2005, CURR OPIN BIOTECH, V16, P55, DOI 10.1016/j.copbio.2005.01.001
   Banica F.G., 2012, CHEM SENSORS BIOSENS
   Brouard D, 2011, ACS NANO, V5, P1888, DOI 10.1021/nn102776m
   Cooper MA, 2002, NAT REV DRUG DISCOV, V1, P515, DOI 10.1038/nrd838
   Cox WG, 2004, BIOTECHNIQUES, V36, P114, DOI 10.2144/04361RR02
   Doria G, 2012, SENSORS-BASEL, V12, P1657, DOI 10.3390/s120201657
   Fayyaz S., 2012, ENHANCING RAMAN FLUO
   Geddes C.D., 2010, METAL ENHANCED FLUOR
   Geddes CD, 2002, J FLUORESC, V12, P121, DOI 10.1023/A:1016875709579
   Gorton L, 2005, COMP ANAL C, V44, pXXI, DOI 10.1016/S0166-526X(05)44016-7
   Grande M, 2013, MICROELECTRON ENG, V111, P234, DOI 10.1016/j.mee.2013.03.172
   Guo LH, 2016, BIOSENS BIOELECTRON, V79, P266, DOI 10.1016/j.bios.2015.12.027
   Hoa XD, 2007, BIOSENS BIOELECTRON, V23, P151, DOI 10.1016/j.bios.2007.07.001
   Huang CJ, 2012, APPL PHYS LETT, V100, DOI 10.1063/1.4707382
   Huffman D. R ., 2004, ABSORPTION SCATTERIN
   Kim DK, 2007, ANAL CHEM, V79, P1855, DOI 10.1021/ac061909o
   Li YY, 2010, GOLD BULL, V43, P29, DOI 10.1007/BF03214964
   Luppa PB, 2001, CLIN CHIM ACTA, V314, P1, DOI 10.1016/S0009-8981(01)00629-5
   Marazuela MD, 2002, ANAL BIOANAL CHEM, V372, P664, DOI 10.1007/s00216-002-1235-9
   Martinsson E., 2014, NANOPLASMONIC SENSIN
   Moerner WE, 2007, P NATL ACAD SCI USA, V104, P12596, DOI 10.1073/pnas.0610081104
   Mohanty SP, 2006, IEEE POTENTIALS, V25, P35, DOI 10.1109/MP.2006.1649009
   Narayanaswamy R., 2004, OPTICAL SENSORS IND
   Ouyang H, 2006, APPL PHYS LETT, V88, DOI 10.1063/1.2196069
   Sapsford K, 2008, OPTICAL BIOSENSORS: TODAY AND TOMORROW, 2ND EDITION, P139, DOI 10.1016/B978-044453125-4.50005-X
   Sharma H, 2013, SENSOR ACTUAT B-CHEM, V183, P535, DOI 10.1016/j.snb.2013.03.137
   Stranik O, 2005, SENSOR ACTUAT B-CHEM, V107, P148, DOI 10.1016/j.snb.2004.08.032
   Urban G. A., MEAS SCI TECHNOL, V20
   Velasco-Garcia MN, 2009, SEMIN CELL DEV BIOL, V20, P27, DOI 10.1016/j.semcdb.2009.01.013
   Vo-Dinh T., 2015, BIOMEDICAL PHOTONICS
   Zeng SW, 2014, CHEM SOC REV, V43, P3426, DOI 10.1039/c3cs60479a
NR 32
TC 3
Z9 3
U1 0
U2 1
PU SPIE-INT SOC OPTICAL ENGINEERING
PI BELLINGHAM
PA 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
SN 0277-786X
BN 978-1-62841-991-7
J9 PROC SPIE
PY 2016
VL 9756
AR 975623
DI 10.1117/12.2213000
PG 11
WC Nanoscience & Nanotechnology; Optics; Physics, Applied
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Science & Technology - Other Topics; Optics; Physics
GA BF4TU
UT WOS:000381696400022
DA 2022-02-03
ER

PT J
AU You, SQ
   Liang, ZQ
   Yang, KY
   Zhang, Y
   Oatts, J
   Han, Y
   Wu, HJ
AF You, Shuqi
   Liang, Zhiqiao
   Yang, Kangyi
   Zhang, Yu
   Oatts, Julius
   Han, Ying
   Wu, Huijuan
TI Novel Discoveries of Anterior Segment Parameters in Fellow Eyes of Acute
   Primary Angle Closure and Chronic Primary Angle Closure Glaucoma
SO INVESTIGATIVE OPHTHALMOLOGY & VISUAL SCIENCE
LA English
DT Article
DE acute primary angle closure; chronic primary angle closure; glaucoma
   anterior segment; ultrasound biomicroscopy; fellow eyes
ID OPTICAL COHERENCE TOMOGRAPHY; ULTRASOUND BIOMICROSCOPY; GLOBAL
   PREVALENCE; SUBTYPES; IRIS; ASSOCIATION; PROJECTIONS; MECHANISMS
AB PURPOSE. To investigate the biometric differences of anterior segment parameters between fellow eyes of acute primary angle closure (F-APAC) and chronic primary angle closure glaucoma (F-CPACG) to get information about differences between APAC and CPAC.
   METHODS. Patients with F-APAC and F-CPACG without prior treatment were enrolled from glaucoma clinics. Parameters were measured on ultrasound biomicroscopy images, including pupil diameter, lens vault (LV), anterior chamber depth, anterior chamber width, iris area, iris thickness (IT 750 and 2000), angle-opening distance (AOD 500 and 750), trabecular-iris space area (TISA 500 and 750), trabecular iris angle (TIA 500 and 750), trabecular-ciliary angle, and ciliary process area. Multivariate logistic regression analysis was performed to determine the most important parameters associated with F-APAC compared with F-CPACG.
   RESULTS. Fifty-five patients with APAC and 55 patients with CPACG were examined. The anterior chamber depth, IT 750, AOD 750, trabecular iris angle 750, and trabecular-ciliary angle were smaller, and LV and ciliary process area were greater in F-APAC as compared with F-CPACG (P <= 0.01). Multivariate logistic regression showed that thinner IT 750, smaller AOD 750, and larger LV were significantly associated with F-APAC (P < 0.01). IT 750 (area under the curve, 0.703) performed relatively better than AOD 750 (area under the curve, 0.696) in distinguishing F-APAC from F-CPACG, with the best cutoff of 0.404 mm and 0.126 mm, respectively.
   CONCLUSIONS. Compared with F-CPACG, F-APAC had thinner peripheral iris, narrower anterior chamber angle, shallower anterior chamber depth, greater LV, larger and anteriorly positioned ciliary body. IT 750, AOD 750, and LV played important roles in distinguishing eyes predisposed to APAC or CPAC.
C1 [You, Shuqi; Liang, Zhiqiao; Yang, Kangyi; Zhang, Yu; Wu, Huijuan] Peking Univ, Dept Ophthalmol, Peoples Hosp, Coll Optometry, 11 Xizhimen South St, Beijing 100044, Peoples R China.
   [You, Shuqi; Liang, Zhiqiao; Yang, Kangyi; Zhang, Yu; Wu, Huijuan] Univ Hlth Sci Ctr, Beijing Key Lab Diag & Therapy Retinal & Choroid, Beijing, Peoples R China.
   [You, Shuqi] Fudan Univ, Dept Ophthalmol, Eye & ENT Hosp, Shanghai, Peoples R China.
   [Oatts, Julius; Han, Ying] Univ Calif San Francisco, Dept Ophthalmol, San Francisco, CA USA.
C3 Peking University; Fudan University; University of California System;
   University of California San Francisco
RP Wu, HJ (corresponding author), Peking Univ, Dept Ophthalmol, Peoples Hosp, Coll Optometry, 11 Xizhimen South St, Beijing 100044, Peoples R China.
EM huijuanwu@vip.sina.com
FU program of development and cultivation of medical innovative varieties
   and industrial support, Beijing Municipal Science and Technology
   Commission [Z191100007619045]; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China (NSFC) [61634006]
FX Supported by the program of development and cultiva-tion of medical
   innovative varieties and industrial support, Beijing Municipal Science
   and Technology Commission [Z191100007619045] ; and National Natural
   Science Foundation of China [61634006] .
CR Bourne RRA, 2017, LANCET GLOB HEALTH, V5, pE888, DOI 10.1016/S2214-109X(17)30293-0
   Chen HJ, 2015, J GLAUCOMA, V24, P233, DOI 10.1097/IJG.0000000000000086
   Dada T, 2007, J CATARACT REFR SURG, V33, P837, DOI 10.1016/j.jcrs.2007.01.021
   EDWARDS RS, 1982, BRIT J OPHTHALMOL, V66, P576, DOI 10.1136/bjo.66.9.576
   Foster PJ, 2002, BRIT J OPHTHALMOL, V86, P238, DOI 10.1136/bjo.86.2.238
   Guzman CP, 2013, INVEST OPHTH VIS SCI, V54, P5281, DOI 10.1167/iovs.13-12285
   He MG, 2019, LANCET, V393, P1609, DOI 10.1016/S0140-6736(18)32607-2
   Henzan IM, 2010, OPHTHALMOLOGY, V117, P1720, DOI 10.1016/j.ophtha.2010.01.045
   Jiang YZ, 2010, INVEST OPHTH VIS SCI, V51, P2035, DOI 10.1167/iovs.09-4145
   Ku JY, 2014, J GLAUCOMA, V23, P583, DOI 10.1097/IJG.0b013e318285fede
   Li MW, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0193006
   LOWE RF, 1966, AM J OPHTHALMOL, V61, P642, DOI 10.1016/0002-9394(66)91200-1
   Marchini G, 1998, OPHTHALMOLOGY, V105, P2091, DOI 10.1016/S0161-6420(98)91132-0
   Mochizuki H, 2011, J GLAUCOMA, V20, P315, DOI 10.1097/IJG.0b013e3181e3d2da
   Moghimi S, 2013, AM J OPHTHALMOL, V155, P664, DOI 10.1016/j.ajo.2012.10.014
   Narayanaswamy A, 2010, ARCH OPHTHALMOL-CHIC, V128, P1321, DOI 10.1001/archophthalmol.2010.231
   Ng WT, 2012, CLIN EXP OPHTHALMOL, V40, pE218, DOI 10.1111/j.1442-9071.2011.02604.x
   Nongpiur ME, 2011, OPHTHALMOLOGY, V118, P474, DOI 10.1016/j.ophtha.2010.07.025
   Nongpiur ME, 2010, OPHTHALMOLOGY, V117, P1967, DOI 10.1016/j.ophtha.2010.02.007
   PAVLIN CJ, 1992, AM J OPHTHALMOL, V113, P381, DOI 10.1016/S0002-9394(14)76159-8
   PAVLIN CJ, 1992, AM J OPHTHALMOL, V113, P390, DOI 10.1016/S0002-9394(14)76160-4
   PAVLIN CJ, 1991, OPHTHALMOLOGY, V98, P287
   Quigley HA, 2006, BRIT J OPHTHALMOL, V90, P262, DOI 10.1136/bjo.2005.081224
   Quigley HA, 2003, J GLAUCOMA, V12, P167, DOI 10.1097/00061198-200304000-00013
   Razeghinejad MR, 2018, SURV OPHTHALMOL, V63, P754, DOI 10.1016/j.survophthal.2018.05.001
   Shabana N, 2012, CLIN EXP OPHTHALMOL, V40, P792, DOI 10.1111/j.1442-9071.2012.02805.x
   Sihota R, 2000, CLIN EXP OPHTHALMOL, V28, P253, DOI 10.1046/j.1442-9071.2000.00324.x
   Sihota R, 2005, J GLAUCOMA, V14, P387, DOI 10.1097/01.ijg.0000176934.14229.32
   Sihota R, 2011, CURR OPIN OPHTHALMOL, V22, P87, DOI 10.1097/ICU.0b013e328343729f
   Sun XH, 2017, PROG RETIN EYE RES, V57, P26, DOI 10.1016/j.preteyeres.2016.12.003
   Tarongoy P, 2009, SURV OPHTHALMOL, V54, P211, DOI 10.1016/j.survophthal.2008.12.002
   Tham YC, 2014, OPHTHALMOLOGY, V121, P2081, DOI 10.1016/j.ophtha.2014.05.013
   Wang BS, 2011, BRIT J OPHTHALMOL, V95, P46, DOI 10.1136/bjo.2009.178129
   Wang Zhonghao, 2013, Eye Sci, V28, P1
NR 34
TC 0
Z9 0
U1 1
U2 1
PU ASSOC RESEARCH VISION OPHTHALMOLOGY INC
PI ROCKVILLE
PA 12300 TWINBROOK PARKWAY, ROCKVILLE, MD 20852-1606 USA
SN 0146-0404
EI 1552-5783
J9 INVEST OPHTH VIS SCI
JI Invest. Ophthalmol. Vis. Sci.
PD NOV
PY 2021
VL 62
IS 14
AR 6
DI 10.1167/iovs.62.14.6
PG 7
WC Ophthalmology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Ophthalmology
GA WX0CK
UT WOS:000718272700002
PM 34730791
OA gold, Green Published
DA 2022-02-03
ER

PT J
AU Sun, YK
   Du, GB
   Cao, Y
   Lin, QZ
   Zhong, LH
   Qiu, J
AF Sun, Yongke
   Du, Guanben
   Cao, Yong
   Lin, Qizhao
   Zhong, Lihui
   Qiu, Jian
TI Wood Product Tracking Using an Improved AKAZE Method in Wood
   Traceability System
SO IEEE ACCESS
LA English
DT Article
DE Wood identification; AKAZE; trust label; blockchain
ID LOG TRACEABILITY; IDENTIFICATION; CLASSIFICATION; FINGERPRINT
AB Tracking of the wood product is an important technology in the trade activity of rare plants. Normally, the factories use Quick Response (QR) and Radio-Frequency Identification (RFID) to identify the individual wood product, but these technologies are not safe enough because they can be easily falsified. It can be seen that traditional methods are hard to catch the detail of the slim wood texture from the wood product. In this study, a novel method is employed to resolve these problems using a biometric feature on the surface of the real wood product to distinguish the individual wood product. AKAZE is used to extract the key-point of wood texture. A sub-area detection technique along with a serialization method is then developed to improve the rate of identification. The sub-area detection technique deals with picking out a sub-region in which there are enough AKAZE points as small as possible. The serialization method is also utilized to reduce the redundant process of feature extraction. The experimental results demonstrate that the values of accuracy, recall, and F1 reach 0.98, 0.96, and 0.96, respectively. The match time that uses serialized function is reduced to 1/3 of which has no application in the original image. The validated results also reveal that our proposed methodology improves the robustness of the wood product identification, and it can be used in Wood Traceability System (WTS) with the blockchain to resolve the digital trust problem and the fast distinction issues of the real wood product.
C1 [Sun, Yongke; Du, Guanben] Southwest Forestry Univ, Yunnan Prov Key Lab Wood Adhes & Glued Prod, Kunming 650224, Yunnan, Peoples R China.
   [Cao, Yong; Lin, Qizhao] Southwest Forestry Univ, Coll Big Data & Intelligence Engn, Kunming 650224, Yunnan, Peoples R China.
   [Zhong, Lihui; Qiu, Jian] Southwest Forestry Univ, Coll Mat Sci & Engn, Kunming 650224, Yunnan, Peoples R China.
C3 Southwest Forestry University - China; Southwest Forestry University -
   China; Southwest Forestry University - China
RP Du, GB (corresponding author), Southwest Forestry Univ, Yunnan Prov Key Lab Wood Adhes & Glued Prod, Kunming 650224, Yunnan, Peoples R China.; Cao, Y (corresponding author), Southwest Forestry Univ, Coll Big Data & Intelligence Engn, Kunming 650224, Yunnan, Peoples R China.; Qiu, J (corresponding author), Southwest Forestry Univ, Coll Mat Sci & Engn, Kunming 650224, Yunnan, Peoples R China.
EM gongben9@hotmail.com; cn_caoyong@126.com; qiujian@swfu.edu.cn
OI Lin, Qizhao/0000-0003-1320-6430
FU Program for Leading Talents of Science and Technology in Yunnan Province
   [2017HA013]; National Natural Science Foundation of ChinaNational
   Natural Science Foundation of China (NSFC) [61462095]; Major Project of
   Science and Technology of Yunnan Province [202002AD080002, 2019ZE005];
   Joint Agricultural Project of Yunnan Provincial Department of Science
   and Technology [2018FG001-108]
FX This work was supported in part by the Program for Leading Talents of
   Science and Technology in Yunnan Province under Grant 2017HA013, in part
   by the National Natural Science Foundation of China under Grant
   61462095, in part by the Major Project of Science and Technology of
   Yunnan Province under Grant 202002AD080002 and Grant 2019ZE005, and in
   part by the Joint Agricultural Project of Yunnan Provincial Department
   of Science and Technology under Grant 2018FG001-108.
CR Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   Appelhanz S, 2016, J CLEAN PROD, V110, P132, DOI 10.1016/j.jclepro.2015.02.034
   Baas P, 2020, IAWA J, V41, P259
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Dormontt EE, 2015, BIOL CONSERV, V191, P790, DOI 10.1016/j.biocon.2015.06.038
   Figorilli S, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18093133
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Geng C, 2009, 2009 16TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOLS 1-6, P3313, DOI 10.1109/ICIP.2009.5413956
   Godbout J, 2018, FOREST CHRON, V94, P75, DOI 10.5558/tfc2018-010
   He T, 2019, HOLZFORSCHUNG, V73, P277, DOI 10.1515/hf-2018-0076
   He W, 2009, IEEE I CONF COMP VIS, P1586, DOI 10.1109/ICCV.2009.5459360
   Hwang SW, 2018, J WOOD SCI, V64, P69, DOI 10.1007/s10086-017-1680-x
   Jo M, 2020, IEEE NETWORK, V34, P76, DOI 10.1109/MNET.2020.9199796
   Kannangara S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-61415-2
   Kaur A., 2020, CRYPTOCURRENCIES BLO, P25, DOI 10.1002/9781119621201.ch2
   Kharbach M, 2018, FOOD CHEM, V263, P8, DOI 10.1016/j.foodchem.2018.04.059
   Lever J, 2016, NAT METHODS, V13, P603, DOI 10.1038/nmeth.3945
   Lowe AJ, 2011, IAWA J, V32, P251, DOI 10.1163/22941932-90000055
   Ma CQ, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20040975
   Magdy S, 2020, IET IMAGE PROCESS, V14, P874, DOI 10.1049/iet-ipr.2019.0575
   Matuska S, 2014, AASRI PROC, V9, P25, DOI 10.1016/j.aasri.2014.09.006
   Meixi C., 2013, P 3 INT C MULT TECHN P 3 INT C MULT TECHN, P1250
   Nabiyev VV, 2017, FORENSIC SCI INT, V278, P280, DOI 10.1016/j.forsciint.2017.07.014
   Oyallon E, 2015, IMAGE PROCESS ON LIN, V5, P176, DOI 10.5201/ipol.2015.69
   Pahlberg T, 2015, COMPUT ELECTRON AGR, V111, P164, DOI 10.1016/j.compag.2014.12.014
   Powers D. M. W., 2011, J MACH LEARN TECHNOL, V2, P37
   Prashar D, 2020, SUSTAINABILITY-BASEL, V12, DOI 10.3390/su12083497
   Puri A., 2019, 2019 INT C ISS CHALL, P1
   Rajagopal H, 2019, WOOD SCI TECHNOL, V53, P967, DOI 10.1007/s00226-019-01110-2
   Redei P. G, 2008, ENCY GENETICS GENOMI, P638
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Saleem, 2018, 2018 INT C COMP MATH, DOI [10.1109/ICOMET.2018.8346440., DOI 10.1109/ICOMET.2018.8346440, 10.1109/ICOMET.2018.8346440]
   Schraml R, 2015, COMPUT ELECTRON AGR, V119, P112, DOI 10.1016/j.compag.2015.10.003
   Schraml R, 2020, MATHEMATICS-BASEL, V8, DOI 10.3390/math8071071
   Schraml R, 2016, MACH VISION APPL, V27, P1289, DOI 10.1007/s00138-016-0814-2
   Sermanet P, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2809, DOI 10.1109/IJCNN.2011.6033589
   Sharma SK, 2020, J INDIAN SOC REMOTE, V48, P1389, DOI 10.1007/s12524-020-01163-y
   Sharma V, 2020, VIB SPECTROSC, V110, DOI 10.1016/j.vibspec.2020.103097
   Simonovic J, 2011, CELLULOSE, V18, P1433, DOI 10.1007/s10570-011-9584-1
   Tnah LH, 2012, WOOD SCI TECHNOL, V46, P813, DOI 10.1007/s00226-011-0447-6
   Trundle E, 2012, PRAIRIE SCHOONER, V86, P85
   Weik H. M, 2000, COMPUTER SCI COMMUNI, P706
   Yang D, 2021, J NETW COMPUT APPL, V173, DOI 10.1016/j.jnca.2020.102817
   Zhang HJ, 2011, 2011 INTERNATIONAL CONFERENCE ON ELECTRONICS, COMMUNICATIONS AND CONTROL (ICECC), P1460, DOI 10.1109/ICECC.2011.6066546
   Zhang L, 2013, PROC CVPR IEEE, P1586, DOI 10.1109/CVPR.2013.208
   Zhang L, 2015, SENSORS-BASEL, V15, P19937, DOI 10.3390/s150819937
   Zhang SC, 2018, IEEE T NEUR NET LEAR, V29, P1774, DOI 10.1109/TNNLS.2017.2673241
   Zhang ZH, 2016, ANN TRANSL MED, V4, DOI 10.21037/atm.2016.03.37
   Zhao L, 2020, J COASTAL RES, P570, DOI 10.2112/SI103-116.1
   Zhuo L, 2016, NEUROCOMPUTING, V173, P511, DOI 10.1016/j.neucom.2015.06.055
NR 50
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2021
VL 9
BP 88552
EP 88563
DI 10.1109/ACCESS.2021.3088236
PG 12
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Telecommunications
GA TJ6DD
UT WOS:000673569000001
OA gold
DA 2022-02-03
ER

PT J
AU Aptel, F
   Chiquet, C
   Beccat, S
   Denis, P
AF Aptel, Florent
   Chiquet, Christophe
   Beccat, Sylvain
   Denis, Philippe
TI Biometric Evaluation of Anterior Chamber Changes after Physiologic Pupil
   Dilation Using Pentacam and Anterior Segment Optical Coherence
   Tomography
SO INVESTIGATIVE OPHTHALMOLOGY & VISUAL SCIENCE
LA English
DT Article
ID PRIMARY ANGLE-CLOSURE; LONG-TERM OUTCOMES; NORMAL EYES; CHINESE
   RESIDENTS; GLAUCOMA; POPULATION; DEPTH; CURVATURE; SINGAPORE; NARROW
AB PURPOSE. We evaluated changes in anterior chamber (AC) morphology and iris volume induced by physiological mydriasis in fellow eyes of acute angle-closure patients, and age-, sex-, and central AC depth-matched primary angle-closure suspects (PACS).
   METHODS. In our study, 21 fellow eyes of patients with acute angle closure; 40 age-, sex-, and central AC depth-matched PACS eyes; and 40 age-and sex-matched normal open-angle eyes were imaged using a Pentacam and anterior segment optical coherence tomography (AS-OCT) under light conditions, and after 5 minutes of darkness using AS-OCT. Iris volume was estimated using AS-OCT and a customized image-processing software.
   RESULTS. Central AC depth, corneal curvature, axial length, and lens thickness did not differ significantly between the PACS and fellow eyes. When going from light to dark, angle opening distance at 500 mu m decreased significantly more in fellow eyes than in PACS (-68% vs. -52%, P < 0.001). When going from light to dark, the mean iris volume increased significantly in the fellow eyes (from 45.34 +/- 2.1 to 47.68 +/- 3.2 mm(3), P < 0.01), whereas it decreased significantly in most PACS eyes (from 45.01 +/- 2.2 to 42.11 +/- 2.3 mm(3), P < 0.01), and in all openangle eyes (from 44.68 +/- 1.16 to 41.67 +/- 1.20 mm(3), P < 0.01). Based on multivariate analysis, significant predictors of angle narrowing under darkness (relative change in angle-opening distance 500) were fellow eyes compared to PACS (beta = -2.98, SE = 0.249, P = 0.005) and higher iris volume increase with pupil dilation (beta = -3.146, SE = 0.432, P = 0.015).
   CONCLUSIONS. Under dark conditions, angles of fellow eyes closed dramatically more than did those of PACS. Iris volume increase per millimeter of pupil dilation is an independent predictor of angle narrowing in darkness. (Invest Ophthalmol Vis Sci. 2012;53:4005-4010) DOI:10.1167/iovs.11-9387
C1 [Aptel, Florent] CHU Grenoble, Univ Hosp, Dept Ophthalmol, Clin Univ Ophtalmol, F-38043 Grenoble 09, France.
   [Aptel, Florent; Chiquet, Christophe] UJF Grenoble 1, Grenoble, France.
   [Beccat, Sylvain] Hop Edouard Herriot, Dept Ophthalmol, Lyon, France.
   [Denis, Philippe] Hop Croix Rousse, Dept Ophthalmol, F-69317 Lyon, France.
C3 CHU Grenoble Alpes; Communaute Universite Grenoble Alpes; Universite
   Grenoble Alpes (UGA); Communaute Universite Grenoble Alpes; Universite
   Grenoble Alpes (UGA); CHU Lyon; CHU Lyon
RP Aptel, F (corresponding author), CHU Grenoble, Univ Hosp, Dept Ophthalmol, Clin Univ Ophtalmol, BP217, F-38043 Grenoble 09, France.
EM faptel@chu-grenoble.fr
RI Chiquet, Christophe/M-7426-2014
OI Chiquet, Christophe/0000-0002-7601-4885; Aptel,
   Florent/0000-0003-1537-9992
CR ALSBIRK PH, 1976, ACTA OPHTHALMOL, P5
   Aptel F, 2011, OPHTHALMOLOGY, V118, P1563, DOI 10.1016/j.ophtha.2011.01.001
   Aptel F, 2010, OPHTHALMOLOGY, V117, pc, DOI 10.1016/j.ophtha.2009.10.030
   Aung T, 2004, OPHTHALMOLOGY, V111, P1464, DOI 10.1016/j.ophtha.2003.12.061
   Buehl W, 2006, AM J OPHTHALMOL, V141, P7, DOI 10.1016/j.ajo.2005.08.048
   CONGDON N, 1992, SURV OPHTHALMOL, V36, P411, DOI 10.1016/S0039-6257(05)80022-0
   Congdon NG, 1997, OPHTHALMOLOGY, V104, P1489, DOI 10.1016/S0161-6420(97)30112-2
   Dandona L, 2000, OPHTHALMOLOGY, V107, P1710, DOI 10.1016/S0161-6420(00)00274-8
   Foster PJ, 2000, ARCH OPHTHALMOL-CHIC, V118, P1105, DOI 10.1001/archopht.118.8.1105
   Friedman DS, 2008, SURV OPHTHALMOL, V53, P250, DOI 10.1016/j.survophthal.2007.10.012
   Friedman DS, 2006, OPHTHALMOLOGY, V113, P1087, DOI 10.1016/j.ophtha.2006.02.016
   Friedman DS, 2003, ARCH OPHTHALMOL-CHIC, V121, P633, DOI 10.1001/archopht.121.5.633
   Kurita N, 2009, J GLAUCOMA, V18, P506, DOI 10.1097/IJG.0b013e318193c141
   Lackner B, 2005, OPTOMETRY VISION SCI, V82, P858, DOI 10.1097/01.opx.0000177804.53192.15
   Lowe R F, 1962, Br J Ophthalmol, V46, P641, DOI 10.1136/bjo.46.11.641
   LOWE RF, 1973, BRIT J OPHTHALMOL, V57, P464, DOI 10.1136/bjo.57.7.464
   LOWE RF, 1972, BRIT J OPHTHALMOL, V56, P409, DOI 10.1136/bjo.56.5.409
   LOWE RF, 1970, BRIT J OPHTHALMOL, V54, P161, DOI 10.1136/bjo.54.3.161
   Meinhardt B, 2006, GRAEF ARCH CLIN EXP, V244, P559, DOI 10.1007/s00417-005-0103-7
   Mou DP, 2010, OPHTHAL SURG LAS IM, V41, P622, DOI 10.3928/15428877-20100929-06
   Nolan WP, 2006, AM J OPHTHALMOL, V141, P896, DOI 10.1016/j.ajo.2005.12.008
   Nongpiur ME, 2011, CURR OPIN OPHTHALMOL, V22, P96, DOI 10.1097/ICU.0b013e32834372b9
   Quigley HA, 2010, OPHTHALMOLOGY, V117, P1, DOI 10.1016/j.ophtha.2009.11.002
   Quigley HA, 2009, AM J OPHTHALMOL, V148, P657, DOI 10.1016/j.ajo.2009.08.009
   Quigley HA, 2009, J GLAUCOMA, V18, P173, DOI 10.1097/IJG.0b013e31818624ce
   Rabsilber TM, 2006, J CATARACT REFR SURG, V32, P456, DOI 10.1016/j.jcrs.2005.12.103
   WILENSKY JT, 1993, AM J OPHTHALMOL, V115, P338, DOI 10.1016/S0002-9394(14)73585-8
   Xu L, 2008, AM J OPHTHALMOL, V145, P929, DOI 10.1016/j.ajo.2008.01.004
   Yip JLY, 2010, BRIT J OPHTHALMOL, V94, P1472, DOI 10.1136/bjo.2009.168682
NR 29
TC 44
Z9 47
U1 0
U2 3
PU ASSOC RESEARCH VISION OPHTHALMOLOGY INC
PI ROCKVILLE
PA 12300 TWINBROOK PARKWAY, ROCKVILLE, MD 20852-1606 USA
SN 0146-0404
EI 1552-5783
J9 INVEST OPHTH VIS SCI
JI Invest. Ophthalmol. Vis. Sci.
PD JUN
PY 2012
VL 53
IS 7
BP 4005
EP 4010
DI 10.1167/iovs.11-9387
PG 6
WC Ophthalmology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Ophthalmology
GA 971CM
UT WOS:000306181200087
PM 22618594
DA 2022-02-03
ER

PT S
AU Mason, CE
   Porter, SG
   Smith, TM
AF Mason, Christopher E.
   Porter, Sandra G.
   Smith, Todd M.
BE Maltsev, N
   Rzhetsky, A
   Gilliam, TC
TI Characterizing Multi-omic Data in Systems Biology
SO SYSTEMS ANALYSIS OF HUMAN MULTIGENE DISORDERS
SE Advances in Experimental Medicine and Biology
LA English
DT Article; Book Chapter
ID DNA-SEQUENCE DIFFERENCES; ONE-STOP SHOP; HUMAN TRANSCRIPTOME;
   GENE-EXPRESSION; WIDESPREAD RNA; HUMAN GENOME; READ ALIGNMENT; GUT
   MICROBIOME; REVEALS; DISEASE
AB In today's biology, studies have shifted to analyzing systems over discrete biochemical reactions and pathways. These studies depend on combining the results from scores of experimental methods that analyze DNA; mRNA; noncoding RNAs, DNA, RNA, and protein interactions; and the nucleotide modifications that form the epigenome into global datasets that represent a diverse array of "omics" data (transcriptional, epigenetic, proteomic, metabolomic). The methods used to collect these data consist of high-throughput data generation platforms that include high-content screening, imaging, flow cytometry, mass spectrometry, and nucleic acid sequencing. Of these, the next-generation DNA sequencing platforms predominate because they provide an inexpensive and scalable way to quickly interrogate the molecular changes at the genetic, epigenetic, and transcriptional level. Furthermore, existing and developing single-molecule sequencing platforms will likely make direct RNA and protein measurements possible, thus increasing the specificity of current assays and making it possible to better characterize "epialterations" that occur in the epigenome and epitranscriptome. These diverse data types present us with the largest challenge: how do we develop software systems and algorithms that can integrate these datasets and begin to support a more democratic model where individuals can capture and track their own medical information through biometric devices and personal genome sequencing? Such systems will need to provide the necessary user interactions to work with the trillions of data points needed to make scientific discoveries. Here, we describe novel approaches in the genesis and processing of such data, models to integrate these data, and the increasing ubiquity of self-reporting and self-measured genomics and health data.
C1 [Mason, Christopher E.] Weill Cornell Med Coll, Dept Physiol & Biophys, New York, NY 10065 USA.
   [Mason, Christopher E.] Weill Cornell Med Coll, HRH Prince Alwaleed Bin Talal Bin Abdulaziz Alsau, New York, NY USA.
   [Porter, Sandra G.; Smith, Todd M.] Digital World Biol, Seattle, WA 98107 USA.
   [Smith, Todd M.] PerkinElmer, Seattle, WA 98119 USA.
C3 Cornell University; Cornell University
RP Mason, CE (corresponding author), Weill Cornell Med Coll, Dept Physiol & Biophys, New York, NY 10065 USA.
EM chm2042@med.cornell.edu; Sandra@digitalworldbiology.com;
   todd.smith@perkinelmer.com
OI Porter, Sandra/0000-0002-3032-1337
FU NATIONAL HUMAN GENOME RESEARCH INSTITUTEUnited States Department of
   Health & Human ServicesNational Institutes of Health (NIH) - USANIH
   National Human Genome Research Institute (NHGRI) [R01HG006798,
   R44HG005297] Funding Source: NIH RePORTER; NATIONAL INSTITUTE OF
   NEUROLOGICAL DISORDERS AND STROKEUnited States Department of Health &
   Human ServicesNational Institutes of Health (NIH) - USANIH National
   Institute of Neurological Disorders & Stroke (NINDS) [R01NS076465]
   Funding Source: NIH RePORTER; NHGRI NIH HHSUnited States Department of
   Health & Human ServicesNational Institutes of Health (NIH) - USANIH
   National Human Genome Research Institute (NHGRI) [2R44HG005297,
   1R01HG006798] Funding Source: Medline; NINDS NIH HHSUnited States
   Department of Health & Human ServicesNational Institutes of Health (NIH)
   - USANIH National Institute of Neurological Disorders & Stroke (NINDS)
   [1R01NS076465] Funding Source: Medline
CR ADAMS MD, 1991, SCIENCE, V252, P1651, DOI 10.1126/science.2047873
   Agius P, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000916
   Akalin A, 2012, PLOS GENET, V8, DOI 10.1371/journal.pgen.1002781
   Altshuler D, 2010, NATURE, V467, P1061, DOI 10.1038/nature09534
   Anders S, 2012, GENOME RES, V22, P2008, DOI 10.1101/gr.133744.111
   Baker M, 2012, NATURE, V491, P171, DOI 10.1038/491171a
   Baker M, 2012, NATURE, V487, P282, DOI 10.1038/487282a
   Bashir A, 2010, BMC GENOMICS, V11, DOI 10.1186/1471-2164-11-385
   Califano A, 2012, NAT GENET, V44, P841, DOI 10.1038/ng.2355
   Charlebois DA, 2011, PHYS REV LETT, V107, DOI 10.1103/PhysRevLett.107.218101
   Cho I, 2012, NAT REV GENET, V13, P260, DOI 10.1038/nrg3182
   Chouchane L, 2011, J TRANSL MED, V9, DOI 10.1186/1479-5876-9-206
   Clark Tyson A, 2011, Genome Integr, V2, P10, DOI 10.1186/2041-9414-2-10
   Cooper S, 2010, NATURE, V466, P756, DOI 10.1038/nature09304
   Craven M, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0041594
   Dominissini D, 2012, NATURE, V485, P201, DOI 10.1038/nature11112
   Dunham I, 2012, NATURE, V489, P57, DOI 10.1038/nature11247
   Fernandez AF, 2012, GENOME RES, V22, P407, DOI 10.1101/gr.119867.110
   Fire A, 1998, NATURE, V391, P806, DOI 10.1038/35888
   Galperin MY, 2012, NUCLEIC ACIDS RES, V40, pD1, DOI 10.1093/nar/gkr1196
   Gentleman RC, 2004, GENOME BIOL, V5, DOI 10.1186/gb-2004-5-10-r80
   Gerstein MB, 2007, GENOME RES, V17, P669, DOI 10.1101/gr.6339607
   Heyn H, 2012, P NATL ACAD SCI USA, V109, P10522, DOI 10.1073/pnas.1120658109
   Homer N, 2008, PLOS GENET, V4, DOI 10.1371/journal.pgen.1000167
   HUNTER T, 1987, CELL, V50, P823, DOI 10.1016/0092-8674(87)90509-5
   Kahvejian A, 2008, NAT BIOTECHNOL, V26, P1125, DOI 10.1038/nbt1494
   Karr JR, 2012, CELL, V150, P389, DOI 10.1016/j.cell.2012.05.044
   Kleinman CL, 2012, SCIENCE, V335, DOI [10.1126/science.1209658, 10.1126/science.1210624]
   Koch C, 2012, SCIENCE, V337, P531, DOI 10.1126/science.1218616
   Koren O, 2012, CELL, V150, P470, DOI 10.1016/j.cell.2012.07.008
   Koren S, 2012, NAT BIOTECHNOL, V30, P692, DOI 10.1038/nbt.2280
   Korlach J, 2012, CURR OPIN STRUC BIOL, V22, P251, DOI 10.1016/j.sbi.2012.04.002
   Langmead B, 2012, NAT METHODS, V9, P357, DOI [10.1038/NMETH.1923, 10.1038/nmeth.1923]
   Levin BR, 2009, PLOS GENET, V5, DOI 10.1371/journal.pgen.1000601
   Li H, 2010, BRIEF BIOINFORM, V11, P473, DOI 10.1093/bib/bbq015
   Li H, 2009, BIOINFORMATICS, V25, P1754, DOI 10.1093/bioinformatics/btp324
   Li MY, 2011, SCIENCE, V333, P53, DOI 10.1126/science.1207018
   Marioni JC, 2008, GENOME RES, V18, P1509, DOI 10.1101/gr.079558.108
   McCarroll SA, 2008, NAT GENET, V40, P1166, DOI 10.1038/ng.238
   McKenna A, 2010, GENOME RES, V20, P1297, DOI 10.1101/gr.107524.110
   Metzker ML, 2010, NAT REV GENET, V11, P31, DOI 10.1038/nrg2626
   Meyer KD, 2012, CELL, V149, P1635, DOI 10.1016/j.cell.2012.05.003
   Miller JR, 2010, GENOMICS, V95, P315, DOI 10.1016/j.ygeno.2010.03.001
   Nagarajan N, 2013, NAT REV GENET, V14, P157, DOI 10.1038/nrg3367
   Nimrod G, 2009, J MOL BIOL, V387, P1040, DOI 10.1016/j.jmb.2009.02.023
   Nobrega MA, 2004, NATURE, V431, P988, DOI 10.1038/nature03022
   Novembre J, 2008, NATURE, V456, P98, DOI 10.1038/nature07331
   Nyholt DR, 2002, HUM HERED, V53, P2, DOI 10.1159/000048598
   Ozsolak F, 2009, NATURE, V461, P814, DOI 10.1038/nature08390
   Petsko GA, 2004, PROTEIN STRUCTURE FU
   Pickrell JK, 2012, SCIENCE, V335, DOI 10.1126/science.1210484
   Porter S., 2009, CURR PROTOC BIOINFOR, P1
   Qin JJ, 2010, NATURE, V464, P59, DOI 10.1038/nature08821
   Saletore Y, 2013, RNA BIOL, V10, P342, DOI 10.4161/rna.23812
   Saletore Y, 2012, GENOME BIOL, V13, DOI 10.1186/gb-2012-13-10-175
   Schadt Eric, 2012, Nat Biotechnol, V30, P769, DOI 10.1038/nbt.2331
   Schatz MC, 2010, NAT BIOTECHNOL, V28, P691, DOI 10.1038/nbt0710-691
   SCHENA M, 1995, SCIENCE, V270, P467, DOI 10.1126/science.270.5235.467
   Shendure J, 2008, NAT METHODS, V5, P585, DOI 10.1038/nmeth0708-585
   Shendure J, 2012, NAT BIOTECHNOL, V30, P1084, DOI 10.1038/nbt.2421
   Snitkin ES, 2012, SCI TRANSL MED, V4, DOI 10.1126/scitranslmed.3004129
   Song CX, 2012, NAT METHODS, V9, P75, DOI [10.1038/NMETH.1779, 10.1038/nmeth.1779]
   Stein LD, 2010, GENOME BIOL, V11, DOI 10.1186/gb-2010-11-5-207
   Suarez-Kurtz G, 2013, BRIT J CLIN PHARMACO, V75, P334, DOI 10.1111/j.1365-2125.2012.04354.x
   Trapnell C, 2012, NAT PROTOC, V7, P562, DOI 10.1038/nprot.2012.016
   Tsai A, 2012, NATURE, V487, P390, DOI 10.1038/nature11172
   VELCULESCU VE, 1995, SCIENCE, V270, P484, DOI 10.1126/science.270.5235.484
   Wang CY, 2012, NUCLEIC ACIDS RES, V40, P6414, DOI 10.1093/nar/gks304
   Wang IM, 2012, MOL SYST BIOL, V8, DOI 10.1038/msb.2012.24
   Yassour M, 2010, GENOME BIOL, V11, DOI 10.1186/gb-2010-11-8-r87
   Zhu BL, 2010, PROTEIN CELL, V1, P718, DOI 10.1007/s13238-010-0093-z
NR 71
TC 19
Z9 20
U1 0
U2 27
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013, UNITED STATES
SN 0065-2598
EI 2214-8019
BN 978-1-4614-8777-7; 978-1-4614-8778-4
J9 ADV EXP MED BIOL
JI Adv.Exp.Med.Biol.
PY 2014
VL 799
BP 15
EP 38
DI 10.1007/978-1-4614-8778-4_2
D2 10.1007/978-1-4614-8778-4
PG 24
WC Genetics & Heredity; Medicine, Research & Experimental
WE Book Citation Index – Science (BKCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Genetics & Heredity; Research & Experimental Medicine
GA BA2PX
UT WOS:000333762700003
PM 24292960
DA 2022-02-03
ER

PT J
AU Hanif, S
   Sohail, F
   Shehrbano
   Tariq, A
   Babar, MI
AF Hanif, Sundas
   Sohail, Fahad
   Shehrbano
   Tariq, Aneeqa
   Babar, Muhammad Imran
TI A New Shoulder Surfing and Mobile Key-Logging Resistant Graphical
   Password Scheme for Smart-Held Devices
SO INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS
LA English
DT Article
DE Authentication; graphical password; shoulder surfing; mobile
   key-logging; security
AB In globalization of information, internet has played a vital role by providing an easy and fast access of information and systems to remote users. However, with ease for authentic users, it has made information resources accessible to unauthorized users too. To authorize legitimate user for the access of information and systems, authentication mechanisms are applied. Many users use their credentials or private information at public places to access their accounts that are protected by passwords. These passwords are usually text-based passwords and their security and effectiveness can be compromised. An attacker can steal text-based passwords using different techniques like shoulder surfing and various key logger software, that are freely available over internet. To improve the security, numerous sophisticated and secure authentication systems have been proposed that employ various biometric authentication systems, token-based authentication system etc. But these solutions providing such high-level security, require special modification in the design and hence, imply additional cost. Textual passwords that are easy to use but vulnerable to attacks like shoulder surfing, various image based, and textual graphical password schemes are proposed. However, none of the existing textual graphical passwords are resistant to shoulder surfing and more importantly to mobile key-logging. In this paper, an improved and robust textual graphical password scheme is proposed that uses sectors and colors and introducing randomization as the primary function for the character display and selection. This property makes the proposed scheme resistant to shoulder surfing and more importantly to mobile key-logging. It can be useful for authentication process of any smart held device application.
C1 [Hanif, Sundas; Sohail, Fahad; Shehrbano; Tariq, Aneeqa; Babar, Muhammad Imran] Army Publ Coll Management & Sci UET Taxila, Dept Comp Sci & Software Engn, Rawalpindi, Pakistan.
RP Hanif, S (corresponding author), Army Publ Coll Management & Sci UET Taxila, Dept Comp Sci & Software Engn, Rawalpindi, Pakistan.
CR Ahmad A., 2016, INT J COMPUTER NETWO
   Darbanian E., 2015, INT S COMP SCI SOFTW
   Eastlake 3rd D, 2001, US SECURE HASH ALGOR
   Gao H., 2009, 4 INT C INN COMP INF
   Gokhale AS, 2016, PROCEDIA COMPUT SCI, V79, P490, DOI 10.1016/j.procs.2016.03.063
   Higashiyama Y., 2015, 3 INT S COMP NETW CA
   Irfan K., 2018, 15 INT C APPL SCI TE
   Kirkpatrick EA, 1894, PSYCHOL REV, V1, P602, DOI [DOI 10.1037/H0068244, 10.1037/h0068244]
   Lashkari A. H., 2011, 11 INT C COMP INF TE
   Mishra A., 2018, INT RES J ENG TECHNO, V5
   Pooja K S, 2015, INT J INFORM TECHNOL, V6
   Por L. Y., 2017, GRAPHICAL PASSWORD P
   Raut S.Y., 2014, INT J ADV RES COMPUT, V5
   RAZA M, 2012, WORLD APPL SCI J, V19, P439, DOI DOI 10.5829/idosi.wasj.2012.19.04.1837
   SHEPARD RN, 1967, J VERB LEARN VERB BE, V6, P156, DOI 10.1016/S0022-5371(67)80067-7
   Suo X., 21 ANN COMP SEC APPL
   Wagh S. H., ICCT 2015 INT J COMP
   Wiedenbeck S., 2006, AVI 06
   Yang GC, 2018, J IMAGE GRAPHICS, V6
   Yang GC, 2017, 4 INT C COMP APPL IN
NR 20
TC 0
Z9 0
U1 0
U2 0
PU SCIENCE & INFORMATION SAI ORGANIZATION LTD
PI WEST YORKSHIRE
PA 19 BOLLING RD, BRADFORD, WEST YORKSHIRE, 00000, ENGLAND
SN 2158-107X
EI 2156-5570
J9 INT J ADV COMPUT SC
JI Int. J. Adv. Comput. Sci. Appl.
PD SEP
PY 2019
VL 10
IS 9
BP 432
EP 437
PG 6
WC Computer Science, Theory & Methods
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA JS0IY
UT WOS:000499999000058
DA 2022-02-03
ER

PT J
AU Jahnke, S
   Roussel, J
   Hombach, T
   Kochs, J
   Fischbach, A
   Huber, G
   Scharr, H
AF Jahnke, Siegfried
   Roussel, Johanna
   Hombach, Thomas
   Kochs, Johannes
   Fischbach, Andreas
   Huber, Gregor
   Scharr, Hanno
TI phenoSeeder - A Robot System for Automated Handling and Phenotyping of
   Individual Seeds
SO PLANT PHYSIOLOGY
LA English
DT Article
ID ARABIDOPSIS-THALIANA; MAGNETIC-RESONANCE; SIZE; MASS; PERFORMANCE;
   GERMINATION; SOFTWARE; HISTORY; TRAITS; IMAGES
AB The enormous diversity of seed traits is an intriguing feature and critical for the overwhelming success of higher plants. In particular, seed mass is generally regarded to be key for seedling development but is mostly approximated by using scanning methods delivering only two-dimensional data, often termed seed size. However, three-dimensional traits, such as the volume or mass of single seeds, are very rarely determined in routine measurements. Here, we introduce a device named phenoSeeder, which enables the handling and phenotyping of individual seeds of very different sizes. The system consists of a pick-and-place robot and a modular setup of sensors that can be versatilely extended. Basic biometric traits detected for individual seeds are two-dimensional data from projections, three-dimensional data from volumetric measures, and mass, from which seed density is also calculated. Each seed is tracked by an identifier and, after phenotyping, can be planted, sorted, or individually stored for further evaluation or processing (e.g. in routine seed-to-plant tracking pipelines). By investigating seeds of Arabidopsis (Arabidopsis thaliana), rapeseed (Brassica napus), and barley (Hordeum vulgare), we observed that, even for apparently round-shaped seeds of rapeseed, correlations between the projected area and the mass of seeds were much weaker than between volume and mass. This indicates that simple projections may not deliver good proxies for seed mass. Although throughput is limited, we expect that automated seed phenotyping on a single-seed basis can contribute valuable information for applications in a wide range of wild or crop species, including seed classification, seed sorting, and assessment of seed quality.
C1 [Jahnke, Siegfried; Roussel, Johanna; Hombach, Thomas; Kochs, Johannes; Fischbach, Andreas; Huber, Gregor; Scharr, Hanno] Forschungszentrum Julich, Inst Bio & Geosci, IBG Plant Sci 2, D-52425 Julich, Germany.
   [Roussel, Johanna] Univ Appl Sci, FH Aachen, Fachbereich Med Tech & Technomath, D-52428 Julich, Germany.
C3 Helmholtz Association; Research Center Julich
RP Jahnke, S (corresponding author), Forschungszentrum Julich, Inst Bio & Geosci, IBG Plant Sci 2, D-52425 Julich, Germany.
EM s.jahnke@fz-juelich.de
RI Scharr, Hanno/D-9051-2015
OI Scharr, Hanno/0000-0002-8555-6416; Hombach, Thomas/0000-0003-3816-5989;
   Fischbach, Andreas/0000-0002-7020-8726
FU German Federal Ministry of Education and ResearchFederal Ministry of
   Education & Research (BMBF) [031A053]
FX This work was performed within the German Plant Phenotyping Network
   funded by the German Federal Ministry of Education and Research (project
   identification no. 031A053).
CR Agelet LE, 2014, TALANTA, V121, P288, DOI 10.1016/j.talanta.2013.12.038
   Alonso-Blanco C, 1999, P NATL ACAD SCI USA, V96, P4710, DOI 10.1073/pnas.96.8.4710
   Belin E, 2011, COMPUT ELECTRON AGR, V77, P188, DOI 10.1016/j.compag.2011.05.002
   Benoit L, 2016, MACH VISION APPL, V27, P625, DOI 10.1007/s00138-015-0717-7
   Borisjuk L, 2011, MATERIALS, V4, P1426, DOI 10.3390/ma4081426
   de Jong TJ, 2011, PLANT BIOLOGY, V13, P71, DOI 10.1111/j.1438-8677.2009.00287.x
   Demilly D, 2015, PLANT IMAGE ANALYSIS: FUNDAMENTALS AND APPLICATIONS, P147
   Esau K., 1977, ANATOMY SEED PLANTS
   Friis EM, 2014, J PALEONTOL, V88, P684, DOI 10.1666/13-099
   Fuchs J, 2013, PLANT PHYSIOL, V161, P583, DOI 10.1104/pp.112.210062
   Herridge RP, 2011, PLANT METHODS, V7, DOI 10.1186/1746-4811-7-3
   Igea J., 2016, BIORXIV
   Jako C, 2001, PLANT PHYSIOL, V126, P861, DOI 10.1104/pp.126.2.861
   Jalink H, 1998, SEED SCI RES, V8, P437, DOI 10.1017/S0960258500004402
   Jiang WB, 2013, PLANT PHYSIOL, V162, P1965, DOI 10.1104/pp.113.217703
   Jofuku KD, 2005, P NATL ACAD SCI USA, V102, P3117, DOI 10.1073/pnas.0409893102
   Joosen RVL, 2010, PLANT J, V62, P148, DOI 10.1111/j.1365-313X.2009.04116.x
   Kikuchi K, 2006, ANN BOT-LONDON, V98, P545, DOI 10.1093/aob/mcl145
   Linkies A, 2010, NEW PHYTOL, V186, P817, DOI 10.1111/j.1469-8137.2010.03249.x
   Moles AT, 2005, SCIENCE, V307, P576, DOI 10.1126/science.1104863
   Montesinos-Navarro A, 2011, NEW PHYTOL, V189, P282, DOI 10.1111/j.1469-8137.2010.03479.x
   Moore CR, 2013, G3-GENES GENOM GENET, V3, P109, DOI 10.1534/g3.112.003806
   Norden N, 2009, FUNCT ECOL, V23, P203, DOI 10.1111/j.1365-2435.2008.01477.x
   Paul-Victor C, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0006917
   Rolletschek H, 2015, PLANT BIOTECHNOL J, V13, P188, DOI 10.1111/pbi.12245
   Roussel J, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.00745
   SALISBURY E, 1974, PROC R SOC SER B-BIO, V186, P83, DOI 10.1098/rspb.1974.0039
   Schmidt F, 2013, LECT NOTES INFORM P, VP-211, P303
   Shahin M. A., 2005, Canadian Biosystems Engineering, V47
   STANTON ML, 1984, ECOLOGY, V65, P1105, DOI 10.2307/1938318
   Stuppy WH, 2003, TRENDS PLANT SCI, V8, P2, DOI 10.1016/S1360-1385(02)00004-3
   Tanabata T, 2012, PLANT PHYSIOL, V160, P1871, DOI 10.1104/pp.112.205120
   Verboven P, 2013, NEW PHYTOL, V199, P936, DOI 10.1111/nph.12342
   Weigel D, 2009, GENOME BIOL, V10, DOI 10.1186/gb-2009-10-5-107
   Windt CW, 2015, TREE PHYSIOL, V35, P366, DOI 10.1093/treephys/tpu105
   Yamauchi D, 2012, AIP CONF PROC, V1466, P237, DOI 10.1063/1.4742298
   Yang WN, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms6087
   Young L, 2006, SEED SCI TECHNOL, V34, P633, DOI 10.15258/sst.2006.34.3.10
   Young LW, 2007, J EXP BOT, V58, P2513, DOI 10.1093/jxb/erm116
   Zhang XQ, 2015, PLANT PHYSIOL, V169, P2118, DOI 10.1104/pp.15.00836
NR 40
TC 28
Z9 28
U1 2
U2 20
PU AMER SOC PLANT BIOLOGISTS
PI ROCKVILLE
PA 15501 MONONA DRIVE, ROCKVILLE, MD 20855 USA
SN 0032-0889
EI 1532-2548
J9 PLANT PHYSIOL
JI Plant Physiol.
PD NOV
PY 2016
VL 172
IS 3
BP 1358
EP 1370
DI 10.1104/pp.16.01122
PG 13
WC Plant Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Plant Sciences
GA EG6QZ
UT WOS:000391172300002
PM 27663410
OA Bronze, Green Published
DA 2022-02-03
ER

PT C
AU Alipbeki, O
   Kabzhanova, G
   Alipbekova, C
AF Alipbeki, Ongarbek
   Kabzhanova, Gulnara
   Alipbekova, Chaimgul
GP SGEM
TI USE OF OPERATIONAL REMOTE SENSING IN FORECASTING OF WHEAT PRODUCTION IN
   NOTHERN KAZAKHSTAN
SO INFORMATICS, GEOINFORMATICS AND REMOTE SENSING CONFERENCE PROCEEDINGS,
   SGEM 2016, VOL II
SE International Multidisciplinary Scientific GeoConference-SGEM
LA English
DT Proceedings Paper
CT 16th International Multidisciplinary Scientific Geoconference (SGEM
   2016)
CY JUN 30-JUL 06, 2016
CL Albena, BULGARIA
DE Remote sensing; geoinformation system; agriculture; space monitoring;
   KazEOSat-2; wheat; forecasting
ID INDEX; YIELD
AB Forecasting of crop productivity has great relevance and significance in addressing issues of ensuring food safety and economic stability of the country. In consideration of large land resources and agricultural industrial potential of Kazakhstan, only space monitoring of agricultural production will provide with operational and actual data. Productivity - a quality, comprehensive index, which depends on many factors. In this article we was developed the model of forecasting wheat productivity for Nothern Kazakhstan. In model of forecasting wheat productivity are taken into account the following factors of formation wheat productivity for the steppe and dry steppe agro climatic zones: soil moisture reserves, properties of the soil, agro meteorological data, growth rates and plant phonological phase, biometric characteristics of plants, spatial analysis of the plant condition on the basis of remote sensing data. In the framework of the project were conducted set of time series remote sensing data and ground based observations on the territory covering three regions: Akmola, North Kazakhstan and Kostanay. It turns out that the Kazakhstan satellite KazEOSat-2 has optimum capacities for regional monitoring of agricultural production according to permission (6,5 m), frequency and coverage of the territory (4 days). Landsat-8 applied as the additional data source. Images were previously processed and georeferenced with the ground based information's on a condition of plants, biophysical parameters and agro meteorological observations. The accuracy of determining crop wheat acreages and forecast of productivity was comparable with data of regional Departments of agriculture. As a result of were comprised the maps of the condition crop wheat acreages with forecasted yields.
C1 [Alipbeki, Ongarbek; Kabzhanova, Gulnara] JSC Natl Co Kazakhstan Gharysh Sapary, Astana, Kazakhstan.
   [Alipbekova, Chaimgul] S Seifullin Kazakh Agro Tech Univ, Astana, Kazakhstan.
C3 Saken Seifullin Kazakh Agrotechnical University
RP Alipbeki, O (corresponding author), JSC Natl Co Kazakhstan Gharysh Sapary, Astana, Kazakhstan.
RI Kabzhanova, Gulnara/AAZ-8858-2020
OI Kabzhanova, Gulnara/0000-0001-7002-4591
CR [Anonymous], 2006, STATE LAND CADASTRE, P65
   Boken VK, 2002, INT J REMOTE SENS, V23, P4155, DOI 10.1080/014311602320567955
   Hammer GL, 1996, DEV NATL DROUGHT A A
   Myshlyakov SG, 2012, GEOMATIKA, P87
   Schut AGT, 2009, CROP PASTURE SCI, V60, P60, DOI 10.1071/CP08182
NR 5
TC 1
Z9 1
U1 0
U2 6
PU STEF92 TECHNOLOGY LTD
PI SOFIA
PA 1 ANDREY LYAPCHEV BLVD, SOFIA, 1797, BULGARIA
SN 1314-2704
BN 978-619-7105-59-9
J9 INT MULTI SCI GEOCO
PY 2016
BP 1051
EP 1057
PG 7
WC Geosciences, Multidisciplinary
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Geology
GA BH0LO
UT WOS:000395499500134
DA 2022-02-03
ER

PT C
AU Breithaupt, R
   Sousedik, C
   Meissner, S
AF Breithaupt, Ralph
   Sousedik, Ctirad
   Meissner, Sven
GP IEEE
TI FULL FINGERPRINT SCANNER USING OPTICAL COHERENCE TOMOGRAPHY
SO 2015 INTERNATIONAL WORKSHOP ON BIOMETRICS AND FORENSICS (IWBF)
SE International Workshop on Biometrics and Forensics IWBF
LA English
DT Proceedings Paper
CT 3rd International Workshop on Biometrics and Forensics (IWBF)
CY MAR 03-04, 2015
CL Gjovik, NORWAY
DE biometrics; fingerprint; OCT; optical coherence tomography; presentation
   attack detection; artefacts; spoof; fake; PAD; biometric scanner
AB In recent years, several advances in fingerprint scanning technology regarding cost efficiency, quality&performance and presentation attack detection (PAD) helped to establish fingerprint biometry in more and more areas of application for mainstream as well as high security/governmental purposes. However, there are still cases where the current state of the art is struggling with the technological limitations. Worn out fingers, and even more fingers of infants, are very problematic to scan in sufficient quality. But even more important, in the field of unsupervised high security applications, is the fact that even those devices with the best available PAD protection can still be fooled with cheap artefacts - despite their current high complexity and increasing difficulty of improvements. On the lookout for technological alternatives, the optical coherence tomography (OCT) came into our focus. The OCT is able to look underneath the skin and acquire high resolution 3D images up to a depth of 2mm. First feasibility studies have shown a very high potential for solving the mentioned issues (and more) but have been conducted with existing medical OCT devices, which were, in many aspects, not suitable for for the time-cost-and mobility requirements of real world applications. For this reason, the goal for our current project "OCT-II" is to develop state-of-the-art OCT prototypes solely dedicated for high quality fingerprint acquisition and reliable PAD, with the focus on real world constrains regarding scanning area size, high speed data acquisition & data processing, cost and mobility. This paper will present and discuss the challenges and concepts of this project.
C1 [Breithaupt, Ralph] Fed Off Informat Secur, Postfach 20 03 63, D-53133 Bonn, Germany.
   [Sousedik, Ctirad] Gjovik Univ Coll, NBL, N-2815 Gjovik, Norway.
   [Meissner, Sven] EVONTA Technol GmbH, D-01109 Dresden, Germany.
C3 Norwegian University of Science & Technology (NTNU)
RP Breithaupt, R (corresponding author), Fed Off Informat Secur, Postfach 20 03 63, D-53133 Bonn, Germany.
EM ralph.breithaupt@bsi.bund.de; ctirad.sousedik@hig.no;
   sven.meissner@evonta-technology.de
CR [Anonymous], LUMIDIGM V SERIES
   Bossen A, 2010, IEEE PHOTONIC TECH L, V22, P507, DOI 10.1109/LPT.2010.2041347
   Cheng  Y., 2007, PHOTONICS TECHNOLOGY, V19
   Cheng YZ, 2006, APPL OPTICS, V45, P9238, DOI 10.1364/AO.45.009238
   Cimalla P, 2009, OPT EXPRESS, V17, P19486, DOI 10.1364/OE.17.019486
   Gottschlich C, 2011, IEEE T INF FOREN SEC, V6, P1165, DOI 10.1109/TIFS.2011.2143406
   HARMS F, 2014, P SPIE, V9075
   Liu GJ, 2013, APPL OPTICS, V52, P5473, DOI 10.1364/AO.52.005473
   Liu M., 2010, J BIOPHOTONICS, V22, P1677
   LIU MY, 2010, PHOTONICS TECHNOLOGY, V22, P1677, DOI DOI 10.1109/LPT.2010.2079926
   Marcialis GL, 2009, LECT NOTES COMPUT SC, V5716, P12, DOI 10.1007/978-3-642-04146-4_4
   MEISSNER S, 2013, FRONTIERS ULTRAFAST, V8611
   Nasiri-Avanaki M.-R., 2011, OPT PHOTONICS J, V01, P91
   Peterson  L.E., 2009, INT J KNOWLEDGE ENG, V1
   Schumacher G., 2013, TECH REP, DOI [10.2788/30687, DOI 10.2788/30687]
   Yambay D., 2011, P 5 IAPR INT C BIOM, P208
   [No title captured]
NR 17
TC 2
Z9 2
U1 0
U2 3
PU IEEE
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017 USA
SN 2381-6120
BN 978-1-4799-8105-2
J9 I W BIOMETRIC FORENS
PY 2015
PG 6
WC Computer Science, Theory & Methods; Mathematical & Computational Biology
WE Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Mathematical & Computational Biology
GA BF1QX
UT WOS:000380429100009
DA 2022-02-03
ER

PT J
AU Hurley, DJ
   Nixon, MS
   Carter, JN
AF Hurley, DJ
   Nixon, MS
   Carter, JN
TI Force field feature extraction for ear biometrics
SO COMPUTER VISION AND IMAGE UNDERSTANDING
LA English
DT Article
ID SHAPE; EDGE
AB The overall objective in defining feature space is to reduce the dimensionality of the original pattern space, whilst maintaining discriminatory power for classification. To meet this objective in the context of ear biometrics a new force field transformation treats the image as an array of mutually attracting particles that act as the source of a Gaussian force field. Underlying the force field there is a scalar potential energy field, which in the case of an ear takes the form of a smooth surface that resembles a small mountain with a number of peaks joined by ridges. The peaks correspond to potential energy wells and to extend the analogy the ridges correspond to potential energy channels. Since the transform also turns out to be invertible, and since the surface is otherwise smooth, information theory suggests that much of the information is transferred to these features, thus confirming their efficacy. We previously described how field line feature extraction, using an algorithm similar to gradient descent, exploits the directional properties of the force field to automatically locate these channels and wells, which then form the basis of characteristic ear features. We now show how an analysis of the mechanism of this algorithmic approach leads to a closed analytical description based on the divergence of force direction, which reveals that channels and wells are really manifestations of the same phenomenon. We further show that this new operator, with its own distinct advantages, has a striking similarity to the Marr-Hildreth operator, but with the important difference that it is non-linear. As well as addressing faster implementation, invertibility, and brightness sensitivity, the technique is also validated by performing recognition on a database of ears selected from the XM2VTS face database, and by comparing the results with the more established technique of Principal Components Analysis. This confirms not only that cars do indeed appear to have potential as a biometric, but also that the new approach is well suited to their description, being robust especially in the presence of noise, and having the advantage that the car does not need to be explicitly extracted from the background. (c) 2004 Elsevier Inc. All rights reserved.
C1 Univ Southampton, Dept Elect & Comp Sci, Southampton SO17 1BJ, Hants, England.
C3 University of Southampton
RP Hurley, DJ (corresponding author), Univ Southampton, Dept Elect & Comp Sci, Southampton SO17 1BJ, Hants, England.
EM djh@analyticalengines.co.uk; msn@ecs.soton.ac.uk; jnc@ecs.soton.ac.uk
RI Nixon, Mark S/F-7406-2014
OI Nixon, Mark/0000-0002-9174-5934
CR Ahuja N, 1996, IEEE T PATTERN ANAL, V18, P1211, DOI 10.1109/34.546258
   Ahuja N, 1997, IEEE T PATTERN ANAL, V19, P169, DOI 10.1109/34.574801
   Bertillon A., 1890, PHOTOGRAPHIE JUDICIA
   BURGE M, 2002, P ICPR 2000, P822
   Burger, 1996, BIOMETRICS PERSONAL, P273, DOI DOI 10.1007/0-306-47044-6_13
   Chang K, 2003, IEEE T PATTERN ANAL, V25, P1160, DOI 10.1109/TPAMI.2003.1227990
   DAUGMAN JG, 1999, TR482 U CAMBR COMP L
   HALLIDAY D, 1962, PHYSICS 2
   Hurley DJ, 2002, IMAGE VISION COMPUT, V20, P311, DOI 10.1016/S0262-8856(02)00003-3
   Hurley DJ, 2000, IEEE IMAGE PROC, P25, DOI 10.1109/ICIP.2000.900883
   HURLEY DJ, 1999, P 10 BRIT MACH VIS C, P604
   HURLEY DJ, 2001, THESIS U SOUTHAMPTON
   Iannarelli A., 1989, EAR IDENTIFICATION
   KIMIA BB, 1995, INT J COMPUT VISION, V15, P189, DOI 10.1007/BF01451741
   Loncaric S, 1998, PATTERN RECOGN, V31, P983, DOI 10.1016/S0031-2023(97)00122-2
   Luo B, 1999, PATTERN RECOGN LETT, V20, P635, DOI 10.1016/S0167-8655(99)00028-8
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   MESSER K, 1999, P AVBPA 99 WASH DC
   MORENO B, 1999, P IEEE 33 ANN INT CA
   O'Neill B., 1997, ELEMENTARY DIFFERENT
   Phillips W.R., 1990, ELECTROMAGNETISM
   Resnick, 1977, PHYSICS 1
   Sadiku M., 1989, ELEMENTS ELECTROMAGN
   Siddiqi K, 2002, INT J COMPUT VISION, V48, P215, DOI 10.1023/A:1016376116653
   STRANG G, 1988, LINEAR ALGEBRA ITS A, P84024
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   VANOTERLOO PJ, 1991, CONTOUR ORIENTED APP
   Victor B, 2002, INT C PATT RECOG, P429, DOI 10.1109/ICPR.2002.1044746
   Xu CY, 1997, PROC CVPR IEEE, P66, DOI 10.1109/CVPR.1997.609299
   YAHIA HM, 1998, INT C AC SPEECH SIGN
NR 30
TC 158
Z9 174
U1 0
U2 6
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1077-3142
EI 1090-235X
J9 COMPUT VIS IMAGE UND
JI Comput. Vis. Image Underst.
PD JUN
PY 2005
VL 98
IS 3
BP 491
EP 512
DI 10.1016/j.cviu.2004.11.001
PG 22
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering
GA 927CQ
UT WOS:000229170400005
OA Green Accepted, Green Submitted
DA 2022-02-03
ER

PT J
AU Ryu, S
   Kim, JH
   Yu, H
   Jung, HD
   Chang, SW
   Park, JJ
   Hong, S
   Cho, HJ
   Choi, YJ
   Choi, J
   Lee, JS
AF Ryu, Susie
   Kim, Jun Hong
   Yu, Heejin
   Jung, Hwi-Dong
   Chang, Suk Won
   Park, Jeong Jin
   Hong, Soonhyuk
   Cho, Hyung-Ju
   Choi, Yoon Jeong
   Choi, Jongeun
   Lee, Joon Sang
TI Diagnosis of obstructive sleep apnea with prediction of flow
   characteristics according to airway morphology automatically extracted
   from medical images: Computational fluid dynamics and artificial
   intelligence approach
SO COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
LA English
DT Article
DE Obstructive sleep apnea syndrome; Auto-segmentation; Upper-airway
   morphology; Computational fluid dynamics
ID SEGMENTATION; PATIENT
AB Background: Obstructive sleep apnea syndrome (OSAS) is being observed in an increasing number of cases. It can be diagnosed using several methods such as polysomnography. Objectives: To overcome the challenges of time and cost faced by conventional diagnostic methods, this paper proposes computational fluid dynamics (CFD) and machine-learning approaches that are derived from the upper-airway morphology with automatic segmentation using deep learning. Method: We adopted a 3D UNet deep-learning model to perform medical image segmentation. 3D UNet prevents the feature-extraction loss that may occur by concatenating layers and extracts the anteroposterior coordination and width of the airway morphology. To create flow characteristics of the upper airway training data, we analyzed the changes in flow characteristics according to the upper-airway morphology using CFD. A multivariate Gaussian process regression (MVGPR) model was used to train the flow characteristic values. The trained MVGPR enables the prompt prediction of the aerodynamic features of the upper airway without simulation. Unlike conventional regression methods, MVGPR can be trained by considering the correlation between the flow characteristics. As a diagnostic step, a support vector machine (SVM) with predicted aerodynamic and biometric features was used in this study to classify patients as healthy or suffering from moderate OSAS. SVM is beneficial as it is easy to learn even with a small dataset, and it can diagnose various flow characteristics as factors while enhancing the feature via the kernel function. As the patient dataset is small, the Monte Carlo cross-validation was used to validate the trained model. Furthermore, to overcome the imbalanced data problem, the oversampling method was applied. Result: The segmented upper-airway results of the high-resolution and low-resolution models present overall average dice coefficients of 0.76 +/- 0.041 and 0.74 +/- 0.052, respectively. Furthermore, the classification accuracy, sensitivity, specificity, and F1-score of the diagnosis algorithm were 81.5%, 89.3%, 86.2%, and 87.6%, respectively. Conclusion: The convenience and accuracy of sleep apnea diagnosis are improved using deep learning and machine learning. Further, the proposed method can aid clinicians in making appropriate decisions to evaluate the possible applications of OSAS. (c) 2021 Elsevier B.V. All rights reserved.
C1 [Ryu, Susie; Kim, Jun Hong; Yu, Heejin; Choi, Yoon Jeong; Choi, Jongeun; Lee, Joon Sang] Yonsei Univ, Coll Engn, Sch Mech Engn, 50 Yonsei Ro, Seoul 120749, South Korea.
   [Jung, Hwi-Dong] Yonsei Univ, Oral Sci Res Ctr, Dept Oral & Maxillofacial Surg, Coll Dent, Seoul, South Korea.
   [Chang, Suk Won; Park, Jeong Jin; Hong, Soonhyuk; Cho, Hyung-Ju] Yonsei Univ, Dept Otorhinolaryngol, Coll Med, Seoul, South Korea.
   [Choi, Yoon Jeong; Lee, Joon Sang] Yonsei Univ, Inst Craniofacial Deform, Dept Orthodont, Coll Dent, Seoul, South Korea.
C3 Yonsei University; Yonsei University; Yonsei University Health System;
   Yonsei University; Yonsei University Health System; Yonsei University;
   Yonsei University Health System
RP Lee, JS (corresponding author), Yonsei Univ, Coll Engn, Sch Mech Engn, 50 Yonsei Ro, Seoul 120749, South Korea.
EM joonlee@yonsei.ac.kr
RI Lee, Kee-Joon/AAA-4090-2022
OI Jung, Hwi-Dong/0000-0003-1025-8323; Choi, Jongeun/0000-0002-7532-5315;
   Kim, Jun Hong/0000-0002-7872-4442; Cho, Hyung-Ju/0000-0002-2851-3225
FU LAONPEOPLE INC. (Korea) [2021-11-0406]
FX This work also supported by the University-Industry Project
   (2021-11-0406, Sleep apnea diagnosis and Severity determination software
   using CBCT image of the patient's upper airway) funded by the LAONPEOPLE
   INC. (Korea)
CR Anthimopoulos M, 2019, IEEE J BIOMED HEALTH, V23, P714, DOI 10.1109/JBHI.2018.2818620
   Ball CG, 2008, COMPUT FLUIDS, V37, P943, DOI 10.1016/j.compfluid.2007.07.021
   Barewal Reva Malhotra, 2019, Dent Clin North Am, V63, P297, DOI 10.1016/j.cden.2018.11.009
   Bonilla Edwin V, 2007, NEURAL INFORM PROCES, P153
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chen ZX, 2020, NEURAL COMPUT APPL, V32, P3005, DOI 10.1007/s00521-019-04687-8
   Chi L, 2011, EUR RESPIR J, V38, P348, DOI 10.1183/09031936.00119210
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Faizal WM, 2020, COMPUT METH PROG BIO, V196, DOI 10.1016/j.cmpb.2020.105627
   Faizal WM, 2019, COMPUT METH PROG BIO, V180, DOI 10.1016/j.cmpb.2019.105036
   Fedorov A, 2012, MAGN RESON IMAGING, V30, P1323, DOI 10.1016/j.mri.2012.05.001
   Guo XP, 2018, SLEEP MED, V48, P27, DOI 10.1016/j.sleep.2018.04.011
   He HB, 2008, IEEE IJCNN, P1322, DOI 10.1109/IJCNN.2008.4633969
   Ibragimov B, 2015, MED IMAGE ANAL, V20, P198, DOI 10.1016/j.media.2014.11.006
   Jeong SJ, 2007, MED ENG PHYS, V29, P637, DOI 10.1016/j.medengphy.2006.08.017
   Jodas DS, 2017, MED IMAGE ANAL, V40, P60, DOI 10.1016/j.media.2017.06.006
   Khandoker AH, 2009, IEEE T INF TECHNOL B, V13, P37, DOI 10.1109/TITB.2008.2004495
   Kim YW, 2020, COMPUT METH PROG BIO, V191, DOI 10.1016/j.cmpb.2020.105421
   Kim YW, 2020, RSC ADV, V10, P4014, DOI 10.1039/c9ra08999c
   Kingma D., 2015, ADAM METHOD STOCHAST
   Kleinstreuer C, 2003, INT J MULTIPHAS FLOW, V29, P271, DOI 10.1016/S0301-9322(02)00131-3
   Koo SK, 2017, EUR ARCH OTO-RHINO-L, V274, P1735, DOI 10.1007/s00405-016-4335-4
   Kursa MB, 2010, J STAT SOFTW, V36, P1, DOI 10.18637/jss.v036.i11
   Lee YJG, 2017, PSYCHIAT INVEST, V14, P656, DOI 10.4306/pi.2017.14.5.656
   Li YF, 2019, IEEE J BIOMED HEALTH, V23, P175, DOI 10.1109/JBHI.2018.2790968
   Liu WT, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0176991
   Liu Y, 2018, RESP PHYSIOL NEUROBI, V249, P54, DOI 10.1016/j.resp.2018.01.005
   Alonso-Alvarez ML, 2017, SLEEP MED, V37, P1, DOI 10.1016/j.sleep.2017.06.002
   Ma B., 2019, P IEEE INT C BIOINF P IEEE INT C BIOINF
   Mencar C, 2020, HEALTH INFORM J, V26, P298, DOI 10.1177/1460458218824725
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Moreno-Munoz P, 2018, ADV NEUR IN, V31
   Mylavarapu G, 2009, J BIOMECH, V42, P1553, DOI 10.1016/j.jbiomech.2009.03.035
   Na JS, 2019, J APPL PHYSIOL, V126, P330, DOI 10.1152/japplphysiol.00522.2018
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen Q, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3062414
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tuncer SA, 2019, MED HYPOTHESES, V127, P15, DOI 10.1016/j.mehy.2019.03.026
   Veasey SC, 2019, NEW ENGL J MED, V380, P1442, DOI 10.1056/NEJMcp1816152
   Wilcox DC, 2008, AIAA J, V46, P2823, DOI 10.2514/1.36541
   Wootton DM, 2016, J APPL PHYSIOL, V121, P925, DOI 10.1152/japplphysiol.00190.2016
   Wu S, 2021, BIOMED SIGNAL PROCES, V65, DOI 10.1016/j.bspc.2020.102370
   Xia GH, 2010, ANN BIOMED ENG, V38, P1836, DOI 10.1007/s10439-010-9956-y
   Xu QS, 2001, CHEMOMETR INTELL LAB, V56, P1, DOI 10.1016/S0169-7439(00)00122-2
   Yaggi HK, 2005, NEW ENGL J MED, V353, P2034, DOI 10.1056/NEJMoa043104
   Yeom SH, 2019, J APPL PHYSIOL, V127, P959, DOI 10.1152/japplphysiol.01033.2018
   Yun J, 2019, MED IMAGE ANAL, V51, P13, DOI 10.1016/j.media.2018.10.006
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
NR 48
TC 0
Z9 0
U1 6
U2 6
PU ELSEVIER IRELAND LTD
PI CLARE
PA ELSEVIER HOUSE, BROOKVALE PLAZA, EAST PARK SHANNON, CO, CLARE, 00000,
   IRELAND
SN 0169-2607
EI 1872-7565
J9 COMPUT METH PROG BIO
JI Comput. Meth. Programs Biomed.
PD SEP
PY 2021
VL 208
AR 106243
DI 10.1016/j.cmpb.2021.106243
PG 13
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Theory & Methods; Engineering, Biomedical; Medical Informatics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Medical Informatics
GA UA9WM
UT WOS:000685504400017
PM 34218170
DA 2022-02-03
ER

PT J
AU Diaz, EA
   Donoso, G
   Saenz, C
   Aponte, PM
AF Diaz, Eduardo A.
   Donoso, Gustavo
   Saenz, Carolina
   Aponte, Pedro M.
TI Spermatogenesis in a vulnerable South American cervid, dwarf red brocket
   (Mazama rufina)
SO ANATOMIA HISTOLOGIA EMBRYOLOGIA
LA English
DT Article
DE cervids; Mazama; seminiferous epithelium; spermatogenesis; stages;
   testis
ID GERM-CELL TRANSPLANTATION; REPRODUCTIVE BIOTECHNOLOGIES; EVOLUTIONARY
   HISTORY; SERTOLI-CELLS; DEER; TESTIS; CYCLE; SPERMATOGONIA;
   CONSERVATION; GOUAZOUBIRA
AB The brocket deer (Genus Mazama) is a highly diverse cervid group distributed from Mexico to Argentina, with a downward population trend. However, literature on the basic reproductive biology of the genus is scarce. This work aimed to study biometric, histological and stereological aspects of the testes of Dwarf Red Brocket (Mazama rufina). Testes from free-ranging adult brockets (n = 3) were retrieved from necropsies. Testes were histologically processed. From histological images, several stereological parameters were estimated, and seminiferous epithelium cycle morphology was described. Testes volumes were between 8.2 and 18.4 ml and weights from 8.3 to 19.4 g. Gonadosomatic index (% paired-testes weight to body weight) went from 0.17 to 0.64. The tubular cross-sectional diameter was 179.8 +/- 2.8 mu m. Estimated volume densities for parenchyma and interstitium were 78.8% and 21.2% respectively. There were (in millions/ml) 96.0 +/- 13.1 germ cells and 37.7 +/- 6.0 somatic cells. Specific cell densities were (all expressed in millions/ml) as follows: spermatogonia 13.1 +/- 4.2; primary spermatocytes 43.1 +/- 5.0; round spermatids 36.8 +/- 8.0 (lower density near the caudal pole, p < 0.01); sustentacular (Sertoli) cells 16.8 +/- 4.1 and interstitial endocrine (Leydig) cells 17.4 +/- 3.4. Sertoli cell index (germ cells per Sertoli cell) was 6.72. Eight stages of the cycle were described, and frequencies estimated, resembling those of goats. M. rufina adult testis anatomy is similar to that of other cervids and domestic ruminants, with an apparently lower spermatogenic efficiency. This work is a first approximation to the physiology of the testis of M. rufina. Basic knowledge of the reproductive physiology of vulnerable species may allow biotechnological approaches for the restitution of animal populations.
C1 [Diaz, Eduardo A.; Aponte, Pedro M.] Univ San Francisco Quito USFQ, Colegio Ciencias Salud, Escuela Med Vet, Quito, Ecuador.
   [Diaz, Eduardo A.; Donoso, Gustavo; Saenz, Carolina] Univ San Francisco Quito USFQ, Hosp Fauna Silvestre Tueri, Inst IBIOTROP, Quito, Ecuador.
   [Aponte, Pedro M.] Univ San Francisco Quito USFQ, Colegio Ciencias Biol & Ambientales, Campus Cumbaya, Quito, Ecuador.
   [Aponte, Pedro M.] Univ San Francisco Quito USFQ, Inst Invest Biomed iBIOMED, Quito, Ecuador.
RP Aponte, PM (corresponding author), Univ San Francisco Quito USFQ, Colegio Ciencias Biol & Ambientales, Campus Cumbaya, Quito, Ecuador.
EM pmaponte@usfq.edu.ec
OI Diaz, Eduardo A./0000-0003-0486-1513; Aponte, Pedro/0000-0001-6905-1672
FU Universidad San Francisco de Quito [4347]
FX Universidad San Francisco de Quito, Grant/Award Number: 4347
CR Ajala O.O., 2015, TURKISH J AGR FOOD S, V3, P886, DOI [10.24925/turjaf.v3i11.886-890.516, DOI 10.24925/TURJAF.V3I11.886-890.516]
   Alfonso Diaz Eduardo, 2020, Journal of Threatened Taxa, V12, P16885, DOI 10.11609/jott.5552.12.13.16885-16890
   Alvarez S., 2015, IUCN RED LIST THREAT
   Andrabi SMH, 2007, ANIM REPROD SCI, V99, P223, DOI 10.1016/j.anireprosci.2006.07.002
   Antonelli A, 2018, PEERJ, V6, DOI 10.7717/peerj.5644
   Aponte PM, 2008, REPRODUCTION, V136, P543, DOI 10.1530/REP-07-0419
   BERNDTSON WE, 1977, J ANIM SCI, V44, P818, DOI 10.2527/jas1977.445818x
   BILASPURI GS, 1986, THERIOGENOLOGY, V25, P485, DOI 10.1016/0093-691X(86)90133-0
   Cabrera NC, 2017, J VET DIAGN INVEST, V29, P91, DOI 10.1177/1040638716672252
   Caldeira Bianca Cabral, 2010, Rev. Ceres, V57, P569
   Costa KLC, 2011, ANIM REPROD SCI, V127, P202, DOI 10.1016/j.anireprosci.2011.07.016
   Clegg, 1990, HISTOLOGICAL HISTOPA, P1
   Comizzoli P, 2020, ANIM PROD SCI, V60, P1227, DOI 10.1071/AN18674
   Comizzoli P, 2000, REPROD NUTR DEV, V40, P493, DOI 10.1051/rnd:2000113
   CURTIS SK, 1981, J ANIM SCI, V53, P1645, DOI 10.2527/jas1982.5361645x
   Cunha DMD, 2019, ACTA SCI VET, V47, DOI 10.22456/1679-9216.98311
   Dobrinski I, 2005, ANIM REPROD SCI, V89, P137, DOI 10.1016/j.anireprosci.2005.06.020
   Doherty TS, 2017, BIOL CONSERV, V210, P56, DOI 10.1016/j.biocon.2017.04.007
   DUARTE J.M.B., 2010, NEOTROPICAL CERVIDOL
   Duarte JMB, 2008, MOL PHYLOGENET EVOL, V49, P17, DOI 10.1016/j.ympev.2008.07.009
   Franca LR, 1999, TISSUE CELL, V31, P274, DOI 10.1054/tice.1999.0044
   Gallina-Tessaro S, 2020, ECOLOGY AND CONSERVATION OF TROPICAL UNGULATES IN LATIN AMERICA, P395, DOI 10.1007/978-3-030-28868-6_16
   Geuna S, 2015, CELL TISSUE RES, V360, P5, DOI 10.1007/s00441-015-2143-6
   Gonzalez Susana, 2020, Mastozoologia Neotropical, V27, P37, DOI 10.31687/saremMN_SI.20.27.1.05
   Gutierrez EE, 2017, ZOOKEYS, P87, DOI 10.3897/zookeys.697.15124
   HIKIM APS, 1989, ENDOCRINOLOGY, V125, P1829, DOI 10.1210/endo-125-4-1829
   HOCHEREAUDEREVIERS MT, 1978, J REPROD FERTIL, V54, P209, DOI 10.1530/jrf.0.0540209
   Hurtado-Gonzales JL, 2006, EUR J WILDLIFE RES, V52, P171, DOI 10.1007/s10344-006-0034-6
   IUCN, IUCN RED LIST THREAT
   Jorge MLSP, 2013, BIOL CONSERV, V163, P49, DOI 10.1016/j.biocon.2013.04.018
   Lara N.L.M., 2018, ENCY REPROD, P105, DOI [10.1016/B978-0-12-801238-3.64567-1, DOI 10.1016/B978-0-12-801238-3.64567-1]
   Leal M. C., 2004, Animal Reproduction, V1, P122
   Li LH, 2005, ANNU REV CELL DEV BI, V21, P605, DOI 10.1146/annurev.cellbio.21.012704.131525
   Lizcano Diego J., 2010, P177
   Machado AAN, 2011, ANIM REPROD SCI, V127, P73, DOI 10.1016/j.anireprosci.2011.06.008
   Aponte PM, 2015, WORLD J STEM CELLS, V7, P669, DOI 10.4252/wjsc.v7.i4.669
   Marcos R, 2012, J ANAT, V220, P303, DOI 10.1111/j.1469-7580.2012.01475.x
   MCLACHLAN RI, 1994, BIOL REPROD, V51, P945, DOI 10.1095/biolreprod51.5.945
   Mendes-Oliveira Ana Cristina, 2012, Mastozool. neotrop., V19, P105
   Okwun OE, 1996, J ANDROL, V17, P301
   Okwun OE, 1996, J REPROD FERTIL, V107, P137, DOI 10.1530/jrf.0.1070137
   Onyango DW, 2000, ANN ANAT, V182, P235, DOI 10.1016/S0940-9602(00)80026-6
   Pereira RJG, 2020, MAMM BIOL, V100, P253, DOI 10.1007/s42991-020-00020-2
   Polziehn RO, 1998, MOL PHYLOGENET EVOL, V10, P249, DOI 10.1006/mpev.1998.0527
   Rola LD, 2013, ANIM PROD SCI, V53, P472, DOI 10.1071/AN12247
   Rull V, 2020, NEOTROPICAL DIVERSIF
   RUSSELL LD, 1990, AM J ANAT, V188, P21, DOI 10.1002/aja.1001880104
   Shinohara T, 2006, P NATL ACAD SCI USA, V103, P13624, DOI 10.1073/pnas.0604205103
   Silva RC, 2012, J ANDROL, V33, P264, DOI 10.2164/jandrol.110.012898
   STALLINGS JR, 1986, J MAMMAL, V67, P172, DOI 10.2307/1381016
   Sukumar R., 2016, TROPICAL CONSERVATIO, P255
   Ungerfeld R, 2020, ANIM REPROD, V17, DOI [10.1590/1984-3143-AR2020-0021, 10.1590/1984-3143-ar2020-0021]
   Wahyuni Sri, 2018, Veterinary Medicine International, P3024532, DOI 10.1155/2018/3024532
   Wildt D, 1995, REPROD FERT DEVELOP, V7, P811, DOI 10.1071/RD9950811
   WROBEL KH, 1993, REPROD DOMEST ANIM, V28, P1, DOI 10.1111/j.1439-0531.1993.tb00986.x
   WROBEL KH, 1995, ANN ANAT, V177, P19, DOI 10.1016/S0940-9602(11)80126-3
   Zapata-Rios G, 2016, BIOL CONSERV, V193, P9, DOI 10.1016/j.biocon.2015.10.016
NR 57
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0340-2096
EI 1439-0264
J9 ANAT HISTOL EMBRYOL
JI Anat. Histol. Embryol.
PD JAN
PY 2022
VL 51
IS 1
BP 91
EP 102
DI 10.1111/ahe.12766
EA NOV 2021
PG 12
WC Anatomy & Morphology; Veterinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Anatomy & Morphology; Veterinary Sciences
GA YE3VU
UT WOS:000722065000001
PM 34820886
DA 2022-02-03
ER

PT J
AU LECAMPIONALSUMARD, T
AF LECAMPIONALSUMARD, T
TI 3 HYELLA TAXA (ENDOLITHIC CYANOPHYTES) FROM TROPICAL ENVIRONMENTS
   (LIZARD-ISLAND, GREAT-BARRIER-REEF)
SO ARCHIV FUR HYDROBIOLOGIE
LA English
DT Article; Proceedings Paper
CT 11TH SYMP OF THE INTERNATIONAL ASSOC FOR CYANOPHYTE : CYANOBACTERIA
   RESEARCH
CY JUL 30-AUG 10, 1989
CL MAX PLANCK INST LIMNOL, PLON, GERMANY
HO MAX PLANCK INST LIMNOL
DE CYANOBACTERIA; CYANOPHYTES; MARINS; ENDOLITHIC; HYELLA; AUSTRALIA;
   TROPICAL
ID ATLANTIC
AB In reef environment, the role of boring microorganisms in the process of degradation of coral has long been recognised (DUERDEN 1902).  There has however been little work done on the taxonomy of these microorganisms.  The only studies that might be mentioned are those by WEBER-VAN BOSSE (1932), LUKAS (1974), LUKAS & GOLUBIC (1981, 1983), LUKAS & HOFFMAN (1984), CHU & WU (1984), HOFFMAN (1985) and NIELSEN (1987).
   It was felt therefore that a study of the taxonomy of the principal microorganisms found in tropical environment was indispensable.
   The first stage of this investigation is restricted to a study of the various species of the genus Hyella that bore into the shells.
   Empty shells of molluscs and fragments of coral skeletons were collected in the intertidal zone at Lizard Island, Northern Great Barrier Reef, Australia (Fig. 1).
   The samples were fixed in 2.5% buffered glutaraldehyde, and were examined by light microscopy.  Preparation for light microscopy were made by decalcifying shell fragments using PRENYI's solution (10% nitric acid, 0.5% chromic acid, 95% ethanol in the proportion 4:3:3).
   Biometric evaluation was carried out on natural populations of Hyella.  Measurements were made on in-scale camera lucida projections of light microscopy images.  Cell measurements (width, length; mean, standard deviation of sample mean, from end cells, second cells, and filament cells) were obtained from 3 species of Hyella.
   Marine algae and cyanobacteria that penetrate into calcareous material are common in most geographical areas.  Various species bore into mollusc shells collected in the intertidal zone at Lizard Island, including Mastigocoleus testarum, Cyanosaccus piriformis, Hyella caespitosa and three new species or morphotypes of Hyella.
   In the present paper only the morphological properties of these morphotypes of Hyella are compared with each other and with those of other species of Hyella described from tropical environment.
C1 CTR OCEANOL MARSEILLE,MARINE ENDOUME STN,F-13007 MARSEILLE,FRANCE.
CR ANAGNOSTIDIS K, 1988, ARCH HYDROBIOL S, V80
   CHU HJ, 1984, HYDROBIOLOGIA, V116, P227, DOI 10.1007/BF00027671
   Davies P.J., 1983, Coral Reefs, V2, P27, DOI 10.1007/BF00304729
   Duerden JE, 1902, B AM MUS NAT HIST, V16, P323
   HOFFMAN EJ, 1985, SOC ECON PALEONT MIN, V35, P179
   KOMAREK J, 1986, ARCH HYDROBIOL S, V73
   LECAMPIONALSUMA.T, 1985, ARCH HYDROBIOL S, V71
   LUKAS KJ, 1974, J PHYCOL, V10, P331, DOI 10.1111/j.1529-8817.1974.tb02722.x
   LUKAS KJ, 1981, J PHYCOL, V17, P224, DOI 10.1111/j.0022-3646.1981.00224.x
   LUKAS KJ, 1969, BERMUDA BIOL STA RES, V2, P145
   LUKAS KJ, 1983, J PHYCOL, V19, P126
   NIELSEN R, 1987, NEW ZEAL J BOT, V25, P425, DOI 10.1080/0028825X.1987.10413359
   WATERBURY JB, 1978, MICROBIOL REV, V42, P2, DOI 10.1128/MMBR.42.1.2-44.1978
   WEBER-VAN BOSSE A., 1932, MEM MM ROY HIST NAT BELGIQUE HORS SER, V6, P1
NR 14
TC 1
Z9 1
U1 0
U2 1
PU E SCHWEIZERBART'SCHE VERLAGS
PI STUTTGART
PA NAEGELE U OBERMILLER JOHANNESSTRASSE 3A, D 70176 STUTTGART, GERMANY
SN 0003-9136
J9 ARCH HYDROBIOL
JI Arch. Hydrobiol.
PD DEC
PY 1991
SU 92
BP 159
EP 166
PG 8
WC Limnology; Marine & Freshwater Biology
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Marine & Freshwater Biology
GA HW777
UT WOS:A1991HW77700013
DA 2022-02-03
ER

PT J
AU Perlman, S
   Raviv-Zilka, L
   Levinsky, D
   Gidron, A
   Achiron, R
   Gilboa, Y
   Kivilevitch, Z
AF Perlman, Sharon
   Raviv-Zilka, Lisa
   Levinsky, Denis
   Gidron, Ayelet
   Achiron, Reuven
   Gilboa, Yinon
   Kivilevitch, Zvi
TI The birth canal: correlation between the pubic arch angle, the
   interspinous diameter, and the obstetrical conjugate: a computed
   tomography biometric study in reproductive age women
SO JOURNAL OF MATERNAL-FETAL & NEONATAL MEDICINE
LA English
DT Article
DE Computed tomography; birth canal; pelvimetry; pubic arch angle;
   inter-spinous diameter; obstetrical conjugate
ID 3-DIMENSIONAL TRANSPERINEAL ULTRASOUND; PREDICT CEPHALOPELVIC
   DISPROPORTION; EXTERNAL PELVIMETRY; DIAGNOSTIC-ACCURACY; IMAGING
   PELVIMETRY; MATERNAL HEIGHT; RISK; DELIVERY; POSITION; VALUES
AB Background: Assessment of pelvic configuration is an important factor in the prediction of a successful vaginal birth. However, manual evaluation of the pelvis is practically a vanishing art, and imaging techniques are not available as a real-time bed-side tool. Unlike the obstetrical conjugate diameter (OC) and inter spinous diameter (ISD), the pubic arch angle (PAA) can be easily measured by transperineal ultrasound. Objectives: Three-dimensional computed tomography bone reconstructions were used to measure the three main birth canal diameters, evaluate the correlation between them, and establish the normal reference range for the inlet, mid-, and pelvic outlet. Study design: Measurements of the PAA, obstetric conjugate (OC), and ISD were performed offline using three-dimensional post processing reconstruction in bone algorithm application of the pelvis on examinations performed for suspected renal colic in nonpregnant reproductive age woman. The mean of two measurements was used for statistical analysis which included reproducibility of measurements, regression curve estimation between PAA, OC, and ISD, and calculation of the respective reference range centiles for each PAA degree. Results: Two hundred ninety-eight women comprised the study group. The mean +/- SD of the PAA, ISD, and OC were 104.9 degrees (+/- 7.4), 103.8 mm (+/- 7.3), and 129.9 mm (+/- 8.3), respectively. The intra- and interobserver agreement defined by the intraclass correlation coefficient (ICC) was excellent for all parameters (range 0.905-0.993). A significant positive correlation was found between PAA and ISD and between PAA and OCD (Pearson's correlation = 0.373 (p < .001), and 0.163 (p = .022), respectively). The best regression formula was found with quadratic regression for inter spinous diameter (ISD): 34.122778 + (0.962182*PAA - 0.002830*PAA(2)), and linear regression for obstetric conjugate (OC): 110.638397 + 0.183156*PAA. Modeled mean, SD, and reference centiles of the ISD and OCD were calculated using the above regression models as function of the PAA. Conclusions: We report significant correlation between the three pelvic landmarks with greatest impact on the prediction of a successful vaginal delivery: the PAA which is easily measured sonographically and the ISD and OC which are not measurable by ultrasound. This correlation may serve as a basis for future studies to assess its utility and prognostic value for a safe vaginal delivery.
C1 [Perlman, Sharon; Levinsky, Denis; Gidron, Ayelet; Achiron, Reuven; Gilboa, Yinon; Kivilevitch, Zvi] Cha Sheba Med Ctr, Dept Obstet & Gynecol, Prenatal Diagnost Unit, Tel Hashomer, Israel.
   [Perlman, Sharon; Raviv-Zilka, Lisa; Achiron, Reuven; Gilboa, Yinon] Tel Aviv Univ, Sackler Sch Med, Tel Aviv, Israel.
   [Raviv-Zilka, Lisa] Safra Childrens Hosp, Chaim Sheba Med Ctr, Dept Diagnost Imaging, Tel Hashomer, Israel.
C3 Tel Aviv University; Sackler Faculty of Medicine; Chaim Sheba Medical
   Center
RP Perlman, S (corresponding author), Cha Sheba Med Ctr, Dept Obstet & Gynecol, Prenatal Diagnost Unit, Tel Hashomer, Israel.
EM drsharonperlman@gmail.com
RI Perlman, Sharon/AAQ-2396-2020; Perlman, Sharon/AAP-5740-2020
OI Perlman, Sharon/0000-0002-8023-0679
CR Albrich SB, 2015, ULTRASOUND OBST GYN, V46, P496, DOI 10.1002/uog.14814
   ALTMAN DG, 1994, BRIT J OBSTET GYNAEC, V101, P29, DOI 10.1111/j.1471-0528.1994.tb13006.x
   Blackadar CS, 2004, FAM MED, V36, P505
   BULL H C, 1949, Postgrad Med J, V25, P310
   Choi S, 2013, AM J PERINAT, V30, P191, DOI 10.1055/s-0032-1322518
   Dolea C, 2003, EVIDENCE INFORM POLI
   Ferguson JE, 2000, CLIN OBSTET GYNECOL, V43, P247, DOI 10.1097/00003081-200006000-00004
   FLOBERG J, 1986, ACTA OBSTET GYN SCAN, V65, P321, DOI 10.3109/00016348609157352
   Ghi T, 2016, ULTRASOUND OBST GYN, V48, P511, DOI 10.1002/uog.15808
   Ghi T, 2015, FETAL DIAGN THER, V38, P195, DOI 10.1159/000380947
   Gilboa Y, 2013, ULTRASOUND OBST GYN, V41, P442, DOI 10.1002/uog.12304
   Gilboa Y, 2014, OJOG, V04, P757, DOI 10.4236/ojog.2014
   Gomes M, 2015, PEDIATR RADIOL, V45, P1916, DOI 10.1007/s00247-015-3403-z
   Huerta-Enochian GS, 2006, AM J OBSTET GYNECOL, V194, P1689, DOI 10.1016/j.ajog.2006.03.008
   Jeyabalan A, 2005, J MATERN-FETAL NEO M, V17, P381, DOI 10.1080/14767050500124051
   Keller TM, 2003, RADIOLOGY, V227, P37, DOI 10.1148/radiol.2271011658
   Korhonen U, 2014, ARCH GYNECOL OBSTET, V290, P643, DOI 10.1007/s00404-014-3271-z
   Korhonen U, 2015, ACTA OBSTET GYN SCAN, V94, P615, DOI 10.1111/aogs.12608
   KRISHNAMURTHY S, 1991, BRIT J OBSTET GYNAEC, V98, P716, DOI 10.1111/j.1471-0528.1991.tb13462.x
   Lenhard M, 2009, RADIOL MED, V114, P827, DOI 10.1007/s11547-009-0390-x
   Lenhard MS, 2010, EUR J RADIOL, V74, pE108, DOI 10.1016/j.ejrad.2009.04.042
   LENKE RR, 1986, J REPROD MED, V31, P958
   Liselele HB, 2000, BRIT J OBSTET GYNAEC, V107, P947, DOI 10.1111/j.1471-0528.2000.tb10394.x
   Liselele HB, 2000, ACTA OBSTET GYN SCAN, V79, P673
   Michel SCA, 2002, AM J ROENTGENOL, V179, DOI 10.2214/ajr.179.4.1791063
   Msamati B C, 2005, East Afr Med J, V82, P643
   Pattinson R. C., 2000, COCHRANE DB SYST REV, V2
   Poma P A, 1982, J Natl Med Assoc, V74, P173
   Reitter A, 2014, AM J OBSTET GYNECOL, V211, DOI 10.1016/j.ajog.2014.06.029
   Royston P, 1998, ULTRASOUND OBST GYN, V11, P30, DOI 10.1046/j.1469-0705.1998.11010030.x
   Rozenholc AT, 2007, BJOG-INT J OBSTET GY, V114, P630, DOI 10.1111/j.1471-0528.2007.01294.x
   Salk I, 2016, MED PRIN PRACT, V25, P40, DOI 10.1159/000440808
   Small C, 2012, FORENSIC SCI INT, V222, DOI 10.1016/j.forsciint.2012.06.002
   Sporri S, 2002, AM J ROENTGENOL, V179, P137
   Youssef A, 2016, FETAL DIAGN THER, V40, P150, DOI 10.1159/000441517
   Zaretsky MV, 2005, OBSTET GYNECOL, V106, P919, DOI 10.1097/01.AOG.0000182575.81843.e7
NR 36
TC 4
Z9 4
U1 0
U2 10
PU TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN 1476-7058
EI 1476-4954
J9 J MATERN-FETAL NEO M
JI J. Matern.-Fetal Neonatal Med.
PD OCT 2
PY 2019
VL 32
IS 19
BP 3255
EP 3265
DI 10.1080/14767058.2018.1462322
PG 11
WC Obstetrics & Gynecology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Obstetrics & Gynecology
GA IG2GL
UT WOS:000473614700019
PM 29621904
DA 2022-02-03
ER

PT J
AU Nazir, S
   Khan, MN
   Anwar, S
   Adnan, A
   Asadi, S
   Shahzad, S
   Ali, S
AF Nazir, Shah
   Khan, Muhammad Nawaz
   Anwar, Sajid
   Adnan, Awais
   Asadi, Shahla
   Shahzad, Sara
   Ali, Shaukat
TI Big Data Visualization in Cardiology-A Systematic Review and Future
   Directions
SO IEEE ACCESS
LA English
DT Review
DE Big data; medical big data; visualization; healthcare; cardiology;
   systematic literature review
ID PREDICTION; HEART
AB The digital transformations and use of healthcare information system, electronic medical records, wearable technology, and smart devices are increasing with the passage of time. A variety of sources of big data in healthcare are available, such as biometric data, registration data, electronic health record, medical imaging, patient reported data, biomarker data, clinical data, and administrative data. Visualization of data is a key tool for producing images, diagrams, or animations to convey messages from the viewed insight. The role of cardiology in healthcare is obvious for living and life. The function of heart is the control of blood supply to the entire parts of the body. Recent speedy growth in healthcare and the development of computation in the field of cardiology enable researchers and practitioners to mine and visualize new insights from patient data. The role of visualization is to capture the important information from the data and to visualize it for the easiness of doctors and practitioners. To help the doctors and practitioners, the proposed study presents a detailed report of the existing literature on visualization of data in the field of cardiology. This report will support the doctors and practitioners in decision-making process and to make it easier. This detailed study will eventually summarize the results of the existing literature published related to visualization of data in the cardiology. This research uses the systematic literature protocol and the data was collected from the studies published during the year 2009 to 2018 (10 years). The proposed study selected 53 primary studies from different repositories according to the defined exclusion, inclusion, and quality criteria. The proposed study focused mainly on the research work been done on visualization of big data in the field of cardiology, presented a summary of the techniques used for visualization of data in cardiology, and highlight the benefits of visualizations in cardiology. The current research summarizes and organizes the available literature in the form of published materials related to big data visualization in cardiology. The proposed research will help the researchers to view the available research studies on the subject of medical big data in cardiology and then can ultimately be used as evidence in future research. The results of the proposed research show that there is an increase in articles published yearly wise and several studies exist related to medical big data in cardiology. The derivations from the studies are presented in the paper.
C1 [Nazir, Shah] Univ Swabi, Dept Comp Sci, Khyber Pakhtunkhwa 23450, Pakistan.
   [Khan, Muhammad Nawaz; Anwar, Sajid; Adnan, Awais] Inst Management Sci, Khyber Pakhtunkhwa 25000, Pakistan.
   [Asadi, Shahla] Univ Putra Malaysia, Fac Comp Sci & Informat Technol, Seri Kembangan 43400, Malaysia.
   [Shahzad, Sara] Univ Peshawar, Dept Comp Sci, Peshawar 25000, Pakistan.
   [Ali, Shaukat] Islamia Coll, Dept Comp Sci, Peshawar 25120, Pakistan.
C3 Universiti Putra Malaysia; University of Peshawar; University of
   Peshawar
RP Nazir, S (corresponding author), Univ Swabi, Dept Comp Sci, Khyber Pakhtunkhwa 23450, Pakistan.
EM snshahnzr@gmail.com
RI Nawaz, Muhammad/AAA-8063-2019; Nazir, Shah/D-2020-2015
OI Nawaz, Muhammad/0000-0003-3658-694X; Nazir, Shah/0000-0003-0126-9944
CR Abdallah Y, 2015, PROCEDIA COMPUT SCI, V65, P546, DOI 10.1016/j.procs.2015.09.129
   Ahmadi H, 2018, COMPUT METH PROG BIO, V161, P145, DOI 10.1016/j.cmpb.2018.04.013
   Allen EA, 2012, NEURON, V74, P603, DOI 10.1016/j.neuron.2012.05.001
   [Anonymous], 2018, THOMSON SCI RELEASES
   Asadi S, 2019, IEEE ACCESS, V7, P35242, DOI 10.1109/ACCESS.2019.2897729
   Asante-Korang Alfred, 2016, Cardiol Young, V26, P1597, DOI 10.1017/S1047951116001736
   Azzam T., 2013, NEW DIRECTIONS EVALU, V2013, P7, DOI [10.1002/ev.20065, DOI 10.1002/EV.20065]
   Banerjee I., 2014, 3D MULTISCALE PHYSL, p297 
   Basori A.H., 2014, VIRTUAL AUGMENTED RE, V68, P79, DOI [10.1007/978-3-642-54816-1_5, DOI 10.1007/978-3-642-54816-1_5]
   Borawska A, 2018, PROCEDIA COMPUT SCI, V126, P1758, DOI 10.1016/j.procs.2018.08.101
   Bouraoui B, 2010, COMPUT MED IMAG GRAP, V34, P377, DOI 10.1016/j.compmedimag.2010.01.001
   Brigham Tara J., 2016, Medical Reference Services Quarterly, V35, P215, DOI 10.1080/02763869.2016.1152146
   Bui AAT, 2010, MEDICAL IMAGING INFORMATICS, P139, DOI 10.1007/978-1-4419-0385-3_4
   Chow M, 2010, COMPUT EDUC, V54, P733, DOI 10.1016/j.compedu.2009.08.022
   Ciecholewski M, 2013, INT J COMPUT MATH, V90, P1734, DOI 10.1080/00207160.2012.742189
   D'hooge J, 2011, BIOL MED PHYS BIOMED, P81, DOI 10.1007/978-3-642-15816-2_3
   Dag A, 2016, DECIS SUPPORT SYST, V86, P1, DOI 10.1016/j.dss.2016.02.007
   de Lemos JA, 2015, CIRCULATION, V132, P2289, DOI 10.1161/CIRCULATIONAHA.115.019648
   Dyba T, 2008, INFORM SOFTWARE TECH, V50, P833, DOI 10.1016/j.infsof.2008.01.006
   Griffin PM, 2016, HEALTHCARE SYSTEMS ENGINEERING, P1
   Harris J, 2018, CARDIOVASC DIAGN THE, V8, pS212, DOI 10.21037/cdt.2017.09.10
   Hilbert M, 2011, SCIENCE, V332, P60, DOI 10.1126/science.1200970
   Honeyman-Buck J, 2012, J DIGIT IMAGING, V25, P689
   IDC, 2014, AN FUT
   Ilyas M, 2018, REV ENVIRON HEALTH, V33, P383, DOI 10.1515/reveh-2017-0035
   Jenkins M. W., 2015, OPTICAL COHERENCE TO, DOI [10.1007/978-3-319-06419-2_67, DOI 10.1007/978-3-319-06419-2_67]
   Johnson CR, 2009, ADVANCES IN BIOMEDICAL ENGINEERING, P209
   Keim D, 2013, IEEE COMPUT GRAPH, V33, P20, DOI 10.1109/MCG.2013.54
   Kim JH, 2012, PROTEIN SCI, V21, P1540, DOI 10.1002/pro.2144
   Kim J, 2017, J AM COLL CARDIOL, V69, P899, DOI 10.1016/j.jacc.2017.01.006
   Kitchenham B., 2007, TECH REP EBSE 2007 0
   Kitchenham B, 2009, INFORM SOFTWARE TECH, V51, P7, DOI 10.1016/j.infsof.2008.09.009
   Klimov D, 2010, ARTIF INTELL MED, V49, P11, DOI 10.1016/j.artmed.2010.02.001
   Kopanitsa G, 2016, STUD HEALTH TECHNOL, V224, P189, DOI 10.3233/978-1-61499-653-8-189
   Kopanitsa G, 2014, STUD HEALTH TECHNOL, V200, P158, DOI 10.3233/978-1-61499-393-3-158
   Kopanitsa G, 2014, STUD HEALTH TECHNOL, V200, P155, DOI 10.3233/978-1-61499-393-3-155
   Kopanitsa G, 2012, STUD HEALTH TECHNOL, V180, P199, DOI 10.3233/978-1-61499-101-4-199
   Krittanawong C, 2018, CURR HYPERTENS REP, V20, DOI 10.1007/s11906-018-0875-x
   Labiod L, 2015, IEEE T NEUR NET LEAR, V26, P2194, DOI 10.1109/TNNLS.2014.2359918
   Lau E, 2016, CIRCULATION, V134, P362, DOI 10.1161/CIRCULATIONAHA.116.021892
   Lawonn K, 2018, COMPUT GRAPH FORUM, V37, P413, DOI 10.1111/cgf.13306
   Lee J, 2016, BMC MED INFORM DECIS, V16, DOI 10.1186/s12911-016-0256-9
   Li B, 2019, J BIOMED INFORM, V93, DOI 10.1016/j.jbi.2019.103149
   Li B, 2018, INT J MED INFORM, V112, P114, DOI 10.1016/j.ijmedinf.2018.01.016
   Liu H., 2012, CAD APPL, V9, P79
   Manovich L, 2011, VISUAL STUD, V26, P36, DOI 10.1080/1472586X.2011.548488
   Mayer-Schonberger V, 2016, EUR HEART J, V37, P996, DOI 10.1093/eurheartj/ehv648
   Mishra Sundeep, 2016, Indian Heart J, V68, P586, DOI 10.1016/j.ihj.2016.06.014
   Mohanty S., 2013, BIG DATA IMPERATIVES, p45 
   Mora ML, 2015, IFMBE PROC, V50, P48, DOI 10.1007/978-981-287-573-0_12
   Nazir S, 2019, ARAB J SCI ENG, V44, P3905, DOI 10.1007/s13369-019-03718-9
   Nguyen HH, 2016, TRENDS CARDIOVAS MED, V26, P376, DOI 10.1016/j.tcm.2015.11.002
   Nilashi M, 2018, BIOCYBERN BIOMED ENG, V38, P1, DOI 10.1016/j.bbe.2017.09.002
   Nilashi M, 2017, COMPUT CHEM ENG, V106, P212, DOI 10.1016/j.compchemeng.2017.06.011
   Pantazos K., 2012, THESIS
   Pretorius AJ, 2017, COMPUT GRAPH FORUM, V36, P46, DOI 10.1111/cgf.12784
   Rodrigues JJPC, 2016, SENSOR NETW SET, P123
   Rodriguez J., 2016, DATA VISUALIZATION P
   Sagiroglu S, 2013, PROCEEDINGS OF THE 2013 INTERNATIONAL CONFERENCE ON COLLABORATION TECHNOLOGIES AND SYSTEMS (CTS), P42
   Scruggs SB, 2015, CIRC RES, V116, P1115, DOI 10.1161/CIRCRESAHA.115.306013
   Shah RU, 2017, EUR HEART J, V38, P1865, DOI 10.1093/eurheartj/ehx284
   Skalna I, 2015, STUD FUZZ SOFT COMP, V333, P135, DOI 10.1007/978-3-319-26494-3_8
   Soto-Iglesias D, 2016, MED IMAGE ANAL, V32, P131, DOI 10.1016/j.media.2016.03.010
   Thorvaldsdottir H, 2013, BRIEF BIOINFORM, V14, P178, DOI 10.1093/bib/bbs017
   VanBuren V, 2012, METHODS MOL BIOL, V843, P291, DOI 10.1007/978-1-61779-523-7_25
   Walton S, 2014, PROG BIOPHYS MOL BIO, V115, P349, DOI 10.1016/j.pbiomolbio.2014.07.009
   Yang F, 2017, BIOMED ENG ONLINE, V16, DOI 10.1186/s12938-017-0368-1
   Yao CH, 2019, FUTURE GENER COMP SY, V94, P140, DOI 10.1016/j.future.2018.11.011
   Zastrow M, 2015, NATURE, V519, P119, DOI 10.1038/519119a
NR 69
TC 7
Z9 7
U1 7
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2019
VL 7
BP 115945
EP 115958
DI 10.1109/ACCESS.2019.2936133
PG 14
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Telecommunications
GA IV4FY
UT WOS:000484230200030
OA gold
DA 2022-02-03
ER

PT J
AU Arlt, S
   Noser, H
   Wienke, A
   Radetzki, F
   Hofmann, GO
   Mendel, T
AF Arlt, Stephan
   Noser, Hansrudi
   Wienke, Andreas
   Radetzki, Florian
   Hofmann, Gunther Olaf
   Mendel, Thomas
TI Secure corridor for infraacetabular screws in acetabular fracture
   fixation-a 3-D radiomorphometric analysis of 124 pelvic CT datasets
SO JOURNAL OF ORTHOPAEDIC SURGERY AND RESEARCH
LA English
DT Article
DE Infraacetabular screw; Acetabulum; Acetabular fracture; Virtual bone
   corridor; Corridor volume; Computed tomography; Computer-aided imaging
ID BONE MASS-DISTRIBUTION; COLUMN FRACTURES; PLACEMENT; DATABASE;
   REDUCTION; DYSPLASIA; STRENGTH; MODELS; SACRUM
AB Background: Acetabular fracture surgery is directed toward anatomical reduction and stable fixation to allow for the early functional rehabilitation of an injured hip joint. Recent biomechanical investigations have shown the superiority of using an additional screw in the infraacetabular (IA) region, thereby transfixing the separated columns to strengthen the construct by closing the periacetabular fixation frame. However, the inter-individual existence and variance concerning secure IA screw corridors are poorly understood.
   Methods: This computer-aided 3-D radiomorphometric study examined 124 CT Digital Imaging and Communications in Medicine (DICOM) datasets of intact human pelves (248 acetabula) to visualize the spatial IA corridors as the sum of all intraosseous screw positions. DICOM files were pre-processed using the Amira (R) 4.2 visualization software. Final corridor computation was accomplished using a custom-made software algorithm. The volumetric measurement data of each corridor were calculated for further statistical analyses. Correlations between the volumetric values and the biometric data were investigated. Furthermore, the influence of hip dysplasia on the IA corridor configuration was analyzed.
   Results: The IA corridors consistently showed a double-cone shape with the isthmus located at the acetabular fovea. In 97% of male and 91% of female acetabula, a corridor for a 3.5-mm screw could be found. The number of IA corridors was significantly lower in females for screw diameters >= 4.5 mm. The mean 3.5-mm screw corridor volume was 16 cm(3) in males and 9.2 cm(3) in female pelves. Corridor volumes were significantly positively correlated with body height and weight and with the diameter of Kohler's teardrop on standard AP pelvic X-rays. No correlation was observed between hip dysplasia and the IA corridor extent.
   Conclusion: IA corridors are consistently smaller in females. However, 3.5-mm small fragment screws may still be used as the standard implant because sex-specific differences are significant only with screw diameters >= 4.5 mm. Congenital hip dysplasia does not affect secure IA screw insertion. The described method allows 3-D shape analyses with highly reliable results. The visualization of secure IA corridors may support the spatial awareness of surgeons. Volumetric data allow the reliable assessment of individual IA corridors using standard AP X-ray views, which aids preoperative planning.
C1 [Arlt, Stephan; Hofmann, Gunther Olaf; Mendel, Thomas] BG Klinikum Bergmannstrost Halle gGmbH, Dept Trauma Surg, Merseburger Str 165, D-06112 Halle, Saale, Germany.
   [Arlt, Stephan; Hofmann, Gunther Olaf; Mendel, Thomas] Univ Klinikum Jena, Dept Trauma Surg, Klinikum 1, D-07747 Jena, Germany.
   [Noser, Hansrudi] AO Res Inst, Clavadelerstr 8, CH-7270 Davos, Switzerland.
   [Wienke, Andreas] Martin Luther Univ Halle Wittenberg, Inst Med Epidemiol Biometry & Informat, Magdeburger Str 8, D-06112 Halle, Saale, Germany.
   [Radetzki, Florian] Martin Luther Univ Halle Wittenberg, Dept Orthopaed & Trauma Surg, Ernst Grube Str 40, D-06120 Halle, Saale, Germany.
C3 Friedrich Schiller University of Jena; Martin Luther University Halle
   Wittenberg; Martin Luther University Halle Wittenberg
RP Arlt, S (corresponding author), BG Klinikum Bergmannstrost Halle gGmbH, Dept Trauma Surg, Merseburger Str 165, D-06112 Halle, Saale, Germany.; Arlt, S (corresponding author), Univ Klinikum Jena, Dept Trauma Surg, Klinikum 1, D-07747 Jena, Germany.
EM stephan.arlt@gmail.com
RI Mendel, Thomas/ABA-5709-2020; Wienke, Andreas/AAN-3707-2020
OI Mendel, Thomas/0000-0002-5212-5898; 
FU AO Foundation [AR2011_4]
FX The present study was made possible by funding from the AO Foundation
   (AO Research Grant Project No. AR2011_4).
CR Bresenham JE, 1998, SEMINAL GRAPHICS POI, P1
   Culemann U, 2011, J TRAUMA, V70, P244, DOI 10.1097/TA.0b013e3181f45f91
   Dora C, 2000, J ORTHOP TRAUMA, V14, P483, DOI 10.1097/00005131-200009000-00004
   Ferguson TA, 2010, J BONE JOINT SURG BR, V92B, P250, DOI 10.1302/0301-620X.92B2.22488
   Gansslen A, 2009, OPER ORTHOPADE TRAUM, V21, P270, DOI 10.1007/s00064-009-1804-6
   Gras F, 2015, CLIN ORTHOP RELAT R, V473, P361, DOI 10.1007/s11999-014-3932-z
   Gras F, 2012, J TRAUMA ACUTE CARE, V72, P1664, DOI 10.1097/TA.0b013e3182463b45
   HELFET DL, 1992, J BONE JOINT SURG AM, V74A, P753, DOI 10.2106/00004623-199274050-00015
   JUDET R, 1964, J BONE JOINT SURG AM, V46, P1615, DOI 10.2106/00004623-196446080-00001
   Laird A, 2005, J BONE JOINT SURG BR, V87B, P969, DOI 10.1302/0301-620X.87B7.16017
   LETOURNEL E, 1980, CLIN ORTHOP RELAT R, P81
   Letournel E, 1993, FRACTURES ACETABULUM
   Marintschev I, 2012, INJURY, V43, P470, DOI 10.1016/j.injury.2011.11.009
   MATTA JM, 1986, CLIN ORTHOP RELAT R, P230
   Matta JM, 1996, J BONE JOINT SURG AM, V78A, P1632, DOI 10.2106/00004623-199611000-00002
   Mears DC, 2003, CLIN ORTHOP RELAT R, P173, DOI 10.1097/01.blo.0000043057.62337.3a
   Mendel T, 2013, INJURY, V44, P1773, DOI 10.1016/j.injury.2013.08.006
   Mendel T, 2011, INJURY, V42, P1164, DOI 10.1016/j.injury.2010.03.016
   Mendel T, 2013, INJURY, V44, P957, DOI 10.1016/j.injury.2012.11.013
   Messmer P, 2007, J DIGIT IMAGING, V20, P17, DOI 10.1007/s10278-006-0771-9
   Noser H, 2011, J DIGIT IMAGING, V24, P665, DOI 10.1007/s10278-010-9327-0
   Ochs BG, 2010, INJURY, V41, P839, DOI 10.1016/j.injury.2010.04.010
   Ovre S, 2008, INJURY, V39, P922, DOI 10.1016/j.injury.2007.12.006
   Pagenkopf Eric, 2006, HSS J, V2, P161, DOI 10.1007/s11420-006-9010-7
   Plontke SK, 2014, OTOL NEUROTOL, V35, P1251, DOI 10.1097/MAO.0000000000000405
   Radetzki F, 2013, SURG RADIOL ANAT, V35, P963, DOI 10.1007/s00276-013-1118-0
   Radetzki F, 2015, ARCH ORTHOP TRAUM SU, V135, P667, DOI 10.1007/s00402-015-2185-y
   Radetzki F, 2014, ARCH ORTHOP TRAUM SU, V134, P1115, DOI 10.1007/s00402-014-2022-8
   Rommens PM, 1999, UNFALLCHIRURG, V102, P591, DOI 10.1007/s001130050455
   Stabe-Heyl J, 2006, ORTHOPADE, V35, P566, DOI 10.1007/s00132-006-0933-y
   Wagner D, 2016, J BONE JOINT SURG AM, V98, P584, DOI 10.2106/JBJS.15.00726
   Wagner D, 2014, J ORTHOP RES, V32, P1543, DOI 10.1002/jor.22667
   Werner CML, 2012, SKELETAL RADIOL, V41, P1273, DOI 10.1007/s00256-012-1420-7
NR 33
TC 8
Z9 12
U1 0
U2 1
PU BIOMED CENTRAL LTD
PI LONDON
PA 236 GRAYS INN RD, FLOOR 6, LONDON WC1X 8HL, ENGLAND
SN 1749-799X
J9 J ORTHOP SURG RES
JI J. Orthop. Surg. Res.
PD MAY 21
PY 2018
VL 13
AR 119
DI 10.1186/s13018-018-0833-y
PG 10
WC Orthopedics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Orthopedics
GA GH0UV
UT WOS:000433118600002
PM 29784006
OA gold, Green Published
DA 2022-02-03
ER

PT J
AU Nissen, M
   Sander, V
   Rogge, P
   Alrefai, M
   Trobs, RB
AF Nissen, Matthias
   Sander, Volker
   Rogge, Phillip
   Alrefai, Mohamad
   Troebs, Ralf-Bodo
TI Neutrophil to Lymphocyte Ratio and Platelet to Lymphocyte Ratio Might
   Predict Pediatric Ovarian Torsion: A Single-Institution Experience and
   Review of the Literature
SO JOURNAL OF PEDIATRIC AND ADOLESCENT GYNECOLOGY
LA English
DT Review
DE Ovarian cyst; Ovarian torsion; Adnexal torsion; Pediatric; Neutrophil to
   lymphocyte ratio; Platelet to lymphocyte ratio; Lymphocyte to CRP ratio
ID C-REACTIVE PROTEIN; SCORING SYSTEM; CYSTS; DIAGNOSIS; MANAGEMENT;
   INFLAMMATION; SURGERY; INFANTS; VALUES; MARKER
AB Study Objective: To determine clinical and laboratory characteristics of ovarian torsion (OT; n = 28) compared with a non-OT control (OC; n = 64) group. Design: Retrospective single-center review performed between January 2006 and December 2016. Setting: Academic department of pediatric surgery. Participants and Interventions: Postoperative diagnosis of pediatric ovarian pathology (International Classification of Diseases, 10th Revision code N83) in 88 patients who underwent 92 surgeries for suspected OT, aged from 3 days to 17.8 years. Main Outcome Measures: Predictive value for OT according to biometric, procedural, and laboratory parameters at the time of admission. Results: Compared with OC, OT in patients aged older than 1 year was associated with elevated values regarding white blood cell count, neutrophils, neutrophil to lymphocyte ratio (NLR; all P < .001), platelet to lymphocyte ratio (PLR; P = .003), platelets (P = .011), and a trend toward raised C-reactive protein (P = .054), whereas lymphocytes and lymphocyte to C-reactive protein ratio (both P < .001) were decreased. Using receiver operating characteristic analysis for differentiating OC from OT, besides lymphocytes and NLR (both area under the curve O 0.9), PLR elicited strongest discriminatory accuracy (area under the curve = 0.946 f 0.037; P < .001; sensitivity 82%; specificity 90%). At binary logistic regression analysis PLR (P = .018) was independently predictive of OT. OT was suspected on ultrasound imaging in 15/18 (83%), showed a right-sided dominance in 13/18 (72%), and was associated with younger age (P = .003). No differences regarding laboratory or procedural parameters in patients aged younger than 1 year were discerned. Conclusion: Blood count indices such as PLR, NLR, and lymphocyte to C-reactive protein ratio might be helpful in identification of in-flammatory processes as induced by ischemia in OT. Together with ultrasound and clinical features, these parameters constitute potential predictors of OT in girls aged older than 1 year.
C1 [Nissen, Matthias; Sander, Volker; Rogge, Phillip; Alrefai, Mohamad] Ruhr Univ Bochum, Marien Hosp, Dept Pediat Surg, St Elisabeth Grp, Marienpl 2, D-58452 Witten, Germany.
   [Troebs, Ralf-Bodo] St Johannes Hosp, Dept Pediat Surg, Helios Grp, Duisburg, Germany.
C3 Ruhr University Bochum; St. Marien Hospital; St. Johannes Hospital
RP Nissen, M (corresponding author), Ruhr Univ Bochum, Marien Hosp, Dept Pediat Surg, St Elisabeth Grp, Marienpl 2, D-58452 Witten, Germany.
EM matthias.nissen@elisabethgruppe.de
RI Nissen, Matthias/AAL-6635-2021
OI Nissen, Matthias/0000-0001-5458-8127
FU  [K13319]
FX M.N. is currently receiving a research fellowship from the
   RuhrUniversita?t Bochum (grant number: K13319) . The funding source had
   no involvement in study design, data collection, analysis,
   interpretation, writing of the report, nor the decision to submit the
   article for publication.
CR Akin Mustafa Ali, 2010, J Clin Res Pediatr Endocrinol, V2, P28, DOI 10.4274/jcrpe.v2i1.28
   Akobeng AK, 2007, ACTA PAEDIATR, V96, P644, DOI 10.1111/j.1651-2227.2006.00178.x
   Augene E, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0219763
   Avcil S, 2018, PSYCHIAT CLIN NEUROS, V72, P522, DOI 10.1111/pcn.12659
   Azurah AGN, 2015, WORLD J PEDIATR, V11, P35, DOI 10.1007/s12519-014-0536-3
   Bagolan P, 2002, J PEDIATR SURG, V37, P25, DOI 10.1053/jpsu.2002.29421
   Bakacak M, 2015, ARCH GYNECOL OBSTET, V291, P99, DOI 10.1007/s00404-014-3400-8
   Bitkin A, 2018, ANDROLOGIA, V50, DOI 10.1111/and.12819
   Bolli P, 2017, MEDICINE, V96, DOI 10.1097/MD.0000000000008299
   Buck BH, 2008, STROKE, V39, P355, DOI 10.1161/STROKEAHA.107.490128
   Campbell BT, 2015, J PEDIATR SURG, V50, P1374, DOI 10.1016/j.jpedsurg.2015.04.018
   Cass Darrell L, 2005, Semin Pediatr Surg, V14, P86
   Cass DL, 2001, J PEDIATR SURG, V36, P693, DOI 10.1053/jpsu.2001.22939
   Chen SC, 2016, MEDICINE, V95, P2727
   Daldal E, 2020, HEALTHCARE-BASEL, V8, DOI 10.3390/healthcare8010039
   Daponte A, 2006, FERTIL STERIL, V85, P1469, DOI 10.1016/j.fertnstert.2005.10.056
   Ercan O, 2015, INT J CLIN EXP MED, V8, P16095
   Erdim I, 2017, INT J PEDIATR OTORHI, V98, P85, DOI 10.1016/j.ijporl.2017.04.043
   Gasparyan AY, 2019, ANN LAB MED, V39, P345, DOI 10.3343/alm.2019.39.4.345
   Gawaz M, 2005, J CLIN INVEST, V115, P3378, DOI 10.1172/JCI27196
   GRAIF M, 1988, AM J ROENTGENOL, V150, P647, DOI 10.2214/ajr.150.3.647
   Ha R, 2019, INT J PEDIATR OTORHI, V120, P134, DOI 10.1016/j.ijporl.2019.02.023
   Heling KS, 2002, ULTRASOUND OBST GYN, V20, P47, DOI 10.1046/j.1469-0705.2002.00725.x
   Hong SH, 2018, PEDIATR INT, V60, P791, DOI 10.1111/ped.13652
   Koksal H, 2018, ULUS TRAVMA ACIL CER, V24, P207, DOI 10.5505/tjtes.2017.93937
   Lindemann S, 2007, J THROMB HAEMOST, V5, P203, DOI 10.1111/j.1538-7836.2007.02517.x
   Liu BJ, 2016, SCI REP-UK, V6, DOI 10.1038/srep39862
   Nozoe T, 2019, J MED INVESTIG, V66, P264, DOI 10.2152/jmi.66.264
   NUSSBAUM AR, 1988, RADIOLOGY, V168, P817, DOI 10.1148/radiology.168.3.3043551
   Okugawa Y, 2020, ANN SURG, V272, P342, DOI 10.1097/SLA.0000000000003239
   Oltmann SC, 2009, J PEDIATR SURG, V44, P1212, DOI 10.1016/j.jpedsurg.2009.02.028
   Pepys MB, 2003, J CLIN INVEST, V111, P1805, DOI 10.1172/JCI200318921
   Servaes S, 2007, PEDIATR RADIOL, V37, P446, DOI 10.1007/s00247-007-0429-x
   Soysal S, 2018, TURK J OBSTET GYNECO, V15, P91, DOI 10.4274/tjod.95881
   Tamhane UU, 2008, AM J CARDIOL, V102, P653, DOI 10.1016/j.amjcard.2008.05.006
   Tayyar AT, 2017, MILITARY MED RES, V23, P1, DOI [10.21613/GORM.2016.655, DOI 10.21613/GORM.2016.655]
   Tobiume T, 2011, TOHOKU J EXP MED, V225, P211, DOI 10.1620/tjem.225.211
   Trinh TW, 2015, RADIOGRAPHICS, V35, P621, DOI 10.1148/rg.352140073
   Tyraskis A, 2017, PRENATAL DIAG, V37, P951, DOI 10.1002/pd.5143
   Wu S, 2019, FRONT PEDIATR, V7, DOI 10.3389/fped.2019.00514
   Xie XM, 2017, MED SCI MONITOR, V23, P5558, DOI 10.12659/MSM.905728
   Yildirim M, 2021, HERNIA, V25, P733, DOI 10.1007/s10029-020-02174-x
   Yorulmaz A, 2019, CLIN RHEUMATOL, V38, P701, DOI 10.1007/s10067-018-4338-1
NR 43
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER SCIENCE INC
PI NEW YORK
PA STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA
SN 1083-3188
EI 1873-4332
J9 J PEDIATR ADOL GYNEC
JI J. Pediatr Adolesc. Gynecol.
PD JUN
PY 2021
VL 34
IS 3
BP 334
EP 340
DI 10.1016/j.jpag.2020.12.003
PG 7
WC Obstetrics & Gynecology; Pediatrics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Obstetrics & Gynecology; Pediatrics
GA UY1BQ
UT WOS:000701267200012
PM 33316415
DA 2022-02-03
ER

PT J
AU Gentle, A
   McBrien, NA
AF Gentle, A
   McBrien, NA
TI Modulation of scleral DNA synthesis in development of and recovery from
   induced axial myopia in the tree shrew
SO EXPERIMENTAL EYE RESEARCH
LA English
DT Article
DE myopia; sclera; DNA; tree shrew; [H-3] thymidine; cell proliferation
ID FORM-DEPRIVATION MYOPIA; FIBROBLAST GROWTH-FACTOR; EYE ENLARGEMENT;
   CHICKS; COLLAGEN; MATRIX; MECHANISM
AB Visually modulated scleral extracellular matrix remodelling is associated with the development of, and recovery ii om, induced axial myopia in the tree shrew, a commonly used mammalian model of refractive error development. The involvement of scleral cell proliferation in this process was investigated using [H-3] thymidine. Tree shrews were monocularly deprived of pattern vision, using translucent occluders, or the retinal image was optically defocused, using negative lenses, over a period of 5 days, A further group was monocularly deprived for 5 days, then allowed 3 days of binocular recovery, A control group of binocularly open animals was employed to establish normal parameters. On the final day of the experimental period, [RH] thymidine was administered by intraperitoneal injection, then optical and biometric measures were taken and tissue samples collected for assay. Incorporation of [RH] thymidine into cellular DNA was measured in proteinase K digests, following precipitation with trichloroacetic acid. After 5 days, significant amounts of myopia were present in the treated eyes of both form-deprived [- 7.0 +/- 0.7 Dioptres (D), group mean +/- S.E.M.; P < 0.01] and lens-defocused animals (- 6.2 +/- 0.9 D: P < 0.01). After 3 days of recovery, 50% of the refractive error had been compensated for, predominantly through shortening of the vitreous chamber in the treated eye. Reduced levels of [H-3] thymidine incorporation were observed in sclera From both groups of myopic animals (form-deprived. - 34.3 +/- 9.9%; P < 0.05 and lens-defocus, - 32.8 +/- 4.5%; P < 0.005). Increased levels of [H-3] thymidine incorporation were Found in the sclera of recovering animals (+ 144.0 +/- 43.2%; P < 0.05) The results show that changes in regulation of cell proliferation, during the development of myopia. are visually mediated and inversely related to the direction of change in ocular size. This implies that alterations in the scleral fibroblast population are involved in thr modulation of scleral matrix turnover during myopia development. (C) 1999 Academic Press.
C1 Univ Melbourne, Dept Optometry & Vis Sci, Melbourne, Vic 3053, Australia.
C3 University of Melbourne
RP McBrien, NA (corresponding author), Univ Melbourne, Dept Optometry & Vis Sci, Corner Keppel & Cardigan St, Melbourne, Vic 3053, Australia.
CR Alexander CM, 1991, CELL BIOL EXTRACELLU, P255
   AVETISOV ES, 1984, METAB PEDIATR SYST O, V7, P183
   CHRISTENSEN AM, 1991, INVEST OPHTH VIS SCI, V32, P2143
   CORNELL LM, 1994, INVEST OPHTH VIS SCI, V35, P2068
   Cottriall CL, 1996, INVEST OPHTH VIS SCI, V37, P1368
   Curtin, 1985, MYOPIAS BASIC SCI CL, P247
   CURTIN BJ, 1979, ARCH OPHTHALMOL-CHIC, V97, P912, DOI 10.1001/archopht.1979.01020010470017
   GRADL G, 1995, CURR BIOL, V5, P526, DOI 10.1016/S0960-9822(95)00105-9
   Guggenheim JA, 1996, INVEST OPHTH VIS SCI, V37, P1380
   HAMPTON GR, 1983, OPHTHALMOLOGY, V90, P923
   Jilka RL, 1998, J BONE MINER RES, V13, P793, DOI 10.1359/jbmr.1998.13.5.793
   KANG RN, 1993, INVEST OPHTH VIS SCI, V34, P1209
   Kusakari T, 1997, EXP EYE RES, V64, P465, DOI 10.1006/exer.1996.0242
   LABARCA C, 1980, ANAL BIOCHEM, V102, P344, DOI 10.1016/0003-2697(80)90165-7
   Marzani D, 1997, INVEST OPHTH VIS SCI, V38, P1726
   MAURER HR, 1981, CELL TISSUE KINET, V14, P111, DOI 10.1111/j.1365-2184.1981.tb00516.x
   MCBRIEN NA, 1994, EXP EYE RES, V59, P475, DOI 10.1006/exer.1994.1133
   MCBRIEN NA, 1993, INVEST OPHTH VIS SCI, V34, P205
   MCBRIEN NA, 1992, VISION RES, V32, P843, DOI 10.1016/0042-6989(92)90027-G
   MCBRIEN NA, 1995, INVEST OPHTHALMOL, V36, P760
   MCBRIEN NA, 1998, MYOPIA UPDATES, P278
   Norton T. T., 1982, CHANGING CONCEPTS NE, P377
   NORTON TT, 1995, VISION RES, V35, P1271, DOI 10.1016/0042-6989(94)00243-F
   NORTON TT, 1998, INVEST OPHTHALMOL  S, V39, P505
   Ohyama K, 1997, J BONE MINER RES, V12, P1647, DOI 10.1359/jbmr.1997.12.10.1647
   PIERRO L, 1992, RETINA-J RET VIT DIS, V12, P12, DOI 10.1097/00006982-199212010-00003
   RADA JA, 1995, INVEST OPHTH VIS SCI, V36, P1555
   RADA JA, 1992, CURR EYE RES, V11, P767, DOI 10.3109/02713689209000750
   RADA JA, 1991, DEV BIOL, V147, P303, DOI 10.1016/0012-1606(91)90288-E
   RAVIOLA E, 1985, NEW ENGL J MED, V312, P1609, DOI 10.1056/NEJM198506203122505
   REEDER AP, 1994, INVEST OPHTH VIS SCI, V35, P1801
   ROHRER B, 1994, EXP EYE RES, V58, P553, DOI 10.1006/exer.1994.1049
   RUOSLAHTI E, 1987, SCIENCE, V238, P491, DOI 10.1126/science.2821619
   SEKO Y, 1995, INVEST OPHTH VIS SCI, V36, P1183
   SIEGWART JT, 1994, LAB ANIM SCI, V44, P213
   SPORN MB, 1988, NATURE, V332, P217, DOI 10.1038/332217a0
   Yokoyama Y, 1997, J NEUROCHEM, V68, P2212
   YOUNG RD, 1985, J CELL SCI, V74, P95
NR 38
TC 48
Z9 58
U1 0
U2 1
PU ACADEMIC PRESS LTD
PI LONDON
PA 24-28 OVAL RD, LONDON NW1 7DX, ENGLAND
SN 0014-4835
J9 EXP EYE RES
JI Exp. Eye Res.
PD FEB
PY 1999
VL 68
IS 2
BP 155
EP 163
DI 10.1006/exer.1998.0587
PG 9
WC Ophthalmology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Ophthalmology
GA 179UN
UT WOS:000079348000002
PM 10068481
DA 2022-02-03
ER

PT J
AU Hocaoglu, Y
   Herrmann, K
   Walther, S
   Hennenberg, M
   Gratzke, C
   Bauer, R
   Stief, C
   Roosen, A
AF Hocaoglu, Yasemin
   Herrmann, Karin
   Walther, Sebastian
   Hennenberg, Martin
   Gratzke, Christian
   Bauer, Ricarda
   Stief, Christian
   Roosen, Alexander
TI Contraction of the anterior prostate is required for the initiation of
   micturition
SO BJU INTERNATIONAL
LA English
DT Article
DE prostate; micturition cycle; bladder neck
ID GUINEA-PIG PROSTATE; URINARY CONTINENCE; MOUSE PROSTATE; ACETYLCHOLINE;
   STIMULATION; PROTEINS; FRUCTOSE
AB Objective
   To investigate if in vitro contractile strength of the prostate and the prostatic urethra might correlate with the shortening of the ventral prostate seen on real-time magnetic resonance imaging (rtMRI). Micturition is a complex process that includes anatomical and neurological interactions for successful voiding. Recently we described on rtMRI that vertical contraction of the ventral prostate precedes initiation of male micturition and may contribute to the funnelling of the bladder neck.
   Patients and Methods
   In all, 10 patients undergoing radical prostatectomy (RP) were enrolled. Approval was obtained from all patients and by the local Ethics Committee.
   Preoperative rtMRI during voiding was performed as described before in eight patients undergoing RP, measuring the difference of the cranio-caudal distance of the ventral prostate (VP). To roughly estimate the amount of force required to deform the prostate in a vertical direction as seen on rtMRI, we uniaxially compressed the organ immediately after surgery by the same distance, assuming incompressibility and isotropy of prostatic tissue.
   A muscle strip (3 3 mm) from the ventral prostate, dorsal prostate and prostatic urethra was obtained after pathological evaluation. Contraction was elicited by electrical-field stimulation (EFS: 0.1 ms pulses at 2, 4, 8, 16, 32 and 64 Hz for 4 s).
   Results
   There was a mean cranio-caudal contraction of the ventral prostate by 7.6 mm at the onset of micturition on rtMRI (P = 0.002).
   The mean (SD) contractile force of strips elicited by EFS at 32 Hz was 1472.44 (706.88) mN for the ventral prostate, 1044.24 (894.66) mN for the dorsal prostate, and 639.10 (785.06) mN for the prostatic urethra (P = 0.02). Extrapolating these values to the whole organ diameter, we calculated comparable force as observed in compression experiments.
   Conclusions
   Our functional and biometric in vitro analyses of prostate tissue showed sufficient contractile strength of the ventral prostate to induce a shortening of the organ as seen on rtMRI.
   There was significant higher contractile strength in the ventral prostate than in the dorsal prostate or the proximal urethra. The consistency of in vivo and in vitro results underlines the significance of the ventral prostate for the initiation of normal micturition.
C1 [Hocaoglu, Yasemin; Herrmann, Karin; Walther, Sebastian; Hennenberg, Martin; Gratzke, Christian; Bauer, Ricarda; Stief, Christian; Roosen, Alexander] Univ Munich, Dept Urol, D-81377 Munich, Germany.
C3 University of Munich
RP Hocaoglu, Y (corresponding author), Univ Munich, Dept Urol, Marchioninistr 15, D-81377 Munich, Germany.
EM yasmin.hocaoglu@med.uni-muenchen.de
RI Hennenberg, Martin/M-3474-2016
FU Deutsche ForschungsgemeinschaftGerman Research Foundation (DFG) [RO
   3589-212]
FX This study is supported by the Deutsche Forschungsgemeinschaft (grant RO
   3589-212).
CR Andersson KE, 2004, PHARMACOL REV, V56, P581, DOI 10.1124/pr.56.4.4
   Buljubasich R, 2004, EUR J PHARMACOL, V499, P335, DOI 10.1016/j.ejphar.2004.07.080
   CHOW PH, 1993, INT J ANDROL, V16, P41, DOI 10.1111/j.1365-2605.1993.tb01151.x
   Chueh SC, 1996, EUR J PHARMACOL, V305, P177, DOI 10.1016/0014-2999(96)00197-5
   Daniels GF, 1990, SCI FDN UROLOGY, P351
   Dorschner W, 2001, Adv Anat Embryol Cell Biol, V159, pIII
   DORSCHNER W, 1994, UROL INT, V52, P154, DOI 10.1159/000282596
   Dorschner W, 2001, UROLOGE A, V40, P223, DOI 10.1007/s001200050466
   Dorschner W, 2001, ADV ANAT EMBRYOL CEL, V159, P1
   Haynes JM, 2005, CLIN EXP PHARMACOL P, V32, P797, DOI 10.1111/j.1440-1681.2005.04268.x
   Hocaoglu Y, 2012, BJU INT, V109, P234, DOI 10.1111/j.1464-410X.2011.10255.x
   MCNEAL JE, 1981, PROSTATE, V2, P35, DOI 10.1002/pros.2990020105
   Miller K, 2005, J BIOMECH, V38, P153, DOI 10.1016/j.jbiomech.2004.03.004
   Shafik A, 2006, UROLOGY, V67, P793, DOI 10.1016/j.urology.2005.10.016
   TAUBER PF, 1975, J REPROD FERTIL, V43, P249
   Ventura L, 2003, BRIT J PHARMACOL, V138, P1277, DOI 10.1038/sj.bjp.0705167
   Ventura S, 2000, CLIN EXP PHARMACOL P, V27, P917, DOI 10.1046/j.1440-1681.2000.03361.x
   Walther S, 2011, 26 ANN C EUR ASS UR
   White CW, 2011, J PHARMACOL EXP THER, V339, P870, DOI 10.1124/jpet.111.186841
   White CW, 2010, J PHARMACOL EXP THER, V335, P489, DOI 10.1124/jpet.110.172130
NR 20
TC 12
Z9 12
U1 0
U2 4
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1464-4096
EI 1464-410X
J9 BJU INT
JI BJU Int.
PD JUN
PY 2013
VL 111
IS 7
BP 1117
EP 1123
DI 10.1111/j.1464-410X.2012.11698.x
PG 7
WC Urology & Nephrology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Urology & Nephrology
GA 142PQ
UT WOS:000318809900022
PM 23356864
DA 2022-02-03
ER

PT J
AU Hadi, M
   Mosaddegh, H
   Abbassi, N
AF Hadi, Mehdi
   Mosaddegh, Hossein
   Abbassi, Nasrollah
TI Microfacies and biofabric of nummulite accumulations (Bank) from the
   Eocene deposits of Western Alborz (NW Iran)
SO JOURNAL OF AFRICAN EARTH SCIENCES
LA English
DT Article
DE Larger benthic foraminifera; Microfacies; Nummulite accumulations;
   Intraskeletal porosity; Ziarat formation; Western Alborz
ID EL-GARIA FORMATION; LARGER BENTHIC FORAMINIFERA; CARBONATE PLATFORM;
   HYDRODYNAMIC BEHAVIOR; DYNAMIC INDICATORS; OFFSHORE TUNISIA; MIDDLE
   EOCENE; TESTS; SEDIMENTS; TERTIARY
AB The nummulite bank from the Eocene Ziarat Formation is described for the first time from Alborz, Iran, enhancing the record of these nummulite-rich accumulations in the Eocene of the circum-Tethyan carbonate platform. Five microfacies types have been defined within the shallow-water carbonate deposits of the Ziarat formation located in the western Alborz zone. Microfacies type 1 contains the most diverse Alveolina species associated with predominance of Nummulites A-forms. Microfacies type 2 is characterized by the presence of bivalve (oysters) fragments. Microfacies type 3 is supported by the high abundance of nummulitids. Microfacies type 4 is dominated by the occurrence of encrusting foraminifera-algal with flat growth forms that are mainly formed within the acervulinids assemblage. Finally,, there is the presence of orthophragminids and nummuitids represented by microfacies type 5. Microfacies data obtained from the investigation area show that nummulite banks were formed within the back, core and fore-bank palaeoenvironments. The classification method of this paper is based on use biometric, biofabric, taphonomic and palaeoecological characteristics of larger benthic foraminifera. In addition, the calculated intraskeletal porosity by the use of numerous sections and FE-SEM images of Nummulites tests were displacement of tests in order to achieve a better understanding of paleo-conditions that occurred during sedimentation. We conclude that differences among bank frameworks suggest that small biconvex A-forms of Nummulites tests along with alveolinids were living in shallow, euphotic waters, whereas robust and ovate nummulitid tests thrived and concentrated in the intermediate (40-80 m) water with biofabrics in the min-scales, which indicates the influence of waves and currents in combination with wave-winnowing processes. More distal accumulations, the fore-bank were characterized by orthophragminid and nummulitid tests in the deeper part of the photic zone.
   The larger benthic foraminifera, as confirmed by Discocyclina javana (VERBEEK), D. cf. dispansa (SOWERBY), Assilina ex. gr. exponens (SOWERBY), Nummulites ex. gr. globulus)SCHAUB(, Alveolina ellipsoidalis (SCHWAGER), A. subpyrenaica (LEYMERIE), A. pisiformis (HOTTINGER), A. tumida (HOTTINGER), A. cemali (SIREL et ACAR), A. laxa (HOTTINGER), A. ex. gr. cremae)CHECCHIA-RISPOLI(, suggest the early Ilerdian-Middle Eocene age of these sediments. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Hadi, Mehdi] Ferdowsi Univ Mashhad, Dept Geol, Fac Sci, Mashhad, Iran.
   [Mosaddegh, Hossein] Kharazmi Univ, Dept Geol, Fac Earth Sci, Tehran, Iran.
   [Abbassi, Nasrollah] Univ Zanjan, Dept Geol, Fac Sci, Zanjan, Iran.
C3 Ferdowsi University Mashhad; Kharazmi University; University Zanjan
RP Mosaddegh, H (corresponding author), Kharazmi Univ, Dept Geol, Fac Earth Sci, Tehran, Iran.
EM mehdijiadi_s@yahoo.com; mosaddegh@khu.ac.ir; abbasi@znu.ac.ir
RI Nasrollah, Abbassi/AAO-6710-2020; Hadi, Mehdi/R-8258-2019; Abbassi,
   Nasrollah/Y-8530-2018
OI Nasrollah, Abbassi/0000-0002-9994-9122; Abbassi,
   Nasrollah/0000-0002-9994-9122
CR Aghanabati A., 2004, GEOLOGY IRAN, P586
   AIGNER T, 1985, J SEDIMENT PETROL, V55, P131
   Aigner T., 1982, CYCLIC EVENT STRATIF, P248
   Anketell JM, 2000, J PETROL GEOL, V23, P425, DOI 10.1111/j.1747-5457.2000.tb00495.x
   Arni P., 1965, Mem Bur Rech Geol Minier, V32, P7
   Asiabanha A, 2012, LITHOS, V148, P98, DOI 10.1016/j.lithos.2012.05.014
   Bassi D, 2005, PALAEOGEOGR PALAEOCL, V226, P17, DOI 10.1016/j.palaeo.2005.05.002
   Bassi D, 2013, SEDIMENT GEOL, V297, P1, DOI 10.1016/j.sedgeo.2013.08.012
   Beavington-Penney SJ, 2006, J SEDIMENT RES, V76, P1137, DOI 10.2110/jsr.2006.109
   Beavington-Penney SJ, 2005, SEDIMENTOLOGY, V52, P537, DOI 10.1111/j.1365-3091.2005.00709.x
   Beavington-Penney SJ, 2004, EARTH-SCI REV, V67, P219, DOI 10.1016/j.earscirev.2004.02.005
   Beavington-Penney SJ, 2004, PALAIOS, V19, P143, DOI 10.1669/0883-1351(2004)019<0143:AOTEOA>2.0.CO;2
   Blanc-Vernet L., 1979, GEOLOGIE MEDITERRANE, V6, P171
   Blondeau A., 1972, LES NUMMULITES, P254
   Bosence D. W. J., 1983, COATED GRAINS, P217
   BRAGA JC, 1988, PALAEOGEOGR PALAEOCL, V67, P285, DOI 10.1016/0031-0182(88)90157-5
   Brasier M.D., 1984, Journal of Micropalaeontology, V3, P11
   Brasier M.D., 1975, W INDIES FORAMIN RES, V5, P193
   Briguglio A, 2011, TURK J EARTH SCI, V20, P683, DOI 10.3906/yer-0910-44
   Briguglio A, 2011, MAR MICROPALEONTOL, V81, P63, DOI 10.1016/j.marmicro.2011.07.004
   Briguglio A, 2009, BOLL SOC PALEONTOL I, V48, P105
   BUXTON MWN, 1989, J GEOL SOC LONDON, V146, P746, DOI 10.1144/gsjgs.146.5.0746
   CARPENTER WB, 1850, QUAT J GEOL SOC LOND, V6, P21
   Chapman F., 1986, QUAT J GEOL SOC LOND, V52, P485
   Cosovic V, 2004, FACIES, V50, P61, DOI 10.1007/s10347-004-0006-9
   DAVAUD E, 1995, J SEDIMENT RES A, V65, P136
   DAVIES GR, 1970, AAPG BULL, V13, P85
   Dellenbach J., 1964, THESIS U STRASBOURG, P117
   Drobne K., 2009, GEOLOGY SLOVENIA, P311
   Dunham R.J., 1962, AM ASS PETROLEUM GEO, V1, P108
   Eichenseer H., 1992, FACIES, V27, P119
   Elliott JM, 1996, B MAR SCI, V58, P261
   Fagerstrom J.A., 1987, EVOLUTION REEF COMMU, P600
   Fermont W.J.J., 1982, UTRECHT MICROPALEONT, V27, P1
   Flugel E., 2004, MICROFACIES CARBONAT, P633
   Foster MS, 2001, J PHYCOL, V37, P659, DOI 10.1046/j.1529-8817.2001.00195.x
   Futterer E., 1982, CYCLIC EVENT STRATIF, P175, DOI [DOI 10.1007/978-3-642-75829-4, 10.1007/978-3-642-75829-4_]
   Geel T, 2000, PALAEOGEOGR PALAEOCL, V155, P211, DOI 10.1016/S0031-0182(99)00117-0
   GHOSE BK, 1977, PALAEOGEOGR PALAEOCL, V22, P231, DOI 10.1016/0031-0182(77)90030-X
   Gischler E, 1999, NEUES JAHRB GEOL P-A, V214, P71, DOI 10.1127/njgpa/214/1999/71
   Goldring R, 1991, FOSSILS FIELD INFORM, P218
   HALL SJ, 1994, OCEANOGR MAR BIOL, V32, P179
   HALLOCK P, 1979, Journal of Foraminiferal Research, V9, P61
   HALLOCK P, 1986, J FORAMIN RES, V16, P224, DOI 10.2113/gsjfr.16.3.224
   HALLOCK P, 1981, Journal of Foraminiferal Research, V11, P40
   HALLOCK P, 1988, PALAEOGEOGR PALAEOCL, V63, P275, DOI 10.1016/0031-0182(88)90100-9
   Hallock P, 1999, MODERN FORAMINIFERA, P123
   Hasler C.A., 2004, GEOMETRY INTERNAL DI, V45, P230
   Hontzsch S, 2013, TURK J EARTH SCI, V22, P891, DOI 10.3906/yer-1207-8
   Hohenegger J, 2001, PALAIOS, V16, P53, DOI 10.2307/3515552
   Hohenegger J, 2000, J FORAMIN RES, V30, P3, DOI 10.2113/0300003
   Hottinger L, 1997, B SOC GEOL FR, V168, P491
   Hottinger L, 1999, ECLOGAE GEOL HELV, V92, P385
   Hottinger L., 1973, P443
   Hottinger L., 1977, MEMOIRES MUSEUM NATL, V59, P1
   Hottinger L, 1983, UTRECHT MICROPALEONT, V30, P239
   Jorry S.J., 2004, THESIS, P226
   Jorry S. J., 2003, GEOLOGY NW LIBYA, P99
   Jorry SJ, 2006, FACIES, V52, P221, DOI 10.1007/s10347-005-0035-z
   KIDWELL S M, 1986, Palaios, V1, P228, DOI 10.2307/3514687
   Kulka A., 1985, Kwartalnik Geologiczny, V29, P31
   LAMING DJC, 1966, J SEDIMENT PETROL, V36, P940
   Langer MR, 2000, MICROPALEONTOLOGY, V46, P105
   Less Gy, 1989, MAGY ALL FOLDT INT E, V1987, P313
   LEUTENEGGER S, 1979, MAR BIOL, V54, P11, DOI 10.1007/BF00387046
   Loucks RG, 1998, GEOL SOC SPEC PUBL, V132, P355, DOI 10.1144/GSL.SP.1998.132.01.20
   Lund M, 2000, FACIES, V42, P25, DOI 10.1007/BF02562564
   Marrack EC, 1999, PALAIOS, V14, P159, DOI 10.2307/3515371
   MARTINDALE W, 1992, CORAL REEFS, V11, P167, DOI 10.1007/BF00255472
   Mateu-Vicens G, 2012, SEDIMENTOLOGY, V59, P527, DOI 10.1111/j.1365-3091.2011.01263.x
   MINNERY GA, 1990, J SEDIMENT PETROL, V60, P992
   Molina Eustoquio, 2000, Revue de Micropaleontologie, V43, P381, DOI 10.1016/S0035-1598(00)90194-6
   Moores E.M., 1998, ENCY EARTH SCI SERIE, P825
   MURRAY JW, 1982, J FORAMIN RES, V12, P51, DOI 10.2113/gsjfr.12.1.51
   MURRAY JW, 1987, J GEOL SOC LONDON, V144, P127, DOI 10.1144/gsjgs.144.1.0127
   Nebelsick JH, 2005, FACIES, V51, P197, DOI 10.1007/s10347-005-0069-2
   Papazzoni C.A, 2010, INT SYMP FOR 2010 BO, P154
   POIZAT C, 1970, Tethys, V2, P267
   RACEY A, 1995, MICROPALEONTOLOGY, V41, P1, DOI 10.2307/1485849
   Racey A, 2001, J PETROL GEOL, V24, P79, DOI 10.1111/j.1747-5457.2001.tb00662.x
   Racey A, 2001, J PETROL GEOL, V24, P29, DOI 10.1111/j.1747-5457.2001.tb00660.x
   Rasser MW, 2004, PALAEOGEOGR PALAEOCL, V206, P21, DOI 10.1016/j.palaeo.2003.12.018
   REISS Z., 1984, ECOL STUD, V50
   Roduit N, 2005, SCEN SED 24 IAS M MU, P142
   Roniewicz P., 1969, ACTA GEOL POL, V19, P503
   Schaub H., 1981, MEMOIRES SUISSES PAL, V104, P238
   Scotese C.R., 2001, PALEOGEOGRAPHY, V1, P52
   Seddighi M, 2015, B SOC PALEONTOL ITAL, V54, P103, DOI 10.4435/BSPI.2015.06
   SEVERIN KP, 1989, LETHAIA, V22, P1, DOI 10.1111/j.1502-3931.1989.tb01163.x
   Steller DL, 1995, J EXP MAR BIOL ECOL, V194, P201, DOI 10.1016/0022-0981(95)00086-0
   Trave A, 1996, PALAIOS, V11, P141, DOI 10.2307/3515067
   Vennin E, 2003, PETROL GEOSCI, V9, P145, DOI 10.1144/1354-079302-505
   WELLS NA, 1986, J SEDIMENT PETROL, V56, P318, DOI 10.1306/212F88FF-2B24-11D7-8648000102C1865D
   Yordanova EK, 2002, FACIES, V46, P169, DOI 10.1007/BF02668080
   Yordanova EK, 2007, SEDIMENTOLOGY, V54, P1273, DOI 10.1111/j.1365-3091.2007.00881.x
NR 95
TC 15
Z9 15
U1 0
U2 7
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 1464-343X
EI 1879-1956
J9 J AFR EARTH SCI
JI J. Afr. Earth Sci.
PD DEC
PY 2016
VL 124
BP 216
EP 233
DI 10.1016/j.jafrearsci.2016.09.012
PG 18
WC Geosciences, Multidisciplinary
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Geology
GA EE2BM
UT WOS:000389388700017
DA 2022-02-03
ER

PT J
AU Boufersaoui, S
   Kassar, A
   Mokrane, Z
   Elleboode, R
   Mahe, K
AF Boufersaoui, Samira
   Kassar, Abderrahmane
   Mokrane, Zakia
   Elleboode, Romain
   Mahe, Kelig
TI AGE AND GROWTH OF THE STRIPED SEABREAM, LITHOGNATHUS MORMYRUS
   (ACTINOPTERYGII: PERCIFORMES: SPARIDAE), IN THE CENTRAL COAST OF
   ALGERIA, MEDITERRANEAN SEA
SO ACTA ICHTHYOLOGICA ET PISCATORIA
LA English
DT Article
DE Lithognathus mormyrus; otolith; age; growth; Algerian coast;
   Mediterranean Sea
ID LENGTH-WEIGHT RELATIONSHIPS; REPRODUCTION; MORTALITY; PISCES; HISTORY;
   FISHES; BREAM; RIVER; SOUTH; L.
AB Background. Striped seabream, Lithognathus mormyrus (Linnaeus, 1758), is a bony fish, which has a high economic value on the Algerian coast. Because of the increasing fishing pressure, however, a close monitoring is recommended. The information about the biology of this species, occurring in the south-western Mediterranean Sea and especially in the north African coast, is very limited. The presently reported study provides new estimated data on age and growth parameters of striped seabream in Algeria.
   Materials and methods. A total of 449 specimens of L. mormyrus were sampled for 2 years (January 2013 to December 2014) from the commercial fishery in the central part of the Algerian coast. The samples were collected monthly. The fish ranged in size from 11.5 cm to 34.5 cm and weighed between 21.6 g and 540.3 g. We analysed the sagittal otolith morphology and morphometry to determine a relation with the fish ontogeny. Fish age was determined from the sagittal otoliths to identify growth structures based on digitally processed otolith images aided by the TNPC software.
   Results. No significant difference between the two otoliths (left and right) was detected (ANCOVA, P > 0.05). The correlation between each biometric parameter of the otolith (length and width) and fish length (TL) was significant (ANCOVA, P < 0.05). The evolution of marginal increment analysis (MI) showed that the annual periodicity of the growth ring was between July and December. The growth parameters of the von Bertalanffy model were estimated for each sex separately. In females they assumed the following values: TL infinity = 35.44 cm, K = 0.27 yr(-1), and t(0) = -1.25 yr, while in males-TL infinity = 26.94 cm and K = 0.6 yr(-1), t(0) = -0.45 yr. The asymptotic length was higher in females than in males. Males were represented only by small specimens and less than or equal to four years of age.
   Conclusion. The presently reported results are the first ones on the age and growth of L. mormyrus off the Algerian coast. They will hopefully improve future stock management to get a sustainable fishery.
C1 [Boufersaoui, Samira; Kassar, Abderrahmane] Ecole Natl Super Sci Mer & Amenagement Littoral, Campus Univ,BP 19 Bois Cars, Algiers 16320, Algeria.
   [Mokrane, Zakia] Ctr Natl Rech & Dev Peche & Aquaculture, Bou Ismail, Tipaza, Algeria.
   [Elleboode, Romain; Mahe, Kelig] IFREMER, Lab Ressources Halieut, Pole Sclerochronol, Boulogne Sur Mer, France.
C3 Ecole Nationale Superieure des Sciences de la Mer et Amenagement du
   Littoral (ENSSMAL); Ifremer
RP Boufersaoui, S (corresponding author), Ecole Natl Super Sci Mer & Amenagement Littoral, Campus Univ,BP 19 Bois Cars, Algiers 16320, Algeria.
EM samira.bonfersaoui@gmail.com; a.w.kassar@gmail.com;
   mokrane.zakia@gmail.com; romain.elleboode@ifremer.fr;
   kelig.mahe@ifremer.fr
RI Kassar, Abderrahmane/A-2936-2019
OI Kassar, Abderrahmane/0000-0002-2406-0644; mahe,
   kelig/0000-0002-6506-211X; BOUFERSAOUI, SAMIRA/0000-0002-8726-9138
CR Abecasis D, 2008, FISH RES, V89, P37, DOI 10.1016/j.fishres.2007.08.013
   Aydin M, 2018, AQUAT SCI ENG, V33, P50, DOI 10.18864/ASE201808
   BADALAMENTI F, 1993, BIOL MAR MEDIT, V1, P145
   Bailey KM, 1997, J SEA RES, V37, P269, DOI 10.1016/S1385-1101(97)00018-X
   Bellido JM, 2000, FISH RES, V48, P107, DOI 10.1016/S0165-7836(00)00183-1
   Ben Abdallah-Ben Hadj Hamida O, 2016, CAH BIOL MAR, V57, P113
   Besseau L., 1990, ICHTYOPHYSIOLOGICA A, V13, P109
   Bradai MN., 1998, B LINSTITUT NATL SCI, V4, P12
   Campillo A., 1992, 921211625TF CEEIFREM
   Chessa L.A., 2005, Biologia Marina Mediterranea, V12, P492
   Dorel D, 1986, POISSONS ATLANTIQUE
   Emre Y, 2010, TURK J ZOOL, V34, P93, DOI 10.3906/zoo-0808-13
   ERKOYUNCU I, 1995, BALIKCILIK BIYOLOJIS
   Fischer W., 1987, MEDITERRANEE MER NOI, VII
   Froese R, 2006, J APPL ICHTHYOL, V22, P241, DOI 10.1111/j.1439-0426.2006.00805.x
   Gokce Gokhan, 2007, International Journal of Natural and Engineering Sciences, V1, P51
   Goncalves JMS, 1997, FISH RES, V30, P253, DOI 10.1016/S0165-7836(96)00569-3
   Hamida NB, 2016, J MAR BIOL ASSOC UK, V96, P1491, DOI 10.1017/S0025315415001769
   Harchouche Kamel, 2005, Annales de la Societe des Sciences Naturelles de la Charente-Maritime, V9, P491
   Hossain MY, 2015, J APPL ICHTHYOL, V31, P967, DOI 10.1111/jai.12823
   Houston AI, 1999, MODELS ADAPTIVE BEHA
   Jardas I., 1996, JADRANSKA IHTIAUNA
   Kallianiotis A, 2005, SCI MAR, V69, P391, DOI 10.3989/scimar.2005.69n3391
   Keskin C., 2010, IUFS J BIOL, V69, P87
   KIMURA DK, 1980, FISH B-NOAA, V77, P765
   Kjerfve B., 1994, COASTAL LAGOON PROCE, P1, DOI DOI 10.1016/S0422-9894(08)70006-0
   Koutrakis ET, 2003, J APPL ICHTHYOL, V19, P258, DOI 10.1046/j.1439-0426.2003.00456.x
   Kraljevic M, 1996, FISH RES, V28, P361, DOI 10.1016/S0165-7836(96)00518-8
   KRALJEVIC M, 1995, J APPL ICHTHYOL, V11, P1, DOI 10.1111/j.1439-0426.1995.tb00001.x
   LABOURG PJ, 1985, OCEANOL ACTA, V8, P331
   Li Q, 2013, ACTA ICHTHYOL PISCAT, V43, P65, DOI 10.3750/AIP2013.43.1.09
   Lorenzo JM, 2002, J APPL ICHTHYOL, V18, P204, DOI 10.1046/j.1439-0426.2002.00318.x
   Mariani S, 2006, MAR POLLUT BULL, V53, P121, DOI 10.1016/j.marpolbul.2005.09.019
   Mille T, 2015, J FISH BIOL, V87, P646, DOI 10.1111/jfb.12746
   Monteiro P, 2010, MAR BIOL RES, V6, P53, DOI 10.1080/17451000903039731
   MORALESNIN B, 1990, J FISH BIOL, V36, P191, DOI 10.1111/j.1095-8649.1990.tb05595.x
   MORALESNIN BYO, 1987, S AFR J MARINE SCI, V5, P255, DOI 10.2989/025776187784522207
   Morey G, 2003, FISH RES, V62, P89, DOI 10.1016/S0165-7836(02)00250-3
   Munro J. L., 1983, ICLARM FISHBYTE, V1, P5
   Nelson J.S., 2006, FISHES WORLD
   Newell R.C., 1982, OCEANOL ACTA, V1, P377
   Osman A. M., 2005, EGYPTIAN J AQUATIC R, V31, P274
   Pajuelo JG, 2002, SCI MAR, V66, P27
   Panfili J., 2002, MANUEL SCLEROCHRONOL
   PAPACONSTANTINOU C, 1988, Cybium, V12, P267
   RICKER WE, 1973, J FISH RES BOARD CAN, V30, P409, DOI 10.1139/f73-072
   Russel B., 2014, LITHOGNATHUS MORMYRU, DOI [10.2305/IUCN.UK.2014-3.RLTS.T170160A1284573.en, DOI 10.2305/IUCN.UK.2014-3.RLTS.T170160A1284573.EN]
   Saleh M, 2017, ACTA ICHTHYOL PISCAT, V47, P307, DOI 10.3750/AIEP/02168
   Santos MN, 2002, FISH RES, V59, P289, DOI 10.1016/S0165-7836(01)00401-5
   Spiegel M. R, 1991, THEORIE APPL STAT
   Suau P, 1969, PUBL TEC JUNT EST PE, V8, P163
   Sumer C, 2014, CAH BIOL MAR, V55, P37
   Sumer C, 2012, CAH BIOL MAR, V53, P185
   Tobes I, 2016, J APPL ICHTHYOL, V32, P1256, DOI 10.1111/jai.13172
   Trojette M, 2015, ACTA ICHTHYOL PISCAT, V45, P363, DOI 10.3750/AIP2015.45.4.04
   Turan Cemal, 1999, Turkish Journal of Zoology, V23, P259
   Turkmen Mustafa, 2003, Turkish Journal of Zoology, V27, P323
   Vieira AR, 2009, SCI MAR, V73, P33, DOI 10.3989/scimar.2009.73s2033
   Vitale S, 2011, J APPL ICHTHYOL, V27, P1086, DOI 10.1111/j.1439-0426.2011.01775.x
   Vitale S., 2003, Biologia Marina Mediterranea, V10, P233
   von Bertalanffy L., 1938, HUM BIOL, V10, P181, DOI DOI 10.2307/41447359
NR 61
TC 3
Z9 3
U1 0
U2 1
PU WYDAWNICTWO AKAD ROLNICZEJ W SZCZECINIE
PI SZCZECIN
PA KAZIMIERZA KROLEWICZA 4, SZCZECIN, 71550, POLAND
SN 0137-1592
EI 1734-1515
J9 ACTA ICHTHYOL PISCAT
JI Acta Ichthyol. Piscat.
PY 2018
VL 48
IS 4
BP 319
EP 328
DI 10.3750/AIEP/02485
PG 10
WC Fisheries; Zoology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Fisheries; Zoology
GA HH7KW
UT WOS:000455910700001
OA Green Submitted, gold
DA 2022-02-03
ER

PT J
AU Guo, JM
   Xu, XL
   Zhang, H
   Wang, JM
AF Guo, Jing-Min
   Xu, Xiao-Lan
   Zhang, Hong
   Wang, Jun-Ming
TI Anterior segment changes after pharmacologic mydriasis using Pentacam
   and optical coherence tomography in angle closure suspects
SO INTERNATIONAL JOURNAL OF OPHTHALMOLOGY
LA English
DT Article
DE pharmacologic mydriasis; primary angle closure suspects; anterior
   segment change; iris volume
ID PUPIL-DILATION; IRIS; GLAUCOMA; VOLUME; AREA; RISK; EYES
AB AIM: To compare the dynamic changes of anterior segment parameters especially iris morphology induced by pharmacologic mydriasis between angle closure suspects and normal controls.
   METHODS: The study group comprised 19 eyes of 19 angle closure suspects and 19 eyes of 19 age- and sexmatched normal open-angle eyes. Pentacam and optical coherence tomography measurements before and 30min after instillation of compound tropicamide eye drop were performed and compared. Biometric evaluations of iris tomography and anterior chamber angle were estimated by a customized image-processing software.
   RESULTS: Baseline axial length, iris cross sectional area and volume did not differ significantly between angle closure suspects and normal controls. Angle closure suspects had smaller pupil size, narrower anterior segment dimension and axial length, thinner iris with greater curve in comparison with normal controls. Pharmacologic mydriasis led to significant increments in iris thickness at 750 mu m, anterior chamber depth and volume, whereas significant decrements in iris curve, cross sectional area and volume in both groups. Angle opening distance at 500 mu M was increased significantly in normal controls (from 0.465 +/- 0.115 mm to 0.539 +/- 0.167 mm, P=0.009), but not in angle closure suspects (from 0.125 0.100 mm to 0.145 +/- 0.131 mm, P=0.326). Iris volume change per millimeter of pupil dilation (Delta IV/Delta PD) decreased significantly less in angle closure suspects than normal controls (-2.47 +/- 1.33 mm(2) vs -3.63 +/- 1.58 mm(2), P=0.019). Linear regression analysis showed that the change of angle opening distance at 500 mu M was associated most with the change of central anterior chamber depth (beta= 0.841, P=0.002) and Delta IV/Delta PD (beta=0.028, P=0.002), followed by gender (beta=0. 062, P=0.032).
   CONCLUSION: Smaller iris volume decrement per millimeter of pupil dilation is related significantly with the less anterior angle opening in angle closure suspects after pharmacologic mydriasis. Dynamic iris change may be as a prospective indicator of iris compressibility and angle closure glaucoma.
C1 [Guo, Jing-Min; Xu, Xiao-Lan; Zhang, Hong; Wang, Jun-Ming] Huazhong Univ Sci & Technol, Tongji Med Coll, Tongji Hosp, Dept Ophthalmol, Wuhan 430000, Hubei Province, Peoples R China.
C3 Huazhong University of Science & Technology
RP Guo, JM (corresponding author), Huazhong Univ Sci & Technol, Tongji Med Coll, Tongji Hosp, Dept Ophthalmol, Wuhan 430000, Hubei Province, Peoples R China.
EM 18571570696@163.com.cn
RI Wang, Junming/AAD-9636-2019
CR Aptel F, 2012, INVEST OPHTH VIS SCI, V53, P4005, DOI 10.1167/iovs.11-9387
   Aptel F, 2010, OPHTHALMOLOGY, V117, pc, DOI 10.1016/j.ophtha.2009.10.030
   BRAZIER DJ, 1989, EYE, V3, P288, DOI 10.1038/eye.1989.40
   Bremner F, 2009, CLIN AUTON RES, V19, P88, DOI 10.1007/s10286-009-0515-2
   Cheung CYL, 2010, INVEST OPHTH VIS SCI, V51, P4040, DOI 10.1167/iovs.09-3941
   Chua J, 2008, MOL VIS, V14, P1886
   Foster PJ, 2002, BRIT J OPHTHALMOL, V86, P238, DOI 10.1136/bjo.86.2.238
   GALIN MA, 1961, ARCH OPHTHALMOL-CHIC, V66, P353
   Ganeshrao SB, 2012, OPTOMETRY VISION SCI, V89, P483, DOI 10.1097/OPX.0b013e31824c3731
   He MG, 2008, J GLAUCOMA, V17, P386, DOI 10.1097/IJG.0b013e31815c5f69
   HIGGITT AC, 1954, BRIT J OPHTHALMOL, V38, P242, DOI 10.1136/bjo.38.4.242
   Hirose F, 2013, GRAEF ARCH CLIN EXP, V251, P2395, DOI 10.1007/s00417-013-2378-4
   KARMON G, 1982, BRIT J OPHTHALMOL, V66, P471, DOI 10.1136/bjo.66.7.471
   MAPSTONE R, 1977, BRIT J OPHTHALMOL, V61, P517, DOI 10.1136/bjo.61.8.517
   Mark HH, 2003, MED HYPOTHESES, V60, P305, DOI 10.1016/S0306-9877(02)00338-9
   Narayanaswamy A, 2013, INVEST OPHTH VIS SCI, V54, P708, DOI 10.1167/iovs.12-10844
   Pandit RJ, 2000, DIABETIC MED, V17, P693, DOI 10.1046/j.1464-5491.2000.00368.x
   Quigley HA, 2006, BRIT J OPHTHALMOL, V90, P262, DOI 10.1136/bjo.2005.081224
   Quigley HA, 2009, J GLAUCOMA, V18, P173, DOI 10.1097/IJG.0b013e31818624ce
   See JLS, 2007, BRIT J OPHTHALMOL, V91, P1485, DOI 10.1136/bjo.2006.113654
   Thomas R, 2003, BRIT J OPHTHALMOL, V87, P450, DOI 10.1136/bjo.87.4.450
   Wang DD, 2012, INVEST OPHTH VIS SCI, V53, P3139, DOI 10.1167/iovs.12-9776
   WILENSKY JT, 1993, AM J OPHTHALMOL, V115, P338, DOI 10.1016/S0002-9394(14)73585-8
   Yao BQ, 2009, OPHTHALMOLOGY, V116, P444, DOI 10.1016/j.ophtha.2008.10.019
   Ye T, 1998, Zhonghua Yan Ke Za Zhi, V34, P167
   Zheng C, 2012, INVEST OPHTH VIS SCI, V53, P6756, DOI 10.1167/iovs.12-10415
NR 26
TC 10
Z9 11
U1 0
U2 5
PU IJO PRESS
PI XI AN
PA NO 269 YOUYI EAST RD, XI AN, 710054, PEOPLES R CHINA
SN 2222-3959
EI 2227-4898
J9 INT J OPHTHALMOL-CHI
JI Int. J. Ophthalmol.
PD OCT
PY 2015
VL 8
IS 5
BP 980
EP 984
DI 10.3980/j.issn.2222-3959.2015.05.23
PG 5
WC Ophthalmology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Ophthalmology
GA CS1WD
UT WOS:000361858600023
PM 26558213
DA 2022-02-03
ER

PT J
AU Trivilin, LO
   Pessoa, FD
   da Silva, MA
   Milholli, LA
   Pastor, FM
   Bindaco, ALS
   Martins, S
   Nunes, LD
AF Trivilin, Leonardo Oliveira
   Pessoa Junior, Francisco de Assis
   da Silva, Maria Aparecida
   Milholli, Leandro Andre
   Pastor, Felipe Martins
   Stelzer Bindaco, Adriano Lima
   Martins Filho, Sebastiao
   Nunes, Louisiane de Carvalho
TI Biometry of Adrenal Glands of Mixed-Breed Dogs
SO ACTA SCIENTIAE VETERINARIAE
LA English
DT Article
DE canine; measurement; adrenal glands
ID ULTRASONOGRAPHIC EVALUATION
AB Background: Differences of the size of the adrenal glands, in healthy dogs or in endocrinopathies, undermine correct diagnosis of endocrine disorders and evaluations of the adrenal cortex in relation to its size and possible correlation with endocrinopathies are rare. The aim of the present study was to perform measurements of the length, width, thickness and weight of the adrenal glands of young, adult and elderly mixed-breed dogs and correlate them with the age, sex and weight of animals. In addition, the areas occupied by the zona glomerulosa, zona fasciculata, and zona reticularis of the adrenal cortex were measured in order to establish a microscopic biometric pattern.
   Material, Methods & Results: The right and left adrenal glands of 12 young (six females and six males), 12 adults (six females and six males), and 12 elderly (six females and six males), all mongrels, derived from routine and necropsied in the Animal Pathology Sector of the Veterinary Hospital of the Federal University of Espirito Santo (HOVET-UFES) were weighed and the length, width, and thickness were measured. For the microscopic measurement of the adrenal cortex, 10 randomly selected samples were submitted to routine histological processing and the microscope slides were observed under a light microscope at 5x and 10x objectives, photodocumented and measurements were obtained from a random portion of the adrenal cortex and the zones composing the cortex were measured in triplicate with the aid of the computerized image analysis software. The left adrenal showed a greater average length than the right adrenal in young, adult, and elderly dogs. The size of the glands in the young and adult dogs is not influenced by the sex of the animals, but in older dogs the females had a greater mean width than the males. The weight of the animals presented a positive correlation in relation to the length and weight of the right and left adrenal glands in all studied groups. The age did not influence the length and weight variables of the glands. However, some differences in thickness and width were observed in the elderly group compared to those in other groups. For the microscopic measurements in the right adrenal gland, the cortex was 1.53 mm, being 0.21 mm for the glomerular zone (14.6% of the total adrenal cortex), 1.04 mm for the zona fasciculata (66.9%), and 0.29 mm for the zona reticularis (18.5%). In the left adrenal gland, the cortex was 1.83 mm, being 0.23 mm for the glomerular zone (13.2% of the total adrenal cortex), 1.23 mm for the zona fasciculata (63.96%), and 0.37 for the zona reticularis (22.84%).
   Discussion: Studies on the size of the adrenal gland in dogs using macroscopic biometrics are scarce, and the current study presents results regarding mixed-breed (male and female) dogs of different ages, which presented variation in size and weight, which could also influence the size of the adrenal gland. Thus, it can be observed that a variation of measurements can be found, especially for dogs with no defined breed. These results demonstrate the importance of studies that perform the macroscopic analysis of such glands. It was concluded that there was a clear variation in the size of the adrenal glands in mixed-breed dogs of different ages and both sexes; the left adrenal showed greater lengths and weights than the right gland. The sex of the animals did not influence the size of the glands in young and adult dogs, but in elderly dogs, the females showed a greater mean width than the males. The right and left adrenal cortices are formed at a greater percentage by the zona fasciculata, followed by the zona reticularis and zona glomerulosa.
C1 [Trivilin, Leonardo Oliveira; Pessoa Junior, Francisco de Assis; da Silva, Maria Aparecida; Milholli, Leandro Andre; Pastor, Felipe Martins; Stelzer Bindaco, Adriano Lima; Nunes, Louisiane de Carvalho] Univ Fed Espirito Santo UFES, Programa Posgrad Ciencias Vet, Ctr Ciencias Agr & Engn CCAE, Vitoria, ES, Brazil.
   [Martins Filho, Sebastiao] Univ Fed Vicosa UFV, Dept Estat, Vicosa, MG, Brazil.
C3 Universidade Federal do Espirito Santo; Universidade Federal de Vicosa
RP Trivilin, LO (corresponding author), CCAE UFES, Dept Med Vet, Vitoria, ES, Brazil.
EM leotrivilin@gmail.com
RI da Silva, Maria Aparecida/AAN-7102-2021
OI da Silva, Maria Aparecida/0000-0003-0967-3925
CR Baker DD, 1937, AM J ANAT, V60, P231, DOI 10.1002/aja.1000600204
   BARTHEZ PY, 1995, J AM VET MED ASSOC, V207, P1180
   Burk R., 2003, SMALL ANIMAL RADIOLO, V3rd ed.
   Carvalho C.F, 2014, ULTRASONOGRAFIA PEQU
   DeMarco V., 2014, TRATADO MED INTERNA, P1691
   Douglass JP, 1997, VET RADIOL ULTRASOUN, V38, P124, DOI 10.1111/j.1740-8261.1997.tb00827.x
   Drazner F.H., 1987, SMALL ANIMAL ENDOCRI
   Dyce K.M., 2010, TRATADO ANATOMIA VET, V4th
   FARIA Margarida Ermelinda Lima de Morais de, 2007, THESIS
   Fernandez S, 2016, ARQ BRAS MED VET ZOO, V68, P907, DOI 10.1590/1678-4162-8644
   Gartner L.P, 2014, ATLAS COLORIDO HISTO
   Grooters AM, 1996, J VET INTERN MED, V10, P110, DOI 10.1111/j.1939-1676.1996.tb02041.x
   Herrtage M.E., 1990, MANUAL SMALL ANIMAL, P22
   Herrtage M.E, 2015, MANUAL ENDOCRINOLOGI, P254
   Hoerauf A, 1999, J AM ANIM HOSP ASSOC, V35, P214, DOI 10.5326/15473317-35-3-214
   JUNQUEIRA L. C., 2017, HISTOLOGIA BASICA, V13
   Liebich H.G., 2016, ANATOMIA ANIMAIS DOM
   Lobetti R., 2016, J VET CLIN PRACTICE, V1, P1
   Mogicato G, 2011, VET REC, V168, P130, DOI 10.1136/vr.c4950
   Pagani E, 2017, ANAT HISTOL EMBRYOL, V46, P187, DOI 10.1111/ahe.12256
   Pagani E, 2016, BMC VET RES, V12, DOI 10.1186/s12917-016-0895-1
   Salvagni F. A., 2017, Brazilian Journal of Veterinary Pathology, V10, P69, DOI 10.24070/bjvp.1983-0246.v10i2p69-78
   Soulsby SN, 2015, VET RADIOL ULTRASOUN, V56, P317, DOI 10.1111/vru.12236
   Wenger M, 2010, VET REC, V167, P207, DOI 10.1136/vr.c4235
   YEH HC, 1980, AM J ROENTGENOL, V135, P1167, DOI 10.2214/ajr.135.6.1167
NR 25
TC 0
Z9 0
U1 0
U2 0
PU UNIV FED RIO GRANDE DO SUL
PI PORTO ALEGRE RS
PA FAC VET, CAIXA POSTAL 15017, PORTO ALEGRE RS, 91501-570, BRAZIL
SN 1678-0345
EI 1679-9216
J9 ACTA SCI VET
JI Acta Sci. Vet.
PD JUN 26
PY 2020
VL 48
AR 1742
DI 10.22456/1679-9216.102249
PG 10
WC Veterinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Veterinary Sciences
GA MG9KW
UT WOS:000546352500001
OA gold
DA 2022-02-03
ER
